### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.25 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.62 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.22 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.63 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.39 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.31 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.28 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.31 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.90 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.31 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.30 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.13 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.19 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.92 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  179.31 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.90 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   25.92 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.92 sec*proc (28 tests)

Total Test time (real) = 221.93 sec

real	3m41.967s
user	7m45.003s
sys	0m6.160s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.29 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.21 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.32 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.47 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.08 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.27 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.16 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.12 sec*proc (28 tests)

Total Test time (real) =  51.13 sec

real	0m51.140s
user	1m11.243s
sys	0m5.373s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.083 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.445 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.323 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.329 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.331 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.332 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.333 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.334 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.334 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.336 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.336 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.340 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.341 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.341 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.344 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.344 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.347 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.347 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.347 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.348 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.349 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.757 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.982 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.985 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.985 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.986 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.986 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.987 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.025.987 I llama_model_loader: - type  f32:  124 tensors
0.00.025.988 I llama_model_loader: - type  f16:   73 tensors
0.00.025.989 I print_info: file format = GGUF V3 (latest)
0.00.025.989 I print_info: file type   = F16
0.00.025.990 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.030.087 I load: special tokens cache size = 5
0.00.032.164 I load: token to piece cache size = 0.2032 MB
0.00.032.167 I print_info: arch             = bert
0.00.032.168 I print_info: vocab_only       = 0
0.00.032.168 I print_info: n_ctx_train      = 512
0.00.032.168 I print_info: n_embd           = 384
0.00.032.168 I print_info: n_layer          = 12
0.00.032.171 I print_info: n_head           = 12
0.00.032.172 I print_info: n_head_kv        = 12
0.00.032.172 I print_info: n_rot            = 32
0.00.032.173 I print_info: n_swa            = 0
0.00.032.173 I print_info: n_embd_head_k    = 32
0.00.032.173 I print_info: n_embd_head_v    = 32
0.00.032.174 I print_info: n_gqa            = 1
0.00.032.175 I print_info: n_embd_k_gqa     = 384
0.00.032.176 I print_info: n_embd_v_gqa     = 384
0.00.032.176 I print_info: f_norm_eps       = 1.0e-12
0.00.032.177 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.032.177 I print_info: f_clamp_kqv      = 0.0e+00
0.00.032.177 I print_info: f_max_alibi_bias = 0.0e+00
0.00.032.178 I print_info: f_logit_scale    = 0.0e+00
0.00.032.179 I print_info: n_ff             = 1536
0.00.032.179 I print_info: n_expert         = 0
0.00.032.179 I print_info: n_expert_used    = 0
0.00.032.179 I print_info: causal attn      = 0
0.00.032.179 I print_info: pooling type     = 2
0.00.032.180 I print_info: rope type        = 2
0.00.032.180 I print_info: rope scaling     = linear
0.00.032.180 I print_info: freq_base_train  = 10000.0
0.00.032.181 I print_info: freq_scale_train = 1
0.00.032.181 I print_info: n_ctx_orig_yarn  = 512
0.00.032.183 I print_info: rope_finetuned   = unknown
0.00.032.183 I print_info: ssm_d_conv       = 0
0.00.032.184 I print_info: ssm_d_inner      = 0
0.00.032.184 I print_info: ssm_d_state      = 0
0.00.032.184 I print_info: ssm_dt_rank      = 0
0.00.032.184 I print_info: ssm_dt_b_c_rms   = 0
0.00.032.184 I print_info: model type       = 33M
0.00.032.185 I print_info: model params     = 33.21 M
0.00.032.185 I print_info: general.name     = Bge Small
0.00.032.186 I print_info: vocab type       = WPM
0.00.032.186 I print_info: n_vocab          = 30522
0.00.032.186 I print_info: n_merges         = 0
0.00.032.187 I print_info: BOS token        = 101 '[CLS]'
0.00.032.187 I print_info: UNK token        = 100 '[UNK]'
0.00.032.187 I print_info: SEP token        = 102 '[SEP]'
0.00.032.187 I print_info: PAD token        = 0 '[PAD]'
0.00.032.188 I print_info: MASK token       = 103 '[MASK]'
0.00.032.188 I print_info: LF token         = 0 '[PAD]'
0.00.032.188 I print_info: max token length = 21
0.00.034.182 I load_tensors: offloading 12 repeating layers to GPU
0.00.034.182 I load_tensors: offloading output layer to GPU
0.00.034.183 I load_tensors: offloaded 13/13 layers to GPU
0.00.034.208 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.210 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.034.447 I llama_init_from_model: n_seq_max     = 1
0.00.034.448 I llama_init_from_model: n_ctx         = 512
0.00.034.448 I llama_init_from_model: n_ctx_per_seq = 512
0.00.034.449 I llama_init_from_model: n_batch       = 2048
0.00.034.449 I llama_init_from_model: n_ubatch      = 2048
0.00.034.449 I llama_init_from_model: flash_attn    = 0
0.00.034.450 I llama_init_from_model: freq_base     = 10000.0
0.00.034.450 I llama_init_from_model: freq_scale    = 1
0.00.034.451 I ggml_metal_init: allocating
0.00.034.454 I ggml_metal_init: found device: Apple M4
0.00.034.457 I ggml_metal_init: picking default device: Apple M4
0.00.035.236 I ggml_metal_init: using embedded metal library
0.00.039.266 I ggml_metal_init: GPU name:   Apple M4
0.00.039.268 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.039.269 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.039.269 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.039.270 I ggml_metal_init: simdgroup reduction   = true
0.00.039.270 I ggml_metal_init: simdgroup matrix mul. = true
0.00.039.270 I ggml_metal_init: has bfloat            = true
0.00.039.270 I ggml_metal_init: use bfloat            = true
0.00.039.271 I ggml_metal_init: hasUnifiedMemory      = true
0.00.039.272 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.051.205 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.051.780 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.051.782 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.051.783 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.052.562 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.052.563 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.052.564 I llama_init_from_model: graph nodes  = 429
0.00.052.564 I llama_init_from_model: graph splits = 2
0.00.052.565 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.052.565 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.059.135 I 
0.00.059.150 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.059.784 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.064.599 I llama_perf_context_print:        load time =      43.68 ms
0.00.064.600 I llama_perf_context_print: prompt eval time =       4.68 ms /     9 tokens (    0.52 ms per token,  1921.84 tokens per second)
0.00.064.601 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.064.602 I llama_perf_context_print:       total time =       5.47 ms /    10 tokens
0.00.064.733 I ggml_metal_free: deallocating

real	0m0.242s
user	0m0.047s
sys	0m0.028s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.039 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.088 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.722 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.726 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.727 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.728 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.728 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.730 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.731 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.731 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.732 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.732 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.733 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.734 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.737 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.738 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.738 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.741 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.742 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.743 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.121 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.767 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.768 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.769 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.769 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.769 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.770 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.770 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.770 I llama_model_loader: - type  f32:  124 tensors
0.00.014.771 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.771 I print_info: file format = GGUF V3 (latest)
0.00.014.772 I print_info: file type   = Q8_0
0.00.014.776 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.169 I load: special tokens cache size = 5
0.00.018.351 I load: token to piece cache size = 0.2032 MB
0.00.018.354 I print_info: arch             = bert
0.00.018.354 I print_info: vocab_only       = 0
0.00.018.354 I print_info: n_ctx_train      = 512
0.00.018.355 I print_info: n_embd           = 384
0.00.018.355 I print_info: n_layer          = 12
0.00.018.358 I print_info: n_head           = 12
0.00.018.358 I print_info: n_head_kv        = 12
0.00.018.359 I print_info: n_rot            = 32
0.00.018.359 I print_info: n_swa            = 0
0.00.018.359 I print_info: n_embd_head_k    = 32
0.00.018.359 I print_info: n_embd_head_v    = 32
0.00.018.360 I print_info: n_gqa            = 1
0.00.018.363 I print_info: n_embd_k_gqa     = 384
0.00.018.363 I print_info: n_embd_v_gqa     = 384
0.00.018.364 I print_info: f_norm_eps       = 1.0e-12
0.00.018.366 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.366 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.366 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.366 I print_info: f_logit_scale    = 0.0e+00
0.00.018.367 I print_info: n_ff             = 1536
0.00.018.367 I print_info: n_expert         = 0
0.00.018.367 I print_info: n_expert_used    = 0
0.00.018.367 I print_info: causal attn      = 0
0.00.018.367 I print_info: pooling type     = 2
0.00.018.367 I print_info: rope type        = 2
0.00.018.367 I print_info: rope scaling     = linear
0.00.018.369 I print_info: freq_base_train  = 10000.0
0.00.018.369 I print_info: freq_scale_train = 1
0.00.018.369 I print_info: n_ctx_orig_yarn  = 512
0.00.018.370 I print_info: rope_finetuned   = unknown
0.00.018.370 I print_info: ssm_d_conv       = 0
0.00.018.370 I print_info: ssm_d_inner      = 0
0.00.018.370 I print_info: ssm_d_state      = 0
0.00.018.370 I print_info: ssm_dt_rank      = 0
0.00.018.370 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.370 I print_info: model type       = 33M
0.00.018.371 I print_info: model params     = 33.21 M
0.00.018.371 I print_info: general.name     = Bge Small
0.00.018.372 I print_info: vocab type       = WPM
0.00.018.372 I print_info: n_vocab          = 30522
0.00.018.372 I print_info: n_merges         = 0
0.00.018.372 I print_info: BOS token        = 101 '[CLS]'
0.00.018.372 I print_info: UNK token        = 100 '[UNK]'
0.00.018.372 I print_info: SEP token        = 102 '[SEP]'
0.00.018.373 I print_info: PAD token        = 0 '[PAD]'
0.00.018.373 I print_info: MASK token       = 103 '[MASK]'
0.00.018.373 I print_info: LF token         = 0 '[PAD]'
0.00.018.375 I print_info: max token length = 21
0.00.019.712 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.712 I load_tensors: offloading output layer to GPU
0.00.019.712 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.720 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.721 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.868 I llama_init_from_model: n_seq_max     = 1
0.00.019.869 I llama_init_from_model: n_ctx         = 512
0.00.019.869 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.870 I llama_init_from_model: n_batch       = 2048
0.00.019.870 I llama_init_from_model: n_ubatch      = 2048
0.00.019.870 I llama_init_from_model: flash_attn    = 0
0.00.019.870 I llama_init_from_model: freq_base     = 10000.0
0.00.019.871 I llama_init_from_model: freq_scale    = 1
0.00.019.871 I ggml_metal_init: allocating
0.00.019.875 I ggml_metal_init: found device: Apple M4
0.00.019.877 I ggml_metal_init: picking default device: Apple M4
0.00.020.506 I ggml_metal_init: using embedded metal library
0.00.022.858 I ggml_metal_init: GPU name:   Apple M4
0.00.022.860 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.860 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.860 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.861 I ggml_metal_init: simdgroup reduction   = true
0.00.022.861 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.861 I ggml_metal_init: has bfloat            = true
0.00.022.861 I ggml_metal_init: use bfloat            = true
0.00.022.862 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.862 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.262 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.740 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.742 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.744 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.333 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.334 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.334 I llama_init_from_model: graph nodes  = 429
0.00.034.334 I llama_init_from_model: graph splits = 2
0.00.034.336 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.336 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.640 I 
0.00.038.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.180 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.415 I llama_perf_context_print:        load time =      29.55 ms
0.00.042.416 I llama_perf_context_print: prompt eval time =       3.09 ms /     9 tokens (    0.34 ms per token,  2908.86 tokens per second)
0.00.042.417 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.417 I llama_perf_context_print:       total time =       3.77 ms /    10 tokens
0.00.042.570 I ggml_metal_free: deallocating

real	0m0.054s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.200 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.697 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.665 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.673 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.674 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.675 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.675 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.677 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.678 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.678 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.679 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.680 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.683 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.683 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.684 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.550 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.552 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.552 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.553 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.553 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.554 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.554 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.554 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.555 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.555 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.555 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.556 I llama_model_loader: - type  f32:   40 tensors
0.00.049.556 I llama_model_loader: - type  f16:   30 tensors
0.00.049.557 I print_info: file format = GGUF V3 (latest)
0.00.049.558 I print_info: file type   = F16
0.00.049.559 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.065.312 W load: empty token at index 5
0.00.069.503 W load: model vocab missing newline token, using special_pad_id instead
0.00.070.760 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.070.791 I load: special tokens cache size = 5
0.00.332.998 I load: token to piece cache size = 1.5060 MB
0.00.333.007 I print_info: arch             = jina-bert-v2
0.00.333.007 I print_info: vocab_only       = 0
0.00.333.008 I print_info: n_ctx_train      = 8192
0.00.333.008 I print_info: n_embd           = 384
0.00.333.008 I print_info: n_layer          = 4
0.00.333.015 I print_info: n_head           = 12
0.00.333.015 I print_info: n_head_kv        = 12
0.00.333.016 I print_info: n_rot            = 32
0.00.333.016 I print_info: n_swa            = 0
0.00.333.016 I print_info: n_embd_head_k    = 32
0.00.333.016 I print_info: n_embd_head_v    = 32
0.00.333.017 I print_info: n_gqa            = 1
0.00.333.017 I print_info: n_embd_k_gqa     = 384
0.00.333.019 I print_info: n_embd_v_gqa     = 384
0.00.333.020 I print_info: f_norm_eps       = 1.0e-12
0.00.333.020 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.333.021 I print_info: f_clamp_kqv      = 0.0e+00
0.00.333.021 I print_info: f_max_alibi_bias = 8.0e+00
0.00.333.021 I print_info: f_logit_scale    = 0.0e+00
0.00.333.022 I print_info: n_ff             = 1536
0.00.333.022 I print_info: n_expert         = 0
0.00.333.022 I print_info: n_expert_used    = 0
0.00.333.022 I print_info: causal attn      = 0
0.00.333.022 I print_info: pooling type     = -1
0.00.333.023 I print_info: rope type        = -1
0.00.333.023 I print_info: rope scaling     = linear
0.00.333.023 I print_info: freq_base_train  = 10000.0
0.00.333.024 I print_info: freq_scale_train = 1
0.00.333.024 I print_info: n_ctx_orig_yarn  = 8192
0.00.333.024 I print_info: rope_finetuned   = unknown
0.00.333.024 I print_info: ssm_d_conv       = 0
0.00.333.024 I print_info: ssm_d_inner      = 0
0.00.333.025 I print_info: ssm_d_state      = 0
0.00.333.025 I print_info: ssm_dt_rank      = 0
0.00.333.025 I print_info: ssm_dt_b_c_rms   = 0
0.00.333.025 I print_info: model type       = 33M
0.00.333.026 I print_info: model params     = 32.90 M
0.00.333.026 I print_info: general.name     = Jina Bert Implementation
0.00.333.031 I print_info: vocab type       = BPE
0.00.333.031 I print_info: n_vocab          = 61056
0.00.333.031 I print_info: n_merges         = 39382
0.00.333.031 I print_info: BOS token        = 0 '<s>'
0.00.333.032 I print_info: EOS token        = 2 '</s>'
0.00.333.032 I print_info: UNK token        = 3 '<unk>'
0.00.333.032 I print_info: SEP token        = 2 '</s>'
0.00.333.032 I print_info: PAD token        = 1 '<pad>'
0.00.333.032 I print_info: MASK token       = 4 '<mask>'
0.00.333.033 I print_info: EOG token        = 2 '</s>'
0.00.333.033 I print_info: max token length = 45
0.00.333.977 I load_tensors: offloading 4 repeating layers to GPU
0.00.333.977 I load_tensors: offloading output layer to GPU
0.00.333.977 I load_tensors: offloaded 5/5 layers to GPU
0.00.333.997 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.333.998 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.334.191 I llama_init_from_model: n_seq_max     = 1
0.00.334.192 I llama_init_from_model: n_ctx         = 8192
0.00.334.192 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.334.192 I llama_init_from_model: n_batch       = 2048
0.00.334.192 I llama_init_from_model: n_ubatch      = 2048
0.00.334.193 I llama_init_from_model: flash_attn    = 0
0.00.334.193 I llama_init_from_model: freq_base     = 10000.0
0.00.334.193 I llama_init_from_model: freq_scale    = 1
0.00.334.194 I ggml_metal_init: allocating
0.00.334.197 I ggml_metal_init: found device: Apple M4
0.00.334.198 I ggml_metal_init: picking default device: Apple M4
0.00.334.867 I ggml_metal_init: using embedded metal library
0.00.337.548 I ggml_metal_init: GPU name:   Apple M4
0.00.337.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.337.550 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.337.550 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.337.550 I ggml_metal_init: simdgroup reduction   = true
0.00.337.551 I ggml_metal_init: simdgroup matrix mul. = true
0.00.337.551 I ggml_metal_init: has bfloat            = true
0.00.337.551 I ggml_metal_init: use bfloat            = true
0.00.337.551 I ggml_metal_init: hasUnifiedMemory      = true
0.00.337.552 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.347.942 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.350.582 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.350.584 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.350.587 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.351.163 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.351.164 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.351.165 I llama_init_from_model: graph nodes  = 154
0.00.351.165 I llama_init_from_model: graph splits = 2
0.00.351.166 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.360.875 I 
0.00.360.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.038 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.039 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.042 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.042 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.046 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.048 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.550 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.364.972 I llama_perf_context_print:        load time =     337.17 ms
0.00.364.973 I llama_perf_context_print: prompt eval time =       3.41 ms /    62 tokens (    0.06 ms per token, 18160.52 tokens per second)
0.00.364.974 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.364.975 I llama_perf_context_print:       total time =       4.10 ms /    63 tokens
0.00.365.170 I ggml_metal_free: deallocating

real	0m1.082s
user	0m0.340s
sys	0m0.042s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.166 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.285 I main: llama backend init
0.00.000.290 I main: load the model and apply lora adapter, if any
0.00.038.974 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.277 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.295 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.299 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.300 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.301 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.301 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.302 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.305 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.306 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.306 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.307 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.310 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.061.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.064.062 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.071.924 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.071.928 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.071.929 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.071.929 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.071.930 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.071.931 I llama_model_loader: - type  f32:  194 tensors
0.00.071.931 I llama_model_loader: - type  f16:   98 tensors
0.00.071.933 I print_info: file format = GGUF V3 (latest)
0.00.071.935 I print_info: file type   = all F32 (guessed)
0.00.071.937 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.100.982 I load: special tokens cache size = 25
0.00.108.096 I load: token to piece cache size = 0.2984 MB
0.00.108.099 I print_info: arch             = gptneox
0.00.108.099 I print_info: vocab_only       = 0
0.00.108.099 I print_info: n_ctx_train      = 2048
0.00.108.099 I print_info: n_embd           = 2048
0.00.108.099 I print_info: n_layer          = 24
0.00.108.102 I print_info: n_head           = 16
0.00.108.103 I print_info: n_head_kv        = 16
0.00.108.103 I print_info: n_rot            = 32
0.00.108.104 I print_info: n_swa            = 0
0.00.108.104 I print_info: n_embd_head_k    = 128
0.00.108.104 I print_info: n_embd_head_v    = 128
0.00.108.105 I print_info: n_gqa            = 1
0.00.108.105 I print_info: n_embd_k_gqa     = 2048
0.00.108.106 I print_info: n_embd_v_gqa     = 2048
0.00.108.106 I print_info: f_norm_eps       = 1.0e-05
0.00.108.107 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.107 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.108 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.108 I print_info: f_logit_scale    = 0.0e+00
0.00.108.110 I print_info: n_ff             = 8192
0.00.108.110 I print_info: n_expert         = 0
0.00.108.110 I print_info: n_expert_used    = 0
0.00.108.111 I print_info: causal attn      = 1
0.00.108.111 I print_info: pooling type     = 0
0.00.108.111 I print_info: rope type        = 2
0.00.108.111 I print_info: rope scaling     = linear
0.00.108.111 I print_info: freq_base_train  = 10000.0
0.00.108.112 I print_info: freq_scale_train = 1
0.00.108.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.112 I print_info: rope_finetuned   = unknown
0.00.108.112 I print_info: ssm_d_conv       = 0
0.00.108.114 I print_info: ssm_d_inner      = 0
0.00.108.114 I print_info: ssm_d_state      = 0
0.00.108.114 I print_info: ssm_dt_rank      = 0
0.00.108.114 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.114 I print_info: model type       = 1.4B
0.00.108.114 I print_info: model params     = 1.41 B
0.00.108.114 I print_info: general.name     = 1.4B
0.00.108.115 I print_info: vocab type       = BPE
0.00.108.115 I print_info: n_vocab          = 50304
0.00.108.115 I print_info: n_merges         = 50009
0.00.108.116 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.116 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.116 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.116 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.116 I print_info: LF token         = 128 'Ä'
0.00.108.116 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.117 I print_info: max token length = 1024
0.00.110.777 I load_tensors: offloading 24 repeating layers to GPU
0.00.110.777 I load_tensors: offloading output layer to GPU
0.00.110.777 I load_tensors: offloaded 25/25 layers to GPU
0.00.110.795 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.110.796 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.111.099 I llama_init_from_model: n_seq_max     = 1
0.00.111.100 I llama_init_from_model: n_ctx         = 2048
0.00.111.100 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.111.101 I llama_init_from_model: n_batch       = 2048
0.00.111.101 I llama_init_from_model: n_ubatch      = 512
0.00.111.101 I llama_init_from_model: flash_attn    = 0
0.00.111.101 I llama_init_from_model: freq_base     = 10000.0
0.00.111.102 I llama_init_from_model: freq_scale    = 1
0.00.111.102 I ggml_metal_init: allocating
0.00.111.105 I ggml_metal_init: found device: Apple M4
0.00.111.107 I ggml_metal_init: picking default device: Apple M4
0.00.111.812 I ggml_metal_init: using embedded metal library
0.00.122.128 I ggml_metal_init: GPU name:   Apple M4
0.00.122.130 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.122.131 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.122.131 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.122.131 I ggml_metal_init: simdgroup reduction   = true
0.00.122.131 I ggml_metal_init: simdgroup matrix mul. = true
0.00.122.131 I ggml_metal_init: has bfloat            = true
0.00.122.132 I ggml_metal_init: use bfloat            = true
0.00.122.132 I ggml_metal_init: hasUnifiedMemory      = true
0.00.122.132 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.146.514 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.166.035 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.166.043 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.166.062 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.167.000 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.167.002 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.167.002 I llama_init_from_model: graph nodes  = 967
0.00.167.002 I llama_init_from_model: graph splits = 2
0.00.167.006 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.167.126 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.167.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.248.173 I main: llama threadpool init, n_threads = 4
0.00.248.217 I 
0.00.248.241 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.248.241 I 
0.00.248.311 I sampler seed: 1234
0.00.248.316 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.248.340 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.248.342 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.248.342 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.073.935 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.02.073.936 I llama_perf_context_print:        load time =     209.19 ms
0.02.073.936 I llama_perf_context_print: prompt eval time =      43.52 ms /     7 tokens (    6.22 ms per token,   160.85 tokens per second)
0.02.073.938 I llama_perf_context_print:        eval time =    1779.29 ms /    63 runs   (   28.24 ms per token,    35.41 tokens per second)
0.02.073.938 I llama_perf_context_print:       total time =    1825.77 ms /    70 tokens
0.02.074.172 I ggml_metal_free: deallocating

real	0m2.380s
user	0m0.145s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.508 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.290 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.571 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.577 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.579 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.580 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.580 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.586 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.586 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.588 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.588 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.589 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.589 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.590 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.594 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.595 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.596 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.702 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.713 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.685 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.687 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.688 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.688 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.689 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.689 I llama_model_loader: - type  f32:  194 tensors
0.00.054.690 I llama_model_loader: - type  f16:   98 tensors
0.00.054.691 I print_info: file format = GGUF V3 (latest)
0.00.054.692 I print_info: file type   = all F32 (guessed)
0.00.054.693 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.909 I load: special tokens cache size = 25
0.00.088.606 I load: token to piece cache size = 0.2984 MB
0.00.088.609 I print_info: arch             = gptneox
0.00.088.609 I print_info: vocab_only       = 0
0.00.088.609 I print_info: n_ctx_train      = 2048
0.00.088.609 I print_info: n_embd           = 2048
0.00.088.609 I print_info: n_layer          = 24
0.00.088.612 I print_info: n_head           = 16
0.00.088.613 I print_info: n_head_kv        = 16
0.00.088.613 I print_info: n_rot            = 32
0.00.088.613 I print_info: n_swa            = 0
0.00.088.613 I print_info: n_embd_head_k    = 128
0.00.088.614 I print_info: n_embd_head_v    = 128
0.00.088.616 I print_info: n_gqa            = 1
0.00.088.617 I print_info: n_embd_k_gqa     = 2048
0.00.088.617 I print_info: n_embd_v_gqa     = 2048
0.00.088.618 I print_info: f_norm_eps       = 1.0e-05
0.00.088.618 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.618 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.624 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.624 I print_info: f_logit_scale    = 0.0e+00
0.00.088.625 I print_info: n_ff             = 8192
0.00.088.625 I print_info: n_expert         = 0
0.00.088.625 I print_info: n_expert_used    = 0
0.00.088.625 I print_info: causal attn      = 1
0.00.088.626 I print_info: pooling type     = 0
0.00.088.626 I print_info: rope type        = 2
0.00.088.626 I print_info: rope scaling     = linear
0.00.088.626 I print_info: freq_base_train  = 10000.0
0.00.088.627 I print_info: freq_scale_train = 1
0.00.088.627 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.627 I print_info: rope_finetuned   = unknown
0.00.088.627 I print_info: ssm_d_conv       = 0
0.00.088.627 I print_info: ssm_d_inner      = 0
0.00.088.627 I print_info: ssm_d_state      = 0
0.00.088.628 I print_info: ssm_dt_rank      = 0
0.00.088.628 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.628 I print_info: model type       = 1.4B
0.00.088.628 I print_info: model params     = 1.41 B
0.00.088.632 I print_info: general.name     = 1.4B
0.00.088.632 I print_info: vocab type       = BPE
0.00.088.632 I print_info: n_vocab          = 50304
0.00.088.633 I print_info: n_merges         = 50009
0.00.088.633 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.633 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.633 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.633 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.633 I print_info: LF token         = 128 'Ä'
0.00.088.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.634 I print_info: max token length = 1024
0.00.090.327 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.328 I load_tensors: offloading output layer to GPU
0.00.090.328 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.338 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.339 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.090.635 I llama_init_from_model: n_seq_max     = 1
0.00.090.636 I llama_init_from_model: n_ctx         = 128
0.00.090.636 I llama_init_from_model: n_ctx_per_seq = 128
0.00.090.637 I llama_init_from_model: n_batch       = 128
0.00.090.637 I llama_init_from_model: n_ubatch      = 128
0.00.090.637 I llama_init_from_model: flash_attn    = 0
0.00.090.637 I llama_init_from_model: freq_base     = 10000.0
0.00.090.638 I llama_init_from_model: freq_scale    = 1
0.00.090.638 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.090.638 I ggml_metal_init: allocating
0.00.090.641 I ggml_metal_init: found device: Apple M4
0.00.090.643 I ggml_metal_init: picking default device: Apple M4
0.00.091.230 I ggml_metal_init: using embedded metal library
0.00.093.816 I ggml_metal_init: GPU name:   Apple M4
0.00.093.818 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.093.818 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.093.819 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.093.819 I ggml_metal_init: simdgroup reduction   = true
0.00.093.819 I ggml_metal_init: simdgroup matrix mul. = true
0.00.093.819 I ggml_metal_init: has bfloat            = true
0.00.093.819 I ggml_metal_init: use bfloat            = true
0.00.093.820 I ggml_metal_init: hasUnifiedMemory      = true
0.00.093.820 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.116 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.421 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.425 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.440 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.360 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.360 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.361 I llama_init_from_model: graph nodes  = 967
0.00.105.361 I llama_init_from_model: graph splits = 2
0.00.105.362 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.362 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.373.551 I 
0.01.373.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.373.640 I perplexity: tokenizing the input ..
0.01.387.028 I perplexity: tokenization took 13.383 ms
0.01.387.035 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.507.856 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.509.574 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.509.625 I llama_perf_context_print:        load time =    1350.24 ms
0.01.509.627 I llama_perf_context_print: prompt eval time =     119.93 ms /   128 tokens (    0.94 ms per token,  1067.32 tokens per second)
0.01.509.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.509.628 I llama_perf_context_print:       total time =     136.08 ms /   129 tokens
0.01.510.326 I ggml_metal_free: deallocating

real	0m1.700s
user	0m0.123s
sys	0m0.231s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.810 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.742 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.753 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.757 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.760 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.697 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.634 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.636 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.637 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.638 I llama_model_loader: - type  f32:  194 tensors
0.00.035.638 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.639 I print_info: file format = GGUF V3 (latest)
0.00.035.639 I print_info: file type   = Q8_0
0.00.035.640 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.148 I load: special tokens cache size = 25
0.00.062.197 I load: token to piece cache size = 0.2984 MB
0.00.062.201 I print_info: arch             = gptneox
0.00.062.201 I print_info: vocab_only       = 0
0.00.062.202 I print_info: n_ctx_train      = 2048
0.00.062.202 I print_info: n_embd           = 2048
0.00.062.202 I print_info: n_layer          = 24
0.00.062.208 I print_info: n_head           = 16
0.00.062.209 I print_info: n_head_kv        = 16
0.00.062.209 I print_info: n_rot            = 32
0.00.062.209 I print_info: n_swa            = 0
0.00.062.209 I print_info: n_embd_head_k    = 128
0.00.062.210 I print_info: n_embd_head_v    = 128
0.00.062.210 I print_info: n_gqa            = 1
0.00.062.211 I print_info: n_embd_k_gqa     = 2048
0.00.062.212 I print_info: n_embd_v_gqa     = 2048
0.00.062.212 I print_info: f_norm_eps       = 1.0e-05
0.00.062.216 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.216 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.216 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.217 I print_info: f_logit_scale    = 0.0e+00
0.00.062.218 I print_info: n_ff             = 8192
0.00.062.218 I print_info: n_expert         = 0
0.00.062.218 I print_info: n_expert_used    = 0
0.00.062.218 I print_info: causal attn      = 1
0.00.062.218 I print_info: pooling type     = 0
0.00.062.219 I print_info: rope type        = 2
0.00.062.219 I print_info: rope scaling     = linear
0.00.062.221 I print_info: freq_base_train  = 10000.0
0.00.062.221 I print_info: freq_scale_train = 1
0.00.062.221 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.222 I print_info: rope_finetuned   = unknown
0.00.062.222 I print_info: ssm_d_conv       = 0
0.00.062.222 I print_info: ssm_d_inner      = 0
0.00.062.222 I print_info: ssm_d_state      = 0
0.00.062.222 I print_info: ssm_dt_rank      = 0
0.00.062.222 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.223 I print_info: model type       = 1.4B
0.00.062.223 I print_info: model params     = 1.41 B
0.00.062.223 I print_info: general.name     = 1.4B
0.00.062.224 I print_info: vocab type       = BPE
0.00.062.224 I print_info: n_vocab          = 50304
0.00.062.224 I print_info: n_merges         = 50009
0.00.062.224 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.224 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.224 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.225 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.225 I print_info: LF token         = 128 'Ä'
0.00.062.225 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.225 I print_info: max token length = 1024
0.00.064.579 I load_tensors: offloading 24 repeating layers to GPU
0.00.064.579 I load_tensors: offloading output layer to GPU
0.00.064.580 I load_tensors: offloaded 25/25 layers to GPU
0.00.064.591 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.064.593 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.064.942 I llama_init_from_model: n_seq_max     = 1
0.00.064.942 I llama_init_from_model: n_ctx         = 2048
0.00.064.942 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.064.943 I llama_init_from_model: n_batch       = 2048
0.00.064.943 I llama_init_from_model: n_ubatch      = 512
0.00.064.943 I llama_init_from_model: flash_attn    = 0
0.00.064.943 I llama_init_from_model: freq_base     = 10000.0
0.00.064.943 I llama_init_from_model: freq_scale    = 1
0.00.064.944 I ggml_metal_init: allocating
0.00.064.948 I ggml_metal_init: found device: Apple M4
0.00.064.949 I ggml_metal_init: picking default device: Apple M4
0.00.065.684 I ggml_metal_init: using embedded metal library
0.00.068.229 I ggml_metal_init: GPU name:   Apple M4
0.00.068.231 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.232 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.232 I ggml_metal_init: simdgroup reduction   = true
0.00.068.232 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.233 I ggml_metal_init: has bfloat            = true
0.00.068.233 I ggml_metal_init: use bfloat            = true
0.00.068.233 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.234 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.911 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.331 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.339 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.367 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.470 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.105.472 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.105.473 I llama_init_from_model: graph nodes  = 967
0.00.105.473 I llama_init_from_model: graph splits = 2
0.00.105.478 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.105.607 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.608 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.554.984 I main: llama threadpool init, n_threads = 4
0.01.555.085 I 
0.01.555.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.555.150 I 
0.01.555.686 I sampler seed: 1234
0.01.555.693 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.555.768 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.555.773 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.555.773 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.653.112 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56528.66 tokens per second)
0.02.653.113 I llama_perf_context_print:        load time =    1545.16 ms
0.02.653.114 I llama_perf_context_print: prompt eval time =      49.92 ms /     7 tokens (    7.13 ms per token,   140.22 tokens per second)
0.02.653.114 I llama_perf_context_print:        eval time =    1044.42 ms /    63 runs   (   16.58 ms per token,    60.32 tokens per second)
0.02.653.118 I llama_perf_context_print:       total time =    1098.14 ms /    70 tokens
0.02.653.374 I ggml_metal_free: deallocating

real	0m2.672s
user	0m0.122s
sys	0m0.273s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.139 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.069 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.095 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.097 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.098 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.098 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.101 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.101 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.102 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.785 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.821 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.824 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.824 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.825 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.825 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.826 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.826 I llama_model_loader: - type  f32:  194 tensors
0.00.033.827 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.827 I print_info: file format = GGUF V3 (latest)
0.00.033.828 I print_info: file type   = Q8_0
0.00.033.829 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.058.028 I load: special tokens cache size = 25
0.00.064.172 I load: token to piece cache size = 0.2984 MB
0.00.064.176 I print_info: arch             = gptneox
0.00.064.176 I print_info: vocab_only       = 0
0.00.064.176 I print_info: n_ctx_train      = 2048
0.00.064.176 I print_info: n_embd           = 2048
0.00.064.177 I print_info: n_layer          = 24
0.00.064.182 I print_info: n_head           = 16
0.00.064.182 I print_info: n_head_kv        = 16
0.00.064.183 I print_info: n_rot            = 32
0.00.064.183 I print_info: n_swa            = 0
0.00.064.183 I print_info: n_embd_head_k    = 128
0.00.064.183 I print_info: n_embd_head_v    = 128
0.00.064.184 I print_info: n_gqa            = 1
0.00.064.185 I print_info: n_embd_k_gqa     = 2048
0.00.064.189 I print_info: n_embd_v_gqa     = 2048
0.00.064.189 I print_info: f_norm_eps       = 1.0e-05
0.00.064.190 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.190 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.190 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.191 I print_info: f_logit_scale    = 0.0e+00
0.00.064.191 I print_info: n_ff             = 8192
0.00.064.192 I print_info: n_expert         = 0
0.00.064.192 I print_info: n_expert_used    = 0
0.00.064.192 I print_info: causal attn      = 1
0.00.064.194 I print_info: pooling type     = 0
0.00.064.194 I print_info: rope type        = 2
0.00.064.194 I print_info: rope scaling     = linear
0.00.064.194 I print_info: freq_base_train  = 10000.0
0.00.064.195 I print_info: freq_scale_train = 1
0.00.064.196 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.197 I print_info: rope_finetuned   = unknown
0.00.064.197 I print_info: ssm_d_conv       = 0
0.00.064.197 I print_info: ssm_d_inner      = 0
0.00.064.197 I print_info: ssm_d_state      = 0
0.00.064.197 I print_info: ssm_dt_rank      = 0
0.00.064.197 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.197 I print_info: model type       = 1.4B
0.00.064.198 I print_info: model params     = 1.41 B
0.00.064.198 I print_info: general.name     = 1.4B
0.00.064.198 I print_info: vocab type       = BPE
0.00.064.199 I print_info: n_vocab          = 50304
0.00.064.199 I print_info: n_merges         = 50009
0.00.064.200 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.200 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.200 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.200 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.201 I print_info: LF token         = 128 'Ä'
0.00.064.201 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.201 I print_info: max token length = 1024
0.00.066.687 I load_tensors: offloading 24 repeating layers to GPU
0.00.066.687 I load_tensors: offloading output layer to GPU
0.00.066.687 I load_tensors: offloaded 25/25 layers to GPU
0.00.066.699 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.700 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.067.029 I llama_init_from_model: n_seq_max     = 1
0.00.067.030 I llama_init_from_model: n_ctx         = 128
0.00.067.030 I llama_init_from_model: n_ctx_per_seq = 128
0.00.067.030 I llama_init_from_model: n_batch       = 128
0.00.067.030 I llama_init_from_model: n_ubatch      = 128
0.00.067.030 I llama_init_from_model: flash_attn    = 0
0.00.067.031 I llama_init_from_model: freq_base     = 10000.0
0.00.067.031 I llama_init_from_model: freq_scale    = 1
0.00.067.031 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.032 I ggml_metal_init: allocating
0.00.067.035 I ggml_metal_init: found device: Apple M4
0.00.067.038 I ggml_metal_init: picking default device: Apple M4
0.00.067.727 I ggml_metal_init: using embedded metal library
0.00.070.421 I ggml_metal_init: GPU name:   Apple M4
0.00.070.422 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.423 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.423 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.423 I ggml_metal_init: simdgroup reduction   = true
0.00.070.424 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.424 I ggml_metal_init: has bfloat            = true
0.00.070.424 I ggml_metal_init: use bfloat            = true
0.00.070.425 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.425 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.708 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.225 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.082.228 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.082.245 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.348 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.083.349 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.083.349 I llama_init_from_model: graph nodes  = 967
0.00.083.349 I llama_init_from_model: graph splits = 2
0.00.083.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.083.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.938.791 I 
0.00.938.816 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.938.820 I perplexity: tokenizing the input ..
0.00.946.848 I perplexity: tokenization took 8.027 ms
0.00.946.852 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.071.485 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.072.672 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.072.701 I llama_perf_context_print:        load time =     926.72 ms
0.01.072.702 I llama_perf_context_print: prompt eval time =     124.41 ms /   128 tokens (    0.97 ms per token,  1028.87 tokens per second)
0.01.072.703 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.072.703 I llama_perf_context_print:       total time =     133.91 ms /   129 tokens
0.01.073.185 I ggml_metal_free: deallocating

real	0m1.091s
user	0m0.093s
sys	0m0.165s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.018.447 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.814 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.026.820 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.827 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.828 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.828 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.828 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.831 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.832 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.832 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.832 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.833 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.833 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.834 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.836 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.374 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.506 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.508 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.508 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.508 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.508 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.509 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.036.509 I llama_model_loader: - type  f32:  194 tensors
0.00.036.510 I llama_model_loader: - type q4_0:   97 tensors
0.00.036.510 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.510 I print_info: file format = GGUF V3 (latest)
0.00.036.511 I print_info: file type   = Q4_0
0.00.036.512 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.060.863 I load: special tokens cache size = 25
0.00.069.101 I load: token to piece cache size = 0.2984 MB
0.00.069.105 I print_info: arch             = gptneox
0.00.069.105 I print_info: vocab_only       = 0
0.00.069.105 I print_info: n_ctx_train      = 2048
0.00.069.106 I print_info: n_embd           = 2048
0.00.069.106 I print_info: n_layer          = 24
0.00.069.110 I print_info: n_head           = 16
0.00.069.111 I print_info: n_head_kv        = 16
0.00.069.111 I print_info: n_rot            = 32
0.00.069.111 I print_info: n_swa            = 0
0.00.069.111 I print_info: n_embd_head_k    = 128
0.00.069.111 I print_info: n_embd_head_v    = 128
0.00.069.112 I print_info: n_gqa            = 1
0.00.069.113 I print_info: n_embd_k_gqa     = 2048
0.00.069.114 I print_info: n_embd_v_gqa     = 2048
0.00.069.115 I print_info: f_norm_eps       = 1.0e-05
0.00.069.116 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.116 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.118 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.118 I print_info: f_logit_scale    = 0.0e+00
0.00.069.119 I print_info: n_ff             = 8192
0.00.069.119 I print_info: n_expert         = 0
0.00.069.119 I print_info: n_expert_used    = 0
0.00.069.119 I print_info: causal attn      = 1
0.00.069.120 I print_info: pooling type     = 0
0.00.069.120 I print_info: rope type        = 2
0.00.069.120 I print_info: rope scaling     = linear
0.00.069.121 I print_info: freq_base_train  = 10000.0
0.00.069.121 I print_info: freq_scale_train = 1
0.00.069.121 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.122 I print_info: rope_finetuned   = unknown
0.00.069.122 I print_info: ssm_d_conv       = 0
0.00.069.122 I print_info: ssm_d_inner      = 0
0.00.069.122 I print_info: ssm_d_state      = 0
0.00.069.122 I print_info: ssm_dt_rank      = 0
0.00.069.122 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.123 I print_info: model type       = 1.4B
0.00.069.123 I print_info: model params     = 1.41 B
0.00.069.125 I print_info: general.name     = 1.4B
0.00.069.126 I print_info: vocab type       = BPE
0.00.069.126 I print_info: n_vocab          = 50304
0.00.069.126 I print_info: n_merges         = 50009
0.00.069.126 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.127 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.127 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.127 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.127 I print_info: LF token         = 128 'Ä'
0.00.069.128 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.128 I print_info: max token length = 1024
0.00.071.674 I load_tensors: offloading 24 repeating layers to GPU
0.00.071.674 I load_tensors: offloading output layer to GPU
0.00.071.675 I load_tensors: offloaded 25/25 layers to GPU
0.00.071.686 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.071.687 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.072.048 I llama_init_from_model: n_seq_max     = 1
0.00.072.049 I llama_init_from_model: n_ctx         = 2048
0.00.072.049 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.072.050 I llama_init_from_model: n_batch       = 2048
0.00.072.050 I llama_init_from_model: n_ubatch      = 512
0.00.072.050 I llama_init_from_model: flash_attn    = 0
0.00.072.051 I llama_init_from_model: freq_base     = 10000.0
0.00.072.051 I llama_init_from_model: freq_scale    = 1
0.00.072.051 I ggml_metal_init: allocating
0.00.072.054 I ggml_metal_init: found device: Apple M4
0.00.072.057 I ggml_metal_init: picking default device: Apple M4
0.00.072.860 I ggml_metal_init: using embedded metal library
0.00.075.912 I ggml_metal_init: GPU name:   Apple M4
0.00.075.914 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.915 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.915 I ggml_metal_init: simdgroup reduction   = true
0.00.075.915 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.915 I ggml_metal_init: has bfloat            = true
0.00.075.915 I ggml_metal_init: use bfloat            = true
0.00.075.916 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.916 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.623 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.113.194 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.200 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.227 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.114.396 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.114.398 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.114.398 I llama_init_from_model: graph nodes  = 967
0.00.114.398 I llama_init_from_model: graph splits = 2
0.00.114.402 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.114.530 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.114.531 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.175 I main: llama threadpool init, n_threads = 4
0.00.703.215 I 
0.00.703.240 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.703.240 I 
0.00.703.457 I sampler seed: 1234
0.00.703.464 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.502 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.503 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.503 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.388.924 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.388.924 I llama_perf_context_print:        load time =     684.72 ms
0.01.388.925 I llama_perf_context_print: prompt eval time =      39.80 ms /     7 tokens (    5.69 ms per token,   175.89 tokens per second)
0.01.388.926 I llama_perf_context_print:        eval time =     642.57 ms /    63 runs   (   10.20 ms per token,    98.04 tokens per second)
0.01.388.929 I llama_perf_context_print:       total time =     685.75 ms /    70 tokens
0.01.389.149 I ggml_metal_free: deallocating

real	0m1.411s
user	0m0.122s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.145 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.995 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.997 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.997 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.998 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.999 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.000 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.003 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.003 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.004 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.007 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.754 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.783 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.480 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.482 I llama_model_loader: - type  f32:  194 tensors
0.00.026.482 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.483 I print_info: file format = GGUF V3 (latest)
0.00.026.483 I print_info: file type   = Q4_0
0.00.026.484 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.045.713 I load: special tokens cache size = 25
0.00.051.680 I load: token to piece cache size = 0.2984 MB
0.00.051.683 I print_info: arch             = gptneox
0.00.051.683 I print_info: vocab_only       = 0
0.00.051.683 I print_info: n_ctx_train      = 2048
0.00.051.683 I print_info: n_embd           = 2048
0.00.051.684 I print_info: n_layer          = 24
0.00.051.686 I print_info: n_head           = 16
0.00.051.687 I print_info: n_head_kv        = 16
0.00.051.687 I print_info: n_rot            = 32
0.00.051.687 I print_info: n_swa            = 0
0.00.051.689 I print_info: n_embd_head_k    = 128
0.00.051.689 I print_info: n_embd_head_v    = 128
0.00.051.690 I print_info: n_gqa            = 1
0.00.051.691 I print_info: n_embd_k_gqa     = 2048
0.00.051.696 I print_info: n_embd_v_gqa     = 2048
0.00.051.697 I print_info: f_norm_eps       = 1.0e-05
0.00.051.697 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.698 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.698 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.698 I print_info: f_logit_scale    = 0.0e+00
0.00.051.700 I print_info: n_ff             = 8192
0.00.051.701 I print_info: n_expert         = 0
0.00.051.701 I print_info: n_expert_used    = 0
0.00.051.701 I print_info: causal attn      = 1
0.00.051.701 I print_info: pooling type     = 0
0.00.051.701 I print_info: rope type        = 2
0.00.051.701 I print_info: rope scaling     = linear
0.00.051.702 I print_info: freq_base_train  = 10000.0
0.00.051.702 I print_info: freq_scale_train = 1
0.00.051.705 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.710 I print_info: rope_finetuned   = unknown
0.00.051.710 I print_info: ssm_d_conv       = 0
0.00.051.711 I print_info: ssm_d_inner      = 0
0.00.051.711 I print_info: ssm_d_state      = 0
0.00.051.711 I print_info: ssm_dt_rank      = 0
0.00.051.711 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.711 I print_info: model type       = 1.4B
0.00.051.712 I print_info: model params     = 1.41 B
0.00.051.712 I print_info: general.name     = 1.4B
0.00.051.713 I print_info: vocab type       = BPE
0.00.051.713 I print_info: n_vocab          = 50304
0.00.051.713 I print_info: n_merges         = 50009
0.00.051.713 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.713 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.714 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.714 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.714 I print_info: LF token         = 128 'Ä'
0.00.051.714 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.714 I print_info: max token length = 1024
0.00.053.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.672 I load_tensors: offloading output layer to GPU
0.00.053.672 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.683 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.684 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.053.964 I llama_init_from_model: n_seq_max     = 1
0.00.053.965 I llama_init_from_model: n_ctx         = 128
0.00.053.965 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.965 I llama_init_from_model: n_batch       = 128
0.00.053.965 I llama_init_from_model: n_ubatch      = 128
0.00.053.965 I llama_init_from_model: flash_attn    = 0
0.00.053.966 I llama_init_from_model: freq_base     = 10000.0
0.00.053.966 I llama_init_from_model: freq_scale    = 1
0.00.053.966 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.967 I ggml_metal_init: allocating
0.00.053.970 I ggml_metal_init: found device: Apple M4
0.00.053.972 I ggml_metal_init: picking default device: Apple M4
0.00.054.526 I ggml_metal_init: using embedded metal library
0.00.056.846 I ggml_metal_init: GPU name:   Apple M4
0.00.056.847 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.848 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.848 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.848 I ggml_metal_init: simdgroup reduction   = true
0.00.056.849 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.849 I ggml_metal_init: has bfloat            = true
0.00.056.849 I ggml_metal_init: use bfloat            = true
0.00.056.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.123 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.353 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.357 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.373 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.265 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.266 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.266 I llama_init_from_model: graph nodes  = 967
0.00.068.267 I llama_init_from_model: graph splits = 2
0.00.068.268 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.268 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.528 I 
0.00.594.572 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.594.578 I perplexity: tokenizing the input ..
0.00.602.682 I perplexity: tokenization took 8.102 ms
0.00.602.691 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.725.283 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.726.428 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.726.451 I llama_perf_context_print:        load time =     583.38 ms
0.00.726.452 I llama_perf_context_print: prompt eval time =     122.36 ms /   128 tokens (    0.96 ms per token,  1046.06 tokens per second)
0.00.726.453 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.726.455 I llama_perf_context_print:       total time =     131.93 ms /   129 tokens
0.00.726.888 I ggml_metal_free: deallocating

real	0m0.742s
user	0m0.077s
sys	0m0.090s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.871 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.546 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.022.551 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.557 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.557 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.558 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.564 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.283 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.028 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.029 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.030 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.030 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.030 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.031 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.031 I llama_model_loader: - type  f32:  194 tensors
0.00.031.031 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.031 I llama_model_loader: - type q6_K:    1 tensors
0.00.031.032 I print_info: file format = GGUF V3 (latest)
0.00.031.032 I print_info: file type   = Q4_1
0.00.031.033 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.050.572 I load: special tokens cache size = 25
0.00.056.476 I load: token to piece cache size = 0.2984 MB
0.00.056.479 I print_info: arch             = gptneox
0.00.056.479 I print_info: vocab_only       = 0
0.00.056.479 I print_info: n_ctx_train      = 2048
0.00.056.480 I print_info: n_embd           = 2048
0.00.056.480 I print_info: n_layer          = 24
0.00.056.483 I print_info: n_head           = 16
0.00.056.484 I print_info: n_head_kv        = 16
0.00.056.484 I print_info: n_rot            = 32
0.00.056.484 I print_info: n_swa            = 0
0.00.056.484 I print_info: n_embd_head_k    = 128
0.00.056.484 I print_info: n_embd_head_v    = 128
0.00.056.485 I print_info: n_gqa            = 1
0.00.056.486 I print_info: n_embd_k_gqa     = 2048
0.00.056.487 I print_info: n_embd_v_gqa     = 2048
0.00.056.487 I print_info: f_norm_eps       = 1.0e-05
0.00.056.488 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.056.488 I print_info: f_clamp_kqv      = 0.0e+00
0.00.056.488 I print_info: f_max_alibi_bias = 0.0e+00
0.00.056.488 I print_info: f_logit_scale    = 0.0e+00
0.00.056.489 I print_info: n_ff             = 8192
0.00.056.489 I print_info: n_expert         = 0
0.00.056.490 I print_info: n_expert_used    = 0
0.00.056.490 I print_info: causal attn      = 1
0.00.056.492 I print_info: pooling type     = 0
0.00.056.492 I print_info: rope type        = 2
0.00.056.492 I print_info: rope scaling     = linear
0.00.056.493 I print_info: freq_base_train  = 10000.0
0.00.056.493 I print_info: freq_scale_train = 1
0.00.056.493 I print_info: n_ctx_orig_yarn  = 2048
0.00.056.493 I print_info: rope_finetuned   = unknown
0.00.056.493 I print_info: ssm_d_conv       = 0
0.00.056.494 I print_info: ssm_d_inner      = 0
0.00.056.494 I print_info: ssm_d_state      = 0
0.00.056.494 I print_info: ssm_dt_rank      = 0
0.00.056.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.056.496 I print_info: model type       = 1.4B
0.00.056.496 I print_info: model params     = 1.41 B
0.00.056.496 I print_info: general.name     = 1.4B
0.00.056.497 I print_info: vocab type       = BPE
0.00.056.497 I print_info: n_vocab          = 50304
0.00.056.497 I print_info: n_merges         = 50009
0.00.056.497 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.056.497 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.056.498 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.056.498 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.056.501 I print_info: LF token         = 128 'Ä'
0.00.056.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.056.502 I print_info: max token length = 1024
0.00.058.418 I load_tensors: offloading 24 repeating layers to GPU
0.00.058.419 I load_tensors: offloading output layer to GPU
0.00.058.419 I load_tensors: offloaded 25/25 layers to GPU
0.00.058.429 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.058.431 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.058.718 I llama_init_from_model: n_seq_max     = 1
0.00.058.718 I llama_init_from_model: n_ctx         = 2048
0.00.058.718 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.058.718 I llama_init_from_model: n_batch       = 2048
0.00.058.719 I llama_init_from_model: n_ubatch      = 512
0.00.058.719 I llama_init_from_model: flash_attn    = 0
0.00.058.719 I llama_init_from_model: freq_base     = 10000.0
0.00.058.719 I llama_init_from_model: freq_scale    = 1
0.00.058.720 I ggml_metal_init: allocating
0.00.058.723 I ggml_metal_init: found device: Apple M4
0.00.058.725 I ggml_metal_init: picking default device: Apple M4
0.00.059.310 I ggml_metal_init: using embedded metal library
0.00.061.629 I ggml_metal_init: GPU name:   Apple M4
0.00.061.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.631 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.632 I ggml_metal_init: simdgroup reduction   = true
0.00.061.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.632 I ggml_metal_init: has bfloat            = true
0.00.061.632 I ggml_metal_init: use bfloat            = true
0.00.061.632 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.159 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.148 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.153 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.171 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.228 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.229 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.230 I llama_init_from_model: graph nodes  = 967
0.00.091.230 I llama_init_from_model: graph splits = 2
0.00.091.233 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.372 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.373 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.847.604 I main: llama threadpool init, n_threads = 4
0.00.847.641 I 
0.00.847.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.847.662 I 
0.00.847.804 I sampler seed: 1234
0.00.847.808 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.847.846 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.847.847 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.847.847 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.578.267 I llama_perf_sampler_print:    sampling time =       1.06 ms /    71 runs   (    0.01 ms per token, 66918.00 tokens per second)
0.01.578.268 I llama_perf_context_print:        load time =     838.73 ms
0.01.578.269 I llama_perf_context_print: prompt eval time =      39.50 ms /     7 tokens (    5.64 ms per token,   177.22 tokens per second)
0.01.578.269 I llama_perf_context_print:        eval time =     688.13 ms /    63 runs   (   10.92 ms per token,    91.55 tokens per second)
0.01.578.270 I llama_perf_context_print:       total time =     730.67 ms /    70 tokens
0.01.578.475 I ggml_metal_free: deallocating

real	0m1.594s
user	0m0.108s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.947 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.794 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.798 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.800 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.800 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.802 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.805 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.805 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.806 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.810 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.810 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.810 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.346 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.925 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.926 I llama_model_loader: - type  f32:  194 tensors
0.00.023.927 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.927 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.927 I print_info: file format = GGUF V3 (latest)
0.00.023.928 I print_info: file type   = Q4_1
0.00.023.929 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.042 I load: special tokens cache size = 25
0.00.048.862 I load: token to piece cache size = 0.2984 MB
0.00.048.865 I print_info: arch             = gptneox
0.00.048.866 I print_info: vocab_only       = 0
0.00.048.866 I print_info: n_ctx_train      = 2048
0.00.048.866 I print_info: n_embd           = 2048
0.00.048.866 I print_info: n_layer          = 24
0.00.048.869 I print_info: n_head           = 16
0.00.048.869 I print_info: n_head_kv        = 16
0.00.048.869 I print_info: n_rot            = 32
0.00.048.870 I print_info: n_swa            = 0
0.00.048.872 I print_info: n_embd_head_k    = 128
0.00.048.872 I print_info: n_embd_head_v    = 128
0.00.048.873 I print_info: n_gqa            = 1
0.00.048.873 I print_info: n_embd_k_gqa     = 2048
0.00.048.879 I print_info: n_embd_v_gqa     = 2048
0.00.048.880 I print_info: f_norm_eps       = 1.0e-05
0.00.048.880 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.881 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.881 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.881 I print_info: f_logit_scale    = 0.0e+00
0.00.048.882 I print_info: n_ff             = 8192
0.00.048.883 I print_info: n_expert         = 0
0.00.048.883 I print_info: n_expert_used    = 0
0.00.048.883 I print_info: causal attn      = 1
0.00.048.883 I print_info: pooling type     = 0
0.00.048.883 I print_info: rope type        = 2
0.00.048.887 I print_info: rope scaling     = linear
0.00.048.887 I print_info: freq_base_train  = 10000.0
0.00.048.888 I print_info: freq_scale_train = 1
0.00.048.888 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.888 I print_info: rope_finetuned   = unknown
0.00.048.888 I print_info: ssm_d_conv       = 0
0.00.048.888 I print_info: ssm_d_inner      = 0
0.00.048.888 I print_info: ssm_d_state      = 0
0.00.048.888 I print_info: ssm_dt_rank      = 0
0.00.048.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.890 I print_info: model type       = 1.4B
0.00.048.890 I print_info: model params     = 1.41 B
0.00.048.891 I print_info: general.name     = 1.4B
0.00.048.891 I print_info: vocab type       = BPE
0.00.048.891 I print_info: n_vocab          = 50304
0.00.048.891 I print_info: n_merges         = 50009
0.00.048.892 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.892 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.892 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.892 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.893 I print_info: LF token         = 128 'Ä'
0.00.048.893 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.894 I print_info: max token length = 1024
0.00.050.834 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.834 I load_tensors: offloading output layer to GPU
0.00.050.835 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.846 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.847 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.051.137 I llama_init_from_model: n_seq_max     = 1
0.00.051.138 I llama_init_from_model: n_ctx         = 128
0.00.051.138 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.138 I llama_init_from_model: n_batch       = 128
0.00.051.138 I llama_init_from_model: n_ubatch      = 128
0.00.051.139 I llama_init_from_model: flash_attn    = 0
0.00.051.139 I llama_init_from_model: freq_base     = 10000.0
0.00.051.139 I llama_init_from_model: freq_scale    = 1
0.00.051.139 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.140 I ggml_metal_init: allocating
0.00.051.143 I ggml_metal_init: found device: Apple M4
0.00.051.144 I ggml_metal_init: picking default device: Apple M4
0.00.051.739 I ggml_metal_init: using embedded metal library
0.00.054.056 I ggml_metal_init: GPU name:   Apple M4
0.00.054.057 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.058 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.058 I ggml_metal_init: simdgroup reduction   = true
0.00.054.058 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.058 I ggml_metal_init: has bfloat            = true
0.00.054.058 I ggml_metal_init: use bfloat            = true
0.00.054.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.652 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.064.901 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.903 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.918 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.065.876 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.065.877 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.065.877 I llama_init_from_model: graph nodes  = 967
0.00.065.877 I llama_init_from_model: graph splits = 2
0.00.065.878 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.065.879 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.365 I 
0.00.676.402 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.412 I perplexity: tokenizing the input ..
0.00.684.444 I perplexity: tokenization took 8.031 ms
0.00.684.448 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.241 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.808.384 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.808.413 I llama_perf_context_print:        load time =     667.41 ms
0.00.808.414 I llama_perf_context_print: prompt eval time =     122.57 ms /   128 tokens (    0.96 ms per token,  1044.34 tokens per second)
0.00.808.415 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.415 I llama_perf_context_print:       total time =     132.05 ms /   129 tokens
0.00.808.925 I ggml_metal_free: deallocating

real	0m0.822s
user	0m0.077s
sys	0m0.121s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.626 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.059 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.064 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.066 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.068 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.072 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.072 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.073 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.073 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.073 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.077 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.838 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.833 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.584 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.585 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.586 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.586 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.586 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.587 I llama_model_loader: - type  f32:  194 tensors
0.00.026.587 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.588 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.588 I print_info: file format = GGUF V3 (latest)
0.00.026.589 I print_info: file type   = Q5_0
0.00.026.590 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.045.057 I load: special tokens cache size = 25
0.00.050.990 I load: token to piece cache size = 0.2984 MB
0.00.050.993 I print_info: arch             = gptneox
0.00.050.994 I print_info: vocab_only       = 0
0.00.050.994 I print_info: n_ctx_train      = 2048
0.00.050.994 I print_info: n_embd           = 2048
0.00.050.994 I print_info: n_layer          = 24
0.00.050.997 I print_info: n_head           = 16
0.00.050.998 I print_info: n_head_kv        = 16
0.00.050.998 I print_info: n_rot            = 32
0.00.050.998 I print_info: n_swa            = 0
0.00.050.999 I print_info: n_embd_head_k    = 128
0.00.050.999 I print_info: n_embd_head_v    = 128
0.00.051.000 I print_info: n_gqa            = 1
0.00.051.000 I print_info: n_embd_k_gqa     = 2048
0.00.051.001 I print_info: n_embd_v_gqa     = 2048
0.00.051.002 I print_info: f_norm_eps       = 1.0e-05
0.00.051.002 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.002 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.002 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.003 I print_info: f_logit_scale    = 0.0e+00
0.00.051.003 I print_info: n_ff             = 8192
0.00.051.003 I print_info: n_expert         = 0
0.00.051.004 I print_info: n_expert_used    = 0
0.00.051.005 I print_info: causal attn      = 1
0.00.051.007 I print_info: pooling type     = 0
0.00.051.007 I print_info: rope type        = 2
0.00.051.007 I print_info: rope scaling     = linear
0.00.051.008 I print_info: freq_base_train  = 10000.0
0.00.051.008 I print_info: freq_scale_train = 1
0.00.051.008 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.008 I print_info: rope_finetuned   = unknown
0.00.051.008 I print_info: ssm_d_conv       = 0
0.00.051.008 I print_info: ssm_d_inner      = 0
0.00.051.009 I print_info: ssm_d_state      = 0
0.00.051.009 I print_info: ssm_dt_rank      = 0
0.00.051.009 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.009 I print_info: model type       = 1.4B
0.00.051.009 I print_info: model params     = 1.41 B
0.00.051.010 I print_info: general.name     = 1.4B
0.00.051.010 I print_info: vocab type       = BPE
0.00.051.010 I print_info: n_vocab          = 50304
0.00.051.010 I print_info: n_merges         = 50009
0.00.051.011 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.011 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.011 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.011 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.011 I print_info: LF token         = 128 'Ä'
0.00.051.012 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.012 I print_info: max token length = 1024
0.00.052.972 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.972 I load_tensors: offloading output layer to GPU
0.00.052.972 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.983 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.984 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.272 I llama_init_from_model: n_seq_max     = 1
0.00.053.273 I llama_init_from_model: n_ctx         = 2048
0.00.053.273 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.273 I llama_init_from_model: n_batch       = 2048
0.00.053.273 I llama_init_from_model: n_ubatch      = 512
0.00.053.274 I llama_init_from_model: flash_attn    = 0
0.00.053.274 I llama_init_from_model: freq_base     = 10000.0
0.00.053.274 I llama_init_from_model: freq_scale    = 1
0.00.053.275 I ggml_metal_init: allocating
0.00.053.278 I ggml_metal_init: found device: Apple M4
0.00.053.280 I ggml_metal_init: picking default device: Apple M4
0.00.053.863 I ggml_metal_init: using embedded metal library
0.00.056.185 I ggml_metal_init: GPU name:   Apple M4
0.00.056.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.188 I ggml_metal_init: simdgroup reduction   = true
0.00.056.188 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.188 I ggml_metal_init: has bfloat            = true
0.00.056.188 I ggml_metal_init: use bfloat            = true
0.00.056.189 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.718 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.590 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.596 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.617 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.752 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.754 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.755 I llama_init_from_model: graph nodes  = 967
0.00.085.755 I llama_init_from_model: graph splits = 2
0.00.085.758 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.921 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.752.769 I main: llama threadpool init, n_threads = 4
0.00.752.818 I 
0.00.752.842 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.752.842 I 
0.00.753.083 I sampler seed: 1234
0.00.753.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.753.098 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.753.100 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.753.100 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.532.721 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.532.722 I llama_perf_context_print:        load time =     742.14 ms
0.01.532.722 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.18 tokens per second)
0.01.532.723 I llama_perf_context_print:        eval time =     733.31 ms /    63 runs   (   11.64 ms per token,    85.91 tokens per second)
0.01.532.723 I llama_perf_context_print:       total time =     779.96 ms /    70 tokens
0.01.532.929 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.108s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.827 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.473 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.478 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.479 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.480 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.480 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.481 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.482 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.484 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.485 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.485 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.486 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.487 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.488 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.489 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.282 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.998 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.998 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.999 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.999 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.999 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.000 I llama_model_loader: - type  f32:  194 tensors
0.00.024.000 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.000 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.001 I print_info: file format = GGUF V3 (latest)
0.00.024.001 I print_info: file type   = Q5_0
0.00.024.003 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.042.348 I load: special tokens cache size = 25
0.00.048.468 I load: token to piece cache size = 0.2984 MB
0.00.048.471 I print_info: arch             = gptneox
0.00.048.471 I print_info: vocab_only       = 0
0.00.048.471 I print_info: n_ctx_train      = 2048
0.00.048.471 I print_info: n_embd           = 2048
0.00.048.471 I print_info: n_layer          = 24
0.00.048.474 I print_info: n_head           = 16
0.00.048.475 I print_info: n_head_kv        = 16
0.00.048.475 I print_info: n_rot            = 32
0.00.048.475 I print_info: n_swa            = 0
0.00.048.476 I print_info: n_embd_head_k    = 128
0.00.048.478 I print_info: n_embd_head_v    = 128
0.00.048.478 I print_info: n_gqa            = 1
0.00.048.479 I print_info: n_embd_k_gqa     = 2048
0.00.048.480 I print_info: n_embd_v_gqa     = 2048
0.00.048.481 I print_info: f_norm_eps       = 1.0e-05
0.00.048.481 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.481 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.481 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.481 I print_info: f_logit_scale    = 0.0e+00
0.00.048.482 I print_info: n_ff             = 8192
0.00.048.482 I print_info: n_expert         = 0
0.00.048.482 I print_info: n_expert_used    = 0
0.00.048.483 I print_info: causal attn      = 1
0.00.048.483 I print_info: pooling type     = 0
0.00.048.483 I print_info: rope type        = 2
0.00.048.483 I print_info: rope scaling     = linear
0.00.048.488 I print_info: freq_base_train  = 10000.0
0.00.048.489 I print_info: freq_scale_train = 1
0.00.048.489 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.489 I print_info: rope_finetuned   = unknown
0.00.048.491 I print_info: ssm_d_conv       = 0
0.00.048.491 I print_info: ssm_d_inner      = 0
0.00.048.491 I print_info: ssm_d_state      = 0
0.00.048.491 I print_info: ssm_dt_rank      = 0
0.00.048.491 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.492 I print_info: model type       = 1.4B
0.00.048.492 I print_info: model params     = 1.41 B
0.00.048.492 I print_info: general.name     = 1.4B
0.00.048.493 I print_info: vocab type       = BPE
0.00.048.493 I print_info: n_vocab          = 50304
0.00.048.493 I print_info: n_merges         = 50009
0.00.048.493 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.493 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.493 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.494 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.495 I print_info: LF token         = 128 'Ä'
0.00.048.495 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.496 I print_info: max token length = 1024
0.00.050.445 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.445 I load_tensors: offloading output layer to GPU
0.00.050.446 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.457 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.050.458 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.050.769 I llama_init_from_model: n_seq_max     = 1
0.00.050.770 I llama_init_from_model: n_ctx         = 128
0.00.050.770 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.770 I llama_init_from_model: n_batch       = 128
0.00.050.770 I llama_init_from_model: n_ubatch      = 128
0.00.050.771 I llama_init_from_model: flash_attn    = 0
0.00.050.771 I llama_init_from_model: freq_base     = 10000.0
0.00.050.771 I llama_init_from_model: freq_scale    = 1
0.00.050.771 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.772 I ggml_metal_init: allocating
0.00.050.774 I ggml_metal_init: found device: Apple M4
0.00.050.776 I ggml_metal_init: picking default device: Apple M4
0.00.051.331 I ggml_metal_init: using embedded metal library
0.00.053.670 I ggml_metal_init: GPU name:   Apple M4
0.00.053.672 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.672 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.672 I ggml_metal_init: simdgroup reduction   = true
0.00.053.673 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.673 I ggml_metal_init: has bfloat            = true
0.00.053.673 I ggml_metal_init: use bfloat            = true
0.00.053.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.674 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.250 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.509 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.512 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.528 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.388 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.389 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.389 I llama_init_from_model: graph nodes  = 967
0.00.064.389 I llama_init_from_model: graph splits = 2
0.00.064.390 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.349 I 
0.00.684.389 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.393 I perplexity: tokenizing the input ..
0.00.692.435 I perplexity: tokenization took 8.04 ms
0.00.692.438 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.827.225 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.828.430 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.828.454 I llama_perf_context_print:        load time =     675.51 ms
0.00.828.455 I llama_perf_context_print: prompt eval time =     134.56 ms /   128 tokens (    1.05 ms per token,   951.26 tokens per second)
0.00.828.456 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.828.456 I llama_perf_context_print:       total time =     144.11 ms /   129 tokens
0.00.828.867 I ggml_metal_free: deallocating

real	0m0.842s
user	0m0.075s
sys	0m0.109s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.638 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.824 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.829 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.831 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.831 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.832 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.832 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.832 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.835 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.835 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.836 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.836 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.836 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.837 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.837 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.840 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.840 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.840 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.578 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.572 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.269 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.269 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.270 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.270 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.271 I llama_model_loader: - type  f32:  194 tensors
0.00.024.271 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.271 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.272 I print_info: file format = GGUF V3 (latest)
0.00.024.272 I print_info: file type   = Q5_1
0.00.024.273 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.042.787 I load: special tokens cache size = 25
0.00.048.754 I load: token to piece cache size = 0.2984 MB
0.00.048.757 I print_info: arch             = gptneox
0.00.048.757 I print_info: vocab_only       = 0
0.00.048.757 I print_info: n_ctx_train      = 2048
0.00.048.758 I print_info: n_embd           = 2048
0.00.048.758 I print_info: n_layer          = 24
0.00.048.760 I print_info: n_head           = 16
0.00.048.761 I print_info: n_head_kv        = 16
0.00.048.762 I print_info: n_rot            = 32
0.00.048.762 I print_info: n_swa            = 0
0.00.048.763 I print_info: n_embd_head_k    = 128
0.00.048.765 I print_info: n_embd_head_v    = 128
0.00.048.766 I print_info: n_gqa            = 1
0.00.048.766 I print_info: n_embd_k_gqa     = 2048
0.00.048.767 I print_info: n_embd_v_gqa     = 2048
0.00.048.767 I print_info: f_norm_eps       = 1.0e-05
0.00.048.768 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.768 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.768 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.768 I print_info: f_logit_scale    = 0.0e+00
0.00.048.769 I print_info: n_ff             = 8192
0.00.048.769 I print_info: n_expert         = 0
0.00.048.769 I print_info: n_expert_used    = 0
0.00.048.771 I print_info: causal attn      = 1
0.00.048.773 I print_info: pooling type     = 0
0.00.048.773 I print_info: rope type        = 2
0.00.048.773 I print_info: rope scaling     = linear
0.00.048.774 I print_info: freq_base_train  = 10000.0
0.00.048.774 I print_info: freq_scale_train = 1
0.00.048.774 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.774 I print_info: rope_finetuned   = unknown
0.00.048.775 I print_info: ssm_d_conv       = 0
0.00.048.775 I print_info: ssm_d_inner      = 0
0.00.048.775 I print_info: ssm_d_state      = 0
0.00.048.775 I print_info: ssm_dt_rank      = 0
0.00.048.780 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.780 I print_info: model type       = 1.4B
0.00.048.781 I print_info: model params     = 1.41 B
0.00.048.781 I print_info: general.name     = 1.4B
0.00.048.782 I print_info: vocab type       = BPE
0.00.048.782 I print_info: n_vocab          = 50304
0.00.048.782 I print_info: n_merges         = 50009
0.00.048.783 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.783 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.783 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.783 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.783 I print_info: LF token         = 128 'Ä'
0.00.048.784 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.784 I print_info: max token length = 1024
0.00.050.795 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.796 I load_tensors: offloading output layer to GPU
0.00.050.796 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.806 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.808 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.051.099 I llama_init_from_model: n_seq_max     = 1
0.00.051.100 I llama_init_from_model: n_ctx         = 2048
0.00.051.100 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.100 I llama_init_from_model: n_batch       = 2048
0.00.051.100 I llama_init_from_model: n_ubatch      = 512
0.00.051.100 I llama_init_from_model: flash_attn    = 0
0.00.051.101 I llama_init_from_model: freq_base     = 10000.0
0.00.051.101 I llama_init_from_model: freq_scale    = 1
0.00.051.102 I ggml_metal_init: allocating
0.00.051.105 I ggml_metal_init: found device: Apple M4
0.00.051.107 I ggml_metal_init: picking default device: Apple M4
0.00.051.721 I ggml_metal_init: using embedded metal library
0.00.054.044 I ggml_metal_init: GPU name:   Apple M4
0.00.054.046 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.046 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.046 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.047 I ggml_metal_init: simdgroup reduction   = true
0.00.054.047 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.047 I ggml_metal_init: has bfloat            = true
0.00.054.047 I ggml_metal_init: use bfloat            = true
0.00.054.048 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.549 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.191 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.198 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.218 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.084.239 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.084.240 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.084.240 I llama_init_from_model: graph nodes  = 967
0.00.084.241 I llama_init_from_model: graph splits = 2
0.00.084.244 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.084.375 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.084.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.448 I main: llama threadpool init, n_threads = 4
0.00.700.499 I 
0.00.700.536 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.537 I 
0.00.700.833 I sampler seed: 1234
0.00.700.837 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.867 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.870 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.870 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.536.263 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60322.85 tokens per second)
0.01.536.264 I llama_perf_context_print:        load time =     691.81 ms
0.01.536.264 I llama_perf_context_print: prompt eval time =      46.20 ms /     7 tokens (    6.60 ms per token,   151.50 tokens per second)
0.01.536.265 I llama_perf_context_print:        eval time =     786.26 ms /    63 runs   (   12.48 ms per token,    80.13 tokens per second)
0.01.536.269 I llama_perf_context_print:       total time =     835.82 ms /    70 tokens
0.01.536.491 I ggml_metal_free: deallocating

real	0m1.554s
user	0m0.108s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.613 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.366 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.371 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.373 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.373 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.374 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.374 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.374 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.375 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.376 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.376 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.376 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.377 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.377 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.377 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.381 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.167 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.940 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.942 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.943 I llama_model_loader: - type  f32:  194 tensors
0.00.025.943 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.943 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.944 I print_info: file format = GGUF V3 (latest)
0.00.025.944 I print_info: file type   = Q5_1
0.00.025.945 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.135 I load: special tokens cache size = 25
0.00.051.211 I load: token to piece cache size = 0.2984 MB
0.00.051.214 I print_info: arch             = gptneox
0.00.051.214 I print_info: vocab_only       = 0
0.00.051.214 I print_info: n_ctx_train      = 2048
0.00.051.214 I print_info: n_embd           = 2048
0.00.051.215 I print_info: n_layer          = 24
0.00.051.217 I print_info: n_head           = 16
0.00.051.218 I print_info: n_head_kv        = 16
0.00.051.218 I print_info: n_rot            = 32
0.00.051.218 I print_info: n_swa            = 0
0.00.051.218 I print_info: n_embd_head_k    = 128
0.00.051.219 I print_info: n_embd_head_v    = 128
0.00.051.219 I print_info: n_gqa            = 1
0.00.051.220 I print_info: n_embd_k_gqa     = 2048
0.00.051.221 I print_info: n_embd_v_gqa     = 2048
0.00.051.221 I print_info: f_norm_eps       = 1.0e-05
0.00.051.222 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.222 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.227 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.229 I print_info: f_logit_scale    = 0.0e+00
0.00.051.230 I print_info: n_ff             = 8192
0.00.051.230 I print_info: n_expert         = 0
0.00.051.230 I print_info: n_expert_used    = 0
0.00.051.230 I print_info: causal attn      = 1
0.00.051.230 I print_info: pooling type     = 0
0.00.051.231 I print_info: rope type        = 2
0.00.051.231 I print_info: rope scaling     = linear
0.00.051.231 I print_info: freq_base_train  = 10000.0
0.00.051.235 I print_info: freq_scale_train = 1
0.00.051.235 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.235 I print_info: rope_finetuned   = unknown
0.00.051.235 I print_info: ssm_d_conv       = 0
0.00.051.236 I print_info: ssm_d_inner      = 0
0.00.051.236 I print_info: ssm_d_state      = 0
0.00.051.237 I print_info: ssm_dt_rank      = 0
0.00.051.237 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.238 I print_info: model type       = 1.4B
0.00.051.238 I print_info: model params     = 1.41 B
0.00.051.238 I print_info: general.name     = 1.4B
0.00.051.239 I print_info: vocab type       = BPE
0.00.051.239 I print_info: n_vocab          = 50304
0.00.051.239 I print_info: n_merges         = 50009
0.00.051.239 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.239 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.243 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.245 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.245 I print_info: LF token         = 128 'Ä'
0.00.051.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.245 I print_info: max token length = 1024
0.00.053.211 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.211 I load_tensors: offloading output layer to GPU
0.00.053.212 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.222 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.223 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.053.496 I llama_init_from_model: n_seq_max     = 1
0.00.053.497 I llama_init_from_model: n_ctx         = 128
0.00.053.497 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.497 I llama_init_from_model: n_batch       = 128
0.00.053.497 I llama_init_from_model: n_ubatch      = 128
0.00.053.497 I llama_init_from_model: flash_attn    = 0
0.00.053.498 I llama_init_from_model: freq_base     = 10000.0
0.00.053.498 I llama_init_from_model: freq_scale    = 1
0.00.053.498 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.499 I ggml_metal_init: allocating
0.00.053.501 I ggml_metal_init: found device: Apple M4
0.00.053.503 I ggml_metal_init: picking default device: Apple M4
0.00.054.036 I ggml_metal_init: using embedded metal library
0.00.056.340 I ggml_metal_init: GPU name:   Apple M4
0.00.056.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.342 I ggml_metal_init: simdgroup reduction   = true
0.00.056.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.343 I ggml_metal_init: has bfloat            = true
0.00.056.343 I ggml_metal_init: use bfloat            = true
0.00.056.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.706 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.943 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.948 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.964 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.807 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.808 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.808 I llama_init_from_model: graph nodes  = 967
0.00.067.808 I llama_init_from_model: graph splits = 2
0.00.067.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.810 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.644.833 I 
0.00.644.867 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.644.874 I perplexity: tokenizing the input ..
0.00.652.709 I perplexity: tokenization took 7.833 ms
0.00.652.714 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.710 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.788.881 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.788.907 I llama_perf_context_print:        load time =     634.21 ms
0.00.788.908 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.76 tokens per second)
0.00.788.908 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.909 I llama_perf_context_print:       total time =     144.08 ms /   129 tokens
0.00.789.380 I ggml_metal_free: deallocating

real	0m0.804s
user	0m0.077s
sys	0m0.116s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.010.712 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.607 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.615 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.616 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.616 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.617 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.617 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.618 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.618 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.618 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.619 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.619 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.621 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.622 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.622 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.325 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.040 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.041 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.041 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.042 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.042 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.042 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.043 I llama_model_loader: - type  f32:  194 tensors
0.00.026.043 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.044 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.044 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.044 I print_info: file format = GGUF V3 (latest)
0.00.026.045 I print_info: file type   = Q2_K - Medium
0.00.026.046 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.510 I load: special tokens cache size = 25
0.00.051.476 I load: token to piece cache size = 0.2984 MB
0.00.051.478 I print_info: arch             = gptneox
0.00.051.479 I print_info: vocab_only       = 0
0.00.051.479 I print_info: n_ctx_train      = 2048
0.00.051.479 I print_info: n_embd           = 2048
0.00.051.479 I print_info: n_layer          = 24
0.00.051.482 I print_info: n_head           = 16
0.00.051.483 I print_info: n_head_kv        = 16
0.00.051.483 I print_info: n_rot            = 32
0.00.051.483 I print_info: n_swa            = 0
0.00.051.483 I print_info: n_embd_head_k    = 128
0.00.051.483 I print_info: n_embd_head_v    = 128
0.00.051.484 I print_info: n_gqa            = 1
0.00.051.485 I print_info: n_embd_k_gqa     = 2048
0.00.051.486 I print_info: n_embd_v_gqa     = 2048
0.00.051.486 I print_info: f_norm_eps       = 1.0e-05
0.00.051.486 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.487 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.487 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.487 I print_info: f_logit_scale    = 0.0e+00
0.00.051.488 I print_info: n_ff             = 8192
0.00.051.488 I print_info: n_expert         = 0
0.00.051.488 I print_info: n_expert_used    = 0
0.00.051.488 I print_info: causal attn      = 1
0.00.051.488 I print_info: pooling type     = 0
0.00.051.488 I print_info: rope type        = 2
0.00.051.488 I print_info: rope scaling     = linear
0.00.051.489 I print_info: freq_base_train  = 10000.0
0.00.051.489 I print_info: freq_scale_train = 1
0.00.051.491 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.491 I print_info: rope_finetuned   = unknown
0.00.051.491 I print_info: ssm_d_conv       = 0
0.00.051.492 I print_info: ssm_d_inner      = 0
0.00.051.492 I print_info: ssm_d_state      = 0
0.00.051.492 I print_info: ssm_dt_rank      = 0
0.00.051.492 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.492 I print_info: model type       = 1.4B
0.00.051.493 I print_info: model params     = 1.41 B
0.00.051.493 I print_info: general.name     = 1.4B
0.00.051.493 I print_info: vocab type       = BPE
0.00.051.495 I print_info: n_vocab          = 50304
0.00.051.495 I print_info: n_merges         = 50009
0.00.051.495 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.495 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.496 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.496 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.496 I print_info: LF token         = 128 'Ä'
0.00.051.496 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.497 I print_info: max token length = 1024
0.00.053.427 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.427 I load_tensors: offloading output layer to GPU
0.00.053.428 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.438 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.439 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.764 I llama_init_from_model: n_seq_max     = 1
0.00.053.765 I llama_init_from_model: n_ctx         = 2048
0.00.053.765 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.765 I llama_init_from_model: n_batch       = 2048
0.00.053.766 I llama_init_from_model: n_ubatch      = 512
0.00.053.766 I llama_init_from_model: flash_attn    = 0
0.00.053.766 I llama_init_from_model: freq_base     = 10000.0
0.00.053.766 I llama_init_from_model: freq_scale    = 1
0.00.053.767 I ggml_metal_init: allocating
0.00.053.770 I ggml_metal_init: found device: Apple M4
0.00.053.772 I ggml_metal_init: picking default device: Apple M4
0.00.054.395 I ggml_metal_init: using embedded metal library
0.00.056.763 I ggml_metal_init: GPU name:   Apple M4
0.00.056.764 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.765 I ggml_metal_init: simdgroup reduction   = true
0.00.056.765 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.765 I ggml_metal_init: has bfloat            = true
0.00.056.765 I ggml_metal_init: use bfloat            = true
0.00.056.766 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.766 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.697 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.457 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.463 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.480 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.614 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.616 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.616 I llama_init_from_model: graph nodes  = 967
0.00.088.617 I llama_init_from_model: graph splits = 2
0.00.088.620 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.755 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.444.495 I main: llama threadpool init, n_threads = 4
0.00.444.531 I 
0.00.444.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.444.551 I 
0.00.444.785 I sampler seed: 1234
0.00.444.790 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.802 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.803 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.803 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.113.735 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.113.736 I llama_perf_context_print:        load time =     433.78 ms
0.01.113.736 I llama_perf_context_print: prompt eval time =      35.86 ms /     7 tokens (    5.12 ms per token,   195.21 tokens per second)
0.01.113.737 I llama_perf_context_print:        eval time =     630.52 ms /    63 runs   (   10.01 ms per token,    99.92 tokens per second)
0.01.113.738 I llama_perf_context_print:       total time =     669.24 ms /    70 tokens
0.01.113.991 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.109s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.619 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.621 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.622 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.624 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.624 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.627 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.631 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.631 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.050 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.050 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.050 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.050 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.051 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.051 I llama_model_loader: - type  f32:  194 tensors
0.00.024.052 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.052 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.052 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.052 I print_info: file format = GGUF V3 (latest)
0.00.024.053 I print_info: file type   = Q2_K - Medium
0.00.024.053 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.272 I load: special tokens cache size = 25
0.00.049.262 I load: token to piece cache size = 0.2984 MB
0.00.049.265 I print_info: arch             = gptneox
0.00.049.265 I print_info: vocab_only       = 0
0.00.049.265 I print_info: n_ctx_train      = 2048
0.00.049.266 I print_info: n_embd           = 2048
0.00.049.266 I print_info: n_layer          = 24
0.00.049.268 I print_info: n_head           = 16
0.00.049.269 I print_info: n_head_kv        = 16
0.00.049.269 I print_info: n_rot            = 32
0.00.049.270 I print_info: n_swa            = 0
0.00.049.270 I print_info: n_embd_head_k    = 128
0.00.049.270 I print_info: n_embd_head_v    = 128
0.00.049.271 I print_info: n_gqa            = 1
0.00.049.271 I print_info: n_embd_k_gqa     = 2048
0.00.049.274 I print_info: n_embd_v_gqa     = 2048
0.00.049.275 I print_info: f_norm_eps       = 1.0e-05
0.00.049.275 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.275 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.275 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.276 I print_info: f_logit_scale    = 0.0e+00
0.00.049.276 I print_info: n_ff             = 8192
0.00.049.277 I print_info: n_expert         = 0
0.00.049.278 I print_info: n_expert_used    = 0
0.00.049.278 I print_info: causal attn      = 1
0.00.049.278 I print_info: pooling type     = 0
0.00.049.278 I print_info: rope type        = 2
0.00.049.278 I print_info: rope scaling     = linear
0.00.049.279 I print_info: freq_base_train  = 10000.0
0.00.049.279 I print_info: freq_scale_train = 1
0.00.049.280 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.280 I print_info: rope_finetuned   = unknown
0.00.049.280 I print_info: ssm_d_conv       = 0
0.00.049.280 I print_info: ssm_d_inner      = 0
0.00.049.284 I print_info: ssm_d_state      = 0
0.00.049.284 I print_info: ssm_dt_rank      = 0
0.00.049.285 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.285 I print_info: model type       = 1.4B
0.00.049.285 I print_info: model params     = 1.41 B
0.00.049.286 I print_info: general.name     = 1.4B
0.00.049.286 I print_info: vocab type       = BPE
0.00.049.287 I print_info: n_vocab          = 50304
0.00.049.287 I print_info: n_merges         = 50009
0.00.049.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.288 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.288 I print_info: LF token         = 128 'Ä'
0.00.049.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.289 I print_info: max token length = 1024
0.00.051.151 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.151 I load_tensors: offloading output layer to GPU
0.00.051.151 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.162 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.163 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.453 I llama_init_from_model: n_seq_max     = 1
0.00.051.454 I llama_init_from_model: n_ctx         = 128
0.00.051.454 I llama_init_from_model: n_ctx_per_seq = 128
0.00.051.454 I llama_init_from_model: n_batch       = 128
0.00.051.454 I llama_init_from_model: n_ubatch      = 128
0.00.051.455 I llama_init_from_model: flash_attn    = 0
0.00.051.455 I llama_init_from_model: freq_base     = 10000.0
0.00.051.455 I llama_init_from_model: freq_scale    = 1
0.00.051.456 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.456 I ggml_metal_init: allocating
0.00.051.459 I ggml_metal_init: found device: Apple M4
0.00.051.461 I ggml_metal_init: picking default device: Apple M4
0.00.052.031 I ggml_metal_init: using embedded metal library
0.00.054.381 I ggml_metal_init: GPU name:   Apple M4
0.00.054.383 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.383 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.384 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.384 I ggml_metal_init: simdgroup reduction   = true
0.00.054.384 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.384 I ggml_metal_init: has bfloat            = true
0.00.054.384 I ggml_metal_init: use bfloat            = true
0.00.054.385 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.385 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.900 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.148 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.152 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.169 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.073 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.074 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.074 I llama_init_from_model: graph nodes  = 967
0.00.066.075 I llama_init_from_model: graph splits = 2
0.00.066.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.076 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.374.497 I 
0.00.374.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.374.534 I perplexity: tokenizing the input ..
0.00.381.994 I perplexity: tokenization took 7.458 ms
0.00.381.997 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.513.999 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.515.262 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.515.300 I llama_perf_context_print:        load time =     365.62 ms
0.00.515.301 I llama_perf_context_print: prompt eval time =     131.78 ms /   128 tokens (    1.03 ms per token,   971.32 tokens per second)
0.00.515.302 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.515.302 I llama_perf_context_print:       total time =     140.81 ms /   129 tokens
0.00.515.733 I ggml_metal_free: deallocating

real	0m0.528s
user	0m0.077s
sys	0m0.063s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.266 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.947 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.952 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.954 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.954 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.955 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.955 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.959 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.959 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.960 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.960 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.960 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.961 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.961 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.964 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.964 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.964 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.715 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.721 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.448 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.450 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.450 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.451 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.451 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.451 I llama_model_loader: - type  f32:  194 tensors
0.00.025.452 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.452 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.452 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.452 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.453 I print_info: file format = GGUF V3 (latest)
0.00.025.454 I print_info: file type   = Q3_K - Medium
0.00.025.454 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.107 I load: special tokens cache size = 25
0.00.050.103 I load: token to piece cache size = 0.2984 MB
0.00.050.106 I print_info: arch             = gptneox
0.00.050.106 I print_info: vocab_only       = 0
0.00.050.106 I print_info: n_ctx_train      = 2048
0.00.050.107 I print_info: n_embd           = 2048
0.00.050.107 I print_info: n_layer          = 24
0.00.050.110 I print_info: n_head           = 16
0.00.050.111 I print_info: n_head_kv        = 16
0.00.050.111 I print_info: n_rot            = 32
0.00.050.111 I print_info: n_swa            = 0
0.00.050.111 I print_info: n_embd_head_k    = 128
0.00.050.113 I print_info: n_embd_head_v    = 128
0.00.050.114 I print_info: n_gqa            = 1
0.00.050.115 I print_info: n_embd_k_gqa     = 2048
0.00.050.115 I print_info: n_embd_v_gqa     = 2048
0.00.050.116 I print_info: f_norm_eps       = 1.0e-05
0.00.050.116 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.116 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.116 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.117 I print_info: f_logit_scale    = 0.0e+00
0.00.050.117 I print_info: n_ff             = 8192
0.00.050.119 I print_info: n_expert         = 0
0.00.050.120 I print_info: n_expert_used    = 0
0.00.050.120 I print_info: causal attn      = 1
0.00.050.121 I print_info: pooling type     = 0
0.00.050.121 I print_info: rope type        = 2
0.00.050.121 I print_info: rope scaling     = linear
0.00.050.122 I print_info: freq_base_train  = 10000.0
0.00.050.127 I print_info: freq_scale_train = 1
0.00.050.128 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.128 I print_info: rope_finetuned   = unknown
0.00.050.128 I print_info: ssm_d_conv       = 0
0.00.050.129 I print_info: ssm_d_inner      = 0
0.00.050.129 I print_info: ssm_d_state      = 0
0.00.050.129 I print_info: ssm_dt_rank      = 0
0.00.050.129 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.129 I print_info: model type       = 1.4B
0.00.050.130 I print_info: model params     = 1.41 B
0.00.050.130 I print_info: general.name     = 1.4B
0.00.050.132 I print_info: vocab type       = BPE
0.00.050.132 I print_info: n_vocab          = 50304
0.00.050.132 I print_info: n_merges         = 50009
0.00.050.133 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.133 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.133 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.133 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.134 I print_info: LF token         = 128 'Ä'
0.00.050.134 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.134 I print_info: max token length = 1024
0.00.052.127 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.127 I load_tensors: offloading output layer to GPU
0.00.052.127 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.138 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.139 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.437 I llama_init_from_model: n_seq_max     = 1
0.00.052.438 I llama_init_from_model: n_ctx         = 2048
0.00.052.438 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.438 I llama_init_from_model: n_batch       = 2048
0.00.052.438 I llama_init_from_model: n_ubatch      = 512
0.00.052.438 I llama_init_from_model: flash_attn    = 0
0.00.052.439 I llama_init_from_model: freq_base     = 10000.0
0.00.052.439 I llama_init_from_model: freq_scale    = 1
0.00.052.439 I ggml_metal_init: allocating
0.00.052.442 I ggml_metal_init: found device: Apple M4
0.00.052.444 I ggml_metal_init: picking default device: Apple M4
0.00.053.060 I ggml_metal_init: using embedded metal library
0.00.055.449 I ggml_metal_init: GPU name:   Apple M4
0.00.055.450 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.451 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.451 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.451 I ggml_metal_init: simdgroup reduction   = true
0.00.055.452 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.452 I ggml_metal_init: has bfloat            = true
0.00.055.452 I ggml_metal_init: use bfloat            = true
0.00.055.452 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.453 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.828 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.197 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.210 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.237 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.171 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.172 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.173 I llama_init_from_model: graph nodes  = 967
0.00.085.173 I llama_init_from_model: graph splits = 2
0.00.085.177 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.525.343 I main: llama threadpool init, n_threads = 4
0.00.525.386 I 
0.00.525.411 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.525.411 I 
0.00.525.621 I sampler seed: 1234
0.00.525.625 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.525.666 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.525.667 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.525.667 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.272.758 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.01.272.759 I llama_perf_context_print:        load time =     516.07 ms
0.01.272.760 I llama_perf_context_print: prompt eval time =      44.48 ms /     7 tokens (    6.35 ms per token,   157.38 tokens per second)
0.01.272.761 I llama_perf_context_print:        eval time =     699.73 ms /    63 runs   (   11.11 ms per token,    90.03 tokens per second)
0.01.272.761 I llama_perf_context_print:       total time =     747.42 ms /    70 tokens
0.01.273.004 I ggml_metal_free: deallocating

real	0m1.289s
user	0m0.107s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.700 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.670 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.675 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.677 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.677 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.678 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.679 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.679 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.680 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.680 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.680 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.681 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.681 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.683 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.683 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.684 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.347 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.041 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.042 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.043 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.043 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.044 I llama_model_loader: - type  f32:  194 tensors
0.00.024.044 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.045 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.045 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.045 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.046 I print_info: file format = GGUF V3 (latest)
0.00.024.046 I print_info: file type   = Q3_K - Medium
0.00.024.048 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.042.437 I load: special tokens cache size = 25
0.00.048.480 I load: token to piece cache size = 0.2984 MB
0.00.048.482 I print_info: arch             = gptneox
0.00.048.482 I print_info: vocab_only       = 0
0.00.048.483 I print_info: n_ctx_train      = 2048
0.00.048.483 I print_info: n_embd           = 2048
0.00.048.483 I print_info: n_layer          = 24
0.00.048.485 I print_info: n_head           = 16
0.00.048.486 I print_info: n_head_kv        = 16
0.00.048.486 I print_info: n_rot            = 32
0.00.048.486 I print_info: n_swa            = 0
0.00.048.487 I print_info: n_embd_head_k    = 128
0.00.048.487 I print_info: n_embd_head_v    = 128
0.00.048.488 I print_info: n_gqa            = 1
0.00.048.488 I print_info: n_embd_k_gqa     = 2048
0.00.048.489 I print_info: n_embd_v_gqa     = 2048
0.00.048.490 I print_info: f_norm_eps       = 1.0e-05
0.00.048.493 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.493 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.493 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.494 I print_info: f_logit_scale    = 0.0e+00
0.00.048.494 I print_info: n_ff             = 8192
0.00.048.495 I print_info: n_expert         = 0
0.00.048.495 I print_info: n_expert_used    = 0
0.00.048.495 I print_info: causal attn      = 1
0.00.048.495 I print_info: pooling type     = 0
0.00.048.495 I print_info: rope type        = 2
0.00.048.496 I print_info: rope scaling     = linear
0.00.048.497 I print_info: freq_base_train  = 10000.0
0.00.048.498 I print_info: freq_scale_train = 1
0.00.048.498 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.498 I print_info: rope_finetuned   = unknown
0.00.048.498 I print_info: ssm_d_conv       = 0
0.00.048.498 I print_info: ssm_d_inner      = 0
0.00.048.498 I print_info: ssm_d_state      = 0
0.00.048.499 I print_info: ssm_dt_rank      = 0
0.00.048.499 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.499 I print_info: model type       = 1.4B
0.00.048.499 I print_info: model params     = 1.41 B
0.00.048.500 I print_info: general.name     = 1.4B
0.00.048.500 I print_info: vocab type       = BPE
0.00.048.501 I print_info: n_vocab          = 50304
0.00.048.501 I print_info: n_merges         = 50009
0.00.048.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.501 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.501 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.501 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.502 I print_info: LF token         = 128 'Ä'
0.00.048.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.502 I print_info: max token length = 1024
0.00.050.412 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.412 I load_tensors: offloading output layer to GPU
0.00.050.412 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.423 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.424 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.050.714 I llama_init_from_model: n_seq_max     = 1
0.00.050.715 I llama_init_from_model: n_ctx         = 128
0.00.050.715 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.715 I llama_init_from_model: n_batch       = 128
0.00.050.715 I llama_init_from_model: n_ubatch      = 128
0.00.050.715 I llama_init_from_model: flash_attn    = 0
0.00.050.716 I llama_init_from_model: freq_base     = 10000.0
0.00.050.716 I llama_init_from_model: freq_scale    = 1
0.00.050.716 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.717 I ggml_metal_init: allocating
0.00.050.720 I ggml_metal_init: found device: Apple M4
0.00.050.722 I ggml_metal_init: picking default device: Apple M4
0.00.051.295 I ggml_metal_init: using embedded metal library
0.00.053.622 I ggml_metal_init: GPU name:   Apple M4
0.00.053.623 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.623 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.624 I ggml_metal_init: simdgroup reduction   = true
0.00.053.624 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.624 I ggml_metal_init: has bfloat            = true
0.00.053.624 I ggml_metal_init: use bfloat            = true
0.00.053.625 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.354 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.604 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.608 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.624 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.544 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.545 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.545 I llama_init_from_model: graph nodes  = 967
0.00.064.545 I llama_init_from_model: graph splits = 2
0.00.064.546 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.546 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.818 I 
0.00.477.846 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.849 I perplexity: tokenizing the input ..
0.00.485.498 I perplexity: tokenization took 7.647 ms
0.00.485.502 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.617.770 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.618.973 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.619.005 I llama_perf_context_print:        load time =     469.11 ms
0.00.619.006 I llama_perf_context_print: prompt eval time =     132.04 ms /   128 tokens (    1.03 ms per token,   969.41 tokens per second)
0.00.619.007 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.619.010 I llama_perf_context_print:       total time =     141.19 ms /   129 tokens
0.00.619.492 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.075s
sys	0m0.089s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.008.860 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.430 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.434 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.434 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.434 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.435 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.435 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.436 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.438 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.322 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.144 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.145 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.145 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.146 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.146 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.147 I llama_model_loader: - type  f32:  194 tensors
0.00.025.147 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.147 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.147 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.148 I print_info: file format = GGUF V3 (latest)
0.00.025.148 I print_info: file type   = Q4_K - Medium
0.00.025.149 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.598 I load: special tokens cache size = 25
0.00.050.546 I load: token to piece cache size = 0.2984 MB
0.00.050.549 I print_info: arch             = gptneox
0.00.050.550 I print_info: vocab_only       = 0
0.00.050.550 I print_info: n_ctx_train      = 2048
0.00.050.550 I print_info: n_embd           = 2048
0.00.050.550 I print_info: n_layer          = 24
0.00.050.553 I print_info: n_head           = 16
0.00.050.554 I print_info: n_head_kv        = 16
0.00.050.554 I print_info: n_rot            = 32
0.00.050.555 I print_info: n_swa            = 0
0.00.050.555 I print_info: n_embd_head_k    = 128
0.00.050.555 I print_info: n_embd_head_v    = 128
0.00.050.556 I print_info: n_gqa            = 1
0.00.050.557 I print_info: n_embd_k_gqa     = 2048
0.00.050.557 I print_info: n_embd_v_gqa     = 2048
0.00.050.558 I print_info: f_norm_eps       = 1.0e-05
0.00.050.558 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.558 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.558 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.559 I print_info: f_logit_scale    = 0.0e+00
0.00.050.559 I print_info: n_ff             = 8192
0.00.050.560 I print_info: n_expert         = 0
0.00.050.560 I print_info: n_expert_used    = 0
0.00.050.561 I print_info: causal attn      = 1
0.00.050.561 I print_info: pooling type     = 0
0.00.050.563 I print_info: rope type        = 2
0.00.050.563 I print_info: rope scaling     = linear
0.00.050.563 I print_info: freq_base_train  = 10000.0
0.00.050.564 I print_info: freq_scale_train = 1
0.00.050.564 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.564 I print_info: rope_finetuned   = unknown
0.00.050.564 I print_info: ssm_d_conv       = 0
0.00.050.565 I print_info: ssm_d_inner      = 0
0.00.050.565 I print_info: ssm_d_state      = 0
0.00.050.565 I print_info: ssm_dt_rank      = 0
0.00.050.565 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.565 I print_info: model type       = 1.4B
0.00.050.566 I print_info: model params     = 1.41 B
0.00.050.566 I print_info: general.name     = 1.4B
0.00.050.566 I print_info: vocab type       = BPE
0.00.050.567 I print_info: n_vocab          = 50304
0.00.050.567 I print_info: n_merges         = 50009
0.00.050.567 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.567 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.568 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.568 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.568 I print_info: LF token         = 128 'Ä'
0.00.050.568 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.569 I print_info: max token length = 1024
0.00.052.624 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.624 I load_tensors: offloading output layer to GPU
0.00.052.625 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.635 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.636 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.958 I llama_init_from_model: n_seq_max     = 1
0.00.052.958 I llama_init_from_model: n_ctx         = 2048
0.00.052.958 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.959 I llama_init_from_model: n_batch       = 2048
0.00.052.959 I llama_init_from_model: n_ubatch      = 512
0.00.052.959 I llama_init_from_model: flash_attn    = 0
0.00.052.959 I llama_init_from_model: freq_base     = 10000.0
0.00.052.960 I llama_init_from_model: freq_scale    = 1
0.00.052.960 I ggml_metal_init: allocating
0.00.052.963 I ggml_metal_init: found device: Apple M4
0.00.052.965 I ggml_metal_init: picking default device: Apple M4
0.00.053.574 I ggml_metal_init: using embedded metal library
0.00.055.921 I ggml_metal_init: GPU name:   Apple M4
0.00.055.923 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.924 I ggml_metal_init: simdgroup reduction   = true
0.00.055.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.924 I ggml_metal_init: has bfloat            = true
0.00.055.924 I ggml_metal_init: use bfloat            = true
0.00.055.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.925 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.314 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.210 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.216 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.240 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.278 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.279 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.279 I llama_init_from_model: graph nodes  = 967
0.00.086.280 I llama_init_from_model: graph splits = 2
0.00.086.282 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.403 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.404 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.098 I main: llama threadpool init, n_threads = 4
0.00.610.143 I 
0.00.610.186 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.187 I 
0.00.610.452 I sampler seed: 1234
0.00.610.458 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.610.495 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.610.500 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.610.500 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.370.956 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.370.957 I llama_perf_context_print:        load time =     601.23 ms
0.01.370.957 I llama_perf_context_print: prompt eval time =      47.11 ms /     7 tokens (    6.73 ms per token,   148.58 tokens per second)
0.01.370.958 I llama_perf_context_print:        eval time =     710.28 ms /    63 runs   (   11.27 ms per token,    88.70 tokens per second)
0.01.370.958 I llama_perf_context_print:       total time =     760.86 ms /    70 tokens
0.01.371.173 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.116 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.141 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.149 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.151 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.152 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.152 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.153 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.153 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.157 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.157 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.158 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.161 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.161 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.161 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.851 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.492 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.493 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.494 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.495 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.495 I llama_model_loader: - type  f32:  194 tensors
0.00.025.495 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.496 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.496 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.496 I print_info: file format = GGUF V3 (latest)
0.00.025.497 I print_info: file type   = Q4_K - Medium
0.00.025.498 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.493 I load: special tokens cache size = 25
0.00.050.262 I load: token to piece cache size = 0.2984 MB
0.00.050.265 I print_info: arch             = gptneox
0.00.050.265 I print_info: vocab_only       = 0
0.00.050.265 I print_info: n_ctx_train      = 2048
0.00.050.265 I print_info: n_embd           = 2048
0.00.050.265 I print_info: n_layer          = 24
0.00.050.269 I print_info: n_head           = 16
0.00.050.269 I print_info: n_head_kv        = 16
0.00.050.270 I print_info: n_rot            = 32
0.00.050.270 I print_info: n_swa            = 0
0.00.050.270 I print_info: n_embd_head_k    = 128
0.00.050.270 I print_info: n_embd_head_v    = 128
0.00.050.271 I print_info: n_gqa            = 1
0.00.050.271 I print_info: n_embd_k_gqa     = 2048
0.00.050.272 I print_info: n_embd_v_gqa     = 2048
0.00.050.273 I print_info: f_norm_eps       = 1.0e-05
0.00.050.275 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.275 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.275 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.275 I print_info: f_logit_scale    = 0.0e+00
0.00.050.276 I print_info: n_ff             = 8192
0.00.050.276 I print_info: n_expert         = 0
0.00.050.276 I print_info: n_expert_used    = 0
0.00.050.277 I print_info: causal attn      = 1
0.00.050.277 I print_info: pooling type     = 0
0.00.050.277 I print_info: rope type        = 2
0.00.050.277 I print_info: rope scaling     = linear
0.00.050.278 I print_info: freq_base_train  = 10000.0
0.00.050.278 I print_info: freq_scale_train = 1
0.00.050.278 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.278 I print_info: rope_finetuned   = unknown
0.00.050.280 I print_info: ssm_d_conv       = 0
0.00.050.280 I print_info: ssm_d_inner      = 0
0.00.050.280 I print_info: ssm_d_state      = 0
0.00.050.281 I print_info: ssm_dt_rank      = 0
0.00.050.281 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.281 I print_info: model type       = 1.4B
0.00.050.281 I print_info: model params     = 1.41 B
0.00.050.282 I print_info: general.name     = 1.4B
0.00.050.282 I print_info: vocab type       = BPE
0.00.050.282 I print_info: n_vocab          = 50304
0.00.050.282 I print_info: n_merges         = 50009
0.00.050.283 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.283 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.286 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.287 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.287 I print_info: LF token         = 128 'Ä'
0.00.050.287 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.287 I print_info: max token length = 1024
0.00.052.279 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.279 I load_tensors: offloading output layer to GPU
0.00.052.279 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.290 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.291 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.575 I llama_init_from_model: n_seq_max     = 1
0.00.052.576 I llama_init_from_model: n_ctx         = 128
0.00.052.576 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.576 I llama_init_from_model: n_batch       = 128
0.00.052.576 I llama_init_from_model: n_ubatch      = 128
0.00.052.576 I llama_init_from_model: flash_attn    = 0
0.00.052.577 I llama_init_from_model: freq_base     = 10000.0
0.00.052.577 I llama_init_from_model: freq_scale    = 1
0.00.052.577 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.578 I ggml_metal_init: allocating
0.00.052.581 I ggml_metal_init: found device: Apple M4
0.00.052.587 I ggml_metal_init: picking default device: Apple M4
0.00.053.144 I ggml_metal_init: using embedded metal library
0.00.055.515 I ggml_metal_init: GPU name:   Apple M4
0.00.055.516 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.517 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.517 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.517 I ggml_metal_init: simdgroup reduction   = true
0.00.055.517 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.518 I ggml_metal_init: has bfloat            = true
0.00.055.518 I ggml_metal_init: use bfloat            = true
0.00.055.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.519 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.371 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.641 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.655 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.630 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.631 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.631 I llama_init_from_model: graph nodes  = 967
0.00.067.631 I llama_init_from_model: graph splits = 2
0.00.067.633 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.633 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.550.838 I 
0.00.550.869 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.550.875 I perplexity: tokenizing the input ..
0.00.559.143 I perplexity: tokenization took 8.267 ms
0.00.559.153 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.119 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.694.320 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.694.341 I llama_perf_context_print:        load time =     540.72 ms
0.00.694.342 I llama_perf_context_print: prompt eval time =     133.74 ms /   128 tokens (    1.04 ms per token,   957.08 tokens per second)
0.00.694.343 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.694.344 I llama_perf_context_print:       total time =     143.50 ms /   129 tokens
0.00.694.773 I ggml_metal_free: deallocating

real	0m0.709s
user	0m0.077s
sys	0m0.096s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.945 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.397 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.405 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.405 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.406 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.407 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.407 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.407 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.408 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.408 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.410 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.411 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.415 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.416 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.417 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.207 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.967 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.968 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.968 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.969 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.969 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.969 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.970 I llama_model_loader: - type  f32:  194 tensors
0.00.025.970 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.970 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.971 I print_info: file format = GGUF V3 (latest)
0.00.025.971 I print_info: file type   = Q5_K - Medium
0.00.025.972 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.415 I load: special tokens cache size = 25
0.00.051.379 I load: token to piece cache size = 0.2984 MB
0.00.051.381 I print_info: arch             = gptneox
0.00.051.381 I print_info: vocab_only       = 0
0.00.051.382 I print_info: n_ctx_train      = 2048
0.00.051.382 I print_info: n_embd           = 2048
0.00.051.382 I print_info: n_layer          = 24
0.00.051.385 I print_info: n_head           = 16
0.00.051.386 I print_info: n_head_kv        = 16
0.00.051.387 I print_info: n_rot            = 32
0.00.051.387 I print_info: n_swa            = 0
0.00.051.388 I print_info: n_embd_head_k    = 128
0.00.051.388 I print_info: n_embd_head_v    = 128
0.00.051.388 I print_info: n_gqa            = 1
0.00.051.389 I print_info: n_embd_k_gqa     = 2048
0.00.051.395 I print_info: n_embd_v_gqa     = 2048
0.00.051.395 I print_info: f_norm_eps       = 1.0e-05
0.00.051.396 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.396 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.396 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.396 I print_info: f_logit_scale    = 0.0e+00
0.00.051.397 I print_info: n_ff             = 8192
0.00.051.397 I print_info: n_expert         = 0
0.00.051.398 I print_info: n_expert_used    = 0
0.00.051.398 I print_info: causal attn      = 1
0.00.051.398 I print_info: pooling type     = 0
0.00.051.398 I print_info: rope type        = 2
0.00.051.398 I print_info: rope scaling     = linear
0.00.051.399 I print_info: freq_base_train  = 10000.0
0.00.051.399 I print_info: freq_scale_train = 1
0.00.051.399 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.400 I print_info: rope_finetuned   = unknown
0.00.051.401 I print_info: ssm_d_conv       = 0
0.00.051.401 I print_info: ssm_d_inner      = 0
0.00.051.401 I print_info: ssm_d_state      = 0
0.00.051.401 I print_info: ssm_dt_rank      = 0
0.00.051.402 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.402 I print_info: model type       = 1.4B
0.00.051.402 I print_info: model params     = 1.41 B
0.00.051.402 I print_info: general.name     = 1.4B
0.00.051.408 I print_info: vocab type       = BPE
0.00.051.410 I print_info: n_vocab          = 50304
0.00.051.410 I print_info: n_merges         = 50009
0.00.051.410 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.411 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.411 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.411 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.412 I print_info: LF token         = 128 'Ä'
0.00.051.412 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.412 I print_info: max token length = 1024
0.00.053.398 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.398 I load_tensors: offloading output layer to GPU
0.00.053.399 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.410 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.411 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.694 I llama_init_from_model: n_seq_max     = 1
0.00.053.695 I llama_init_from_model: n_ctx         = 2048
0.00.053.695 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.695 I llama_init_from_model: n_batch       = 2048
0.00.053.696 I llama_init_from_model: n_ubatch      = 512
0.00.053.696 I llama_init_from_model: flash_attn    = 0
0.00.053.696 I llama_init_from_model: freq_base     = 10000.0
0.00.053.696 I llama_init_from_model: freq_scale    = 1
0.00.053.697 I ggml_metal_init: allocating
0.00.053.699 I ggml_metal_init: found device: Apple M4
0.00.053.702 I ggml_metal_init: picking default device: Apple M4
0.00.054.290 I ggml_metal_init: using embedded metal library
0.00.056.613 I ggml_metal_init: GPU name:   Apple M4
0.00.056.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.615 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.615 I ggml_metal_init: simdgroup reduction   = true
0.00.056.615 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.615 I ggml_metal_init: has bfloat            = true
0.00.056.616 I ggml_metal_init: use bfloat            = true
0.00.056.616 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.032 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.453 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.460 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.479 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.543 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.544 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.544 I llama_init_from_model: graph nodes  = 967
0.00.086.545 I llama_init_from_model: graph splits = 2
0.00.086.548 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.676 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.677 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.048 I main: llama threadpool init, n_threads = 4
0.00.681.087 I 
0.00.681.113 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.114 I 
0.00.681.342 I sampler seed: 1234
0.00.681.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.358 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.358 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.358 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.534.587 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.534.588 I llama_perf_context_print:        load time =     671.10 ms
0.01.534.589 I llama_perf_context_print: prompt eval time =      55.52 ms /     7 tokens (    7.93 ms per token,   126.08 tokens per second)
0.01.534.589 I llama_perf_context_print:        eval time =     794.71 ms /    63 runs   (   12.61 ms per token,    79.27 tokens per second)
0.01.534.591 I llama_perf_context_print:       total time =     853.55 ms /    70 tokens
0.01.534.774 I ggml_metal_free: deallocating

real	0m1.552s
user	0m0.108s
sys	0m0.146s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.897 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.859 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.867 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.869 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.869 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.870 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.872 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.872 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.873 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.875 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.875 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.876 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.676 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.567 I llama_model_loader: - type  f32:  194 tensors
0.00.024.567 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.567 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.568 I print_info: file format = GGUF V3 (latest)
0.00.024.568 I print_info: file type   = Q5_K - Medium
0.00.024.569 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.680 I load: special tokens cache size = 25
0.00.049.694 I load: token to piece cache size = 0.2984 MB
0.00.049.696 I print_info: arch             = gptneox
0.00.049.697 I print_info: vocab_only       = 0
0.00.049.697 I print_info: n_ctx_train      = 2048
0.00.049.697 I print_info: n_embd           = 2048
0.00.049.697 I print_info: n_layer          = 24
0.00.049.700 I print_info: n_head           = 16
0.00.049.700 I print_info: n_head_kv        = 16
0.00.049.701 I print_info: n_rot            = 32
0.00.049.701 I print_info: n_swa            = 0
0.00.049.701 I print_info: n_embd_head_k    = 128
0.00.049.701 I print_info: n_embd_head_v    = 128
0.00.049.702 I print_info: n_gqa            = 1
0.00.049.703 I print_info: n_embd_k_gqa     = 2048
0.00.049.705 I print_info: n_embd_v_gqa     = 2048
0.00.049.706 I print_info: f_norm_eps       = 1.0e-05
0.00.049.706 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.708 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.708 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.708 I print_info: f_logit_scale    = 0.0e+00
0.00.049.709 I print_info: n_ff             = 8192
0.00.049.709 I print_info: n_expert         = 0
0.00.049.709 I print_info: n_expert_used    = 0
0.00.049.710 I print_info: causal attn      = 1
0.00.049.710 I print_info: pooling type     = 0
0.00.049.710 I print_info: rope type        = 2
0.00.049.710 I print_info: rope scaling     = linear
0.00.049.712 I print_info: freq_base_train  = 10000.0
0.00.049.713 I print_info: freq_scale_train = 1
0.00.049.713 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.714 I print_info: rope_finetuned   = unknown
0.00.049.714 I print_info: ssm_d_conv       = 0
0.00.049.714 I print_info: ssm_d_inner      = 0
0.00.049.714 I print_info: ssm_d_state      = 0
0.00.049.714 I print_info: ssm_dt_rank      = 0
0.00.049.714 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.715 I print_info: model type       = 1.4B
0.00.049.715 I print_info: model params     = 1.41 B
0.00.049.715 I print_info: general.name     = 1.4B
0.00.049.716 I print_info: vocab type       = BPE
0.00.049.716 I print_info: n_vocab          = 50304
0.00.049.716 I print_info: n_merges         = 50009
0.00.049.716 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.717 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.717 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.717 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.717 I print_info: LF token         = 128 'Ä'
0.00.049.717 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.718 I print_info: max token length = 1024
0.00.051.753 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.753 I load_tensors: offloading output layer to GPU
0.00.051.753 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.764 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.765 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.057 I llama_init_from_model: n_seq_max     = 1
0.00.052.058 I llama_init_from_model: n_ctx         = 128
0.00.052.058 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.058 I llama_init_from_model: n_batch       = 128
0.00.052.058 I llama_init_from_model: n_ubatch      = 128
0.00.052.058 I llama_init_from_model: flash_attn    = 0
0.00.052.059 I llama_init_from_model: freq_base     = 10000.0
0.00.052.059 I llama_init_from_model: freq_scale    = 1
0.00.052.059 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.060 I ggml_metal_init: allocating
0.00.052.062 I ggml_metal_init: found device: Apple M4
0.00.052.064 I ggml_metal_init: picking default device: Apple M4
0.00.052.648 I ggml_metal_init: using embedded metal library
0.00.055.009 I ggml_metal_init: GPU name:   Apple M4
0.00.055.011 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.011 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.012 I ggml_metal_init: simdgroup reduction   = true
0.00.055.012 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.012 I ggml_metal_init: has bfloat            = true
0.00.055.012 I ggml_metal_init: use bfloat            = true
0.00.055.013 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.013 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.549 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.799 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.802 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.818 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.066.666 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.066.667 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.066.667 I llama_init_from_model: graph nodes  = 967
0.00.066.668 I llama_init_from_model: graph splits = 2
0.00.066.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.669 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.942 I 
0.00.633.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.988 I perplexity: tokenizing the input ..
0.00.642.061 I perplexity: tokenization took 8.071 ms
0.00.642.064 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.922 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.784.080 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.784.116 I llama_perf_context_print:        load time =     625.04 ms
0.00.784.117 I llama_perf_context_print: prompt eval time =     140.63 ms /   128 tokens (    1.10 ms per token,   910.18 tokens per second)
0.00.784.118 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.784.118 I llama_perf_context_print:       total time =     150.18 ms /   129 tokens
0.00.784.574 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.078s
sys	0m0.109s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.056 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.727 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.292 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.297 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.298 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.299 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.299 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.300 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.300 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.301 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.301 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.301 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.302 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.302 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.303 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.303 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.964 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.964 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.629 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.631 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.631 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.631 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.632 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.632 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.632 I llama_model_loader: - type  f32:  194 tensors
0.00.023.633 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.633 I print_info: file format = GGUF V3 (latest)
0.00.023.634 I print_info: file type   = Q6_K
0.00.023.635 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.186 I load: special tokens cache size = 25
0.00.048.124 I load: token to piece cache size = 0.2984 MB
0.00.048.127 I print_info: arch             = gptneox
0.00.048.127 I print_info: vocab_only       = 0
0.00.048.128 I print_info: n_ctx_train      = 2048
0.00.048.128 I print_info: n_embd           = 2048
0.00.048.128 I print_info: n_layer          = 24
0.00.048.131 I print_info: n_head           = 16
0.00.048.132 I print_info: n_head_kv        = 16
0.00.048.132 I print_info: n_rot            = 32
0.00.048.132 I print_info: n_swa            = 0
0.00.048.133 I print_info: n_embd_head_k    = 128
0.00.048.133 I print_info: n_embd_head_v    = 128
0.00.048.133 I print_info: n_gqa            = 1
0.00.048.134 I print_info: n_embd_k_gqa     = 2048
0.00.048.135 I print_info: n_embd_v_gqa     = 2048
0.00.048.136 I print_info: f_norm_eps       = 1.0e-05
0.00.048.136 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.136 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.136 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.136 I print_info: f_logit_scale    = 0.0e+00
0.00.048.137 I print_info: n_ff             = 8192
0.00.048.137 I print_info: n_expert         = 0
0.00.048.137 I print_info: n_expert_used    = 0
0.00.048.138 I print_info: causal attn      = 1
0.00.048.138 I print_info: pooling type     = 0
0.00.048.138 I print_info: rope type        = 2
0.00.048.138 I print_info: rope scaling     = linear
0.00.048.138 I print_info: freq_base_train  = 10000.0
0.00.048.139 I print_info: freq_scale_train = 1
0.00.048.139 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.139 I print_info: rope_finetuned   = unknown
0.00.048.139 I print_info: ssm_d_conv       = 0
0.00.048.139 I print_info: ssm_d_inner      = 0
0.00.048.139 I print_info: ssm_d_state      = 0
0.00.048.140 I print_info: ssm_dt_rank      = 0
0.00.048.140 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.140 I print_info: model type       = 1.4B
0.00.048.140 I print_info: model params     = 1.41 B
0.00.048.140 I print_info: general.name     = 1.4B
0.00.048.141 I print_info: vocab type       = BPE
0.00.048.141 I print_info: n_vocab          = 50304
0.00.048.141 I print_info: n_merges         = 50009
0.00.048.142 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.142 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.142 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.143 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.143 I print_info: LF token         = 128 'Ä'
0.00.048.144 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.144 I print_info: max token length = 1024
0.00.050.141 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.141 I load_tensors: offloading output layer to GPU
0.00.050.141 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.152 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.153 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.429 I llama_init_from_model: n_seq_max     = 1
0.00.050.430 I llama_init_from_model: n_ctx         = 2048
0.00.050.430 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.050.430 I llama_init_from_model: n_batch       = 2048
0.00.050.430 I llama_init_from_model: n_ubatch      = 512
0.00.050.430 I llama_init_from_model: flash_attn    = 0
0.00.050.431 I llama_init_from_model: freq_base     = 10000.0
0.00.050.431 I llama_init_from_model: freq_scale    = 1
0.00.050.431 I ggml_metal_init: allocating
0.00.050.434 I ggml_metal_init: found device: Apple M4
0.00.050.436 I ggml_metal_init: picking default device: Apple M4
0.00.051.008 I ggml_metal_init: using embedded metal library
0.00.053.326 I ggml_metal_init: GPU name:   Apple M4
0.00.053.328 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.328 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.328 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.329 I ggml_metal_init: simdgroup reduction   = true
0.00.053.329 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.329 I ggml_metal_init: has bfloat            = true
0.00.053.329 I ggml_metal_init: use bfloat            = true
0.00.053.329 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.330 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.825 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.082.018 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.023 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.042 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.083.063 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.083.064 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.083.064 I llama_init_from_model: graph nodes  = 967
0.00.083.064 I llama_init_from_model: graph splits = 2
0.00.083.067 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.083.197 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.083.198 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.703 I main: llama threadpool init, n_threads = 4
0.00.731.743 I 
0.00.731.762 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.763 I 
0.00.731.922 I sampler seed: 1234
0.00.731.926 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.958 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.959 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.960 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.612.154 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.01.612.155 I llama_perf_context_print:        load time =     722.97 ms
0.01.612.156 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.57 tokens per second)
0.01.612.156 I llama_perf_context_print:        eval time =     822.87 ms /    63 runs   (   13.06 ms per token,    76.56 tokens per second)
0.01.612.156 I llama_perf_context_print:       total time =     880.45 ms /    70 tokens
0.01.612.408 I ggml_metal_free: deallocating

real	0m1.629s
user	0m0.108s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4477 (d00a80e8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.870 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.310 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.320 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.322 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.322 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.325 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.325 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.326 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.326 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.328 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.328 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.328 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.024 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.010 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.722 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.723 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.724 I llama_model_loader: - type  f32:  194 tensors
0.00.023.724 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.725 I print_info: file format = GGUF V3 (latest)
0.00.023.725 I print_info: file type   = Q6_K
0.00.023.726 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.042.045 I load: special tokens cache size = 25
0.00.048.093 I load: token to piece cache size = 0.2984 MB
0.00.048.096 I print_info: arch             = gptneox
0.00.048.096 I print_info: vocab_only       = 0
0.00.048.097 I print_info: n_ctx_train      = 2048
0.00.048.097 I print_info: n_embd           = 2048
0.00.048.097 I print_info: n_layer          = 24
0.00.048.100 I print_info: n_head           = 16
0.00.048.101 I print_info: n_head_kv        = 16
0.00.048.101 I print_info: n_rot            = 32
0.00.048.101 I print_info: n_swa            = 0
0.00.048.102 I print_info: n_embd_head_k    = 128
0.00.048.103 I print_info: n_embd_head_v    = 128
0.00.048.104 I print_info: n_gqa            = 1
0.00.048.105 I print_info: n_embd_k_gqa     = 2048
0.00.048.105 I print_info: n_embd_v_gqa     = 2048
0.00.048.106 I print_info: f_norm_eps       = 1.0e-05
0.00.048.106 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.106 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.107 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.107 I print_info: f_logit_scale    = 0.0e+00
0.00.048.108 I print_info: n_ff             = 8192
0.00.048.108 I print_info: n_expert         = 0
0.00.048.108 I print_info: n_expert_used    = 0
0.00.048.108 I print_info: causal attn      = 1
0.00.048.110 I print_info: pooling type     = 0
0.00.048.110 I print_info: rope type        = 2
0.00.048.111 I print_info: rope scaling     = linear
0.00.048.111 I print_info: freq_base_train  = 10000.0
0.00.048.111 I print_info: freq_scale_train = 1
0.00.048.112 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.112 I print_info: rope_finetuned   = unknown
0.00.048.112 I print_info: ssm_d_conv       = 0
0.00.048.112 I print_info: ssm_d_inner      = 0
0.00.048.112 I print_info: ssm_d_state      = 0
0.00.048.112 I print_info: ssm_dt_rank      = 0
0.00.048.113 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.113 I print_info: model type       = 1.4B
0.00.048.114 I print_info: model params     = 1.41 B
0.00.048.115 I print_info: general.name     = 1.4B
0.00.048.116 I print_info: vocab type       = BPE
0.00.048.116 I print_info: n_vocab          = 50304
0.00.048.116 I print_info: n_merges         = 50009
0.00.048.116 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.118 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.118 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.118 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.118 I print_info: LF token         = 128 'Ä'
0.00.048.119 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.119 I print_info: max token length = 1024
0.00.050.131 I load_tensors: offloading 24 repeating layers to GPU
0.00.050.132 I load_tensors: offloading output layer to GPU
0.00.050.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.050.143 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.050.144 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.050.424 I llama_init_from_model: n_seq_max     = 1
0.00.050.424 I llama_init_from_model: n_ctx         = 128
0.00.050.425 I llama_init_from_model: n_ctx_per_seq = 128
0.00.050.425 I llama_init_from_model: n_batch       = 128
0.00.050.425 I llama_init_from_model: n_ubatch      = 128
0.00.050.425 I llama_init_from_model: flash_attn    = 0
0.00.050.425 I llama_init_from_model: freq_base     = 10000.0
0.00.050.426 I llama_init_from_model: freq_scale    = 1
0.00.050.426 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.050.426 I ggml_metal_init: allocating
0.00.050.429 I ggml_metal_init: found device: Apple M4
0.00.050.431 I ggml_metal_init: picking default device: Apple M4
0.00.050.997 I ggml_metal_init: using embedded metal library
0.00.053.328 I ggml_metal_init: GPU name:   Apple M4
0.00.053.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.329 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.330 I ggml_metal_init: simdgroup reduction   = true
0.00.053.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.330 I ggml_metal_init: has bfloat            = true
0.00.053.330 I ggml_metal_init: use bfloat            = true
0.00.053.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.129 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.063.345 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.347 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.362 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.064.261 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.064.262 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.064.262 I llama_init_from_model: graph nodes  = 967
0.00.064.263 I llama_init_from_model: graph splits = 2
0.00.064.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.064.264 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.583.680 I 
0.00.583.709 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.712 I perplexity: tokenizing the input ..
0.00.591.350 I perplexity: tokenization took 7.637 ms
0.00.591.359 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.820 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.733.088 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.733.121 I llama_perf_context_print:        load time =     574.81 ms
0.00.733.123 I llama_perf_context_print: prompt eval time =     140.24 ms /   128 tokens (    1.10 ms per token,   912.75 tokens per second)
0.00.733.124 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.125 I llama_perf_context_print:       total time =     149.44 ms /   129 tokens
0.00.733.585 I ggml_metal_free: deallocating

real	0m0.747s
user	0m0.075s
sys	0m0.103s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4477 (d00a80e8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e6076c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e607dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e608380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e608930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e608ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e609490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e609a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e609ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e60a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e60aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e60afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e60b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e60bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e60c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e60cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e60d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e60ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e60e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e60ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e60f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e60faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e610210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e6111d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e6118f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e6121c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e612e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e613370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e613630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e613ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e613d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e614620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e614b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e614e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e6152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e615760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e615c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e6160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e616540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e6169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e616e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e617320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e6177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e617a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e618090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e6186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e618fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e6195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e619be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e61a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e61a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e61ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e61b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e61bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e61c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e61c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e61c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e61ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e61d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e61d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e61dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e61e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e61e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e61eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e61eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e61f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e61f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e61fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e620270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e620710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e620bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e621050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e6215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e621af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e622040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e622590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e622ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e623030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e623580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e623ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e624020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e624570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e624ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e625010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e625560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e625ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e626000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e626550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e626aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e626ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e627540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e627a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e627fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e628530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e628a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e628fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e618cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e629440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e629bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e62a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e62a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e62abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e62b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e62b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e62bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e62c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e62c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e62cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e62d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e62d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e62dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e62e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e62e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e62ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e62eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e62f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e62f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e62fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e630160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e630600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ce04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ce044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ce05160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ce05420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ce056e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ce05b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ce05fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ce06430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ce068a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ce06d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ce07180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ce075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ce07a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ce07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ce08340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ce087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ce08c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ce09090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ce09500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ce09970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ce09de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ce0a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ce0a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ce0ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ce0afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ce0b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ce0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ce0bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ce0c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ce0c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ce0ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ce0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ce0d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ce0d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ce0dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ce0e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ce0e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ce0e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ce0edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ce0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ce0f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ce0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ce0ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ce103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ce10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ce10cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ce11140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ce115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ce11a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ce11e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ce12300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ce12770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ce12be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ce13050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ce134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ce13930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ce13da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ce14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ce14680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ce14af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ce14f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ce153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ce15840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ce15cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ce16120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ce16590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ce16a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ce16e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ce172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ce17750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ce17bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ce18030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ce184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ce18910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ce18d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ce191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ce19660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ce19ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ce19f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ce1a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ce1a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ce1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ce1b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ce1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ce1bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ce1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ce1c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ce1c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ce1cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ce1d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ce1dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ce1e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ce1e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ce1ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ce1f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ce1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ce1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ce20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ce209f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ce20fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ce21570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ce21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ce220f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ce226b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ce22c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ce23230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ce237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ce23db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ce24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ce24930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ce24ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ce254b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ce25a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ce26030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ce265f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ce26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ce27170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ce27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ce27cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ce282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e630b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e6310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e6315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e631b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e632090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e6325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e632b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e633080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e6335d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e633b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e634070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e6345c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e634b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e635060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e6355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e635b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e6365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e636af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e637040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e637590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e637ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e638030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e638580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e638ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e639020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e6394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e639960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e639e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e63a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e63a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e63abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e63b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e63b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e63b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e63be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e63c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e63c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e63cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e63d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e63d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e63dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e63e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e63e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e63f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e63f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e63fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e640200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e6404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e640ad0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.113.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.113.372 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ce268b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ce234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ce20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ce21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ce1eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ce24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ce251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ce27430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ce21df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ce22f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ce24bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ce25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ce27fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ce22970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ce23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ce1fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ce1ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ce262f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ce26e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ce20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ce25770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ce28c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ce291b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ce296f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ce29c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ce2a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ce2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ce2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ce2a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ce2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ce2af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ce2b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ce223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ce2b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ce2b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ce2ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ce2bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ce2bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ce2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ce2c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ce2c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ce2cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ce2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ce2d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ce2d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ce2d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ce2d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ce2db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ce2ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ce2e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ce2e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ce2e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ce2e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ce2ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ce2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ce2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ce2f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ce2f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ce2f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ce2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ce2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ce301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ce30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ce30730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ce309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ce30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ce30f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ce31230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ce314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ce317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ce31a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ce31d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ce31ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ce32540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ce32800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ce32ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ce32d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ce33040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ce33300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ce335c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ce33880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ce33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ce33e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ce34350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ce348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ce34df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ce35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ce35890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ce35de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ce36330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ce36880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ce36dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ce37320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ce37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ce37dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ce38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ce38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ce38db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ce39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ce39850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ce39da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ce3a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ce3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ce3ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ce3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ce3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ce3bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ce3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ce3c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ce3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ce3d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ce3d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ce3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ce3e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ce3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ce3e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ce3eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ce3efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ce3f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ce3f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ce3fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ce40180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ce405f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ce40a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ce40ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ce41340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ce417b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ce41c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ce42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ce42500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ce42970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ce42de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ce43250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ce436c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ce43b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ce43fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ce44410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ce44880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ce44cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ce45160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ce455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ce45a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ce45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ce46320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ce46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ce46c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ce47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ce474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ce47950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ce47dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ce48230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ce486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ce48b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ce48f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ce493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ce49860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ce49cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ce4a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ce4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ce4aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ce4ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ce4b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ce4b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ce4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ce4c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ce4c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ce4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ce4d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ce4d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ce4db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ce4e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ce4e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ce4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ce4ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ce4f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ce4f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ce4fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ce50310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ce50810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ce50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ce51210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ce51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ce51c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ce52110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ce52610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ce52b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ce53010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ce53510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ce53a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ce53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ce54410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ce54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ce54e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ce55310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ce558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ce55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ce56420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ce569d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ce56fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ce575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ce57c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ce583f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ce58890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ce58b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ce59160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ce59770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ce59f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ce5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ce5a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ce5ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ce5b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ce5ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ce5bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ce5c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ce5ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ce5cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ce5d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ce5da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ce5df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ce5e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ce5ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ce5ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ce5f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ce5fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ce5ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ce604a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ce609f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ce60f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ce61490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ce619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ce61f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ce62480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ce629d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ce62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ce63470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ce639c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ce63f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ce64460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ce649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ce64f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ce65450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ce659a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ce65ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ce66440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ce66990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ce66ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ce67430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ce67980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ce67ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ce68420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ce68970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ce68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ce69410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ce69960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ce69eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10ce6a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10ce6a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10ce6aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10ce6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10ce6b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10ce6be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10ce6c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10ce6c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10ce6ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10ce6d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10ce6d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10ce6de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10ce6e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10ce6e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10ce6ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10ce6f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10ce6f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10ce6fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10ce6fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10ce70370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10ce70810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10ce70cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10ce71150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10ce715f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10ce71a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10ce71f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10ce723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10ce72920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10ce73040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10ce73760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10ce73e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10ce745a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10ce74860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10ce75050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10ce75310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ce75920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ce55b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ce555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ce566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ce279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ce4c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ce58e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ce572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ce755d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ce56c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ce578b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ce59420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ce74b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ce57ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ce76090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ce766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ce76980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ce76c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ce76f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ce771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ce77480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ce77740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ce77a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ce77cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ce77f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ce78240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ce78500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ce787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ce78a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ce78d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ce79000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ce792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ce79580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ce79840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ce79b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ce79dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ce7a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ce7a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ce7a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ce7a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ce7ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ce7ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ce7b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ce7b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ce7b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ce7b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ce7bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ce7bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ce7c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ce7c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ce7c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ce7c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ce7cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ce7cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ce7d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ce7d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ce7d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10ce7da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10ce7dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10ce7dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10ce7e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10ce7e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10ce7e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10ce7eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10ce7ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10ce7f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10ce7f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10ce7f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10ce7f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10ce7fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10ce7fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10ce800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10ce80380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10ce80640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10ce80900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10ce80bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10ce80e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10ce81140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10ce81400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10ce816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10ce81980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10ce81c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10ce81f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10ce821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10ce82480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10ce82740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10ce82a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10ce82cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10ce82f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10ce83240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10ce83500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10ce837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10ce83a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10ce83d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10ce84000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10ce842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10ce84580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10ce84840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10ce84b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10ce84dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10ce85080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10ce85340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10ce85600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10ce858c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10ce85b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10ce85e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10ce86100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10ce863c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10ce86680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10ce86940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10ce86c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10ce86ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10ce87180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10ce87440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10ce87700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10ce879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10ce87c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10ce87f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10ce88200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10ce884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10ce88780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10ce88a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10ce88d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10ce88fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10ce89280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10ce89540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10ce89800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10ce89ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10ce89d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10ce8a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10ce8a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10ce8a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10ce8a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10ce8ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10ce8ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10ce8b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10ce8b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10ce8b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10ce8b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10ce8bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10ce8be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10ce8c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10ce8c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10ce8c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10ce8c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10ce8cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10ce8cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10ce8d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10ce8d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10ce8d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10ce8da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10ce8dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10ce8df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10ce8e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10ce8e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10ce8e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10ce8ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10ce8ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10ce8f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10ce8f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10ce8f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10ce8f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10ce8fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10ce8fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10ce90080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10ce90340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10ce90600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10ce908c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10ce90b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10ce90e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10ce91100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10ce913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10ce91680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10ce91940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10ce91c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10ce91ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10ce92180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10ce92440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10ce92700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10ce929c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10ce92c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10ce92f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10ce93200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10ce934c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10ce93780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10ce93a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10ce93d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10ce93fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10ce94280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10ce94540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10ce94800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10ce94ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10ce94d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10ce95040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10ce95300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10ce955c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10ce95880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10ce95b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10ce95e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10ce960c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10ce96380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10ce96640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10ce96900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10ce96bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10ce96e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10ce97140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10ce97400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10ce976c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10ce97980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10ce97c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10ce97f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10ce984d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10ce98790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10ce98a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10ce98d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10ce98fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10ce99290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10ce99550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10ce99810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10ce99ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10ce99d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10ce9a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10ce9a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10ce9a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10ce9a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10ce9ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10ce9ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10ce9b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10ce9b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10ce9b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10ce9b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10ce9bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10ce9be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10ce9c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10ce9c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10ce9c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10ce9c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10ce9cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10ce9cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10ce9d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10ce9d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10ce9d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10ce9da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10ce9dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10ce9df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10ce9e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10ce9e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10ce9e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10ce9ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10ce9ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10ce9f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10ce9f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10ce9f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10ce9f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10ce9fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10ce9fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10cea0090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10cea0350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10cea0610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10cea08d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10cea0b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10cea0e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10cea13a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10cea18f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10cea1e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10cea2390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10cea28e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10cea2e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10cea30f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10cea33b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10cea3670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10cea3ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10cea3f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10cea43c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10cea4830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10cea4ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10cea5110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10cea5580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10cea59f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10cea5e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10cea62d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10cea6740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10cea6bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10cea7020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10cea7490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10cea8180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10cea88a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10cea8fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10cea9280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10cea96f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10cea9cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10ceaa300 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.980s
user	0m0.271s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4477 (d00a80e8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125107590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125107ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125108250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125108800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125108db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125109360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x125109910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125109ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12510a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12510a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12510ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12510b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12510be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12510c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12510ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12510d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12510dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12510e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12510ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12510f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12510f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1251100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x125110800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1251110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1251117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x125111a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x125112090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x125112d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x125113240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x125113500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1251139a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x125113c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1251144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125114a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125114cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125115190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125115630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125115ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125115f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x125116410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1251168b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x125116d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1251171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x125117690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125117950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125117f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125118570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125118e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1251194a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125119ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12511a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12511a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12511ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12511b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12511bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12511bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12511c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12511c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12511ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12511d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12511d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12511dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12511e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12511e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12511ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12511eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12511f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12511f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12511fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x125120140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1251205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x125120a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x125120f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x125121470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1251219c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x125121f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x125122460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1251229b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x125122f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x125123450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1251239a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x125123ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x125124440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x125124990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x125124ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x125125430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x125125980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x125125ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x125126420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125126970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x125126ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125127410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125127960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125127eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125128400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125128950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125128ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x125118b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125129310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125129ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12512a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12512a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12512aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12512b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12512b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12512baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12512bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12512c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12512ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12512cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12512d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12512da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12512dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12512e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12512e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12512edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12512f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12512f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12512fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125130030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1251304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125130970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x125130e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1251312b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125131750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x125131bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x125132090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x125132530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1251329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x125132e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x125133310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1251337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x125133c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1251340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x125134590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x125134a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x125134ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x125135370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x125135810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x125135cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x125136150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1251365f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x125136a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x125136f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1251373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x125137870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x125137d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1251381b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125138650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125138af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125138f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125139430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1251398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125139d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12513a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12513a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12513ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12513aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12513b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12513b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12513bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12513c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12513c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12513cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12513d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12513d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12513d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12513de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12513e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12513e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12513ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12513f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12513f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12513f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12513fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125140330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1251407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x125140c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125141110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1251415b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125141a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125141ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125142390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x125142830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x125142cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x125143170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x125143610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x125143ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x125143f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1251443f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x125144890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x125144d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1251451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x125145720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x125145c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1251461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x125146710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1251469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x125146fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1251475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125147c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1251483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x125148890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x125148b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x125149160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125149770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125149f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12514a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12514a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12514ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12514b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12514ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12514bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12514c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12514ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12514cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12514d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12514da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12514df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12514e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12514ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12514ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12514f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12514fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12514ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1251504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1251509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125150f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125151490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1251519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125151f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125152480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1251529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125152f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125153470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1251539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125153f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125154460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1251549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125154f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x125155450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1251559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125155ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125156440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x125156990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125156ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125157430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125157980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125157ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125158420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x125158970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x125158ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x125159410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x125159960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x125159eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12515a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12515a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12515aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12515b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12515b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12515be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12515c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12515c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12515ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12515d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12515d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12515de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12515e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12515e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12515ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12515f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12515f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12515fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12515fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x125160370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125160810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x125160cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125161150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1251615f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125161a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125161f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1251623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125162920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x125163040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125163760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125163e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1251645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125164860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125165050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125165310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125165920 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.087.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1236088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123608d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1236091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12360b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12360bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12360c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12360c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12360c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12360cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12360d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12360d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12360dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12360e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12360f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12360f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12360ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1236106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123610dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1236114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123611cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1236123e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123612b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123613220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123613940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123614060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123614320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1236145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123614a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123614ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123615330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1236157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123615cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123616140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123616400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123616870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123616ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123617150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1236175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123617a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123617ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123618310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123618780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123618bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1236194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123619940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123619db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12361a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12361a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12361ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12361af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12361b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12361b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12361bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12361c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12361c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12361cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12361d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12361d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12361d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12361dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12361e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12361e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12361eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12361ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12361f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12361f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12361fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1236200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123620550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1236209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123620e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1236212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123621710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123621b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123621ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1236228d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123622d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1236231b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123623620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123623a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123623f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123624370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1236247e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123624c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1236250c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123625530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236259a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123625e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123626280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1236266f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123626b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123626fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123627440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1236278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123627d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123628190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123628600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123628a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123628ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1236297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123629c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12362a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12362a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12362a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12362adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12362b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12362b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12362bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12362bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12362c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12362c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12362cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12362d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12362d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12362da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12362dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12362e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12362e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12362ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12362f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12362f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12362f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12362fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123630240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1236306b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123630b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123630f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123631400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123631870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123631ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123632150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1236325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123632a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123632ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123633310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123633780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123633bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123634060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1236344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123634940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123634db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123635220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123635690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123635b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123635f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1236363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123636850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123636cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123637130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1236375a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123637a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123637e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1236382f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123638760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123638bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123639040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1236394b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123639920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123639d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12363a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12363a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12363aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12363af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12363bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12363be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12363c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12363c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12363c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12363ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12363d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12363d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12363dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12363e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12363e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12363e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12363ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12363f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12363f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12363fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12363ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123640390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123640800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123640c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1236410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123641550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1236419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123641e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1236422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123642710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123642b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123642ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123643460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1236438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123643d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x125148e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1251472a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1251655d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x125146c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1251478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12511a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12511a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12511c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125149420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125111d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125118830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125119150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x125119760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125117c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12511afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x125119d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125110d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12510b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12511cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1251295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125164b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125113f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1251141e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125149a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125147ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125112350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125112610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1251128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125165d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x125166040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125166300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1251665c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125166880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x125166b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125166e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1251670c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x125167380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125167640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125167900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125167bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x125167e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125168140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125168400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1251686c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x125168980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x125168c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x125168f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1251691c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x125169480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x125169740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x125169a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x125169cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x125169f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12516a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12516a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12516a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12516aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12516ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12516b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12516b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12516b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12516b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12516bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12516bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12516c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12516c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12516c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12516c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12516cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12516ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12516d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12516d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12516d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12516d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12516dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12516dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12516e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12516e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12516e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12516e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12516ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12516ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12516f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12516f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12516f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12516fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12516fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12516ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125170280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125170540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125170800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125170ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125170d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125171040 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1237083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123708840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123708cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123709120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123709590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123709a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123709e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12370a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12370a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12370abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12370b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12370b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12370c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12370ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12370d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12370d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12370e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12370e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12370eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12370f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12370fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123710430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123710b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123711270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123711990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123711c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123711f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123712380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1237127f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123712c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123713160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123713670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123713ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123713da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123714210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123714680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123714be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1237150e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1237155e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123715ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123715fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1237164e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1237169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123716ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1237173e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123717850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123717cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123718130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1237185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123718a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123718e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1237192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123719760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123719bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12371a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12371a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12371acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12371af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12371b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12371bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12371c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12371c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12371cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12371cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12371d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12371d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12371ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12371e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12371e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12371ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12371f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12371f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12371f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12371fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123720430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123720980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123720ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123721420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123721970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123721ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123722410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123722960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123722eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123723400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123723950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123723ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1237243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123724940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123724e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1237253e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123725930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123725e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1237263d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123726920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123726e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1237273c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123727910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123727e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1237283b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123728900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123728e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1237293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1237298f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123729e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12372a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12372a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12372ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12372b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12372b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12372be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12372c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12372c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12372ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12372d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12372d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12372dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12372e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12372e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12372e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12372ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12372f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12372f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12372fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1237300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123730590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123730a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123730ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123731370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123731810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123731cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123732150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1237325f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123732a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123732f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1237333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123733870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123733d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1237341b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123734650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123734af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123734f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123735430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1237358d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123735d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123736210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1237366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123736b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123736ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123737490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123737dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123738270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123738710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123738bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123739050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1237394f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123739990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123739e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12373a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12373a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12373ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12373b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12373b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12373b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12373be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12373c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12373c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12373cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12373d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12373d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12373da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12373def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12373e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12373e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12373ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12373f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12373f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12373fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12373ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1237403f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123740890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123740d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1237411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123741670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123741b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123741fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123742450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1237428f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123742d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123743230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1237436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123743b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123744010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123744560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123744ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123745000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123745550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123745810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123745e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123746430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123746a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123747230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1237476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123747990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123747fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1237485b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123748da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123749240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1237496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123749b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12374a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12374a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12374add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12374b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12374b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12374bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12374c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12374c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12374cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12374d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12374d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12374dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12374e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12374e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12374ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12374f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12374f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12374fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1237502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123750820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123750d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1237512c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123751810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123751d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1237522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123752800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123752d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1237532a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1237537f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123753d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123754290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1237547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123754d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123755280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1237557d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123755d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123756270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1237567c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123756d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123757260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1237577b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123757d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123758250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1237587a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123758cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123759240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123759790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123759ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12375a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12375a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12375acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12375b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12375b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12375bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12375c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12375c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12375ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12375d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12375d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12375da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12375df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12375e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12375e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12375ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12375f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12375f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12375faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12375ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123760430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1237608d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123760d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123761210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123761760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123761e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1237625a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123762cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1237633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1237636a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123763e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123764150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123764760 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.905s
user	0m0.243s
sys	0m0.135s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.53 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.55 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.08 sec*proc (2 tests)

Total Test time (real) =   1.09 sec
        1.11 real         0.69 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.25 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.27 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.16 user         0.04 sys
```
