Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.511s
user	0m0.954s
sys	0m1.181s
++ nproc
+ make -j10
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Built target sha1
[  5%] Built target build_info
[  5%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target xxhash
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 19%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 31%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking C executable ../bin/test-c
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 35%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Built target test-c
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 51%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 57%] Linking CXX executable ../bin/test-chat-template
[ 58%] Linking CXX executable ../bin/test-gguf
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../bin/test-quantize-perf
[ 64%] Built target test-backend-ops
[ 64%] Built target test-chat-template
[ 64%] Built target test-gguf
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-autorelease
[ 64%] Built target test-barrier
[ 64%] Built target test-model-load-cancel
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target llama-batched-bench
[ 65%] Built target test-quantize-perf
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Built target test-rope
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-eval-callback
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-imatrix
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Built target llama-infill
[ 77%] Built target llama-lookahead
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-bench
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 78%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-merge
[ 83%] Built target llama-lookup
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-parallel
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-passkey
[ 83%] Built target llama-cli
[ 84%] Generating index.html.gz.hpp
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-perplexity
[ 85%] Built target llama-quantize
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Built target llama-retrieval
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-run
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Built target llama-save-load-state
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tts
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 95%] Built target llama-gen-docs
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-cvector-generator
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.062s
user	0m6.449s
sys	0m10.827s

main: quantize time =  2878.23 ms
main:    total time =  2878.23 ms

main: quantize time =  1675.95 ms
main:    total time =  1675.95 ms

main: quantize time =  2396.84 ms
main:    total time =  2396.84 ms

main: quantize time =  1472.89 ms
main:    total time =  1472.89 ms

main: quantize time =  1750.08 ms
main:    total time =  1750.08 ms

main: quantize time =  5707.33 ms
main:    total time =  5707.33 ms

main: quantize time =  5464.30 ms
main:    total time =  5464.30 ms

main: quantize time =  6426.69 ms
main:    total time =  6426.69 ms

main: quantize time =  5544.79 ms
main:    total time =  5544.79 ms

main: quantize time =  4282.16 ms
main:    total time =  4282.16 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.173 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.330 I main: llama backend init
0.00.000.335 I main: load the model and apply lora adapter, if any
0.00.056.902 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.075.642 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.075.651 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.075.654 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.075.655 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.075.656 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.075.657 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.075.657 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.075.660 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.075.660 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.075.661 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.075.662 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.075.662 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.075.663 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.075.666 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.075.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.075.671 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.075.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.084.450 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.086.623 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.094.682 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.094.686 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.094.686 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.094.687 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.094.687 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.094.689 I llama_model_loader: - type  f32:  194 tensors
0.00.094.689 I llama_model_loader: - type  f16:   98 tensors
0.00.094.691 I print_info: file format = GGUF V3 (latest)
0.00.094.692 I print_info: file type   = all F32 (guessed)
0.00.094.693 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.109.229 I load: special tokens cache size = 25
0.00.117.437 I load: token to piece cache size = 0.2984 MB
0.00.117.441 I print_info: arch             = gptneox
0.00.117.441 I print_info: vocab_only       = 0
0.00.117.441 I print_info: n_ctx_train      = 2048
0.00.117.441 I print_info: n_embd           = 2048
0.00.117.442 I print_info: n_layer          = 24
0.00.117.445 I print_info: n_head           = 16
0.00.117.445 I print_info: n_head_kv        = 16
0.00.117.447 I print_info: n_rot            = 32
0.00.117.447 I print_info: n_swa            = 0
0.00.117.447 I print_info: n_embd_head_k    = 128
0.00.117.447 I print_info: n_embd_head_v    = 128
0.00.117.448 I print_info: n_gqa            = 1
0.00.117.449 I print_info: n_embd_k_gqa     = 2048
0.00.117.450 I print_info: n_embd_v_gqa     = 2048
0.00.117.451 I print_info: f_norm_eps       = 1.0e-05
0.00.117.451 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.117.452 I print_info: f_clamp_kqv      = 0.0e+00
0.00.117.452 I print_info: f_max_alibi_bias = 0.0e+00
0.00.117.452 I print_info: f_logit_scale    = 0.0e+00
0.00.117.453 I print_info: n_ff             = 8192
0.00.117.453 I print_info: n_expert         = 0
0.00.117.453 I print_info: n_expert_used    = 0
0.00.117.453 I print_info: causal attn      = 1
0.00.117.453 I print_info: pooling type     = 0
0.00.117.453 I print_info: rope type        = 2
0.00.117.454 I print_info: rope scaling     = linear
0.00.117.454 I print_info: freq_base_train  = 10000.0
0.00.117.454 I print_info: freq_scale_train = 1
0.00.117.454 I print_info: n_ctx_orig_yarn  = 2048
0.00.117.455 I print_info: rope_finetuned   = unknown
0.00.117.457 I print_info: ssm_d_conv       = 0
0.00.117.457 I print_info: ssm_d_inner      = 0
0.00.117.457 I print_info: ssm_d_state      = 0
0.00.117.457 I print_info: ssm_dt_rank      = 0
0.00.117.457 I print_info: ssm_dt_b_c_rms   = 0
0.00.117.458 I print_info: model type       = 1.4B
0.00.117.458 I print_info: model params     = 1.41 B
0.00.117.458 I print_info: general.name     = 1.4B
0.00.117.459 I print_info: vocab type       = BPE
0.00.117.459 I print_info: n_vocab          = 50304
0.00.117.460 I print_info: n_merges         = 50009
0.00.117.461 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.117.462 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.117.462 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.117.462 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.117.462 I print_info: LF token         = 128 'Ä'
0.00.117.462 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.117.463 I print_info: max token length = 1024
0.00.149.513 I load_tensors: offloading 24 repeating layers to GPU
0.00.149.516 I load_tensors: offloading output layer to GPU
0.00.149.517 I load_tensors: offloaded 25/25 layers to GPU
0.00.149.538 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.149.540 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.149.841 I llama_init_from_model: n_seq_max     = 1
0.00.149.842 I llama_init_from_model: n_ctx         = 2048
0.00.149.842 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.149.842 I llama_init_from_model: n_batch       = 2048
0.00.149.843 I llama_init_from_model: n_ubatch      = 512
0.00.149.843 I llama_init_from_model: flash_attn    = 0
0.00.149.843 I llama_init_from_model: freq_base     = 10000.0
0.00.149.843 I llama_init_from_model: freq_scale    = 1
0.00.149.844 I ggml_metal_init: allocating
0.00.149.863 I ggml_metal_init: found device: Apple M4
0.00.149.868 I ggml_metal_init: picking default device: Apple M4
0.00.150.424 I ggml_metal_init: using embedded metal library
0.00.159.415 I ggml_metal_init: GPU name:   Apple M4
0.00.159.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.159.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.159.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.159.418 I ggml_metal_init: simdgroup reduction   = true
0.00.159.418 I ggml_metal_init: simdgroup matrix mul. = true
0.00.159.418 I ggml_metal_init: has residency sets    = true
0.00.159.418 I ggml_metal_init: has bfloat            = true
0.00.159.418 I ggml_metal_init: use bfloat            = true
0.00.159.419 I ggml_metal_init: hasUnifiedMemory      = true
0.00.159.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.182.921 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.210.778 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.210.783 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.210.807 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.214.427 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.214.429 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.214.430 I llama_init_from_model: graph nodes  = 967
0.00.214.430 I llama_init_from_model: graph splits = 2
0.00.214.434 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.214.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.214.563 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.273.636 I main: llama threadpool init, n_threads = 4
0.00.273.685 I 
0.00.273.714 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.273.714 I 
0.00.273.754 I sampler seed: 1234
0.00.273.758 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.273.782 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.273.783 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.273.783 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.074.737 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58774.83 tokens per second)
0.02.074.738 I llama_perf_context_print:        load time =     215.71 ms
0.02.074.739 I llama_perf_context_print: prompt eval time =      43.54 ms /     7 tokens (    6.22 ms per token,   160.77 tokens per second)
0.02.074.739 I llama_perf_context_print:        eval time =    1754.57 ms /    63 runs   (   27.85 ms per token,    35.91 tokens per second)
0.02.074.740 I llama_perf_context_print:       total time =    1802.11 ms /    70 tokens
0.02.074.954 I ggml_metal_free: deallocating

real	0m2.385s
user	0m0.140s
sys	0m0.123s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.593 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.753 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.761 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.764 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.764 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.764 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.766 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.766 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.767 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.654 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.723 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.550 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.550 I llama_model_loader: - type  f32:  194 tensors
0.00.033.550 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.551 I print_info: file format = GGUF V3 (latest)
0.00.033.552 I print_info: file type   = Q8_0
0.00.033.553 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.686 I load: special tokens cache size = 25
0.00.047.624 I load: token to piece cache size = 0.2984 MB
0.00.047.629 I print_info: arch             = gptneox
0.00.047.630 I print_info: vocab_only       = 0
0.00.047.630 I print_info: n_ctx_train      = 2048
0.00.047.630 I print_info: n_embd           = 2048
0.00.047.630 I print_info: n_layer          = 24
0.00.047.636 I print_info: n_head           = 16
0.00.047.637 I print_info: n_head_kv        = 16
0.00.047.637 I print_info: n_rot            = 32
0.00.047.637 I print_info: n_swa            = 0
0.00.047.637 I print_info: n_embd_head_k    = 128
0.00.047.638 I print_info: n_embd_head_v    = 128
0.00.047.638 I print_info: n_gqa            = 1
0.00.047.639 I print_info: n_embd_k_gqa     = 2048
0.00.047.640 I print_info: n_embd_v_gqa     = 2048
0.00.047.640 I print_info: f_norm_eps       = 1.0e-05
0.00.047.641 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.641 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.641 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.641 I print_info: f_logit_scale    = 0.0e+00
0.00.047.642 I print_info: n_ff             = 8192
0.00.047.642 I print_info: n_expert         = 0
0.00.047.642 I print_info: n_expert_used    = 0
0.00.047.642 I print_info: causal attn      = 1
0.00.047.642 I print_info: pooling type     = 0
0.00.047.643 I print_info: rope type        = 2
0.00.047.643 I print_info: rope scaling     = linear
0.00.047.643 I print_info: freq_base_train  = 10000.0
0.00.047.644 I print_info: freq_scale_train = 1
0.00.047.644 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.649 I print_info: rope_finetuned   = unknown
0.00.047.649 I print_info: ssm_d_conv       = 0
0.00.047.649 I print_info: ssm_d_inner      = 0
0.00.047.649 I print_info: ssm_d_state      = 0
0.00.047.649 I print_info: ssm_dt_rank      = 0
0.00.047.649 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.650 I print_info: model type       = 1.4B
0.00.047.650 I print_info: model params     = 1.41 B
0.00.047.650 I print_info: general.name     = 1.4B
0.00.047.651 I print_info: vocab type       = BPE
0.00.047.651 I print_info: n_vocab          = 50304
0.00.047.651 I print_info: n_merges         = 50009
0.00.047.651 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.652 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.652 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.652 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.652 I print_info: LF token         = 128 'Ä'
0.00.047.652 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.653 I print_info: max token length = 1024
0.01.355.317 I load_tensors: offloading 24 repeating layers to GPU
0.01.355.324 I load_tensors: offloading output layer to GPU
0.01.355.324 I load_tensors: offloaded 25/25 layers to GPU
0.01.355.352 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.355.355 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.356.001 I llama_init_from_model: n_seq_max     = 1
0.01.356.004 I llama_init_from_model: n_ctx         = 2048
0.01.356.005 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.356.005 I llama_init_from_model: n_batch       = 2048
0.01.356.006 I llama_init_from_model: n_ubatch      = 512
0.01.356.006 I llama_init_from_model: flash_attn    = 0
0.01.356.007 I llama_init_from_model: freq_base     = 10000.0
0.01.356.007 I llama_init_from_model: freq_scale    = 1
0.01.356.009 I ggml_metal_init: allocating
0.01.356.058 I ggml_metal_init: found device: Apple M4
0.01.356.068 I ggml_metal_init: picking default device: Apple M4
0.01.357.258 I ggml_metal_init: using embedded metal library
0.01.362.298 I ggml_metal_init: GPU name:   Apple M4
0.01.362.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.362.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.362.303 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.362.303 I ggml_metal_init: simdgroup reduction   = true
0.01.362.304 I ggml_metal_init: simdgroup matrix mul. = true
0.01.362.304 I ggml_metal_init: has residency sets    = true
0.01.362.304 I ggml_metal_init: has bfloat            = true
0.01.362.304 I ggml_metal_init: use bfloat            = true
0.01.362.305 I ggml_metal_init: hasUnifiedMemory      = true
0.01.362.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.376.323 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.410.003 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.410.011 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.410.035 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.415.302 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.415.304 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.415.305 I llama_init_from_model: graph nodes  = 967
0.01.415.305 I llama_init_from_model: graph splits = 2
0.01.415.308 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.415.433 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.415.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.461.822 I main: llama threadpool init, n_threads = 4
0.01.461.874 I 
0.01.461.895 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.461.895 I 
0.01.462.012 I sampler seed: 1234
0.01.462.017 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.462.026 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.462.027 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.462.027 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.544.694 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55599.06 tokens per second)
0.02.544.695 I llama_perf_context_print:        load time =    1451.37 ms
0.02.544.696 I llama_perf_context_print: prompt eval time =      39.47 ms /     7 tokens (    5.64 ms per token,   177.36 tokens per second)
0.02.544.696 I llama_perf_context_print:        eval time =    1040.26 ms /    63 runs   (   16.51 ms per token,    60.56 tokens per second)
0.02.544.701 I llama_perf_context_print:       total time =    1083.73 ms /    70 tokens
0.02.544.973 I ggml_metal_free: deallocating

real	0m2.562s
user	0m0.104s
sys	0m0.313s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.019.369 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.037.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.813 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.814 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.818 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.819 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.820 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.820 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.821 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.821 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.821 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.822 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.825 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.825 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.826 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.155 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.275 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.047.543 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.047.545 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.047.545 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.047.545 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.047.546 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.047.546 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.047.547 I llama_model_loader: - type  f32:  194 tensors
0.00.047.547 I llama_model_loader: - type q4_0:   97 tensors
0.00.047.547 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.548 I print_info: file format = GGUF V3 (latest)
0.00.047.548 I print_info: file type   = Q4_0
0.00.047.549 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.057.410 I load: special tokens cache size = 25
0.00.065.656 I load: token to piece cache size = 0.2984 MB
0.00.065.659 I print_info: arch             = gptneox
0.00.065.660 I print_info: vocab_only       = 0
0.00.065.660 I print_info: n_ctx_train      = 2048
0.00.065.660 I print_info: n_embd           = 2048
0.00.065.660 I print_info: n_layer          = 24
0.00.065.665 I print_info: n_head           = 16
0.00.065.666 I print_info: n_head_kv        = 16
0.00.065.666 I print_info: n_rot            = 32
0.00.065.667 I print_info: n_swa            = 0
0.00.065.667 I print_info: n_embd_head_k    = 128
0.00.065.667 I print_info: n_embd_head_v    = 128
0.00.065.668 I print_info: n_gqa            = 1
0.00.065.669 I print_info: n_embd_k_gqa     = 2048
0.00.065.672 I print_info: n_embd_v_gqa     = 2048
0.00.065.672 I print_info: f_norm_eps       = 1.0e-05
0.00.065.673 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.065.673 I print_info: f_clamp_kqv      = 0.0e+00
0.00.065.675 I print_info: f_max_alibi_bias = 0.0e+00
0.00.065.675 I print_info: f_logit_scale    = 0.0e+00
0.00.065.676 I print_info: n_ff             = 8192
0.00.065.676 I print_info: n_expert         = 0
0.00.065.676 I print_info: n_expert_used    = 0
0.00.065.676 I print_info: causal attn      = 1
0.00.065.676 I print_info: pooling type     = 0
0.00.065.677 I print_info: rope type        = 2
0.00.065.677 I print_info: rope scaling     = linear
0.00.065.677 I print_info: freq_base_train  = 10000.0
0.00.065.679 I print_info: freq_scale_train = 1
0.00.065.679 I print_info: n_ctx_orig_yarn  = 2048
0.00.065.679 I print_info: rope_finetuned   = unknown
0.00.065.679 I print_info: ssm_d_conv       = 0
0.00.065.679 I print_info: ssm_d_inner      = 0
0.00.065.680 I print_info: ssm_d_state      = 0
0.00.065.680 I print_info: ssm_dt_rank      = 0
0.00.065.680 I print_info: ssm_dt_b_c_rms   = 0
0.00.065.680 I print_info: model type       = 1.4B
0.00.065.681 I print_info: model params     = 1.41 B
0.00.065.681 I print_info: general.name     = 1.4B
0.00.065.682 I print_info: vocab type       = BPE
0.00.065.686 I print_info: n_vocab          = 50304
0.00.065.687 I print_info: n_merges         = 50009
0.00.065.687 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.065.687 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.065.687 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.065.687 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.065.688 I print_info: LF token         = 128 'Ä'
0.00.065.688 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.065.689 I print_info: max token length = 1024
0.00.689.822 I load_tensors: offloading 24 repeating layers to GPU
0.00.689.836 I load_tensors: offloading output layer to GPU
0.00.689.836 I load_tensors: offloaded 25/25 layers to GPU
0.00.689.860 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.689.861 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.691.070 I llama_init_from_model: n_seq_max     = 1
0.00.691.077 I llama_init_from_model: n_ctx         = 2048
0.00.691.077 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.691.078 I llama_init_from_model: n_batch       = 2048
0.00.691.078 I llama_init_from_model: n_ubatch      = 512
0.00.691.078 I llama_init_from_model: flash_attn    = 0
0.00.691.080 I llama_init_from_model: freq_base     = 10000.0
0.00.691.081 I llama_init_from_model: freq_scale    = 1
0.00.691.083 I ggml_metal_init: allocating
0.00.691.149 I ggml_metal_init: found device: Apple M4
0.00.691.162 I ggml_metal_init: picking default device: Apple M4
0.00.692.818 I ggml_metal_init: using embedded metal library
0.00.699.203 I ggml_metal_init: GPU name:   Apple M4
0.00.699.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.699.207 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.699.208 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.699.209 I ggml_metal_init: simdgroup reduction   = true
0.00.699.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.699.209 I ggml_metal_init: has residency sets    = true
0.00.699.209 I ggml_metal_init: has bfloat            = true
0.00.699.210 I ggml_metal_init: use bfloat            = true
0.00.699.211 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.716.699 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.760.767 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.760.774 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.760.836 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.765.534 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.765.536 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.765.536 I llama_init_from_model: graph nodes  = 967
0.00.765.537 I llama_init_from_model: graph splits = 2
0.00.765.541 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.765.682 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.765.683 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.095 I main: llama threadpool init, n_threads = 4
0.00.811.132 I 
0.00.811.156 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.157 I 
0.00.811.297 I sampler seed: 1234
0.00.811.302 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.344 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.346 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.346 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.498.671 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48999.31 tokens per second)
0.01.498.672 I llama_perf_context_print:        load time =     790.78 ms
0.01.498.672 I llama_perf_context_print: prompt eval time =      49.39 ms /     7 tokens (    7.06 ms per token,   141.73 tokens per second)
0.01.498.673 I llama_perf_context_print:        eval time =     635.02 ms /    63 runs   (   10.08 ms per token,    99.21 tokens per second)
0.01.498.673 I llama_perf_context_print:       total time =     688.52 ms /    70 tokens
0.01.498.933 I ggml_metal_free: deallocating

real	0m1.520s
user	0m0.117s
sys	0m0.233s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.014.432 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.032.076 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.032.081 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.083 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.085 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.085 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.086 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.086 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.088 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.088 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.088 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.088 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.089 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.089 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.090 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.091 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.092 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.092 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.248 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.730 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.731 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.041.731 I llama_model_loader: - type  f32:  194 tensors
0.00.041.732 I llama_model_loader: - type q4_1:   97 tensors
0.00.041.732 I llama_model_loader: - type q6_K:    1 tensors
0.00.041.732 I print_info: file format = GGUF V3 (latest)
0.00.041.733 I print_info: file type   = Q4_1
0.00.041.734 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.051.366 I load: special tokens cache size = 25
0.00.059.161 I load: token to piece cache size = 0.2984 MB
0.00.059.164 I print_info: arch             = gptneox
0.00.059.164 I print_info: vocab_only       = 0
0.00.059.164 I print_info: n_ctx_train      = 2048
0.00.059.164 I print_info: n_embd           = 2048
0.00.059.164 I print_info: n_layer          = 24
0.00.059.167 I print_info: n_head           = 16
0.00.059.168 I print_info: n_head_kv        = 16
0.00.059.169 I print_info: n_rot            = 32
0.00.059.171 I print_info: n_swa            = 0
0.00.059.171 I print_info: n_embd_head_k    = 128
0.00.059.171 I print_info: n_embd_head_v    = 128
0.00.059.172 I print_info: n_gqa            = 1
0.00.059.172 I print_info: n_embd_k_gqa     = 2048
0.00.059.173 I print_info: n_embd_v_gqa     = 2048
0.00.059.178 I print_info: f_norm_eps       = 1.0e-05
0.00.059.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.179 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.181 I print_info: f_logit_scale    = 0.0e+00
0.00.059.182 I print_info: n_ff             = 8192
0.00.059.182 I print_info: n_expert         = 0
0.00.059.182 I print_info: n_expert_used    = 0
0.00.059.182 I print_info: causal attn      = 1
0.00.059.182 I print_info: pooling type     = 0
0.00.059.183 I print_info: rope type        = 2
0.00.059.184 I print_info: rope scaling     = linear
0.00.059.184 I print_info: freq_base_train  = 10000.0
0.00.059.186 I print_info: freq_scale_train = 1
0.00.059.186 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.186 I print_info: rope_finetuned   = unknown
0.00.059.186 I print_info: ssm_d_conv       = 0
0.00.059.186 I print_info: ssm_d_inner      = 0
0.00.059.186 I print_info: ssm_d_state      = 0
0.00.059.187 I print_info: ssm_dt_rank      = 0
0.00.059.187 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.187 I print_info: model type       = 1.4B
0.00.059.188 I print_info: model params     = 1.41 B
0.00.059.188 I print_info: general.name     = 1.4B
0.00.059.188 I print_info: vocab type       = BPE
0.00.059.188 I print_info: n_vocab          = 50304
0.00.059.189 I print_info: n_merges         = 50009
0.00.059.189 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.189 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.189 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.189 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.190 I print_info: LF token         = 128 'Ä'
0.00.059.190 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.190 I print_info: max token length = 1024
0.00.725.037 I load_tensors: offloading 24 repeating layers to GPU
0.00.725.053 I load_tensors: offloading output layer to GPU
0.00.725.054 I load_tensors: offloaded 25/25 layers to GPU
0.00.725.083 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.725.084 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.726.339 I llama_init_from_model: n_seq_max     = 1
0.00.726.352 I llama_init_from_model: n_ctx         = 2048
0.00.726.352 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.726.353 I llama_init_from_model: n_batch       = 2048
0.00.726.354 I llama_init_from_model: n_ubatch      = 512
0.00.726.354 I llama_init_from_model: flash_attn    = 0
0.00.726.356 I llama_init_from_model: freq_base     = 10000.0
0.00.726.357 I llama_init_from_model: freq_scale    = 1
0.00.726.366 I ggml_metal_init: allocating
0.00.726.430 I ggml_metal_init: found device: Apple M4
0.00.726.446 I ggml_metal_init: picking default device: Apple M4
0.00.728.404 I ggml_metal_init: using embedded metal library
0.00.734.931 I ggml_metal_init: GPU name:   Apple M4
0.00.734.937 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.734.938 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.734.939 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.734.939 I ggml_metal_init: simdgroup reduction   = true
0.00.734.940 I ggml_metal_init: simdgroup matrix mul. = true
0.00.734.940 I ggml_metal_init: has residency sets    = true
0.00.734.940 I ggml_metal_init: has bfloat            = true
0.00.734.941 I ggml_metal_init: use bfloat            = true
0.00.734.942 I ggml_metal_init: hasUnifiedMemory      = true
0.00.734.943 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.753.554 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.809.492 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.809.499 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.809.528 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.814.336 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.814.338 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.814.338 I llama_init_from_model: graph nodes  = 967
0.00.814.338 I llama_init_from_model: graph splits = 2
0.00.814.343 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.814.473 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.814.473 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.857.852 I main: llama threadpool init, n_threads = 4
0.00.857.893 I 
0.00.857.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.857.917 I 
0.00.858.036 I sampler seed: 1234
0.00.858.041 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.858.050 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.858.050 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.858.050 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.578.773 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.578.774 I llama_perf_context_print:        load time =     842.44 ms
0.01.578.774 I llama_perf_context_print: prompt eval time =      39.31 ms /     7 tokens (    5.62 ms per token,   178.06 tokens per second)
0.01.578.775 I llama_perf_context_print:        eval time =     678.74 ms /    63 runs   (   10.77 ms per token,    92.82 tokens per second)
0.01.578.775 I llama_perf_context_print:       total time =     721.90 ms /    70 tokens
0.01.579.046 I ggml_metal_free: deallocating

real	0m1.599s
user	0m0.116s
sys	0m0.236s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.314 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.321 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.324 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.325 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.327 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.327 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.327 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.328 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.329 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.331 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.332 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.207 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.036 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.038 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.038 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.038 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.039 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.039 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.039 I llama_model_loader: - type  f32:  194 tensors
0.00.028.040 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.040 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.041 I print_info: file format = GGUF V3 (latest)
0.00.028.041 I print_info: file type   = Q5_0
0.00.028.042 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.139 I load: special tokens cache size = 25
0.00.042.132 I load: token to piece cache size = 0.2984 MB
0.00.042.135 I print_info: arch             = gptneox
0.00.042.136 I print_info: vocab_only       = 0
0.00.042.136 I print_info: n_ctx_train      = 2048
0.00.042.136 I print_info: n_embd           = 2048
0.00.042.136 I print_info: n_layer          = 24
0.00.042.139 I print_info: n_head           = 16
0.00.042.140 I print_info: n_head_kv        = 16
0.00.042.140 I print_info: n_rot            = 32
0.00.042.140 I print_info: n_swa            = 0
0.00.042.140 I print_info: n_embd_head_k    = 128
0.00.042.140 I print_info: n_embd_head_v    = 128
0.00.042.141 I print_info: n_gqa            = 1
0.00.042.142 I print_info: n_embd_k_gqa     = 2048
0.00.042.142 I print_info: n_embd_v_gqa     = 2048
0.00.042.143 I print_info: f_norm_eps       = 1.0e-05
0.00.042.143 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.143 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.144 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.144 I print_info: f_logit_scale    = 0.0e+00
0.00.042.145 I print_info: n_ff             = 8192
0.00.042.145 I print_info: n_expert         = 0
0.00.042.145 I print_info: n_expert_used    = 0
0.00.042.145 I print_info: causal attn      = 1
0.00.042.145 I print_info: pooling type     = 0
0.00.042.146 I print_info: rope type        = 2
0.00.042.148 I print_info: rope scaling     = linear
0.00.042.148 I print_info: freq_base_train  = 10000.0
0.00.042.148 I print_info: freq_scale_train = 1
0.00.042.149 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.149 I print_info: rope_finetuned   = unknown
0.00.042.149 I print_info: ssm_d_conv       = 0
0.00.042.150 I print_info: ssm_d_inner      = 0
0.00.042.151 I print_info: ssm_d_state      = 0
0.00.042.151 I print_info: ssm_dt_rank      = 0
0.00.042.151 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.151 I print_info: model type       = 1.4B
0.00.042.151 I print_info: model params     = 1.41 B
0.00.042.152 I print_info: general.name     = 1.4B
0.00.042.152 I print_info: vocab type       = BPE
0.00.042.152 I print_info: n_vocab          = 50304
0.00.042.152 I print_info: n_merges         = 50009
0.00.042.153 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.153 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.153 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.153 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.153 I print_info: LF token         = 128 'Ä'
0.00.042.154 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.154 I print_info: max token length = 1024
0.00.742.234 I load_tensors: offloading 24 repeating layers to GPU
0.00.742.238 I load_tensors: offloading output layer to GPU
0.00.742.240 I load_tensors: offloaded 25/25 layers to GPU
0.00.742.261 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.742.263 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.743.233 I llama_init_from_model: n_seq_max     = 1
0.00.743.235 I llama_init_from_model: n_ctx         = 2048
0.00.743.235 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.743.235 I llama_init_from_model: n_batch       = 2048
0.00.743.236 I llama_init_from_model: n_ubatch      = 512
0.00.743.236 I llama_init_from_model: flash_attn    = 0
0.00.743.237 I llama_init_from_model: freq_base     = 10000.0
0.00.743.237 I llama_init_from_model: freq_scale    = 1
0.00.743.242 I ggml_metal_init: allocating
0.00.743.254 I ggml_metal_init: found device: Apple M4
0.00.743.262 I ggml_metal_init: picking default device: Apple M4
0.00.744.555 I ggml_metal_init: using embedded metal library
0.00.750.267 I ggml_metal_init: GPU name:   Apple M4
0.00.750.271 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.750.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.750.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.750.273 I ggml_metal_init: simdgroup reduction   = true
0.00.750.274 I ggml_metal_init: simdgroup matrix mul. = true
0.00.750.274 I ggml_metal_init: has residency sets    = true
0.00.750.274 I ggml_metal_init: has bfloat            = true
0.00.750.274 I ggml_metal_init: use bfloat            = true
0.00.750.275 I ggml_metal_init: hasUnifiedMemory      = true
0.00.750.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.766.734 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.820.045 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.820.052 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.820.075 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.824.896 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.824.898 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.824.899 I llama_init_from_model: graph nodes  = 967
0.00.824.899 I llama_init_from_model: graph splits = 2
0.00.824.906 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.825.042 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.825.043 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.874.388 I main: llama threadpool init, n_threads = 4
0.00.874.432 I 
0.00.874.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.874.460 I 
0.00.874.581 I sampler seed: 1234
0.00.874.585 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.874.595 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.874.596 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.874.596 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.670.357 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.670.357 I llama_perf_context_print:        load time =     861.68 ms
0.01.670.358 I llama_perf_context_print: prompt eval time =      53.80 ms /     7 tokens (    7.69 ms per token,   130.10 tokens per second)
0.01.670.359 I llama_perf_context_print:        eval time =     738.88 ms /    63 runs   (   11.73 ms per token,    85.26 tokens per second)
0.01.670.359 I llama_perf_context_print:       total time =     796.87 ms /    70 tokens
0.01.670.587 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.108s
sys	0m0.253s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.478 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.694 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.699 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.703 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.703 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.707 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.707 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.707 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.709 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.710 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.710 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.492 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.484 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.234 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.236 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.236 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.237 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.238 I llama_model_loader: - type  f32:  194 tensors
0.00.025.238 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.238 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.239 I print_info: file format = GGUF V3 (latest)
0.00.025.239 I print_info: file type   = Q5_1
0.00.025.240 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.988 I load: special tokens cache size = 25
0.00.039.007 I load: token to piece cache size = 0.2984 MB
0.00.039.010 I print_info: arch             = gptneox
0.00.039.010 I print_info: vocab_only       = 0
0.00.039.010 I print_info: n_ctx_train      = 2048
0.00.039.010 I print_info: n_embd           = 2048
0.00.039.010 I print_info: n_layer          = 24
0.00.039.013 I print_info: n_head           = 16
0.00.039.014 I print_info: n_head_kv        = 16
0.00.039.014 I print_info: n_rot            = 32
0.00.039.014 I print_info: n_swa            = 0
0.00.039.015 I print_info: n_embd_head_k    = 128
0.00.039.015 I print_info: n_embd_head_v    = 128
0.00.039.016 I print_info: n_gqa            = 1
0.00.039.016 I print_info: n_embd_k_gqa     = 2048
0.00.039.017 I print_info: n_embd_v_gqa     = 2048
0.00.039.018 I print_info: f_norm_eps       = 1.0e-05
0.00.039.018 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.018 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.018 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.019 I print_info: f_logit_scale    = 0.0e+00
0.00.039.019 I print_info: n_ff             = 8192
0.00.039.020 I print_info: n_expert         = 0
0.00.039.020 I print_info: n_expert_used    = 0
0.00.039.020 I print_info: causal attn      = 1
0.00.039.020 I print_info: pooling type     = 0
0.00.039.020 I print_info: rope type        = 2
0.00.039.021 I print_info: rope scaling     = linear
0.00.039.021 I print_info: freq_base_train  = 10000.0
0.00.039.021 I print_info: freq_scale_train = 1
0.00.039.021 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.023 I print_info: rope_finetuned   = unknown
0.00.039.023 I print_info: ssm_d_conv       = 0
0.00.039.023 I print_info: ssm_d_inner      = 0
0.00.039.023 I print_info: ssm_d_state      = 0
0.00.039.023 I print_info: ssm_dt_rank      = 0
0.00.039.023 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.024 I print_info: model type       = 1.4B
0.00.039.024 I print_info: model params     = 1.41 B
0.00.039.024 I print_info: general.name     = 1.4B
0.00.039.025 I print_info: vocab type       = BPE
0.00.039.025 I print_info: n_vocab          = 50304
0.00.039.025 I print_info: n_merges         = 50009
0.00.039.026 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.026 I print_info: LF token         = 128 'Ä'
0.00.039.027 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.029 I print_info: max token length = 1024
0.00.690.703 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.707 I load_tensors: offloading output layer to GPU
0.00.690.708 I load_tensors: offloaded 25/25 layers to GPU
0.00.690.728 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.690.729 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.691.740 I llama_init_from_model: n_seq_max     = 1
0.00.691.742 I llama_init_from_model: n_ctx         = 2048
0.00.691.742 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.691.743 I llama_init_from_model: n_batch       = 2048
0.00.691.743 I llama_init_from_model: n_ubatch      = 512
0.00.691.743 I llama_init_from_model: flash_attn    = 0
0.00.691.744 I llama_init_from_model: freq_base     = 10000.0
0.00.691.745 I llama_init_from_model: freq_scale    = 1
0.00.691.749 I ggml_metal_init: allocating
0.00.691.770 I ggml_metal_init: found device: Apple M4
0.00.691.779 I ggml_metal_init: picking default device: Apple M4
0.00.693.096 I ggml_metal_init: using embedded metal library
0.00.698.983 I ggml_metal_init: GPU name:   Apple M4
0.00.698.987 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.698.988 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.698.989 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.698.989 I ggml_metal_init: simdgroup reduction   = true
0.00.698.990 I ggml_metal_init: simdgroup matrix mul. = true
0.00.698.990 I ggml_metal_init: has residency sets    = true
0.00.698.990 I ggml_metal_init: has bfloat            = true
0.00.698.991 I ggml_metal_init: use bfloat            = true
0.00.698.991 I ggml_metal_init: hasUnifiedMemory      = true
0.00.699.000 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.715.735 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.774.809 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.774.815 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.774.839 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.780.011 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.780.013 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.780.013 I llama_init_from_model: graph nodes  = 967
0.00.780.014 I llama_init_from_model: graph splits = 2
0.00.780.018 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.780.147 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.780.148 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.831.325 I main: llama threadpool init, n_threads = 4
0.00.831.370 I 
0.00.831.396 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.831.397 I 
0.00.831.517 I sampler seed: 1234
0.00.831.522 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.831.559 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.831.562 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.831.562 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.675.433 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.675.434 I llama_perf_context_print:        load time =     820.94 ms
0.01.675.435 I llama_perf_context_print: prompt eval time =      53.13 ms /     7 tokens (    7.59 ms per token,   131.75 tokens per second)
0.01.675.436 I llama_perf_context_print:        eval time =     787.66 ms /    63 runs   (   12.50 ms per token,    79.98 tokens per second)
0.01.675.436 I llama_perf_context_print:       total time =     845.01 ms /    70 tokens
0.01.675.727 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.108s
sys	0m0.276s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.577 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.130 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.135 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.136 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.140 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.141 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.141 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.142 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.143 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.143 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.145 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.145 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.146 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.146 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.148 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.149 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.149 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.908 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.662 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.666 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.666 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.666 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.667 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.667 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.667 I llama_model_loader: - type  f32:  194 tensors
0.00.024.668 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.668 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.668 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.668 I print_info: file format = GGUF V3 (latest)
0.00.024.669 I print_info: file type   = Q2_K - Medium
0.00.024.669 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.385 I load: special tokens cache size = 25
0.00.038.094 I load: token to piece cache size = 0.2984 MB
0.00.038.097 I print_info: arch             = gptneox
0.00.038.097 I print_info: vocab_only       = 0
0.00.038.097 I print_info: n_ctx_train      = 2048
0.00.038.098 I print_info: n_embd           = 2048
0.00.038.098 I print_info: n_layer          = 24
0.00.038.101 I print_info: n_head           = 16
0.00.038.101 I print_info: n_head_kv        = 16
0.00.038.102 I print_info: n_rot            = 32
0.00.038.102 I print_info: n_swa            = 0
0.00.038.102 I print_info: n_embd_head_k    = 128
0.00.038.102 I print_info: n_embd_head_v    = 128
0.00.038.103 I print_info: n_gqa            = 1
0.00.038.104 I print_info: n_embd_k_gqa     = 2048
0.00.038.104 I print_info: n_embd_v_gqa     = 2048
0.00.038.105 I print_info: f_norm_eps       = 1.0e-05
0.00.038.105 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.105 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.105 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.106 I print_info: f_logit_scale    = 0.0e+00
0.00.038.106 I print_info: n_ff             = 8192
0.00.038.107 I print_info: n_expert         = 0
0.00.038.107 I print_info: n_expert_used    = 0
0.00.038.107 I print_info: causal attn      = 1
0.00.038.109 I print_info: pooling type     = 0
0.00.038.109 I print_info: rope type        = 2
0.00.038.109 I print_info: rope scaling     = linear
0.00.038.110 I print_info: freq_base_train  = 10000.0
0.00.038.110 I print_info: freq_scale_train = 1
0.00.038.110 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.111 I print_info: rope_finetuned   = unknown
0.00.038.111 I print_info: ssm_d_conv       = 0
0.00.038.111 I print_info: ssm_d_inner      = 0
0.00.038.111 I print_info: ssm_d_state      = 0
0.00.038.111 I print_info: ssm_dt_rank      = 0
0.00.038.111 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.111 I print_info: model type       = 1.4B
0.00.038.112 I print_info: model params     = 1.41 B
0.00.038.113 I print_info: general.name     = 1.4B
0.00.038.114 I print_info: vocab type       = BPE
0.00.038.114 I print_info: n_vocab          = 50304
0.00.038.114 I print_info: n_merges         = 50009
0.00.038.114 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.114 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.115 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.115 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.115 I print_info: LF token         = 128 'Ä'
0.00.038.115 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.116 I print_info: max token length = 1024
0.00.378.734 I load_tensors: offloading 24 repeating layers to GPU
0.00.378.744 I load_tensors: offloading output layer to GPU
0.00.378.745 I load_tensors: offloaded 25/25 layers to GPU
0.00.378.775 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.378.776 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.380.061 I llama_init_from_model: n_seq_max     = 1
0.00.380.066 I llama_init_from_model: n_ctx         = 2048
0.00.380.067 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.380.067 I llama_init_from_model: n_batch       = 2048
0.00.380.068 I llama_init_from_model: n_ubatch      = 512
0.00.380.068 I llama_init_from_model: flash_attn    = 0
0.00.380.070 I llama_init_from_model: freq_base     = 10000.0
0.00.380.071 I llama_init_from_model: freq_scale    = 1
0.00.380.073 I ggml_metal_init: allocating
0.00.380.125 I ggml_metal_init: found device: Apple M4
0.00.380.141 I ggml_metal_init: picking default device: Apple M4
0.00.382.087 I ggml_metal_init: using embedded metal library
0.00.388.594 I ggml_metal_init: GPU name:   Apple M4
0.00.388.599 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.388.600 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.388.601 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.388.602 I ggml_metal_init: simdgroup reduction   = true
0.00.388.602 I ggml_metal_init: simdgroup matrix mul. = true
0.00.388.603 I ggml_metal_init: has residency sets    = true
0.00.388.603 I ggml_metal_init: has bfloat            = true
0.00.388.603 I ggml_metal_init: use bfloat            = true
0.00.388.604 I ggml_metal_init: hasUnifiedMemory      = true
0.00.388.606 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.407.703 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.463.719 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.463.726 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.463.758 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.468.884 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.468.886 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.468.886 I llama_init_from_model: graph nodes  = 967
0.00.468.886 I llama_init_from_model: graph splits = 2
0.00.468.891 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.469.024 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.469.025 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.521.434 I main: llama threadpool init, n_threads = 4
0.00.521.480 I 
0.00.521.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.521.507 I 
0.00.521.637 I sampler seed: 1234
0.00.521.642 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.521.651 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.521.653 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.521.653 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.215.056 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49754.73 tokens per second)
0.01.215.057 I llama_perf_context_print:        load time =     510.98 ms
0.01.215.057 I llama_perf_context_print: prompt eval time =      44.70 ms /     7 tokens (    6.39 ms per token,   156.60 tokens per second)
0.01.215.058 I llama_perf_context_print:        eval time =     645.88 ms /    63 runs   (   10.25 ms per token,    97.54 tokens per second)
0.01.215.058 I llama_perf_context_print:       total time =     694.50 ms /    70 tokens
0.01.215.346 I ggml_metal_free: deallocating

real	0m1.232s
user	0m0.111s
sys	0m0.181s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.476 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.445 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.455 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.456 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.456 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.456 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.457 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.458 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.458 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.461 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.462 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.463 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.464 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.094 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.095 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.095 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.096 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.096 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.097 I llama_model_loader: - type  f32:  194 tensors
0.00.025.097 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.097 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.097 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.098 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.098 I print_info: file format = GGUF V3 (latest)
0.00.025.099 I print_info: file type   = Q3_K - Medium
0.00.025.099 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.164 I load: special tokens cache size = 25
0.00.039.205 I load: token to piece cache size = 0.2984 MB
0.00.039.208 I print_info: arch             = gptneox
0.00.039.208 I print_info: vocab_only       = 0
0.00.039.208 I print_info: n_ctx_train      = 2048
0.00.039.208 I print_info: n_embd           = 2048
0.00.039.209 I print_info: n_layer          = 24
0.00.039.211 I print_info: n_head           = 16
0.00.039.212 I print_info: n_head_kv        = 16
0.00.039.213 I print_info: n_rot            = 32
0.00.039.214 I print_info: n_swa            = 0
0.00.039.214 I print_info: n_embd_head_k    = 128
0.00.039.214 I print_info: n_embd_head_v    = 128
0.00.039.215 I print_info: n_gqa            = 1
0.00.039.215 I print_info: n_embd_k_gqa     = 2048
0.00.039.216 I print_info: n_embd_v_gqa     = 2048
0.00.039.217 I print_info: f_norm_eps       = 1.0e-05
0.00.039.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.218 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.218 I print_info: f_logit_scale    = 0.0e+00
0.00.039.219 I print_info: n_ff             = 8192
0.00.039.223 I print_info: n_expert         = 0
0.00.039.223 I print_info: n_expert_used    = 0
0.00.039.224 I print_info: causal attn      = 1
0.00.039.226 I print_info: pooling type     = 0
0.00.039.226 I print_info: rope type        = 2
0.00.039.227 I print_info: rope scaling     = linear
0.00.039.227 I print_info: freq_base_train  = 10000.0
0.00.039.227 I print_info: freq_scale_train = 1
0.00.039.228 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.228 I print_info: rope_finetuned   = unknown
0.00.039.228 I print_info: ssm_d_conv       = 0
0.00.039.228 I print_info: ssm_d_inner      = 0
0.00.039.229 I print_info: ssm_d_state      = 0
0.00.039.230 I print_info: ssm_dt_rank      = 0
0.00.039.230 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.231 I print_info: model type       = 1.4B
0.00.039.231 I print_info: model params     = 1.41 B
0.00.039.231 I print_info: general.name     = 1.4B
0.00.039.232 I print_info: vocab type       = BPE
0.00.039.232 I print_info: n_vocab          = 50304
0.00.039.232 I print_info: n_merges         = 50009
0.00.039.232 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.232 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.233 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.233 I print_info: LF token         = 128 'Ä'
0.00.039.235 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.235 I print_info: max token length = 1024
0.00.485.991 I load_tensors: offloading 24 repeating layers to GPU
0.00.486.001 I load_tensors: offloading output layer to GPU
0.00.486.002 I load_tensors: offloaded 25/25 layers to GPU
0.00.486.030 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.486.033 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.487.073 I llama_init_from_model: n_seq_max     = 1
0.00.487.077 I llama_init_from_model: n_ctx         = 2048
0.00.487.077 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.487.078 I llama_init_from_model: n_batch       = 2048
0.00.487.078 I llama_init_from_model: n_ubatch      = 512
0.00.487.079 I llama_init_from_model: flash_attn    = 0
0.00.487.082 I llama_init_from_model: freq_base     = 10000.0
0.00.487.083 I llama_init_from_model: freq_scale    = 1
0.00.487.084 I ggml_metal_init: allocating
0.00.487.100 I ggml_metal_init: found device: Apple M4
0.00.487.110 I ggml_metal_init: picking default device: Apple M4
0.00.488.637 I ggml_metal_init: using embedded metal library
0.00.495.031 I ggml_metal_init: GPU name:   Apple M4
0.00.495.034 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.495.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.495.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.495.037 I ggml_metal_init: simdgroup reduction   = true
0.00.495.037 I ggml_metal_init: simdgroup matrix mul. = true
0.00.495.037 I ggml_metal_init: has residency sets    = true
0.00.495.038 I ggml_metal_init: has bfloat            = true
0.00.495.038 I ggml_metal_init: use bfloat            = true
0.00.495.039 I ggml_metal_init: hasUnifiedMemory      = true
0.00.495.040 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.512.621 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.566.018 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.566.026 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.566.092 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.570.895 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.570.896 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.570.897 I llama_init_from_model: graph nodes  = 967
0.00.570.897 I llama_init_from_model: graph splits = 2
0.00.570.901 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.571.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.571.022 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.493 I main: llama threadpool init, n_threads = 4
0.00.621.543 I 
0.00.621.565 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.565 I 
0.00.621.683 I sampler seed: 1234
0.00.621.687 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.621.697 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.621.697 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.621.701 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.371.578 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52090.98 tokens per second)
0.01.371.578 I llama_perf_context_print:        load time =     612.13 ms
0.01.371.579 I llama_perf_context_print: prompt eval time =      50.55 ms /     7 tokens (    7.22 ms per token,   138.47 tokens per second)
0.01.371.581 I llama_perf_context_print:        eval time =     696.37 ms /    63 runs   (   11.05 ms per token,    90.47 tokens per second)
0.01.371.581 I llama_perf_context_print:       total time =     750.97 ms /    70 tokens
0.01.371.836 I ggml_metal_free: deallocating

real	0m1.389s
user	0m0.109s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.010.209 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.320 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.018.326 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.328 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.329 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.333 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.334 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.335 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.335 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.336 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.336 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.337 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.339 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.119 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.892 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.893 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.894 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.894 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.894 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.894 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.895 I llama_model_loader: - type  f32:  194 tensors
0.00.026.895 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.895 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.895 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.896 I print_info: file format = GGUF V3 (latest)
0.00.026.896 I print_info: file type   = Q4_K - Medium
0.00.026.897 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.702 I load: special tokens cache size = 25
0.00.040.592 I load: token to piece cache size = 0.2984 MB
0.00.040.595 I print_info: arch             = gptneox
0.00.040.595 I print_info: vocab_only       = 0
0.00.040.595 I print_info: n_ctx_train      = 2048
0.00.040.595 I print_info: n_embd           = 2048
0.00.040.595 I print_info: n_layer          = 24
0.00.040.598 I print_info: n_head           = 16
0.00.040.599 I print_info: n_head_kv        = 16
0.00.040.599 I print_info: n_rot            = 32
0.00.040.601 I print_info: n_swa            = 0
0.00.040.601 I print_info: n_embd_head_k    = 128
0.00.040.601 I print_info: n_embd_head_v    = 128
0.00.040.602 I print_info: n_gqa            = 1
0.00.040.603 I print_info: n_embd_k_gqa     = 2048
0.00.040.603 I print_info: n_embd_v_gqa     = 2048
0.00.040.604 I print_info: f_norm_eps       = 1.0e-05
0.00.040.604 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.604 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.605 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.605 I print_info: f_logit_scale    = 0.0e+00
0.00.040.606 I print_info: n_ff             = 8192
0.00.040.606 I print_info: n_expert         = 0
0.00.040.608 I print_info: n_expert_used    = 0
0.00.040.608 I print_info: causal attn      = 1
0.00.040.609 I print_info: pooling type     = 0
0.00.040.609 I print_info: rope type        = 2
0.00.040.610 I print_info: rope scaling     = linear
0.00.040.610 I print_info: freq_base_train  = 10000.0
0.00.040.610 I print_info: freq_scale_train = 1
0.00.040.611 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.611 I print_info: rope_finetuned   = unknown
0.00.040.611 I print_info: ssm_d_conv       = 0
0.00.040.611 I print_info: ssm_d_inner      = 0
0.00.040.616 I print_info: ssm_d_state      = 0
0.00.040.616 I print_info: ssm_dt_rank      = 0
0.00.040.616 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.616 I print_info: model type       = 1.4B
0.00.040.617 I print_info: model params     = 1.41 B
0.00.040.617 I print_info: general.name     = 1.4B
0.00.040.617 I print_info: vocab type       = BPE
0.00.040.617 I print_info: n_vocab          = 50304
0.00.040.618 I print_info: n_merges         = 50009
0.00.040.618 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.618 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: LF token         = 128 'Ä'
0.00.040.620 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.620 I print_info: max token length = 1024
0.00.594.361 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.365 I load_tensors: offloading output layer to GPU
0.00.594.366 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.385 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.594.387 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.595.496 I llama_init_from_model: n_seq_max     = 1
0.00.595.498 I llama_init_from_model: n_ctx         = 2048
0.00.595.499 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.595.499 I llama_init_from_model: n_batch       = 2048
0.00.595.500 I llama_init_from_model: n_ubatch      = 512
0.00.595.501 I llama_init_from_model: flash_attn    = 0
0.00.595.501 I llama_init_from_model: freq_base     = 10000.0
0.00.595.509 I llama_init_from_model: freq_scale    = 1
0.00.595.510 I ggml_metal_init: allocating
0.00.595.527 I ggml_metal_init: found device: Apple M4
0.00.595.536 I ggml_metal_init: picking default device: Apple M4
0.00.596.966 I ggml_metal_init: using embedded metal library
0.00.602.909 I ggml_metal_init: GPU name:   Apple M4
0.00.602.913 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.914 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.915 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.916 I ggml_metal_init: simdgroup reduction   = true
0.00.602.916 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.916 I ggml_metal_init: has residency sets    = true
0.00.602.917 I ggml_metal_init: has bfloat            = true
0.00.602.917 I ggml_metal_init: use bfloat            = true
0.00.602.918 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.921 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.290 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.814 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.673.821 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.673.844 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.252 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.680.255 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.680.255 I llama_init_from_model: graph nodes  = 967
0.00.680.256 I llama_init_from_model: graph splits = 2
0.00.680.260 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.680.394 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.680.394 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.731.452 I main: llama threadpool init, n_threads = 4
0.00.731.496 I 
0.00.731.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.731.527 I 
0.00.731.654 I sampler seed: 1234
0.00.731.658 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.731.690 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.731.693 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.731.693 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.500.350 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.500.350 I llama_perf_context_print:        load time =     720.36 ms
0.01.500.351 I llama_perf_context_print: prompt eval time =      58.87 ms /     7 tokens (    8.41 ms per token,   118.90 tokens per second)
0.01.500.353 I llama_perf_context_print:        eval time =     706.73 ms /    63 runs   (   11.22 ms per token,    89.14 tokens per second)
0.01.500.353 I llama_perf_context_print:       total time =     769.78 ms /    70 tokens
0.01.500.634 I ggml_metal_free: deallocating

real	0m1.519s
user	0m0.108s
sys	0m0.246s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.581 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.591 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.504 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.577 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.434 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.435 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.435 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.436 I llama_model_loader: - type  f32:  194 tensors
0.00.026.437 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.437 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.437 I print_info: file format = GGUF V3 (latest)
0.00.026.438 I print_info: file type   = Q5_K - Medium
0.00.026.439 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.034.617 I load: special tokens cache size = 25
0.00.040.624 I load: token to piece cache size = 0.2984 MB
0.00.040.627 I print_info: arch             = gptneox
0.00.040.627 I print_info: vocab_only       = 0
0.00.040.627 I print_info: n_ctx_train      = 2048
0.00.040.627 I print_info: n_embd           = 2048
0.00.040.628 I print_info: n_layer          = 24
0.00.040.630 I print_info: n_head           = 16
0.00.040.631 I print_info: n_head_kv        = 16
0.00.040.631 I print_info: n_rot            = 32
0.00.040.632 I print_info: n_swa            = 0
0.00.040.632 I print_info: n_embd_head_k    = 128
0.00.040.632 I print_info: n_embd_head_v    = 128
0.00.040.633 I print_info: n_gqa            = 1
0.00.040.634 I print_info: n_embd_k_gqa     = 2048
0.00.040.634 I print_info: n_embd_v_gqa     = 2048
0.00.040.635 I print_info: f_norm_eps       = 1.0e-05
0.00.040.635 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.635 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.635 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.636 I print_info: f_logit_scale    = 0.0e+00
0.00.040.636 I print_info: n_ff             = 8192
0.00.040.636 I print_info: n_expert         = 0
0.00.040.637 I print_info: n_expert_used    = 0
0.00.040.637 I print_info: causal attn      = 1
0.00.040.637 I print_info: pooling type     = 0
0.00.040.637 I print_info: rope type        = 2
0.00.040.637 I print_info: rope scaling     = linear
0.00.040.638 I print_info: freq_base_train  = 10000.0
0.00.040.638 I print_info: freq_scale_train = 1
0.00.040.638 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.638 I print_info: rope_finetuned   = unknown
0.00.040.639 I print_info: ssm_d_conv       = 0
0.00.040.639 I print_info: ssm_d_inner      = 0
0.00.040.639 I print_info: ssm_d_state      = 0
0.00.040.639 I print_info: ssm_dt_rank      = 0
0.00.040.639 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.640 I print_info: model type       = 1.4B
0.00.040.640 I print_info: model params     = 1.41 B
0.00.040.640 I print_info: general.name     = 1.4B
0.00.040.641 I print_info: vocab type       = BPE
0.00.040.641 I print_info: n_vocab          = 50304
0.00.040.641 I print_info: n_merges         = 50009
0.00.040.642 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.642 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.642 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.643 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.643 I print_info: LF token         = 128 'Ä'
0.00.040.643 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.643 I print_info: max token length = 1024
0.00.663.273 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.276 I load_tensors: offloading output layer to GPU
0.00.663.277 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.296 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.663.299 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.664.125 I llama_init_from_model: n_seq_max     = 1
0.00.664.127 I llama_init_from_model: n_ctx         = 2048
0.00.664.127 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.664.128 I llama_init_from_model: n_batch       = 2048
0.00.664.128 I llama_init_from_model: n_ubatch      = 512
0.00.664.128 I llama_init_from_model: flash_attn    = 0
0.00.664.130 I llama_init_from_model: freq_base     = 10000.0
0.00.664.130 I llama_init_from_model: freq_scale    = 1
0.00.664.134 I ggml_metal_init: allocating
0.00.664.172 I ggml_metal_init: found device: Apple M4
0.00.664.183 I ggml_metal_init: picking default device: Apple M4
0.00.665.526 I ggml_metal_init: using embedded metal library
0.00.671.242 I ggml_metal_init: GPU name:   Apple M4
0.00.671.245 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.671.246 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.671.247 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.671.247 I ggml_metal_init: simdgroup reduction   = true
0.00.671.247 I ggml_metal_init: simdgroup matrix mul. = true
0.00.671.248 I ggml_metal_init: has residency sets    = true
0.00.671.248 I ggml_metal_init: has bfloat            = true
0.00.671.248 I ggml_metal_init: use bfloat            = true
0.00.671.249 I ggml_metal_init: hasUnifiedMemory      = true
0.00.671.250 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.687.488 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.738.297 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.738.304 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.738.326 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.742.888 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.742.890 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.742.890 I llama_init_from_model: graph nodes  = 967
0.00.742.890 I llama_init_from_model: graph splits = 2
0.00.742.895 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.028 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.029 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.193 I main: llama threadpool init, n_threads = 4
0.00.801.245 I 
0.00.801.272 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.275 I 
0.00.801.387 I sampler seed: 1234
0.00.801.391 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.801.400 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.801.401 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.801.401 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.644.391 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53423.63 tokens per second)
0.01.644.392 I llama_perf_context_print:        load time =     790.40 ms
0.01.644.393 I llama_perf_context_print: prompt eval time =      51.62 ms /     7 tokens (    7.37 ms per token,   135.60 tokens per second)
0.01.644.393 I llama_perf_context_print:        eval time =     788.32 ms /    63 runs   (   12.51 ms per token,    79.92 tokens per second)
0.01.644.394 I llama_perf_context_print:       total time =     844.19 ms /    70 tokens
0.01.644.623 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.107s
sys	0m0.244s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.300 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.932 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.937 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.938 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.939 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.939 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.942 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.943 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.945 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.947 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.746 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.517 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.519 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.519 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.519 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.520 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.520 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.520 I llama_model_loader: - type  f32:  194 tensors
0.00.025.521 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.521 I print_info: file format = GGUF V3 (latest)
0.00.025.522 I print_info: file type   = Q6_K
0.00.025.522 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.683 I load: special tokens cache size = 25
0.00.039.623 I load: token to piece cache size = 0.2984 MB
0.00.039.626 I print_info: arch             = gptneox
0.00.039.626 I print_info: vocab_only       = 0
0.00.039.627 I print_info: n_ctx_train      = 2048
0.00.039.627 I print_info: n_embd           = 2048
0.00.039.627 I print_info: n_layer          = 24
0.00.039.630 I print_info: n_head           = 16
0.00.039.631 I print_info: n_head_kv        = 16
0.00.039.632 I print_info: n_rot            = 32
0.00.039.632 I print_info: n_swa            = 0
0.00.039.633 I print_info: n_embd_head_k    = 128
0.00.039.633 I print_info: n_embd_head_v    = 128
0.00.039.634 I print_info: n_gqa            = 1
0.00.039.636 I print_info: n_embd_k_gqa     = 2048
0.00.039.636 I print_info: n_embd_v_gqa     = 2048
0.00.039.637 I print_info: f_norm_eps       = 1.0e-05
0.00.039.637 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.638 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.638 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.638 I print_info: f_logit_scale    = 0.0e+00
0.00.039.639 I print_info: n_ff             = 8192
0.00.039.639 I print_info: n_expert         = 0
0.00.039.639 I print_info: n_expert_used    = 0
0.00.039.639 I print_info: causal attn      = 1
0.00.039.639 I print_info: pooling type     = 0
0.00.039.639 I print_info: rope type        = 2
0.00.039.640 I print_info: rope scaling     = linear
0.00.039.640 I print_info: freq_base_train  = 10000.0
0.00.039.640 I print_info: freq_scale_train = 1
0.00.039.640 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.641 I print_info: rope_finetuned   = unknown
0.00.039.641 I print_info: ssm_d_conv       = 0
0.00.039.641 I print_info: ssm_d_inner      = 0
0.00.039.641 I print_info: ssm_d_state      = 0
0.00.039.641 I print_info: ssm_dt_rank      = 0
0.00.039.642 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.644 I print_info: model type       = 1.4B
0.00.039.644 I print_info: model params     = 1.41 B
0.00.039.644 I print_info: general.name     = 1.4B
0.00.039.645 I print_info: vocab type       = BPE
0.00.039.645 I print_info: n_vocab          = 50304
0.00.039.645 I print_info: n_merges         = 50009
0.00.039.646 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.646 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.647 I print_info: LF token         = 128 'Ä'
0.00.039.647 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.647 I print_info: max token length = 1024
0.00.740.205 I load_tensors: offloading 24 repeating layers to GPU
0.00.740.210 I load_tensors: offloading output layer to GPU
0.00.740.210 I load_tensors: offloaded 25/25 layers to GPU
0.00.740.229 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.740.231 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.740.910 I llama_init_from_model: n_seq_max     = 1
0.00.740.912 I llama_init_from_model: n_ctx         = 2048
0.00.740.912 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.740.912 I llama_init_from_model: n_batch       = 2048
0.00.740.913 I llama_init_from_model: n_ubatch      = 512
0.00.740.913 I llama_init_from_model: flash_attn    = 0
0.00.740.914 I llama_init_from_model: freq_base     = 10000.0
0.00.740.914 I llama_init_from_model: freq_scale    = 1
0.00.740.915 I ggml_metal_init: allocating
0.00.740.925 I ggml_metal_init: found device: Apple M4
0.00.740.931 I ggml_metal_init: picking default device: Apple M4
0.00.742.131 I ggml_metal_init: using embedded metal library
0.00.747.474 I ggml_metal_init: GPU name:   Apple M4
0.00.747.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.747.478 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.747.479 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.747.480 I ggml_metal_init: simdgroup reduction   = true
0.00.747.480 I ggml_metal_init: simdgroup matrix mul. = true
0.00.747.480 I ggml_metal_init: has residency sets    = true
0.00.747.481 I ggml_metal_init: has bfloat            = true
0.00.747.481 I ggml_metal_init: use bfloat            = true
0.00.747.481 I ggml_metal_init: hasUnifiedMemory      = true
0.00.747.483 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.767.550 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.821.827 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.821.834 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.821.904 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.827.184 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.827.186 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.827.187 I llama_init_from_model: graph nodes  = 967
0.00.827.187 I llama_init_from_model: graph splits = 2
0.00.827.192 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.827.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.827.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.882.072 I main: llama threadpool init, n_threads = 4
0.00.882.117 I 
0.00.882.140 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.882.143 I 
0.00.882.264 I sampler seed: 1234
0.00.882.269 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.882.278 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.882.278 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.882.282 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.747.812 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54157.13 tokens per second)
0.01.747.813 I llama_perf_context_print:        load time =     871.87 ms
0.01.747.815 I llama_perf_context_print: prompt eval time =      54.10 ms /     7 tokens (    7.73 ms per token,   129.39 tokens per second)
0.01.747.816 I llama_perf_context_print:        eval time =     808.45 ms /    63 runs   (   12.83 ms per token,    77.93 tokens per second)
0.01.747.816 I llama_perf_context_print:       total time =     866.64 ms /    70 tokens
0.01.748.092 I ggml_metal_free: deallocating

real	0m1.767s
user	0m0.107s
sys	0m0.277s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.509 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.014 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.348 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.356 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.361 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.362 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.364 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.365 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.366 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.368 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.371 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.371 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.965 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.955 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.312 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.314 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.315 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.315 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.316 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.316 I llama_model_loader: - type  f32:  194 tensors
0.00.055.317 I llama_model_loader: - type  f16:   98 tensors
0.00.055.318 I print_info: file format = GGUF V3 (latest)
0.00.055.319 I print_info: file type   = all F32 (guessed)
0.00.055.320 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.068.333 I load: special tokens cache size = 25
0.00.076.474 I load: token to piece cache size = 0.2984 MB
0.00.076.477 I print_info: arch             = gptneox
0.00.076.478 I print_info: vocab_only       = 0
0.00.076.478 I print_info: n_ctx_train      = 2048
0.00.076.478 I print_info: n_embd           = 2048
0.00.076.478 I print_info: n_layer          = 24
0.00.076.481 I print_info: n_head           = 16
0.00.076.482 I print_info: n_head_kv        = 16
0.00.076.482 I print_info: n_rot            = 32
0.00.076.483 I print_info: n_swa            = 0
0.00.076.483 I print_info: n_embd_head_k    = 128
0.00.076.483 I print_info: n_embd_head_v    = 128
0.00.076.484 I print_info: n_gqa            = 1
0.00.076.485 I print_info: n_embd_k_gqa     = 2048
0.00.076.485 I print_info: n_embd_v_gqa     = 2048
0.00.076.486 I print_info: f_norm_eps       = 1.0e-05
0.00.076.486 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.076.487 I print_info: f_clamp_kqv      = 0.0e+00
0.00.076.487 I print_info: f_max_alibi_bias = 0.0e+00
0.00.076.487 I print_info: f_logit_scale    = 0.0e+00
0.00.076.489 I print_info: n_ff             = 8192
0.00.076.490 I print_info: n_expert         = 0
0.00.076.490 I print_info: n_expert_used    = 0
0.00.076.490 I print_info: causal attn      = 1
0.00.076.490 I print_info: pooling type     = 0
0.00.076.490 I print_info: rope type        = 2
0.00.076.491 I print_info: rope scaling     = linear
0.00.076.491 I print_info: freq_base_train  = 10000.0
0.00.076.491 I print_info: freq_scale_train = 1
0.00.076.492 I print_info: n_ctx_orig_yarn  = 2048
0.00.076.492 I print_info: rope_finetuned   = unknown
0.00.076.492 I print_info: ssm_d_conv       = 0
0.00.076.492 I print_info: ssm_d_inner      = 0
0.00.076.492 I print_info: ssm_d_state      = 0
0.00.076.493 I print_info: ssm_dt_rank      = 0
0.00.076.493 I print_info: ssm_dt_b_c_rms   = 0
0.00.076.494 I print_info: model type       = 1.4B
0.00.076.495 I print_info: model params     = 1.41 B
0.00.076.495 I print_info: general.name     = 1.4B
0.00.076.496 I print_info: vocab type       = BPE
0.00.076.496 I print_info: n_vocab          = 50304
0.00.076.496 I print_info: n_merges         = 50009
0.00.076.496 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.076.497 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.076.497 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.076.497 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.076.497 I print_info: LF token         = 128 'Ä'
0.00.076.497 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.076.498 I print_info: max token length = 1024
0.01.014.637 I load_tensors: offloading 24 repeating layers to GPU
0.01.014.645 I load_tensors: offloading output layer to GPU
0.01.014.645 I load_tensors: offloaded 25/25 layers to GPU
0.01.014.672 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.014.673 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.015.218 I llama_init_from_model: n_seq_max     = 1
0.01.015.219 I llama_init_from_model: n_ctx         = 128
0.01.015.219 I llama_init_from_model: n_ctx_per_seq = 128
0.01.015.219 I llama_init_from_model: n_batch       = 128
0.01.015.220 I llama_init_from_model: n_ubatch      = 128
0.01.015.220 I llama_init_from_model: flash_attn    = 0
0.01.015.220 I llama_init_from_model: freq_base     = 10000.0
0.01.015.221 I llama_init_from_model: freq_scale    = 1
0.01.015.221 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.015.222 I ggml_metal_init: allocating
0.01.015.268 I ggml_metal_init: found device: Apple M4
0.01.015.274 I ggml_metal_init: picking default device: Apple M4
0.01.016.269 I ggml_metal_init: using embedded metal library
0.01.019.532 I ggml_metal_init: GPU name:   Apple M4
0.01.019.534 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.019.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.019.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.019.535 I ggml_metal_init: simdgroup reduction   = true
0.01.019.535 I ggml_metal_init: simdgroup matrix mul. = true
0.01.019.535 I ggml_metal_init: has residency sets    = true
0.01.019.535 I ggml_metal_init: has bfloat            = true
0.01.019.535 I ggml_metal_init: use bfloat            = true
0.01.019.536 I ggml_metal_init: hasUnifiedMemory      = true
0.01.019.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.030.231 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.031.932 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.031.934 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.031.951 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.033.471 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.033.472 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.033.473 I llama_init_from_model: graph nodes  = 967
0.01.033.473 I llama_init_from_model: graph splits = 2
0.01.033.474 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.033.474 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.066.761 I 
0.01.066.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.066.809 I perplexity: tokenizing the input ..
0.01.071.121 I perplexity: tokenization took 4.311 ms
0.01.071.141 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.189.364 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.190.633 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.190.648 I llama_perf_context_print:        load time =    1043.74 ms
0.01.190.648 I llama_perf_context_print: prompt eval time =     117.94 ms /   128 tokens (    0.92 ms per token,  1085.28 tokens per second)
0.01.190.649 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.190.649 I llama_perf_context_print:       total time =     123.89 ms /   129 tokens
0.01.191.008 I ggml_metal_free: deallocating

real	0m1.387s
user	0m0.101s
sys	0m0.273s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.772 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.867 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.017.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.880 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.880 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.880 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.881 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.882 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.882 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.882 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.883 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.883 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.883 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.885 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.655 I llama_model_loader: - type  f32:  194 tensors
0.00.026.655 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.656 I print_info: file format = GGUF V3 (latest)
0.00.026.656 I print_info: file type   = Q8_0
0.00.026.657 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.148 I load: special tokens cache size = 25
0.00.041.187 I load: token to piece cache size = 0.2984 MB
0.00.041.191 I print_info: arch             = gptneox
0.00.041.191 I print_info: vocab_only       = 0
0.00.041.192 I print_info: n_ctx_train      = 2048
0.00.041.192 I print_info: n_embd           = 2048
0.00.041.192 I print_info: n_layer          = 24
0.00.041.196 I print_info: n_head           = 16
0.00.041.197 I print_info: n_head_kv        = 16
0.00.041.197 I print_info: n_rot            = 32
0.00.041.197 I print_info: n_swa            = 0
0.00.041.197 I print_info: n_embd_head_k    = 128
0.00.041.197 I print_info: n_embd_head_v    = 128
0.00.041.198 I print_info: n_gqa            = 1
0.00.041.199 I print_info: n_embd_k_gqa     = 2048
0.00.041.199 I print_info: n_embd_v_gqa     = 2048
0.00.041.200 I print_info: f_norm_eps       = 1.0e-05
0.00.041.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.201 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.201 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.201 I print_info: f_logit_scale    = 0.0e+00
0.00.041.205 I print_info: n_ff             = 8192
0.00.041.205 I print_info: n_expert         = 0
0.00.041.206 I print_info: n_expert_used    = 0
0.00.041.206 I print_info: causal attn      = 1
0.00.041.206 I print_info: pooling type     = 0
0.00.041.206 I print_info: rope type        = 2
0.00.041.206 I print_info: rope scaling     = linear
0.00.041.206 I print_info: freq_base_train  = 10000.0
0.00.041.207 I print_info: freq_scale_train = 1
0.00.041.208 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.208 I print_info: rope_finetuned   = unknown
0.00.041.209 I print_info: ssm_d_conv       = 0
0.00.041.209 I print_info: ssm_d_inner      = 0
0.00.041.209 I print_info: ssm_d_state      = 0
0.00.041.209 I print_info: ssm_dt_rank      = 0
0.00.041.209 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.209 I print_info: model type       = 1.4B
0.00.041.210 I print_info: model params     = 1.41 B
0.00.041.210 I print_info: general.name     = 1.4B
0.00.041.210 I print_info: vocab type       = BPE
0.00.041.212 I print_info: n_vocab          = 50304
0.00.041.212 I print_info: n_merges         = 50009
0.00.041.212 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.212 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.212 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.212 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.213 I print_info: LF token         = 128 'Ä'
0.00.041.213 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.213 I print_info: max token length = 1024
0.00.965.161 I load_tensors: offloading 24 repeating layers to GPU
0.00.965.167 I load_tensors: offloading output layer to GPU
0.00.965.168 I load_tensors: offloaded 25/25 layers to GPU
0.00.965.194 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.965.196 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.966.053 I llama_init_from_model: n_seq_max     = 1
0.00.966.055 I llama_init_from_model: n_ctx         = 128
0.00.966.055 I llama_init_from_model: n_ctx_per_seq = 128
0.00.966.056 I llama_init_from_model: n_batch       = 128
0.00.966.056 I llama_init_from_model: n_ubatch      = 128
0.00.966.056 I llama_init_from_model: flash_attn    = 0
0.00.966.057 I llama_init_from_model: freq_base     = 10000.0
0.00.966.057 I llama_init_from_model: freq_scale    = 1
0.00.966.058 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.966.061 I ggml_metal_init: allocating
0.00.966.130 I ggml_metal_init: found device: Apple M4
0.00.966.142 I ggml_metal_init: picking default device: Apple M4
0.00.967.336 I ggml_metal_init: using embedded metal library
0.00.971.883 I ggml_metal_init: GPU name:   Apple M4
0.00.971.885 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.971.886 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.971.887 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.971.888 I ggml_metal_init: simdgroup reduction   = true
0.00.971.888 I ggml_metal_init: simdgroup matrix mul. = true
0.00.971.888 I ggml_metal_init: has residency sets    = true
0.00.971.888 I ggml_metal_init: has bfloat            = true
0.00.971.888 I ggml_metal_init: use bfloat            = true
0.00.971.889 I ggml_metal_init: hasUnifiedMemory      = true
0.00.971.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.984.385 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.986.401 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.986.409 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.986.429 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.988.189 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.988.191 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.988.191 I llama_init_from_model: graph nodes  = 967
0.00.988.192 I llama_init_from_model: graph splits = 2
0.00.988.193 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.988.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.010.387 I 
0.01.010.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.010.429 I perplexity: tokenizing the input ..
0.01.015.465 I perplexity: tokenization took 5.034 ms
0.01.015.478 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.152.375 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.153.728 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.153.744 I llama_perf_context_print:        load time =     999.61 ms
0.01.153.746 I llama_perf_context_print: prompt eval time =     136.68 ms /   128 tokens (    1.07 ms per token,   936.53 tokens per second)
0.01.153.746 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.153.747 I llama_perf_context_print:       total time =     143.36 ms /   129 tokens
0.01.154.117 I ggml_metal_free: deallocating

real	0m1.167s
user	0m0.070s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.958 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.180 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.186 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.190 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.193 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.193 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.195 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.196 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.904 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.941 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.742 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.744 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.744 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.745 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.745 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.746 I llama_model_loader: - type  f32:  194 tensors
0.00.025.746 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.746 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.747 I print_info: file format = GGUF V3 (latest)
0.00.025.747 I print_info: file type   = Q4_0
0.00.025.748 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.033.444 I load: special tokens cache size = 25
0.00.039.330 I load: token to piece cache size = 0.2984 MB
0.00.039.334 I print_info: arch             = gptneox
0.00.039.334 I print_info: vocab_only       = 0
0.00.039.334 I print_info: n_ctx_train      = 2048
0.00.039.335 I print_info: n_embd           = 2048
0.00.039.335 I print_info: n_layer          = 24
0.00.039.338 I print_info: n_head           = 16
0.00.039.338 I print_info: n_head_kv        = 16
0.00.039.339 I print_info: n_rot            = 32
0.00.039.339 I print_info: n_swa            = 0
0.00.039.339 I print_info: n_embd_head_k    = 128
0.00.039.339 I print_info: n_embd_head_v    = 128
0.00.039.340 I print_info: n_gqa            = 1
0.00.039.341 I print_info: n_embd_k_gqa     = 2048
0.00.039.341 I print_info: n_embd_v_gqa     = 2048
0.00.039.342 I print_info: f_norm_eps       = 1.0e-05
0.00.039.342 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.343 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.343 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.343 I print_info: f_logit_scale    = 0.0e+00
0.00.039.343 I print_info: n_ff             = 8192
0.00.039.344 I print_info: n_expert         = 0
0.00.039.344 I print_info: n_expert_used    = 0
0.00.039.344 I print_info: causal attn      = 1
0.00.039.344 I print_info: pooling type     = 0
0.00.039.344 I print_info: rope type        = 2
0.00.039.344 I print_info: rope scaling     = linear
0.00.039.345 I print_info: freq_base_train  = 10000.0
0.00.039.345 I print_info: freq_scale_train = 1
0.00.039.345 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.346 I print_info: rope_finetuned   = unknown
0.00.039.348 I print_info: ssm_d_conv       = 0
0.00.039.349 I print_info: ssm_d_inner      = 0
0.00.039.349 I print_info: ssm_d_state      = 0
0.00.039.349 I print_info: ssm_dt_rank      = 0
0.00.039.349 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.349 I print_info: model type       = 1.4B
0.00.039.350 I print_info: model params     = 1.41 B
0.00.039.350 I print_info: general.name     = 1.4B
0.00.039.350 I print_info: vocab type       = BPE
0.00.039.351 I print_info: n_vocab          = 50304
0.00.039.352 I print_info: n_merges         = 50009
0.00.039.353 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.353 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.353 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.353 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.354 I print_info: LF token         = 128 'Ä'
0.00.039.354 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.354 I print_info: max token length = 1024
0.00.615.084 I load_tensors: offloading 24 repeating layers to GPU
0.00.615.098 I load_tensors: offloading output layer to GPU
0.00.615.098 I load_tensors: offloaded 25/25 layers to GPU
0.00.615.130 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.615.131 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.616.224 I llama_init_from_model: n_seq_max     = 1
0.00.616.229 I llama_init_from_model: n_ctx         = 128
0.00.616.230 I llama_init_from_model: n_ctx_per_seq = 128
0.00.616.230 I llama_init_from_model: n_batch       = 128
0.00.616.231 I llama_init_from_model: n_ubatch      = 128
0.00.616.231 I llama_init_from_model: flash_attn    = 0
0.00.616.233 I llama_init_from_model: freq_base     = 10000.0
0.00.616.234 I llama_init_from_model: freq_scale    = 1
0.00.616.234 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.616.237 I ggml_metal_init: allocating
0.00.616.290 I ggml_metal_init: found device: Apple M4
0.00.616.302 I ggml_metal_init: picking default device: Apple M4
0.00.617.955 I ggml_metal_init: using embedded metal library
0.00.623.808 I ggml_metal_init: GPU name:   Apple M4
0.00.623.818 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.819 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.820 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.820 I ggml_metal_init: simdgroup reduction   = true
0.00.623.821 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.821 I ggml_metal_init: has residency sets    = true
0.00.623.821 I ggml_metal_init: has bfloat            = true
0.00.623.821 I ggml_metal_init: use bfloat            = true
0.00.623.826 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.835 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.644.228 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.647.715 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.647.719 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.647.761 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.651.101 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.651.103 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.651.104 I llama_init_from_model: graph nodes  = 967
0.00.651.104 I llama_init_from_model: graph splits = 2
0.00.651.107 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.651.107 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.537 I 
0.00.681.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.631 I perplexity: tokenizing the input ..
0.00.688.331 I perplexity: tokenization took 6.696 ms
0.00.688.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.825.519 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.826.882 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.826.897 I llama_perf_context_print:        load time =     671.57 ms
0.00.826.899 I llama_perf_context_print: prompt eval time =     136.24 ms /   128 tokens (    1.06 ms per token,   939.55 tokens per second)
0.00.826.899 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.826.900 I llama_perf_context_print:       total time =     145.36 ms /   129 tokens
0.00.827.245 I ggml_metal_free: deallocating

real	0m0.843s
user	0m0.080s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.310 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.639 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.644 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.647 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.647 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.648 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.648 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.648 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.649 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.650 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.650 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.650 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.651 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.651 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.651 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.653 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.653 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.654 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.386 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.060 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.061 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.061 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.062 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.062 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.062 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.063 I llama_model_loader: - type  f32:  194 tensors
0.00.025.063 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.063 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.064 I print_info: file format = GGUF V3 (latest)
0.00.025.064 I print_info: file type   = Q4_1
0.00.025.065 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.709 I load: special tokens cache size = 25
0.00.038.606 I load: token to piece cache size = 0.2984 MB
0.00.038.609 I print_info: arch             = gptneox
0.00.038.609 I print_info: vocab_only       = 0
0.00.038.610 I print_info: n_ctx_train      = 2048
0.00.038.610 I print_info: n_embd           = 2048
0.00.038.610 I print_info: n_layer          = 24
0.00.038.613 I print_info: n_head           = 16
0.00.038.614 I print_info: n_head_kv        = 16
0.00.038.614 I print_info: n_rot            = 32
0.00.038.614 I print_info: n_swa            = 0
0.00.038.614 I print_info: n_embd_head_k    = 128
0.00.038.614 I print_info: n_embd_head_v    = 128
0.00.038.615 I print_info: n_gqa            = 1
0.00.038.616 I print_info: n_embd_k_gqa     = 2048
0.00.038.616 I print_info: n_embd_v_gqa     = 2048
0.00.038.617 I print_info: f_norm_eps       = 1.0e-05
0.00.038.617 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.619 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.619 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.619 I print_info: f_logit_scale    = 0.0e+00
0.00.038.620 I print_info: n_ff             = 8192
0.00.038.620 I print_info: n_expert         = 0
0.00.038.620 I print_info: n_expert_used    = 0
0.00.038.622 I print_info: causal attn      = 1
0.00.038.622 I print_info: pooling type     = 0
0.00.038.622 I print_info: rope type        = 2
0.00.038.623 I print_info: rope scaling     = linear
0.00.038.623 I print_info: freq_base_train  = 10000.0
0.00.038.623 I print_info: freq_scale_train = 1
0.00.038.623 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.624 I print_info: rope_finetuned   = unknown
0.00.038.624 I print_info: ssm_d_conv       = 0
0.00.038.624 I print_info: ssm_d_inner      = 0
0.00.038.624 I print_info: ssm_d_state      = 0
0.00.038.624 I print_info: ssm_dt_rank      = 0
0.00.038.624 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.625 I print_info: model type       = 1.4B
0.00.038.625 I print_info: model params     = 1.41 B
0.00.038.625 I print_info: general.name     = 1.4B
0.00.038.625 I print_info: vocab type       = BPE
0.00.038.626 I print_info: n_vocab          = 50304
0.00.038.626 I print_info: n_merges         = 50009
0.00.038.626 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.626 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.626 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.627 I print_info: LF token         = 128 'Ä'
0.00.038.631 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.631 I print_info: max token length = 1024
0.00.719.392 I load_tensors: offloading 24 repeating layers to GPU
0.00.719.395 I load_tensors: offloading output layer to GPU
0.00.719.396 I load_tensors: offloaded 25/25 layers to GPU
0.00.719.419 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.719.421 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.720.462 I llama_init_from_model: n_seq_max     = 1
0.00.720.464 I llama_init_from_model: n_ctx         = 128
0.00.720.465 I llama_init_from_model: n_ctx_per_seq = 128
0.00.720.467 I llama_init_from_model: n_batch       = 128
0.00.720.467 I llama_init_from_model: n_ubatch      = 128
0.00.720.467 I llama_init_from_model: flash_attn    = 0
0.00.720.468 I llama_init_from_model: freq_base     = 10000.0
0.00.720.469 I llama_init_from_model: freq_scale    = 1
0.00.720.470 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.720.470 I ggml_metal_init: allocating
0.00.720.502 I ggml_metal_init: found device: Apple M4
0.00.720.510 I ggml_metal_init: picking default device: Apple M4
0.00.721.761 I ggml_metal_init: using embedded metal library
0.00.727.482 I ggml_metal_init: GPU name:   Apple M4
0.00.727.484 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.727.486 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.727.487 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.727.487 I ggml_metal_init: simdgroup reduction   = true
0.00.727.487 I ggml_metal_init: simdgroup matrix mul. = true
0.00.727.487 I ggml_metal_init: has residency sets    = true
0.00.727.488 I ggml_metal_init: has bfloat            = true
0.00.727.488 I ggml_metal_init: use bfloat            = true
0.00.727.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.727.497 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.539 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.746.742 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.746.745 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.746.771 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.749.769 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.749.770 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.749.771 I llama_init_from_model: graph nodes  = 967
0.00.749.771 I llama_init_from_model: graph splits = 2
0.00.749.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.749.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.532 I 
0.00.776.596 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.612 I perplexity: tokenizing the input ..
0.00.783.530 I perplexity: tokenization took 6.914 ms
0.00.783.553 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.921.229 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.922.468 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.922.481 I llama_perf_context_print:        load time =     767.22 ms
0.00.922.482 I llama_perf_context_print: prompt eval time =     136.73 ms /   128 tokens (    1.07 ms per token,   936.14 tokens per second)
0.00.922.483 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.922.483 I llama_perf_context_print:       total time =     145.95 ms /   129 tokens
0.00.922.862 I ggml_metal_free: deallocating

real	0m0.937s
user	0m0.077s
sys	0m0.174s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.679 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.018.527 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.529 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.530 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.530 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.532 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.533 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.534 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.536 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.537 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.537 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.192 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.909 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.910 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.910 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.911 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.911 I llama_model_loader: - type  f32:  194 tensors
0.00.026.911 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.912 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.912 I print_info: file format = GGUF V3 (latest)
0.00.026.913 I print_info: file type   = Q5_0
0.00.026.914 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.034.576 I load: special tokens cache size = 25
0.00.040.441 I load: token to piece cache size = 0.2984 MB
0.00.040.444 I print_info: arch             = gptneox
0.00.040.444 I print_info: vocab_only       = 0
0.00.040.444 I print_info: n_ctx_train      = 2048
0.00.040.444 I print_info: n_embd           = 2048
0.00.040.444 I print_info: n_layer          = 24
0.00.040.447 I print_info: n_head           = 16
0.00.040.448 I print_info: n_head_kv        = 16
0.00.040.448 I print_info: n_rot            = 32
0.00.040.448 I print_info: n_swa            = 0
0.00.040.449 I print_info: n_embd_head_k    = 128
0.00.040.449 I print_info: n_embd_head_v    = 128
0.00.040.449 I print_info: n_gqa            = 1
0.00.040.450 I print_info: n_embd_k_gqa     = 2048
0.00.040.451 I print_info: n_embd_v_gqa     = 2048
0.00.040.452 I print_info: f_norm_eps       = 1.0e-05
0.00.040.452 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.452 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.452 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.452 I print_info: f_logit_scale    = 0.0e+00
0.00.040.453 I print_info: n_ff             = 8192
0.00.040.453 I print_info: n_expert         = 0
0.00.040.453 I print_info: n_expert_used    = 0
0.00.040.453 I print_info: causal attn      = 1
0.00.040.454 I print_info: pooling type     = 0
0.00.040.454 I print_info: rope type        = 2
0.00.040.454 I print_info: rope scaling     = linear
0.00.040.454 I print_info: freq_base_train  = 10000.0
0.00.040.455 I print_info: freq_scale_train = 1
0.00.040.455 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.455 I print_info: rope_finetuned   = unknown
0.00.040.455 I print_info: ssm_d_conv       = 0
0.00.040.455 I print_info: ssm_d_inner      = 0
0.00.040.458 I print_info: ssm_d_state      = 0
0.00.040.458 I print_info: ssm_dt_rank      = 0
0.00.040.458 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.458 I print_info: model type       = 1.4B
0.00.040.459 I print_info: model params     = 1.41 B
0.00.040.459 I print_info: general.name     = 1.4B
0.00.040.459 I print_info: vocab type       = BPE
0.00.040.459 I print_info: n_vocab          = 50304
0.00.040.459 I print_info: n_merges         = 50009
0.00.040.460 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.460 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.460 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.460 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.460 I print_info: LF token         = 128 'Ä'
0.00.040.461 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.461 I print_info: max token length = 1024
0.00.738.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.738.342 I load_tensors: offloading output layer to GPU
0.00.738.344 I load_tensors: offloaded 25/25 layers to GPU
0.00.738.369 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.738.371 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.739.380 I llama_init_from_model: n_seq_max     = 1
0.00.739.382 I llama_init_from_model: n_ctx         = 128
0.00.739.382 I llama_init_from_model: n_ctx_per_seq = 128
0.00.739.383 I llama_init_from_model: n_batch       = 128
0.00.739.384 I llama_init_from_model: n_ubatch      = 128
0.00.739.384 I llama_init_from_model: flash_attn    = 0
0.00.739.385 I llama_init_from_model: freq_base     = 10000.0
0.00.739.386 I llama_init_from_model: freq_scale    = 1
0.00.739.386 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.739.387 I ggml_metal_init: allocating
0.00.739.399 I ggml_metal_init: found device: Apple M4
0.00.739.407 I ggml_metal_init: picking default device: Apple M4
0.00.740.591 I ggml_metal_init: using embedded metal library
0.00.746.608 I ggml_metal_init: GPU name:   Apple M4
0.00.746.612 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.746.613 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.746.614 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.746.614 I ggml_metal_init: simdgroup reduction   = true
0.00.746.614 I ggml_metal_init: simdgroup matrix mul. = true
0.00.746.615 I ggml_metal_init: has residency sets    = true
0.00.746.615 I ggml_metal_init: has bfloat            = true
0.00.746.615 I ggml_metal_init: use bfloat            = true
0.00.746.616 I ggml_metal_init: hasUnifiedMemory      = true
0.00.746.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.762.561 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.765.867 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.765.871 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.765.894 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.769.008 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.769.010 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.769.010 I llama_init_from_model: graph nodes  = 967
0.00.769.011 I llama_init_from_model: graph splits = 2
0.00.769.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.769.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.653 I 
0.00.795.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.749 I perplexity: tokenizing the input ..
0.00.801.608 I perplexity: tokenization took 5.856 ms
0.00.801.620 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.935.987 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.937.257 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.937.269 I llama_perf_context_print:        load time =     783.97 ms
0.00.937.270 I llama_perf_context_print: prompt eval time =     134.14 ms /   128 tokens (    1.05 ms per token,   954.22 tokens per second)
0.00.937.271 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.937.271 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.937.669 I ggml_metal_free: deallocating

real	0m0.953s
user	0m0.075s
sys	0m0.185s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.491 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.244 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.252 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.252 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.253 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.253 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.254 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.255 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.255 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.255 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.256 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.256 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.258 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.259 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.965 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.694 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.695 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.696 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.696 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.696 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.696 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.697 I llama_model_loader: - type  f32:  194 tensors
0.00.025.697 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.698 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.698 I print_info: file format = GGUF V3 (latest)
0.00.025.699 I print_info: file type   = Q5_1
0.00.025.699 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.350 I load: special tokens cache size = 25
0.00.039.165 I load: token to piece cache size = 0.2984 MB
0.00.039.168 I print_info: arch             = gptneox
0.00.039.168 I print_info: vocab_only       = 0
0.00.039.168 I print_info: n_ctx_train      = 2048
0.00.039.169 I print_info: n_embd           = 2048
0.00.039.169 I print_info: n_layer          = 24
0.00.039.171 I print_info: n_head           = 16
0.00.039.175 I print_info: n_head_kv        = 16
0.00.039.175 I print_info: n_rot            = 32
0.00.039.175 I print_info: n_swa            = 0
0.00.039.177 I print_info: n_embd_head_k    = 128
0.00.039.177 I print_info: n_embd_head_v    = 128
0.00.039.178 I print_info: n_gqa            = 1
0.00.039.178 I print_info: n_embd_k_gqa     = 2048
0.00.039.179 I print_info: n_embd_v_gqa     = 2048
0.00.039.180 I print_info: f_norm_eps       = 1.0e-05
0.00.039.180 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.180 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.181 I print_info: f_logit_scale    = 0.0e+00
0.00.039.181 I print_info: n_ff             = 8192
0.00.039.182 I print_info: n_expert         = 0
0.00.039.182 I print_info: n_expert_used    = 0
0.00.039.182 I print_info: causal attn      = 1
0.00.039.182 I print_info: pooling type     = 0
0.00.039.182 I print_info: rope type        = 2
0.00.039.182 I print_info: rope scaling     = linear
0.00.039.184 I print_info: freq_base_train  = 10000.0
0.00.039.184 I print_info: freq_scale_train = 1
0.00.039.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.185 I print_info: rope_finetuned   = unknown
0.00.039.185 I print_info: ssm_d_conv       = 0
0.00.039.185 I print_info: ssm_d_inner      = 0
0.00.039.185 I print_info: ssm_d_state      = 0
0.00.039.185 I print_info: ssm_dt_rank      = 0
0.00.039.186 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.186 I print_info: model type       = 1.4B
0.00.039.186 I print_info: model params     = 1.41 B
0.00.039.186 I print_info: general.name     = 1.4B
0.00.039.187 I print_info: vocab type       = BPE
0.00.039.187 I print_info: n_vocab          = 50304
0.00.039.187 I print_info: n_merges         = 50009
0.00.039.188 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.188 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.188 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.188 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.188 I print_info: LF token         = 128 'Ä'
0.00.039.189 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.189 I print_info: max token length = 1024
0.00.681.690 I load_tensors: offloading 24 repeating layers to GPU
0.00.681.696 I load_tensors: offloading output layer to GPU
0.00.681.697 I load_tensors: offloaded 25/25 layers to GPU
0.00.681.719 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.681.720 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.682.718 I llama_init_from_model: n_seq_max     = 1
0.00.682.720 I llama_init_from_model: n_ctx         = 128
0.00.682.721 I llama_init_from_model: n_ctx_per_seq = 128
0.00.682.721 I llama_init_from_model: n_batch       = 128
0.00.682.721 I llama_init_from_model: n_ubatch      = 128
0.00.682.722 I llama_init_from_model: flash_attn    = 0
0.00.682.723 I llama_init_from_model: freq_base     = 10000.0
0.00.682.723 I llama_init_from_model: freq_scale    = 1
0.00.682.724 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.682.725 I ggml_metal_init: allocating
0.00.682.736 I ggml_metal_init: found device: Apple M4
0.00.682.748 I ggml_metal_init: picking default device: Apple M4
0.00.683.947 I ggml_metal_init: using embedded metal library
0.00.689.872 I ggml_metal_init: GPU name:   Apple M4
0.00.689.876 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.877 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.878 I ggml_metal_init: simdgroup reduction   = true
0.00.689.879 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.879 I ggml_metal_init: has residency sets    = true
0.00.689.879 I ggml_metal_init: has bfloat            = true
0.00.689.879 I ggml_metal_init: use bfloat            = true
0.00.689.880 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.882 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.706.118 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.709.487 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.709.492 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.709.519 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.712.645 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.712.647 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.712.648 I llama_init_from_model: graph nodes  = 967
0.00.712.648 I llama_init_from_model: graph splits = 2
0.00.712.651 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.712.652 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.740.949 I 
0.00.741.015 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.033 I perplexity: tokenizing the input ..
0.00.747.959 I perplexity: tokenization took 6.924 ms
0.00.747.975 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.893.472 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.894.770 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.894.784 I llama_perf_context_print:        load time =     730.45 ms
0.00.894.785 I llama_perf_context_print: prompt eval time =     144.60 ms /   128 tokens (    1.13 ms per token,   885.21 tokens per second)
0.00.894.786 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.894.786 I llama_perf_context_print:       total time =     153.84 ms /   129 tokens
0.00.895.269 I ggml_metal_free: deallocating

real	0m0.909s
user	0m0.077s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.401 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.075 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.018.080 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.082 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.082 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.083 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.083 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.084 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.085 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.085 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.086 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.086 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.089 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.089 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.090 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.864 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.900 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.586 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.587 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.587 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.588 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.588 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.588 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.589 I llama_model_loader: - type  f32:  194 tensors
0.00.026.589 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.590 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.590 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.590 I print_info: file format = GGUF V3 (latest)
0.00.026.591 I print_info: file type   = Q2_K - Medium
0.00.026.591 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.274 I load: special tokens cache size = 25
0.00.040.080 I load: token to piece cache size = 0.2984 MB
0.00.040.082 I print_info: arch             = gptneox
0.00.040.083 I print_info: vocab_only       = 0
0.00.040.083 I print_info: n_ctx_train      = 2048
0.00.040.083 I print_info: n_embd           = 2048
0.00.040.083 I print_info: n_layer          = 24
0.00.040.086 I print_info: n_head           = 16
0.00.040.087 I print_info: n_head_kv        = 16
0.00.040.087 I print_info: n_rot            = 32
0.00.040.087 I print_info: n_swa            = 0
0.00.040.087 I print_info: n_embd_head_k    = 128
0.00.040.088 I print_info: n_embd_head_v    = 128
0.00.040.090 I print_info: n_gqa            = 1
0.00.040.091 I print_info: n_embd_k_gqa     = 2048
0.00.040.091 I print_info: n_embd_v_gqa     = 2048
0.00.040.092 I print_info: f_norm_eps       = 1.0e-05
0.00.040.092 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.092 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.093 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.093 I print_info: f_logit_scale    = 0.0e+00
0.00.040.094 I print_info: n_ff             = 8192
0.00.040.094 I print_info: n_expert         = 0
0.00.040.094 I print_info: n_expert_used    = 0
0.00.040.094 I print_info: causal attn      = 1
0.00.040.094 I print_info: pooling type     = 0
0.00.040.094 I print_info: rope type        = 2
0.00.040.094 I print_info: rope scaling     = linear
0.00.040.095 I print_info: freq_base_train  = 10000.0
0.00.040.095 I print_info: freq_scale_train = 1
0.00.040.095 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.099 I print_info: rope_finetuned   = unknown
0.00.040.099 I print_info: ssm_d_conv       = 0
0.00.040.099 I print_info: ssm_d_inner      = 0
0.00.040.099 I print_info: ssm_d_state      = 0
0.00.040.100 I print_info: ssm_dt_rank      = 0
0.00.040.100 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.100 I print_info: model type       = 1.4B
0.00.040.100 I print_info: model params     = 1.41 B
0.00.040.100 I print_info: general.name     = 1.4B
0.00.040.101 I print_info: vocab type       = BPE
0.00.040.101 I print_info: n_vocab          = 50304
0.00.040.101 I print_info: n_merges         = 50009
0.00.040.102 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.102 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.102 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.102 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.103 I print_info: LF token         = 128 'Ä'
0.00.040.103 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.103 I print_info: max token length = 1024
0.00.385.581 I load_tensors: offloading 24 repeating layers to GPU
0.00.385.593 I load_tensors: offloading output layer to GPU
0.00.385.594 I load_tensors: offloaded 25/25 layers to GPU
0.00.385.625 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.385.627 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.386.882 I llama_init_from_model: n_seq_max     = 1
0.00.386.887 I llama_init_from_model: n_ctx         = 128
0.00.386.891 I llama_init_from_model: n_ctx_per_seq = 128
0.00.386.892 I llama_init_from_model: n_batch       = 128
0.00.386.892 I llama_init_from_model: n_ubatch      = 128
0.00.386.893 I llama_init_from_model: flash_attn    = 0
0.00.386.908 I llama_init_from_model: freq_base     = 10000.0
0.00.386.909 I llama_init_from_model: freq_scale    = 1
0.00.386.910 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.386.912 I ggml_metal_init: allocating
0.00.386.979 I ggml_metal_init: found device: Apple M4
0.00.386.994 I ggml_metal_init: picking default device: Apple M4
0.00.388.851 I ggml_metal_init: using embedded metal library
0.00.395.496 I ggml_metal_init: GPU name:   Apple M4
0.00.395.501 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.395.502 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.395.503 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.395.504 I ggml_metal_init: simdgroup reduction   = true
0.00.395.504 I ggml_metal_init: simdgroup matrix mul. = true
0.00.395.504 I ggml_metal_init: has residency sets    = true
0.00.395.505 I ggml_metal_init: has bfloat            = true
0.00.395.505 I ggml_metal_init: use bfloat            = true
0.00.395.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.395.507 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.414.620 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.418.135 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.418.139 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.418.165 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.421.430 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.421.433 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.421.433 I llama_init_from_model: graph nodes  = 967
0.00.421.433 I llama_init_from_model: graph splits = 2
0.00.421.437 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.421.437 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.449.005 I 
0.00.449.087 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.449.104 I perplexity: tokenizing the input ..
0.00.455.724 I perplexity: tokenization took 6.616 ms
0.00.455.744 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.589.123 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.590.464 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.590.477 I llama_perf_context_print:        load time =     437.60 ms
0.00.590.478 I llama_perf_context_print: prompt eval time =     132.50 ms /   128 tokens (    1.04 ms per token,   966.07 tokens per second)
0.00.590.479 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.590.479 I llama_perf_context_print:       total time =     141.48 ms /   129 tokens
0.00.590.827 I ggml_metal_free: deallocating

real	0m0.605s
user	0m0.079s
sys	0m0.120s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.424 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.433 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.439 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.446 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.447 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.447 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.447 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.448 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.449 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.449 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.449 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.450 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.450 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.450 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.453 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.453 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.192 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.183 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.925 I llama_model_loader: - type  f32:  194 tensors
0.00.024.925 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.925 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.926 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.926 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.926 I print_info: file format = GGUF V3 (latest)
0.00.024.927 I print_info: file type   = Q3_K - Medium
0.00.024.928 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.577 I load: special tokens cache size = 25
0.00.038.251 I load: token to piece cache size = 0.2984 MB
0.00.038.253 I print_info: arch             = gptneox
0.00.038.254 I print_info: vocab_only       = 0
0.00.038.254 I print_info: n_ctx_train      = 2048
0.00.038.254 I print_info: n_embd           = 2048
0.00.038.254 I print_info: n_layer          = 24
0.00.038.257 I print_info: n_head           = 16
0.00.038.258 I print_info: n_head_kv        = 16
0.00.038.258 I print_info: n_rot            = 32
0.00.038.258 I print_info: n_swa            = 0
0.00.038.259 I print_info: n_embd_head_k    = 128
0.00.038.259 I print_info: n_embd_head_v    = 128
0.00.038.260 I print_info: n_gqa            = 1
0.00.038.260 I print_info: n_embd_k_gqa     = 2048
0.00.038.261 I print_info: n_embd_v_gqa     = 2048
0.00.038.262 I print_info: f_norm_eps       = 1.0e-05
0.00.038.262 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.262 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.262 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.263 I print_info: f_logit_scale    = 0.0e+00
0.00.038.263 I print_info: n_ff             = 8192
0.00.038.264 I print_info: n_expert         = 0
0.00.038.264 I print_info: n_expert_used    = 0
0.00.038.264 I print_info: causal attn      = 1
0.00.038.264 I print_info: pooling type     = 0
0.00.038.264 I print_info: rope type        = 2
0.00.038.265 I print_info: rope scaling     = linear
0.00.038.265 I print_info: freq_base_train  = 10000.0
0.00.038.265 I print_info: freq_scale_train = 1
0.00.038.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.266 I print_info: rope_finetuned   = unknown
0.00.038.266 I print_info: ssm_d_conv       = 0
0.00.038.266 I print_info: ssm_d_inner      = 0
0.00.038.266 I print_info: ssm_d_state      = 0
0.00.038.266 I print_info: ssm_dt_rank      = 0
0.00.038.266 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.267 I print_info: model type       = 1.4B
0.00.038.267 I print_info: model params     = 1.41 B
0.00.038.267 I print_info: general.name     = 1.4B
0.00.038.268 I print_info: vocab type       = BPE
0.00.038.268 I print_info: n_vocab          = 50304
0.00.038.268 I print_info: n_merges         = 50009
0.00.038.268 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.268 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.269 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.269 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.269 I print_info: LF token         = 128 'Ä'
0.00.038.269 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.269 I print_info: max token length = 1024
0.00.486.779 I load_tensors: offloading 24 repeating layers to GPU
0.00.486.792 I load_tensors: offloading output layer to GPU
0.00.486.793 I load_tensors: offloaded 25/25 layers to GPU
0.00.486.820 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.486.822 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.488.102 I llama_init_from_model: n_seq_max     = 1
0.00.488.107 I llama_init_from_model: n_ctx         = 128
0.00.488.107 I llama_init_from_model: n_ctx_per_seq = 128
0.00.488.108 I llama_init_from_model: n_batch       = 128
0.00.488.108 I llama_init_from_model: n_ubatch      = 128
0.00.488.109 I llama_init_from_model: flash_attn    = 0
0.00.488.111 I llama_init_from_model: freq_base     = 10000.0
0.00.488.111 I llama_init_from_model: freq_scale    = 1
0.00.488.112 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.488.119 I ggml_metal_init: allocating
0.00.488.189 I ggml_metal_init: found device: Apple M4
0.00.488.203 I ggml_metal_init: picking default device: Apple M4
0.00.489.887 I ggml_metal_init: using embedded metal library
0.00.495.449 I ggml_metal_init: GPU name:   Apple M4
0.00.495.451 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.495.452 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.495.452 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.495.453 I ggml_metal_init: simdgroup reduction   = true
0.00.495.453 I ggml_metal_init: simdgroup matrix mul. = true
0.00.495.453 I ggml_metal_init: has residency sets    = true
0.00.495.453 I ggml_metal_init: has bfloat            = true
0.00.495.453 I ggml_metal_init: use bfloat            = true
0.00.495.454 I ggml_metal_init: hasUnifiedMemory      = true
0.00.495.454 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.514.801 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.518.365 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.518.375 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.518.428 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.521.676 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.521.678 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.521.678 I llama_init_from_model: graph nodes  = 967
0.00.521.679 I llama_init_from_model: graph splits = 2
0.00.521.682 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.521.682 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.825 I 
0.00.552.863 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.552.873 I perplexity: tokenizing the input ..
0.00.558.068 I perplexity: tokenization took 5.194 ms
0.00.558.083 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.703.049 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.704.352 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.704.368 I llama_perf_context_print:        load time =     543.40 ms
0.00.704.369 I llama_perf_context_print: prompt eval time =     144.74 ms /   128 tokens (    1.13 ms per token,   884.33 tokens per second)
0.00.704.369 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.704.372 I llama_perf_context_print:       total time =     151.54 ms /   129 tokens
0.00.704.725 I ggml_metal_free: deallocating

real	0m0.717s
user	0m0.077s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.699 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.659 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.663 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.664 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.665 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.665 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.666 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.668 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.669 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.674 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.675 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.677 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.677 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.677 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.435 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.447 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.205 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.206 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.207 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.208 I llama_model_loader: - type  f32:  194 tensors
0.00.026.208 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.208 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.208 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.209 I print_info: file format = GGUF V3 (latest)
0.00.026.209 I print_info: file type   = Q4_K - Medium
0.00.026.210 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.188 I load: special tokens cache size = 25
0.00.040.204 I load: token to piece cache size = 0.2984 MB
0.00.040.207 I print_info: arch             = gptneox
0.00.040.207 I print_info: vocab_only       = 0
0.00.040.208 I print_info: n_ctx_train      = 2048
0.00.040.208 I print_info: n_embd           = 2048
0.00.040.208 I print_info: n_layer          = 24
0.00.040.210 I print_info: n_head           = 16
0.00.040.211 I print_info: n_head_kv        = 16
0.00.040.211 I print_info: n_rot            = 32
0.00.040.211 I print_info: n_swa            = 0
0.00.040.212 I print_info: n_embd_head_k    = 128
0.00.040.214 I print_info: n_embd_head_v    = 128
0.00.040.214 I print_info: n_gqa            = 1
0.00.040.215 I print_info: n_embd_k_gqa     = 2048
0.00.040.216 I print_info: n_embd_v_gqa     = 2048
0.00.040.216 I print_info: f_norm_eps       = 1.0e-05
0.00.040.217 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.217 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.217 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.217 I print_info: f_logit_scale    = 0.0e+00
0.00.040.218 I print_info: n_ff             = 8192
0.00.040.218 I print_info: n_expert         = 0
0.00.040.218 I print_info: n_expert_used    = 0
0.00.040.218 I print_info: causal attn      = 1
0.00.040.218 I print_info: pooling type     = 0
0.00.040.218 I print_info: rope type        = 2
0.00.040.220 I print_info: rope scaling     = linear
0.00.040.220 I print_info: freq_base_train  = 10000.0
0.00.040.220 I print_info: freq_scale_train = 1
0.00.040.221 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.221 I print_info: rope_finetuned   = unknown
0.00.040.221 I print_info: ssm_d_conv       = 0
0.00.040.222 I print_info: ssm_d_inner      = 0
0.00.040.222 I print_info: ssm_d_state      = 0
0.00.040.222 I print_info: ssm_dt_rank      = 0
0.00.040.223 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.223 I print_info: model type       = 1.4B
0.00.040.223 I print_info: model params     = 1.41 B
0.00.040.223 I print_info: general.name     = 1.4B
0.00.040.224 I print_info: vocab type       = BPE
0.00.040.225 I print_info: n_vocab          = 50304
0.00.040.225 I print_info: n_merges         = 50009
0.00.040.226 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.226 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.226 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.226 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.226 I print_info: LF token         = 128 'Ä'
0.00.040.227 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.227 I print_info: max token length = 1024
0.00.600.373 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.380 I load_tensors: offloading output layer to GPU
0.00.600.381 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.411 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.600.413 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.601.536 I llama_init_from_model: n_seq_max     = 1
0.00.601.540 I llama_init_from_model: n_ctx         = 128
0.00.601.541 I llama_init_from_model: n_ctx_per_seq = 128
0.00.601.541 I llama_init_from_model: n_batch       = 128
0.00.601.542 I llama_init_from_model: n_ubatch      = 128
0.00.601.542 I llama_init_from_model: flash_attn    = 0
0.00.601.544 I llama_init_from_model: freq_base     = 10000.0
0.00.601.545 I llama_init_from_model: freq_scale    = 1
0.00.601.545 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.601.547 I ggml_metal_init: allocating
0.00.601.602 I ggml_metal_init: found device: Apple M4
0.00.601.616 I ggml_metal_init: picking default device: Apple M4
0.00.603.296 I ggml_metal_init: using embedded metal library
0.00.609.997 I ggml_metal_init: GPU name:   Apple M4
0.00.610.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.003 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.004 I ggml_metal_init: simdgroup reduction   = true
0.00.610.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.004 I ggml_metal_init: has residency sets    = true
0.00.610.005 I ggml_metal_init: has bfloat            = true
0.00.610.005 I ggml_metal_init: use bfloat            = true
0.00.610.006 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.627.001 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.630.369 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.630.374 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.630.404 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.507 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.509 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.510 I llama_init_from_model: graph nodes  = 967
0.00.633.510 I llama_init_from_model: graph splits = 2
0.00.633.513 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.513 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.785 I 
0.00.663.859 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.880 I perplexity: tokenizing the input ..
0.00.670.662 I perplexity: tokenization took 6.779 ms
0.00.670.683 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.542 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.816.822 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.816.838 I llama_perf_context_print:        load time =     653.08 ms
0.00.816.839 I llama_perf_context_print: prompt eval time =     143.94 ms /   128 tokens (    1.12 ms per token,   889.26 tokens per second)
0.00.816.839 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.816.840 I llama_perf_context_print:       total time =     153.06 ms /   129 tokens
0.00.817.301 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.079s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.547 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.421 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.428 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.428 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.429 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.429 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.429 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.430 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.430 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.431 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.431 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.434 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.434 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.434 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.436 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.436 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.437 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.159 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.137 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.844 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.846 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.846 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.846 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.847 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.847 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.847 I llama_model_loader: - type  f32:  194 tensors
0.00.025.847 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.848 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.848 I print_info: file format = GGUF V3 (latest)
0.00.025.849 I print_info: file type   = Q5_K - Medium
0.00.025.849 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.467 I load: special tokens cache size = 25
0.00.039.407 I load: token to piece cache size = 0.2984 MB
0.00.039.410 I print_info: arch             = gptneox
0.00.039.410 I print_info: vocab_only       = 0
0.00.039.410 I print_info: n_ctx_train      = 2048
0.00.039.411 I print_info: n_embd           = 2048
0.00.039.411 I print_info: n_layer          = 24
0.00.039.413 I print_info: n_head           = 16
0.00.039.414 I print_info: n_head_kv        = 16
0.00.039.414 I print_info: n_rot            = 32
0.00.039.414 I print_info: n_swa            = 0
0.00.039.415 I print_info: n_embd_head_k    = 128
0.00.039.415 I print_info: n_embd_head_v    = 128
0.00.039.416 I print_info: n_gqa            = 1
0.00.039.417 I print_info: n_embd_k_gqa     = 2048
0.00.039.418 I print_info: n_embd_v_gqa     = 2048
0.00.039.418 I print_info: f_norm_eps       = 1.0e-05
0.00.039.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.419 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.419 I print_info: f_logit_scale    = 0.0e+00
0.00.039.420 I print_info: n_ff             = 8192
0.00.039.420 I print_info: n_expert         = 0
0.00.039.420 I print_info: n_expert_used    = 0
0.00.039.420 I print_info: causal attn      = 1
0.00.039.421 I print_info: pooling type     = 0
0.00.039.422 I print_info: rope type        = 2
0.00.039.422 I print_info: rope scaling     = linear
0.00.039.423 I print_info: freq_base_train  = 10000.0
0.00.039.423 I print_info: freq_scale_train = 1
0.00.039.423 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.423 I print_info: rope_finetuned   = unknown
0.00.039.424 I print_info: ssm_d_conv       = 0
0.00.039.424 I print_info: ssm_d_inner      = 0
0.00.039.424 I print_info: ssm_d_state      = 0
0.00.039.424 I print_info: ssm_dt_rank      = 0
0.00.039.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.426 I print_info: model type       = 1.4B
0.00.039.426 I print_info: model params     = 1.41 B
0.00.039.426 I print_info: general.name     = 1.4B
0.00.039.427 I print_info: vocab type       = BPE
0.00.039.427 I print_info: n_vocab          = 50304
0.00.039.427 I print_info: n_merges         = 50009
0.00.039.427 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.427 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: LF token         = 128 'Ä'
0.00.039.428 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: max token length = 1024
0.00.663.049 I load_tensors: offloading 24 repeating layers to GPU
0.00.663.052 I load_tensors: offloading output layer to GPU
0.00.663.053 I load_tensors: offloaded 25/25 layers to GPU
0.00.663.072 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.663.073 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.664.041 I llama_init_from_model: n_seq_max     = 1
0.00.664.043 I llama_init_from_model: n_ctx         = 128
0.00.664.044 I llama_init_from_model: n_ctx_per_seq = 128
0.00.664.045 I llama_init_from_model: n_batch       = 128
0.00.664.046 I llama_init_from_model: n_ubatch      = 128
0.00.664.046 I llama_init_from_model: flash_attn    = 0
0.00.664.047 I llama_init_from_model: freq_base     = 10000.0
0.00.664.047 I llama_init_from_model: freq_scale    = 1
0.00.664.048 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.664.049 I ggml_metal_init: allocating
0.00.664.076 I ggml_metal_init: found device: Apple M4
0.00.664.082 I ggml_metal_init: picking default device: Apple M4
0.00.665.277 I ggml_metal_init: using embedded metal library
0.00.670.475 I ggml_metal_init: GPU name:   Apple M4
0.00.670.478 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.670.479 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.670.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.670.481 I ggml_metal_init: simdgroup reduction   = true
0.00.670.481 I ggml_metal_init: simdgroup matrix mul. = true
0.00.670.481 I ggml_metal_init: has residency sets    = true
0.00.670.481 I ggml_metal_init: has bfloat            = true
0.00.670.481 I ggml_metal_init: use bfloat            = true
0.00.670.482 I ggml_metal_init: hasUnifiedMemory      = true
0.00.670.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.685.313 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.688.534 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.688.540 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.688.569 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.691.430 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.691.431 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.691.432 I llama_init_from_model: graph nodes  = 967
0.00.691.432 I llama_init_from_model: graph splits = 2
0.00.691.435 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.691.435 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.724.383 I 
0.00.724.452 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.724.471 I perplexity: tokenizing the input ..
0.00.730.436 I perplexity: tokenization took 5.964 ms
0.00.730.447 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.870.373 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.871.662 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.871.676 I llama_perf_context_print:        load time =     713.83 ms
0.00.871.677 I llama_perf_context_print: prompt eval time =     139.70 ms /   128 tokens (    1.09 ms per token,   916.22 tokens per second)
0.00.871.677 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.871.678 I llama_perf_context_print:       total time =     147.30 ms /   129 tokens
0.00.872.138 I ggml_metal_free: deallocating

real	0m0.886s
user	0m0.073s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.942 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.718 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.018.723 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.724 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.725 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.727 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.727 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.728 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.728 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.729 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.729 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.730 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.730 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.730 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.732 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.732 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.733 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.571 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.628 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.465 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.466 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.466 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.466 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.467 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.467 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.027.468 I llama_model_loader: - type  f32:  194 tensors
0.00.027.468 I llama_model_loader: - type q6_K:   98 tensors
0.00.027.468 I print_info: file format = GGUF V3 (latest)
0.00.027.469 I print_info: file type   = Q6_K
0.00.027.474 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.035.437 I load: special tokens cache size = 25
0.00.041.525 I load: token to piece cache size = 0.2984 MB
0.00.041.528 I print_info: arch             = gptneox
0.00.041.528 I print_info: vocab_only       = 0
0.00.041.529 I print_info: n_ctx_train      = 2048
0.00.041.529 I print_info: n_embd           = 2048
0.00.041.529 I print_info: n_layer          = 24
0.00.041.532 I print_info: n_head           = 16
0.00.041.533 I print_info: n_head_kv        = 16
0.00.041.533 I print_info: n_rot            = 32
0.00.041.533 I print_info: n_swa            = 0
0.00.041.533 I print_info: n_embd_head_k    = 128
0.00.041.533 I print_info: n_embd_head_v    = 128
0.00.041.534 I print_info: n_gqa            = 1
0.00.041.535 I print_info: n_embd_k_gqa     = 2048
0.00.041.536 I print_info: n_embd_v_gqa     = 2048
0.00.041.538 I print_info: f_norm_eps       = 1.0e-05
0.00.041.538 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.538 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.539 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.539 I print_info: f_logit_scale    = 0.0e+00
0.00.041.541 I print_info: n_ff             = 8192
0.00.041.541 I print_info: n_expert         = 0
0.00.041.541 I print_info: n_expert_used    = 0
0.00.041.541 I print_info: causal attn      = 1
0.00.041.542 I print_info: pooling type     = 0
0.00.041.542 I print_info: rope type        = 2
0.00.041.542 I print_info: rope scaling     = linear
0.00.041.542 I print_info: freq_base_train  = 10000.0
0.00.041.543 I print_info: freq_scale_train = 1
0.00.041.543 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.543 I print_info: rope_finetuned   = unknown
0.00.041.543 I print_info: ssm_d_conv       = 0
0.00.041.543 I print_info: ssm_d_inner      = 0
0.00.041.544 I print_info: ssm_d_state      = 0
0.00.041.545 I print_info: ssm_dt_rank      = 0
0.00.041.545 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.546 I print_info: model type       = 1.4B
0.00.041.546 I print_info: model params     = 1.41 B
0.00.041.546 I print_info: general.name     = 1.4B
0.00.041.547 I print_info: vocab type       = BPE
0.00.041.547 I print_info: n_vocab          = 50304
0.00.041.547 I print_info: n_merges         = 50009
0.00.041.547 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.547 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.548 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.548 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.548 I print_info: LF token         = 128 'Ä'
0.00.041.548 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.549 I print_info: max token length = 1024
0.00.223.255 I load_tensors: offloading 24 repeating layers to GPU
0.00.223.263 I load_tensors: offloading output layer to GPU
0.00.223.263 I load_tensors: offloaded 25/25 layers to GPU
0.00.223.286 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.223.289 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.224.126 I llama_init_from_model: n_seq_max     = 1
0.00.224.127 I llama_init_from_model: n_ctx         = 128
0.00.224.128 I llama_init_from_model: n_ctx_per_seq = 128
0.00.224.128 I llama_init_from_model: n_batch       = 128
0.00.224.129 I llama_init_from_model: n_ubatch      = 128
0.00.224.129 I llama_init_from_model: flash_attn    = 0
0.00.224.130 I llama_init_from_model: freq_base     = 10000.0
0.00.224.131 I llama_init_from_model: freq_scale    = 1
0.00.224.131 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.224.133 I ggml_metal_init: allocating
0.00.224.175 I ggml_metal_init: found device: Apple M4
0.00.224.188 I ggml_metal_init: picking default device: Apple M4
0.00.225.309 I ggml_metal_init: using embedded metal library
0.00.230.167 I ggml_metal_init: GPU name:   Apple M4
0.00.230.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.230.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.230.173 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.230.173 I ggml_metal_init: simdgroup reduction   = true
0.00.230.174 I ggml_metal_init: simdgroup matrix mul. = true
0.00.230.174 I ggml_metal_init: has residency sets    = true
0.00.230.174 I ggml_metal_init: has bfloat            = true
0.00.230.174 I ggml_metal_init: use bfloat            = true
0.00.230.175 I ggml_metal_init: hasUnifiedMemory      = true
0.00.230.176 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.243.291 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.245.177 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.245.179 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.245.194 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.247.089 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.247.091 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.247.091 I llama_init_from_model: graph nodes  = 967
0.00.247.091 I llama_init_from_model: graph splits = 2
0.00.247.092 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.247.093 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.274.753 I 
0.00.274.782 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.274.798 I perplexity: tokenizing the input ..
0.00.279.859 I perplexity: tokenization took 5.065 ms
0.00.279.870 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.419.124 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.420.399 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.420.414 I llama_perf_context_print:        load time =     262.81 ms
0.00.420.415 I llama_perf_context_print: prompt eval time =     139.03 ms /   128 tokens (    1.09 ms per token,   920.67 tokens per second)
0.00.420.415 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.420.416 I llama_perf_context_print:       total time =     145.66 ms /   129 tokens
0.00.420.861 I ggml_metal_free: deallocating

real	0m0.437s
user	0m0.071s
sys	0m0.088s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.432 I build: 4577 (d0c08040) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.292 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.415 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.422 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.427 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.428 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.429 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.429 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.431 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.431 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.432 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.432 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.433 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.433 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.436 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.438 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.438 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.291 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.293 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.293 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.294 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.295 I llama_model_loader: - type  f32:  194 tensors
0.00.050.295 I llama_model_loader: - type  f16:   98 tensors
0.00.050.296 I print_info: file format = GGUF V3 (latest)
0.00.050.296 I print_info: file type   = all F32 (guessed)
0.00.050.297 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.061.773 I load: special tokens cache size = 25
0.00.069.145 I load: token to piece cache size = 0.2984 MB
0.00.069.149 I print_info: arch             = gptneox
0.00.069.150 I print_info: vocab_only       = 0
0.00.069.150 I print_info: n_ctx_train      = 2048
0.00.069.150 I print_info: n_embd           = 2048
0.00.069.150 I print_info: n_layer          = 24
0.00.069.153 I print_info: n_head           = 16
0.00.069.153 I print_info: n_head_kv        = 16
0.00.069.153 I print_info: n_rot            = 32
0.00.069.153 I print_info: n_swa            = 0
0.00.069.154 I print_info: n_embd_head_k    = 128
0.00.069.154 I print_info: n_embd_head_v    = 128
0.00.069.155 I print_info: n_gqa            = 1
0.00.069.155 I print_info: n_embd_k_gqa     = 2048
0.00.069.156 I print_info: n_embd_v_gqa     = 2048
0.00.069.157 I print_info: f_norm_eps       = 1.0e-05
0.00.069.157 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.158 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.159 I print_info: f_logit_scale    = 0.0e+00
0.00.069.159 I print_info: n_ff             = 8192
0.00.069.159 I print_info: n_expert         = 0
0.00.069.159 I print_info: n_expert_used    = 0
0.00.069.160 I print_info: causal attn      = 1
0.00.069.160 I print_info: pooling type     = 0
0.00.069.160 I print_info: rope type        = 2
0.00.069.160 I print_info: rope scaling     = linear
0.00.069.160 I print_info: freq_base_train  = 10000.0
0.00.069.161 I print_info: freq_scale_train = 1
0.00.069.161 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.161 I print_info: rope_finetuned   = unknown
0.00.069.161 I print_info: ssm_d_conv       = 0
0.00.069.161 I print_info: ssm_d_inner      = 0
0.00.069.162 I print_info: ssm_d_state      = 0
0.00.069.162 I print_info: ssm_dt_rank      = 0
0.00.069.162 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.162 I print_info: model type       = 1.4B
0.00.069.162 I print_info: model params     = 1.41 B
0.00.069.163 I print_info: general.name     = 1.4B
0.00.069.163 I print_info: vocab type       = BPE
0.00.069.163 I print_info: n_vocab          = 50304
0.00.069.163 I print_info: n_merges         = 50009
0.00.069.164 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.164 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.164 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.164 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.165 I print_info: LF token         = 128 'Ä'
0.00.069.165 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.165 I print_info: max token length = 1024
0.01.432.871 I load_tensors: offloading 24 repeating layers to GPU
0.01.432.874 I load_tensors: offloading output layer to GPU
0.01.432.875 I load_tensors: offloaded 25/25 layers to GPU
0.01.432.898 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.432.899 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.433.496 I llama_init_from_model: n_seq_max     = 1
0.01.433.497 I llama_init_from_model: n_ctx         = 128
0.01.433.497 I llama_init_from_model: n_ctx_per_seq = 128
0.01.433.497 I llama_init_from_model: n_batch       = 128
0.01.433.498 I llama_init_from_model: n_ubatch      = 128
0.01.433.498 I llama_init_from_model: flash_attn    = 0
0.01.433.498 I llama_init_from_model: freq_base     = 10000.0
0.01.433.498 I llama_init_from_model: freq_scale    = 1
0.01.433.499 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.433.500 I ggml_metal_init: allocating
0.01.433.548 I ggml_metal_init: found device: Apple M4
0.01.433.554 I ggml_metal_init: picking default device: Apple M4
0.01.434.466 I ggml_metal_init: using embedded metal library
0.01.437.922 I ggml_metal_init: GPU name:   Apple M4
0.01.437.924 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.437.925 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.437.925 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.437.925 I ggml_metal_init: simdgroup reduction   = true
0.01.437.925 I ggml_metal_init: simdgroup matrix mul. = true
0.01.437.926 I ggml_metal_init: has residency sets    = true
0.01.437.926 I ggml_metal_init: has bfloat            = true
0.01.437.926 I ggml_metal_init: use bfloat            = true
0.01.437.926 I ggml_metal_init: hasUnifiedMemory      = true
0.01.437.927 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.447.855 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.449.537 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.449.539 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.449.553 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.451.016 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.451.017 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.451.018 I llama_init_from_model: graph nodes  = 967
0.01.451.018 I llama_init_from_model: graph splits = 2
0.01.451.019 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.451.019 I 
0.01.451.053 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.451.054 I compute_imatrix: tokenizing the input ..
0.01.454.547 I compute_imatrix: tokenization took 3.492 ms
0.01.454.549 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.718.444 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.720.937 I llama_perf_context_print:        load time =    1697.15 ms
0.01.720.937 I llama_perf_context_print: prompt eval time =     262.84 ms /   128 tokens (    2.05 ms per token,   486.99 tokens per second)
0.01.720.938 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.720.939 I llama_perf_context_print:       total time =    1699.64 ms /   129 tokens
0.01.721.447 I ggml_metal_free: deallocating

real	0m1.913s
user	0m0.125s
sys	0m0.351s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4577 (d0c08040)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14db075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14db07d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14db082b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14db08860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14db08e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14db093c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14db09970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14db09f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14db0a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14db0a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14db0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14db0b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14db0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14db0c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14db0ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14db0d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14db0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14db0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14db0eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14db0f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14db0fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14db10140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14db10860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14db11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14db11820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14db11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14db120f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14db12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14db132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14db13560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14db13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14db13cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14db14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14db14a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14db14d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14db151f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14db15690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14db15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14db15fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14db16470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14db16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14db16db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14db17250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14db176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14db179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14db17fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14db185d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14db18ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14db19500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14db19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14db1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14db1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14db1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14db1b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14db1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14db1bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14db1c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14db1c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14db1cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14db1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14db1d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14db1dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14db1e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14db1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14db1ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14db1ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14db1f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14db1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14db1fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14db201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14db20640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14db20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14db20f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14db214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14db21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14db21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14db224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14db22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14db22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14db234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14db23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14db23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14db244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14db249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14db24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14db25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14db259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14db25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14db26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14db269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14db26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14db27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14db279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14db27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14db28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14db289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14db28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14db18be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14db29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14db29b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14db2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14db2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14db2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14db2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14db2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14db2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14db2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14db2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14db2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14db2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14db2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14db2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14db2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14db2e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14db2e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14db2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14db2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14db2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14db2fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14db30090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14db30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14db309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14db30e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14db31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14db317b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14db31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14db320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14db32590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14db32a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14db32ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14db33370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14db33810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14db33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14db34150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14db345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14db34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14db34f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14db353d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14db35870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14db35d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14db361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14db36650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14db36af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14db36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14db37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14db378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14db37d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14db38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14db386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14db38b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14db38ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14db39490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14db39930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14db39dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14db3a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14db3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14db3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14db3b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14db3b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14db3b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14db3be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14db3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14db3c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14db3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14db3d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14db3d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14db3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14db3de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14db3e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14db3e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14db3ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14db3f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14db3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14db3fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14db3fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14db40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14db40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14db40cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14db41170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14db41610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14db41ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14db41f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14db423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14db42890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14db42d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14db431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14db43670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14db43b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14db43fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14db44450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14db448f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14db44d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14db45230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14db45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14db45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14db46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14db46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14db46a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14db47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14db47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14db47c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14db48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14db488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14db48bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14db491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14db497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14db49fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14db4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14db4a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14db4ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14db4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14db4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14db4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14db4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14db4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14db4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14db4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14db4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14db4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14db4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14db4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14db4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14db4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14db4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14db4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14db50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14db50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14db50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14db514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14db51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14db51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14db524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14db52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14db52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14db534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14db53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14db53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14db544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14db54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14db54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14db554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14db55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14db55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14db564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14db569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14db56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14db57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14db579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14db57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14db58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14db589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14db58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14db59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14db599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14db59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14db5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14db5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14db5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14db5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14db5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14db5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14db5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14db5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14db5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14db5d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14db5d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14db5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14db5e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14db5e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14db5ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14db5f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14db5f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14db5fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14db5ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14db603d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14db60870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14db60d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14db611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14db61650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14db61af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14db61f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14db62430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14db62980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14db630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14db637c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14db63ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14db64600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14db648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14db650b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14db65370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14db65980 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.757.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.757.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14da05bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14da06030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14da064a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14da06910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14da06d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14da071f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14da07660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14da07ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14da07f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14da083b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14da08820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14da08f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14da09a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14da0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14da0a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14da0b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14da0b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14da0bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14da0c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14da0cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14da0d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14da0dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14da0e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14da0ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14da0f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14da0f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14da0f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14da0fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14da0ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14da10410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14da10880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14da10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14da11220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14da114e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14da11950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14da11dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14da12230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14da126a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14da12b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14da12f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14da133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14da13860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14da13cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14da14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14da145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14da14a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14da14e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14da15300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14da15770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14da15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14da16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14da164c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14da16930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14da16da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14da17210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14da17680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14da17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14da180f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14da18560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14da189d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14da18e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14da192b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14da19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14da19b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14da1a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14da1a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14da1a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14da1ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14da1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14da1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14da1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14da1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14da1c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14da1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14da1cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14da1d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14da1d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14da1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14da1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14da1e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14da1e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14da1eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14da1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14da1f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14da1f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14da1fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14da201a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14da20610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14da20a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14da20ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14da21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14da217d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14da21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14da220b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14da22520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14da22990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14da22e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14da23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14da236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14da23b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14da23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14da24430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14da248a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14da24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14da25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14da255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14da25a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14da25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14da26340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14da267b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14da26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14da27090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14da27500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14da27970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14da27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14da28250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14da286c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14da28b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14da28fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14da29410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14da29880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14da29cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14da2a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14da2a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14da2aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14da2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14da2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14da2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14da2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14da2c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14da2c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14da2c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14da2cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14da2d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14da2d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14da2db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14da2df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14da2e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14da2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14da2ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14da2f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14da2f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14da2fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14da2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14da30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14da30770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14da30be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14da31050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14da314c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14da31930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14da31da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14da32210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14da32680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14da32af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14da32f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14da333d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14da33840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14da33cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14da34120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14da34590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14da34a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14da34e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14da352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14da35750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14da35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14da36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14da36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14da36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14da371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14da37650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14da37ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14da37f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14da383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14da38810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14da38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14da390f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14da39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14da399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14da39e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14da3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14da3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14da3ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14da3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14da3b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14da3b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14da3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14da3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14da3c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14da3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14da3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14da3d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14da3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14da3dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14da3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14da3e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14da3e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14da3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14da3f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14da3f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14da3fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14da3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14da40450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14da409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14da40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14da41330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14da417a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14da41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14da42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14da425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14da42ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14da43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14da438e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14da43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14da44460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14da44a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14da44fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14da455a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14da45b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14da46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14da466e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14da46ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14da47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14da47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14da47de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14da483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14da48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14da48f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14da494e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14da49aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14da4a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14da4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14da4abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14da4b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14da4b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14da4bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14da4c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14da4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14da4ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14da4d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14da4d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14da4dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14da4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14da4eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14da4f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14da4f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14da4fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14da50220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14da507e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14da50da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14da51360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14da51920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14da51ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14da524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14da52a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14da53020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14da535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14da53ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14da54160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14da54720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14da54ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14da552a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14da55860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14da55e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14da563e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14da569a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14da56f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14da57520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14da57ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14da57fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14da584e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14da589e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14da58ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14da593e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14da598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14da59de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14da5a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14da5a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14da5ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14da5b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14da5b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14da5bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14da5c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14da5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14da5cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14da5d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14da5de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14da5e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14da5e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14da5f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14da5f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14da5f8d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14dc06480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14dc068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14dc06d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14dc071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14dc09fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14dc0a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14dc0ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14dc0b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14dc0b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14dc0bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14dc0c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14dc0c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14dc0d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14dc0d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14dc0e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14dc0e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14dc0eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14dc0f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14dc0fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14dc104d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14dc10bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14dc11310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14dc11a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14dc12150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14dc12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14dc12b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14dc13140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14dc13750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14dc13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14dc14550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14dc149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14dc14cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14dc15540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14dc15a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14dc15d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14dc161e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14dc16680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14dc16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14dc16fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14dc17460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14dc17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14dc17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14dc18240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14dc186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14dc189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14dc18fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14dc195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14dc19bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14dc1a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14dc1a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14dc1ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14dc1b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14dc1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14dc1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14dc1c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14dc1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14dc1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14dc1d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14dc1da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14dc1e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14dc1e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14dc1eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14dc1f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14dc1f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14dc1f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14dc1fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14dc20280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14dc20720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14dc20bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14dc21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14dc21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14dc219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14dc21e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14dc22390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14dc228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14dc22e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14dc23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14dc238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14dc23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14dc24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14dc248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14dc24e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14dc25360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14dc258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14dc25e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14dc26350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14dc268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14dc26df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14dc27340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14dc27890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14dc27de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14dc28330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14dc28880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14dc28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14dc29320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14dc29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14dc29dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14dc2a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14dc2a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14dc2adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14dc2b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14dc2b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14dc2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14dc2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14dc2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14dc2cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14dc2d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14dc2d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14dc2dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14dc2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14dc2e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14dc2ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14dc2f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14dc2f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14dc2fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14dc300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14dc30540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14dc309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14dc30e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14dc31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14dc317c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14dc31c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14dc32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14dc325a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14dc32a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14dc32ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14dc33380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14dc33820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14dc33cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14dc34160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14dc34600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14dc34aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14dc34f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14dc353e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14dc35880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14dc35d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14dc361c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14dc36660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14dc36b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14dc36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14dc37440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14dc378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14dc37d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14dc38220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14dc386c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14dc38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14dc39000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14dc394a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14dc39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14dc39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14dc3a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14dc3a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14dc3abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14dc3b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14dc3b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14dc3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14dc3be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14dc3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14dc3c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14dc3cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14dc3d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14dc3d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14dc3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14dc3dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14dc3e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14dc3e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14dc3ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14dc3f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14dc3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14dc3fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14dc3ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14dc403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14dc40840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14dc40ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14dc41180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14dc41620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14dc41ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14dc41f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14dc42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14dc428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14dc42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14dc431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14dc43680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14dc43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14dc43fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14dc44460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14dc44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14dc44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14dc45240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14dc456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14dc45b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14dc46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14dc464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14dc46a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14dc46f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14dc474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14dc47a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14dc47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14dc482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14dc488e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14dc48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14dc496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14dc49b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14dc49e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14dc4a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14dc4aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14dc4b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14dc4b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14dc4bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14dc4c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14dc4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14dc4cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14dc4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14dc4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14dc4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14dc4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14dc4e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14dc4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14dc4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14dc4f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14dc4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14dc50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14dc507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14dc50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14dc51240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14dc51790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14dc51ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14dc52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14dc52780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14dc52cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14dc53220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14dc53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14dc53cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14dc54210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14dc54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14dc54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14dc55200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14dc55750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14dc55ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14dc561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14dc56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14dc56c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14dc571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14dc57730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14dc57c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14dc581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14dc58720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14dc58c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14dc591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14dc59710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14dc59c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14dc5a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14dc5a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14dc5ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14dc5b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14dc5b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14dc5bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14dc5c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14dc5c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14dc5cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14dc5d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14dc5d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14dc5dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14dc5e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14dc5e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14dc5ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14dc5f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14dc5f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14dc5faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14dc5ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14dc603e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14dc60880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14dc60d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14dc611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14dc61660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14dc61b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14dc61fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14dc62440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14dc628e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14dc62d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14dc63220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14dc636c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14dc63c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14dc64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14dc64a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14dc65170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14dc65890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14dc65b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14dc66340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14dc66600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14dc66c10 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.806s
user	0m0.280s
sys	0m0.322s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4577 (d0c08040)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123710580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123710c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123711240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1237117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123711da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123712350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123712900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123712eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123713460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123713960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123713e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123714360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123714e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123715630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123715e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123716560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123716c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1237173a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123717ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123718290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1237189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1237190d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1237197f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12371a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12371a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12371aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12371b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12371bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12371c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12371c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12371c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12371cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12371d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12371da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12371dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12371e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12371e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12371eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12371ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12371f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12371f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12371fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1237201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123720940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123721560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123721e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123722490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123722aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1237230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1237236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123723cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1237242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123724ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123724f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123725410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1237256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123725ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1237264d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123726790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123726c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1237270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123727570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123727a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123727eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123728350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1237287f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123728c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123729130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1237295d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123729a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12372a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12372a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12372af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12372b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12372b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12372bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12372c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12372c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12372cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12372d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12372d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12372ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12372e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12372e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12372eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12372f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12372f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12372feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123730400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123730950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123730ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1237313f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123731940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123731e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123721b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123732300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123732ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123733000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123733aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123733ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123734540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123734a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123734fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123735530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123735a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123735fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123736520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123736a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123736fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123737460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123737900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123738240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1237386e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123738b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123739020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1237394c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123739960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123739e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12373a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12373a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12373abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12373b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12373b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12373b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12373be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12373c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12373c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12373cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12373d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12373d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12373da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12373dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12373e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12373e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12373eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12373f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12373f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12373fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12373ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1237403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123740860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123740d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1237411a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123741640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123741ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123741f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123742420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1237428c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123742d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1237436a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123743b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123743fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123744480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123744920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123744dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123745260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123745700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123745ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123746040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1237464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123746980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123746e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1237472c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123747760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123747c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1237480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123748540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1237489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123749320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1237497c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12374a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12374a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12374aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12374aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12374b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12374b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12374bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12374c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12374c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12374caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12374cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12374d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12374d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12374dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12374e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12374e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12374ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12374f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12374f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12374f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12374ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1237505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1237513e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123751880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123751b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123752150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123752760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123752f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1237533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123753890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123753d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1237544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123754a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123754f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1237554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123755a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123755f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1237564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123756a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123756f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1237574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123757a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1237584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1237589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123758f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123759490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1237599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123759f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12375a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12375a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12375af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12375b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12375b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12375bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12375c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12375c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12375cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12375d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12375d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12375def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12375e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12375e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12375eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12375f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12375f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12375fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123760970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123760ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123761410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123761960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123761eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123762400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123762950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123762ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1237633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123763940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123763e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1237643e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123764930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123764e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1237653d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123765920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123765e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1237663c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123766910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123766e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123767300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1237677a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123767c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1237680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123768580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123768a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123768ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123769360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123769800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123769ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12376a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12376a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12376aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12376af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12376b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12376b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12376c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12376c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12376ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12376d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12376d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12376e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12376e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12376e910 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.095.024 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.028 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12376e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12374fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1237508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123723980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123723370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123725990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123752410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12371ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123721820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123722140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123722750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123720c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123722d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123719d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123725fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1237325c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12376db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12371cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12371d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123752a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123750eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12371b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12371b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12371b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12376ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12376f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12376f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12376f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12376f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12376fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12376fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1237700b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123770370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123770630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1237708f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123770bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123770e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123771130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1237713f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1237716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123771970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123771c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123771ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1237721b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123772470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123772730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1237729f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123772cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123772f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123773230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1237734f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1237737b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123773a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123773d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123773ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1237742b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123774570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123774830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123774af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123774db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123775070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123775330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1237755f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1237758b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123775b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123775e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1237760f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1237763b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123776670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123776930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123776bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123776eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123777170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123777430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1237776f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1237779b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123777c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123777f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1237781f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1237784b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123778770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123778a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123778cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123778fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123779270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123779530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1237797f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123779ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123779d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12377a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12377a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12377a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12377a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12377ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12377adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12377b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12377b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12377b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12377b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12377bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12377be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12377c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12377c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12377c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12377c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12377cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12377cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12377d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12377d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12377d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12377d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12377dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12377df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12377e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12377e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12377e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12377ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12377ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12377eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12377f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12377f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12377f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12377faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12377fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123780070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123780330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127e04280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127e046f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123605670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123605b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123605fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123606420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123606890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123606d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123607170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1236075e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123607a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123607ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123608330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1236087a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x127e04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123608dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123609230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1236096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123609b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123609f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12360a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12360a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127e04e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127e053b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127e05820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127e05c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127e06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127e06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127e069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127e06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127e072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127e07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127e07ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127e08010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127e08480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127e088f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127e08d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127e091d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127e09640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127e0a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127e0a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127e0a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127e0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127e0b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127e0b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127e0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127e0c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127e0c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127e0ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127e0d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127e0d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127e0dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127e0e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127e0e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12360acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12360b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12360b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12360ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12360bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12360c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12360c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12360cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12360d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12360d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12360d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12360ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12360e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12360e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12360eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12360ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12360f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12360f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12360fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123610130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1236105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123610a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123610e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1236112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123611760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123611bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123612040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1236124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123612920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1236134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123613760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123613a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123613e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123614300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123614770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123614be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1236154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123615930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123615da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123616210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123616680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123616af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123616f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1236173d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123617840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123617cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123618120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123618590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123618a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123618e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1236192e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123619750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123619bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12361a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12361a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12361a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12361ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12361b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12361b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12361bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12361bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12361c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12361c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12361cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12361d100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12361d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12361d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12361de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12361e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12361e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12361eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12361f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12361f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12361f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12361fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1236201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123620640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123620ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123620f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123621390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123621800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123621c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1236220e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123622550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1236229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123622e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1236232a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123623710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123623b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123623ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123624460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1236248d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123624d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1236251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123625620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123625a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123625f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123626370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1236267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123626c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1236270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123627b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123628250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123628970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123629090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1236297c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123629dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12362a3d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1237805f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1237808b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123780c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123780f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1237811d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123781490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123781750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123781a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123781cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123781f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123782250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123782510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123782ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1237830b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1237836e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1237839a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123783c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123783f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1237841e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1237844a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123784760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123784a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123784ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123784fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123785260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123785520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1237857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123785aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123785d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123786020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1237862e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1237865a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123786860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123786b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123786de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1237870a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123787360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123787620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1237878e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123787ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123787e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123788120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1237883e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1237886a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123788960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123788c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123788ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1237891a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123789460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123789720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1237899e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123789ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123789f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12378a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12378a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12378a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12378aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12378ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12378afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12378b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12378b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12378b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12378bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12378bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12378c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12378c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12378c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12378c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12378cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12378ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12378d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12378d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12378d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12378d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12378dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12378dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12378e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12378e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12378e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12378e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12378ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12378ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12378f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12378f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12378f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12378fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12378fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12378ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123790260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123790520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1237907e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123790aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123790d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123791020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1237912e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1237915a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123791860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123791b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123791de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1237920a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123792360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123792620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1237928e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123792ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123792e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123793120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1237933e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1237936a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123793960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123793c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123793ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1237941a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123794460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123794720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1237949e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123794ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123794f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123795220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1237954e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1237957a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123795a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123795d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123795fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1237962a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123796560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123796820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123796ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123796da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123797060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123797320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1237975e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1237978a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123797b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123797e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1237980e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1237983a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123798660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123798920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123798be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123798ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123799160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123799420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1237996e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1237999a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123799c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123799f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12379a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12379a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12379a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12379aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12379ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12379afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12379b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12379b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12379b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12379baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12379bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12379c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12379c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12379c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12379c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12379cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12379cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12379d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12379d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12379d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12379d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12379dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12379de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12379e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12379e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12379e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12379e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12379ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12379eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12379f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12379f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12379f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12379f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12379fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12379ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1237a0220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1237a04e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1237a07a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1237a0a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1237a0d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1237a0fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1237a12a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1237a1560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1237a1820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1237a1ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1237a1da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1237a2060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1237a2320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1237a25e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1237a28a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1237a2b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1237a30a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1237a35e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1237a3b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1237a4060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1237a4320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1237a4720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1237a49e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1237a4ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1237a5110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1237a5580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1237a5a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1237a5f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1237a64a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1237a7010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1237a72d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1237a7890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1237a7e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1237a8410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1237a89d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1237a8f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1237a9550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1237a9b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1237aa0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1237aa690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1237aac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1237ab210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1237ab7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1237abd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1237ac350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1237ac910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1237aced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1237ad490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1237ada50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1237ae010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1237ae5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1237aeb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1237af150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1237af710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1237afcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1237b0290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1237b0850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1237b0e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1237b13d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1237b1990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1237b1f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1237b2510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1237b2ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1237b3090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1237b3650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1237b3c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1237b41d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1237b4790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1237b4d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1237b5310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1237b58d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1237b5e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1237b6450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1237b6a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1237b6fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1237b7590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1237b7b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1237b8110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1237b86d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1237b8c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1237b9250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1237b9810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1237b9dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1237ba390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1237ba950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1237baf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1237bb4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1237bb9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1237bbed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1237bc3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1237bc8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1237bcdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1237bd2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1237bd7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1237bdcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1237be1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1237be6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1237bebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1237bf0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1237bf5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1237bfad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1237bffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1237c09e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1237c1100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1237c1820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1237c1f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1237c2200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1237c29f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1237c2cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1237c32c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.928s
user	0m0.235s
sys	0m0.157s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
