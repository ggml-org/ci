### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.39 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.76 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.97 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.28 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.22 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.18 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.23 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.27 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  177.58 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.90 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.85 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.33 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.26 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 220.10 sec*proc (27 tests)

Total Test time (real) = 220.11 sec

real	3m40.140s
user	7m33.155s
sys	0m6.227s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.34 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.29 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.21 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.92 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.20 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.20 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.17 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   28.23 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.27 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.07 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.06 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  49.94 sec*proc (27 tests)

Total Test time (real) =  49.95 sec

real	0m49.956s
user	1m12.022s
sys	0m5.921s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.066 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.017.817 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.082 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.089 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.092 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.093 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.094 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.095 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.109 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.110 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.111 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.111 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.112 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.115 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.116 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.116 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.117 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.118 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.118 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.119 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.027.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.028.549 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.551 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.028.552 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.028.552 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.028.553 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.028.553 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.028.553 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.028.554 I llama_model_loader: - type  f32:  124 tensors
0.00.028.555 I llama_model_loader: - type  f16:   73 tensors
0.00.033.085 I llm_load_vocab: special tokens cache size = 5
0.00.035.232 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.035.235 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.035.236 I llm_load_print_meta: arch             = bert
0.00.035.236 I llm_load_print_meta: vocab type       = WPM
0.00.035.237 I llm_load_print_meta: n_vocab          = 30522
0.00.035.237 I llm_load_print_meta: n_merges         = 0
0.00.035.237 I llm_load_print_meta: vocab_only       = 0
0.00.035.237 I llm_load_print_meta: n_ctx_train      = 512
0.00.035.237 I llm_load_print_meta: n_embd           = 384
0.00.035.238 I llm_load_print_meta: n_layer          = 12
0.00.035.241 I llm_load_print_meta: n_head           = 12
0.00.035.242 I llm_load_print_meta: n_head_kv        = 12
0.00.035.242 I llm_load_print_meta: n_rot            = 32
0.00.035.242 I llm_load_print_meta: n_swa            = 0
0.00.035.242 I llm_load_print_meta: n_embd_head_k    = 32
0.00.035.242 I llm_load_print_meta: n_embd_head_v    = 32
0.00.035.243 I llm_load_print_meta: n_gqa            = 1
0.00.035.244 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.035.245 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.035.246 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.035.246 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.035.249 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.035.250 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.035.250 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.035.251 I llm_load_print_meta: n_ff             = 1536
0.00.035.251 I llm_load_print_meta: n_expert         = 0
0.00.035.251 I llm_load_print_meta: n_expert_used    = 0
0.00.035.251 I llm_load_print_meta: causal attn      = 0
0.00.035.253 I llm_load_print_meta: pooling type     = 2
0.00.035.253 I llm_load_print_meta: rope type        = 2
0.00.035.254 I llm_load_print_meta: rope scaling     = linear
0.00.035.254 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.035.255 I llm_load_print_meta: freq_scale_train = 1
0.00.035.255 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.035.255 I llm_load_print_meta: rope_finetuned   = unknown
0.00.035.255 I llm_load_print_meta: ssm_d_conv       = 0
0.00.035.256 I llm_load_print_meta: ssm_d_inner      = 0
0.00.035.256 I llm_load_print_meta: ssm_d_state      = 0
0.00.035.256 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.035.256 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.035.269 I llm_load_print_meta: model type       = 33M
0.00.035.269 I llm_load_print_meta: model ftype      = F16
0.00.035.269 I llm_load_print_meta: model params     = 33.21 M
0.00.035.270 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.035.271 I llm_load_print_meta: general.name     = Bge Small
0.00.035.271 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.035.272 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.035.273 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.035.273 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.035.273 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.035.273 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.035.274 I llm_load_print_meta: max token length = 21
0.00.037.382 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.037.382 I llm_load_tensors: offloading output layer to GPU
0.00.037.383 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.037.407 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.409 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.980 I llama_new_context_with_model: n_seq_max     = 1
0.00.037.981 I llama_new_context_with_model: n_ctx         = 512
0.00.037.982 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.037.982 I llama_new_context_with_model: n_batch       = 2048
0.00.037.982 I llama_new_context_with_model: n_ubatch      = 2048
0.00.037.982 I llama_new_context_with_model: flash_attn    = 0
0.00.037.983 I llama_new_context_with_model: freq_base     = 10000.0
0.00.037.983 I llama_new_context_with_model: freq_scale    = 1
0.00.037.984 I ggml_metal_init: allocating
0.00.037.994 I ggml_metal_init: found device: Apple M4
0.00.037.998 I ggml_metal_init: picking default device: Apple M4
0.00.038.814 I ggml_metal_init: using embedded metal library
0.00.042.450 I ggml_metal_init: GPU name:   Apple M4
0.00.042.453 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.453 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.454 I ggml_metal_init: simdgroup reduction   = true
0.00.042.454 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.454 I ggml_metal_init: has bfloat            = true
0.00.042.455 I ggml_metal_init: use bfloat            = true
0.00.042.455 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.053.532 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.053.534 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.053.536 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.054.334 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.054.335 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.054.336 I llama_new_context_with_model: graph nodes  = 429
0.00.054.336 I llama_new_context_with_model: graph splits = 2
0.00.054.357 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.974 I 
0.00.061.002 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.061.712 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.066.447 I llama_perf_context_print:        load time =      43.15 ms
0.00.066.448 I llama_perf_context_print: prompt eval time =       4.58 ms /     9 tokens (    0.51 ms per token,  1964.21 tokens per second)
0.00.066.449 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.066.449 I llama_perf_context_print:       total time =       5.47 ms /    10 tokens
0.00.066.585 I ggml_metal_free: deallocating

real	0m0.244s
user	0m0.048s
sys	0m0.029s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.563 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.013.717 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.013.720 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.013.721 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.013.722 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.013.722 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.013.724 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.013.725 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.013.732 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.013.732 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.013.732 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.013.733 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.013.733 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.013.736 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.013.736 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.013.736 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.013.737 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.013.737 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.013.737 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.013.738 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.016.317 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.017.038 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.017.040 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.017.040 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.017.040 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.017.041 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.017.041 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.017.041 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.017.042 I llama_model_loader: - type  f32:  124 tensors
0.00.017.042 I llama_model_loader: - type q8_0:   73 tensors
0.00.019.620 I llm_load_vocab: special tokens cache size = 5
0.00.020.957 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.020.960 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.020.960 I llm_load_print_meta: arch             = bert
0.00.020.960 I llm_load_print_meta: vocab type       = WPM
0.00.020.961 I llm_load_print_meta: n_vocab          = 30522
0.00.020.961 I llm_load_print_meta: n_merges         = 0
0.00.020.961 I llm_load_print_meta: vocab_only       = 0
0.00.020.961 I llm_load_print_meta: n_ctx_train      = 512
0.00.020.961 I llm_load_print_meta: n_embd           = 384
0.00.020.961 I llm_load_print_meta: n_layer          = 12
0.00.020.964 I llm_load_print_meta: n_head           = 12
0.00.020.965 I llm_load_print_meta: n_head_kv        = 12
0.00.020.965 I llm_load_print_meta: n_rot            = 32
0.00.020.965 I llm_load_print_meta: n_swa            = 0
0.00.020.965 I llm_load_print_meta: n_embd_head_k    = 32
0.00.020.965 I llm_load_print_meta: n_embd_head_v    = 32
0.00.020.966 I llm_load_print_meta: n_gqa            = 1
0.00.020.966 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.020.967 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.020.967 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.020.968 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.020.968 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.020.968 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.020.968 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.020.969 I llm_load_print_meta: n_ff             = 1536
0.00.020.969 I llm_load_print_meta: n_expert         = 0
0.00.020.969 I llm_load_print_meta: n_expert_used    = 0
0.00.020.969 I llm_load_print_meta: causal attn      = 0
0.00.020.970 I llm_load_print_meta: pooling type     = 2
0.00.020.970 I llm_load_print_meta: rope type        = 2
0.00.020.970 I llm_load_print_meta: rope scaling     = linear
0.00.020.971 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.020.973 I llm_load_print_meta: freq_scale_train = 1
0.00.020.973 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.020.973 I llm_load_print_meta: rope_finetuned   = unknown
0.00.020.973 I llm_load_print_meta: ssm_d_conv       = 0
0.00.020.973 I llm_load_print_meta: ssm_d_inner      = 0
0.00.020.974 I llm_load_print_meta: ssm_d_state      = 0
0.00.020.974 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.020.974 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.020.980 I llm_load_print_meta: model type       = 33M
0.00.020.981 I llm_load_print_meta: model ftype      = Q8_0
0.00.020.981 I llm_load_print_meta: model params     = 33.21 M
0.00.020.982 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.020.982 I llm_load_print_meta: general.name     = Bge Small
0.00.020.982 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.020.982 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.020.982 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.020.983 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.020.983 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.020.983 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.020.983 I llm_load_print_meta: max token length = 21
0.00.022.345 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.022.345 I llm_load_tensors: offloading output layer to GPU
0.00.022.345 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.022.352 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.022.353 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.022.714 I llama_new_context_with_model: n_seq_max     = 1
0.00.022.714 I llama_new_context_with_model: n_ctx         = 512
0.00.022.714 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.022.715 I llama_new_context_with_model: n_batch       = 2048
0.00.022.715 I llama_new_context_with_model: n_ubatch      = 2048
0.00.022.715 I llama_new_context_with_model: flash_attn    = 0
0.00.022.715 I llama_new_context_with_model: freq_base     = 10000.0
0.00.022.716 I llama_new_context_with_model: freq_scale    = 1
0.00.022.716 I ggml_metal_init: allocating
0.00.022.719 I ggml_metal_init: found device: Apple M4
0.00.022.720 I ggml_metal_init: picking default device: Apple M4
0.00.023.312 I ggml_metal_init: using embedded metal library
0.00.025.492 I ggml_metal_init: GPU name:   Apple M4
0.00.025.493 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.025.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.025.494 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.025.494 I ggml_metal_init: simdgroup reduction   = true
0.00.025.495 I ggml_metal_init: simdgroup matrix mul. = true
0.00.025.495 I ggml_metal_init: has bfloat            = true
0.00.025.495 I ggml_metal_init: use bfloat            = true
0.00.025.495 I ggml_metal_init: hasUnifiedMemory      = true
0.00.025.496 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.187 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.190 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.191 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.748 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.749 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.749 I llama_new_context_with_model: graph nodes  = 429
0.00.034.750 I llama_new_context_with_model: graph splits = 2
0.00.034.762 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.083 I 
0.00.039.107 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.039.642 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.891 I llama_perf_context_print:        load time =      27.52 ms
0.00.043.892 I llama_perf_context_print: prompt eval time =       4.10 ms /     9 tokens (    0.46 ms per token,  2192.98 tokens per second)
0.00.043.892 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.893 I llama_perf_context_print:       total time =       4.81 ms /    10 tokens
0.00.044.070 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.028s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.149 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.089 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.679 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.028.684 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.686 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.028.687 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.688 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.028.689 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.028.689 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.028.715 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.028.717 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.028.717 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.028.718 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.028.718 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.028.722 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.028.730 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.028.730 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.028.731 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.732 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.036.029 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.038.182 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.474 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.042.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.042.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.042.477 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.042.477 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.042.478 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.042.478 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.042.478 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.042.479 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.042.479 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.042.479 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.042.480 I llama_model_loader: - type  f32:   41 tensors
0.00.042.480 I llama_model_loader: - type  f16:   29 tensors
0.00.060.497 W llm_load_vocab: empty token at index 5
0.00.065.013 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.066.348 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.066.390 I llm_load_vocab: special tokens cache size = 5
0.00.326.222 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.326.229 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.326.229 I llm_load_print_meta: arch             = jina-bert-v2
0.00.326.230 I llm_load_print_meta: vocab type       = BPE
0.00.326.230 I llm_load_print_meta: n_vocab          = 61056
0.00.326.233 I llm_load_print_meta: n_merges         = 39382
0.00.326.233 I llm_load_print_meta: vocab_only       = 0
0.00.326.233 I llm_load_print_meta: n_ctx_train      = 8192
0.00.326.233 I llm_load_print_meta: n_embd           = 384
0.00.326.233 I llm_load_print_meta: n_layer          = 4
0.00.326.240 I llm_load_print_meta: n_head           = 12
0.00.326.242 I llm_load_print_meta: n_head_kv        = 12
0.00.326.243 I llm_load_print_meta: n_rot            = 32
0.00.326.243 I llm_load_print_meta: n_swa            = 0
0.00.326.243 I llm_load_print_meta: n_embd_head_k    = 32
0.00.326.243 I llm_load_print_meta: n_embd_head_v    = 32
0.00.326.244 I llm_load_print_meta: n_gqa            = 1
0.00.326.245 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.326.245 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.326.247 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.326.247 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.326.247 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.326.252 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.326.252 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.326.253 I llm_load_print_meta: n_ff             = 1536
0.00.326.253 I llm_load_print_meta: n_expert         = 0
0.00.326.254 I llm_load_print_meta: n_expert_used    = 0
0.00.326.254 I llm_load_print_meta: causal attn      = 0
0.00.326.255 I llm_load_print_meta: pooling type     = -1
0.00.326.255 I llm_load_print_meta: rope type        = -1
0.00.326.255 I llm_load_print_meta: rope scaling     = linear
0.00.326.256 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.326.256 I llm_load_print_meta: freq_scale_train = 1
0.00.326.256 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.326.256 I llm_load_print_meta: rope_finetuned   = unknown
0.00.326.256 I llm_load_print_meta: ssm_d_conv       = 0
0.00.326.257 I llm_load_print_meta: ssm_d_inner      = 0
0.00.326.257 I llm_load_print_meta: ssm_d_state      = 0
0.00.326.257 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.326.257 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.326.282 I llm_load_print_meta: model type       = 33M
0.00.326.283 I llm_load_print_meta: model ftype      = F16
0.00.326.283 I llm_load_print_meta: model params     = 32.90 M
0.00.326.284 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.326.284 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.326.284 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.326.284 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.326.284 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.326.284 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.326.285 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.326.285 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.326.285 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.326.285 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.326.285 I llm_load_print_meta: max token length = 45
0.00.327.596 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.327.596 I llm_load_tensors: offloading output layer to GPU
0.00.327.596 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.327.619 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.327.620 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.328.340 I llama_new_context_with_model: n_seq_max     = 1
0.00.328.341 I llama_new_context_with_model: n_ctx         = 8192
0.00.328.341 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.328.341 I llama_new_context_with_model: n_batch       = 2048
0.00.328.341 I llama_new_context_with_model: n_ubatch      = 2048
0.00.328.342 I llama_new_context_with_model: flash_attn    = 0
0.00.328.342 I llama_new_context_with_model: freq_base     = 10000.0
0.00.328.342 I llama_new_context_with_model: freq_scale    = 1
0.00.328.343 I ggml_metal_init: allocating
0.00.328.346 I ggml_metal_init: found device: Apple M4
0.00.328.348 I ggml_metal_init: picking default device: Apple M4
0.00.329.101 I ggml_metal_init: using embedded metal library
0.00.331.572 I ggml_metal_init: GPU name:   Apple M4
0.00.331.573 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.331.573 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.331.574 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.331.574 I ggml_metal_init: simdgroup reduction   = true
0.00.331.574 I ggml_metal_init: simdgroup matrix mul. = true
0.00.331.574 I ggml_metal_init: has bfloat            = true
0.00.331.574 I ggml_metal_init: use bfloat            = true
0.00.331.575 I ggml_metal_init: hasUnifiedMemory      = true
0.00.331.575 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.970 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.341.972 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.341.973 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.342.528 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.342.530 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.342.530 I llama_new_context_with_model: graph nodes  = 154
0.00.342.530 I llama_new_context_with_model: graph splits = 2
0.00.342.548 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.357.734 I 
0.00.357.766 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.357.919 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.357.920 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.357.923 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.357.923 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.357.927 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.357.927 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.482 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.298 I llama_perf_context_print:        load time =     338.64 ms
0.00.362.299 I llama_perf_context_print: prompt eval time =       3.81 ms /    62 tokens (    0.06 ms per token, 16285.79 tokens per second)
0.00.362.300 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.302 I llama_perf_context_print:       total time =       4.56 ms /    63 tokens
0.00.362.519 I ggml_metal_free: deallocating

real	0m1.050s
user	0m0.350s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.151 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.313 I main: llama backend init
0.00.000.325 I main: load the model and apply lora adapter, if any
0.00.031.974 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.206 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.225 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.228 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.229 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.230 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.230 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.254 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.258 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.259 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.260 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.260 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.261 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.262 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.265 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.261 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.062.264 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.264 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.265 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.265 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.266 I llama_model_loader: - type  f32:  194 tensors
0.00.062.267 I llama_model_loader: - type  f16:   98 tensors
0.00.092.318 I llm_load_vocab: special tokens cache size = 25
0.00.099.179 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.099.181 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.099.182 I llm_load_print_meta: arch             = gptneox
0.00.099.182 I llm_load_print_meta: vocab type       = BPE
0.00.099.182 I llm_load_print_meta: n_vocab          = 50304
0.00.099.182 I llm_load_print_meta: n_merges         = 50009
0.00.099.183 I llm_load_print_meta: vocab_only       = 0
0.00.099.183 I llm_load_print_meta: n_ctx_train      = 2048
0.00.099.183 I llm_load_print_meta: n_embd           = 2048
0.00.099.183 I llm_load_print_meta: n_layer          = 24
0.00.099.186 I llm_load_print_meta: n_head           = 16
0.00.099.186 I llm_load_print_meta: n_head_kv        = 16
0.00.099.187 I llm_load_print_meta: n_rot            = 32
0.00.099.187 I llm_load_print_meta: n_swa            = 0
0.00.099.187 I llm_load_print_meta: n_embd_head_k    = 128
0.00.099.187 I llm_load_print_meta: n_embd_head_v    = 128
0.00.099.190 I llm_load_print_meta: n_gqa            = 1
0.00.099.191 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.099.191 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.099.192 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.099.192 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.099.193 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.099.194 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.099.194 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.099.195 I llm_load_print_meta: n_ff             = 8192
0.00.099.195 I llm_load_print_meta: n_expert         = 0
0.00.099.195 I llm_load_print_meta: n_expert_used    = 0
0.00.099.195 I llm_load_print_meta: causal attn      = 1
0.00.099.195 I llm_load_print_meta: pooling type     = 0
0.00.099.196 I llm_load_print_meta: rope type        = 2
0.00.099.197 I llm_load_print_meta: rope scaling     = linear
0.00.099.197 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.099.198 I llm_load_print_meta: freq_scale_train = 1
0.00.099.198 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.099.198 I llm_load_print_meta: rope_finetuned   = unknown
0.00.099.198 I llm_load_print_meta: ssm_d_conv       = 0
0.00.099.198 I llm_load_print_meta: ssm_d_inner      = 0
0.00.099.198 I llm_load_print_meta: ssm_d_state      = 0
0.00.099.198 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.099.198 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.099.211 I llm_load_print_meta: model type       = 1.4B
0.00.099.211 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.099.212 I llm_load_print_meta: model params     = 1.41 B
0.00.099.219 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.099.221 I llm_load_print_meta: general.name     = 1.4B
0.00.099.222 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.099.222 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.099.222 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.099.222 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.099.223 I llm_load_print_meta: LF token         = 128 ''
0.00.099.223 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.099.223 I llm_load_print_meta: max token length = 1024
0.00.101.922 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.923 I llm_load_tensors: offloading output layer to GPU
0.00.101.923 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.940 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.941 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.867 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.868 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.868 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.868 I llama_new_context_with_model: n_batch       = 2048
0.00.102.868 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.868 I llama_new_context_with_model: flash_attn    = 0
0.00.102.869 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.869 I llama_new_context_with_model: freq_scale    = 1
0.00.102.869 I ggml_metal_init: allocating
0.00.102.872 I ggml_metal_init: found device: Apple M4
0.00.102.874 I ggml_metal_init: picking default device: Apple M4
0.00.103.524 I ggml_metal_init: using embedded metal library
0.00.113.669 I ggml_metal_init: GPU name:   Apple M4
0.00.113.671 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.113.671 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.113.672 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.113.672 I ggml_metal_init: simdgroup reduction   = true
0.00.113.672 I ggml_metal_init: simdgroup matrix mul. = true
0.00.113.672 I ggml_metal_init: has bfloat            = true
0.00.113.672 I ggml_metal_init: use bfloat            = true
0.00.113.673 I ggml_metal_init: hasUnifiedMemory      = true
0.00.113.673 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.149.989 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.149.995 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.150.016 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.150.971 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.150.972 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.150.972 I llama_new_context_with_model: graph nodes  = 967
0.00.150.973 I llama_new_context_with_model: graph splits = 2
0.00.150.995 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.226.358 I main: llama threadpool init, n_threads = 4
0.00.226.397 I 
0.00.226.426 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.226.427 I 
0.00.226.510 I sampler seed: 1234
0.00.226.514 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.226.537 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.226.539 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.226.539 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.067.166 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54074.64 tokens per second)
0.02.067.167 I llama_perf_context_print:        load time =     194.37 ms
0.02.067.167 I llama_perf_context_print: prompt eval time =      37.27 ms /     7 tokens (    5.32 ms per token,   187.80 tokens per second)
0.02.067.168 I llama_perf_context_print:        eval time =    1800.35 ms /    63 runs   (   28.58 ms per token,    34.99 tokens per second)
0.02.067.168 I llama_perf_context_print:       total time =    1840.81 ms /    70 tokens
0.02.067.349 I ggml_metal_free: deallocating

real	0m2.372s
user	0m0.142s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.556 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.067 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.672 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.685 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.686 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.703 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.707 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.709 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.712 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.865 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.885 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.541 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.054.543 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.544 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.544 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.545 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.546 I llama_model_loader: - type  f32:  194 tensors
0.00.054.546 I llama_model_loader: - type  f16:   98 tensors
0.00.083.513 I llm_load_vocab: special tokens cache size = 25
0.00.090.213 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.216 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.216 I llm_load_print_meta: arch             = gptneox
0.00.090.217 I llm_load_print_meta: vocab type       = BPE
0.00.090.217 I llm_load_print_meta: n_vocab          = 50304
0.00.090.217 I llm_load_print_meta: n_merges         = 50009
0.00.090.217 I llm_load_print_meta: vocab_only       = 0
0.00.090.217 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.217 I llm_load_print_meta: n_embd           = 2048
0.00.090.217 I llm_load_print_meta: n_layer          = 24
0.00.090.221 I llm_load_print_meta: n_head           = 16
0.00.090.222 I llm_load_print_meta: n_head_kv        = 16
0.00.090.222 I llm_load_print_meta: n_rot            = 32
0.00.090.222 I llm_load_print_meta: n_swa            = 0
0.00.090.222 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.222 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.223 I llm_load_print_meta: n_gqa            = 1
0.00.090.224 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.225 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.225 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.226 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.226 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.226 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.226 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.227 I llm_load_print_meta: n_ff             = 8192
0.00.090.229 I llm_load_print_meta: n_expert         = 0
0.00.090.230 I llm_load_print_meta: n_expert_used    = 0
0.00.090.230 I llm_load_print_meta: causal attn      = 1
0.00.090.230 I llm_load_print_meta: pooling type     = 0
0.00.090.230 I llm_load_print_meta: rope type        = 2
0.00.090.230 I llm_load_print_meta: rope scaling     = linear
0.00.090.231 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.231 I llm_load_print_meta: freq_scale_train = 1
0.00.090.231 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.231 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.232 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.232 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.232 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.232 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.232 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.244 I llm_load_print_meta: model type       = 1.4B
0.00.090.245 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.245 I llm_load_print_meta: model params     = 1.41 B
0.00.090.246 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.246 I llm_load_print_meta: general.name     = 1.4B
0.00.090.248 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.248 I llm_load_print_meta: LF token         = 128 ''
0.00.090.249 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.249 I llm_load_print_meta: max token length = 1024
0.00.092.723 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.723 I llm_load_tensors: offloading output layer to GPU
0.00.092.723 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.733 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.734 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.002 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.003 I llama_new_context_with_model: n_ctx         = 128
0.00.094.003 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.003 I llama_new_context_with_model: n_batch       = 128
0.00.094.003 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.003 I llama_new_context_with_model: flash_attn    = 0
0.00.094.004 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.004 I llama_new_context_with_model: freq_scale    = 1
0.00.094.004 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.005 I ggml_metal_init: allocating
0.00.094.007 I ggml_metal_init: found device: Apple M4
0.00.094.009 I ggml_metal_init: picking default device: Apple M4
0.00.094.595 I ggml_metal_init: using embedded metal library
0.00.096.702 I ggml_metal_init: GPU name:   Apple M4
0.00.096.704 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.704 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.705 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.705 I ggml_metal_init: simdgroup reduction   = true
0.00.096.705 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.705 I ggml_metal_init: has bfloat            = true
0.00.096.705 I ggml_metal_init: use bfloat            = true
0.00.096.706 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.708 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.430 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.432 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.445 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.351 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.352 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.352 I llama_new_context_with_model: graph nodes  = 967
0.00.107.352 I llama_new_context_with_model: graph splits = 2
0.00.107.364 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.829.182 I 
0.00.829.224 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.829.278 I perplexity: tokenizing the input ..
0.00.840.850 I perplexity: tokenization took 11.571 ms
0.00.840.870 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.961.784 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.00.964.047 I Final estimate: PPL = 10.1498 +/- 3.22650

0.00.964.072 I llama_perf_context_print:        load time =     804.10 ms
0.00.964.073 I llama_perf_context_print: prompt eval time =     120.64 ms /   128 tokens (    0.94 ms per token,  1060.97 tokens per second)
0.00.964.074 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.964.075 I llama_perf_context_print:       total time =     134.89 ms /   129 tokens
0.00.964.653 I ggml_metal_free: deallocating

real	0m1.155s
user	0m0.122s
sys	0m0.194s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.749 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.026.228 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.233 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.236 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.238 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.239 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.253 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.253 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.253 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.254 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.254 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.254 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.255 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.256 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.257 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.257 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.160 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.267 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.478 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.480 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.480 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.481 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.482 I llama_model_loader: - type  f32:  194 tensors
0.00.035.482 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.207 I llm_load_vocab: special tokens cache size = 25
0.00.066.700 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.704 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.704 I llm_load_print_meta: arch             = gptneox
0.00.066.704 I llm_load_print_meta: vocab type       = BPE
0.00.066.705 I llm_load_print_meta: n_vocab          = 50304
0.00.066.705 I llm_load_print_meta: n_merges         = 50009
0.00.066.705 I llm_load_print_meta: vocab_only       = 0
0.00.066.705 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.705 I llm_load_print_meta: n_embd           = 2048
0.00.066.708 I llm_load_print_meta: n_layer          = 24
0.00.066.712 I llm_load_print_meta: n_head           = 16
0.00.066.714 I llm_load_print_meta: n_head_kv        = 16
0.00.066.714 I llm_load_print_meta: n_rot            = 32
0.00.066.714 I llm_load_print_meta: n_swa            = 0
0.00.066.714 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.714 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.716 I llm_load_print_meta: n_gqa            = 1
0.00.066.717 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.717 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.718 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.718 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.719 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.719 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.719 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.720 I llm_load_print_meta: n_ff             = 8192
0.00.066.720 I llm_load_print_meta: n_expert         = 0
0.00.066.720 I llm_load_print_meta: n_expert_used    = 0
0.00.066.720 I llm_load_print_meta: causal attn      = 1
0.00.066.720 I llm_load_print_meta: pooling type     = 0
0.00.066.721 I llm_load_print_meta: rope type        = 2
0.00.066.721 I llm_load_print_meta: rope scaling     = linear
0.00.066.723 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.723 I llm_load_print_meta: freq_scale_train = 1
0.00.066.723 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.723 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.723 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.723 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.724 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.724 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.724 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.736 I llm_load_print_meta: model type       = 1.4B
0.00.066.737 I llm_load_print_meta: model ftype      = Q8_0
0.00.066.737 I llm_load_print_meta: model params     = 1.41 B
0.00.066.738 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.066.738 I llm_load_print_meta: general.name     = 1.4B
0.00.066.738 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.738 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.738 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.739 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.739 I llm_load_print_meta: LF token         = 128 ''
0.00.066.739 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.740 I llm_load_print_meta: max token length = 1024
0.00.069.285 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.285 I llm_load_tensors: offloading output layer to GPU
0.00.069.285 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.296 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.297 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.314 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.316 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.316 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.316 I llama_new_context_with_model: n_batch       = 2048
0.00.070.316 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.316 I llama_new_context_with_model: flash_attn    = 0
0.00.070.317 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.317 I llama_new_context_with_model: freq_scale    = 1
0.00.070.318 I ggml_metal_init: allocating
0.00.070.320 I ggml_metal_init: found device: Apple M4
0.00.070.323 I ggml_metal_init: picking default device: Apple M4
0.00.071.051 I ggml_metal_init: using embedded metal library
0.00.073.417 I ggml_metal_init: GPU name:   Apple M4
0.00.073.418 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.419 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.419 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.420 I ggml_metal_init: simdgroup reduction   = true
0.00.073.420 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.420 I ggml_metal_init: has bfloat            = true
0.00.073.420 I ggml_metal_init: use bfloat            = true
0.00.073.420 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.421 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.326 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.332 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.356 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.381 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.382 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.383 I llama_new_context_with_model: graph nodes  = 967
0.00.107.383 I llama_new_context_with_model: graph splits = 2
0.00.107.394 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.271.570 I main: llama threadpool init, n_threads = 4
0.01.271.606 I 
0.01.271.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.271.632 I 
0.01.271.872 I sampler seed: 1234
0.01.271.876 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.271.887 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.271.889 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.271.889 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.345.498 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.02.345.499 I llama_perf_context_print:        load time =    1261.82 ms
0.02.345.500 I llama_perf_context_print: prompt eval time =      33.55 ms /     7 tokens (    4.79 ms per token,   208.63 tokens per second)
0.02.345.500 I llama_perf_context_print:        eval time =    1037.04 ms /    63 runs   (   16.46 ms per token,    60.75 tokens per second)
0.02.345.501 I llama_perf_context_print:       total time =    1073.93 ms /    70 tokens
0.02.345.688 I ggml_metal_free: deallocating

real	0m2.364s
user	0m0.115s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.140 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.886 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.423 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.429 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.436 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.436 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.437 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.437 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.437 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.445 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.447 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.447 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.447 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.448 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.448 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.450 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.451 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.451 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.847 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.029.849 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.849 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.849 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.850 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.850 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.851 I llama_model_loader: - type  f32:  194 tensors
0.00.029.851 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.781 I llm_load_vocab: special tokens cache size = 25
0.00.060.002 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.005 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.005 I llm_load_print_meta: arch             = gptneox
0.00.060.005 I llm_load_print_meta: vocab type       = BPE
0.00.060.005 I llm_load_print_meta: n_vocab          = 50304
0.00.060.005 I llm_load_print_meta: n_merges         = 50009
0.00.060.006 I llm_load_print_meta: vocab_only       = 0
0.00.060.006 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.006 I llm_load_print_meta: n_embd           = 2048
0.00.060.006 I llm_load_print_meta: n_layer          = 24
0.00.060.009 I llm_load_print_meta: n_head           = 16
0.00.060.010 I llm_load_print_meta: n_head_kv        = 16
0.00.060.010 I llm_load_print_meta: n_rot            = 32
0.00.060.010 I llm_load_print_meta: n_swa            = 0
0.00.060.011 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.011 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.011 I llm_load_print_meta: n_gqa            = 1
0.00.060.012 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.012 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.013 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.013 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.013 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.013 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.014 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.014 I llm_load_print_meta: n_ff             = 8192
0.00.060.014 I llm_load_print_meta: n_expert         = 0
0.00.060.014 I llm_load_print_meta: n_expert_used    = 0
0.00.060.015 I llm_load_print_meta: causal attn      = 1
0.00.060.015 I llm_load_print_meta: pooling type     = 0
0.00.060.015 I llm_load_print_meta: rope type        = 2
0.00.060.015 I llm_load_print_meta: rope scaling     = linear
0.00.060.015 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.015 I llm_load_print_meta: freq_scale_train = 1
0.00.060.016 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.016 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.016 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.016 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.016 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.016 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.016 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.028 I llm_load_print_meta: model type       = 1.4B
0.00.060.028 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.029 I llm_load_print_meta: model params     = 1.41 B
0.00.060.029 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.029 I llm_load_print_meta: general.name     = 1.4B
0.00.060.029 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.030 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.030 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.030 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.030 I llm_load_print_meta: LF token         = 128 ''
0.00.060.030 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.030 I llm_load_print_meta: max token length = 1024
0.00.062.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.269 I llm_load_tensors: offloading output layer to GPU
0.00.062.269 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.279 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.280 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.221 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.222 I llama_new_context_with_model: n_ctx         = 128
0.00.063.222 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.222 I llama_new_context_with_model: n_batch       = 128
0.00.063.222 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.222 I llama_new_context_with_model: flash_attn    = 0
0.00.063.223 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.223 I llama_new_context_with_model: freq_scale    = 1
0.00.063.223 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.224 I ggml_metal_init: allocating
0.00.063.229 I ggml_metal_init: found device: Apple M4
0.00.063.232 I ggml_metal_init: picking default device: Apple M4
0.00.063.784 I ggml_metal_init: using embedded metal library
0.00.065.720 I ggml_metal_init: GPU name:   Apple M4
0.00.065.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.722 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.722 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.722 I ggml_metal_init: simdgroup reduction   = true
0.00.065.722 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.723 I ggml_metal_init: has bfloat            = true
0.00.065.723 I ggml_metal_init: use bfloat            = true
0.00.065.723 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.724 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.875 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.073.880 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.073.896 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.074.850 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.074.851 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.074.852 I llama_new_context_with_model: graph nodes  = 967
0.00.074.852 I llama_new_context_with_model: graph splits = 2
0.00.074.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.862.113 I 
0.00.862.138 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.862.150 I perplexity: tokenizing the input ..
0.00.869.845 I perplexity: tokenization took 7.694 ms
0.00.869.855 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.992.090 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.993.493 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.993.511 I llama_perf_context_print:        load time =     851.22 ms
0.00.993.512 I llama_perf_context_print: prompt eval time =     121.99 ms /   128 tokens (    0.95 ms per token,  1049.23 tokens per second)
0.00.993.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.993.513 I llama_perf_context_print:       total time =     131.40 ms /   129 tokens
0.00.993.973 I ggml_metal_free: deallocating

real	0m1.010s
user	0m0.086s
sys	0m0.139s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.011.312 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.809 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.814 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.817 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.817 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.826 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.827 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.828 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.830 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.831 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.831 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.689 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.763 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.688 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.690 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.690 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.691 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.691 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.691 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.692 I llama_model_loader: - type  f32:  194 tensors
0.00.026.693 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.693 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.104 I llm_load_vocab: special tokens cache size = 25
0.00.053.031 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.035 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.035 I llm_load_print_meta: arch             = gptneox
0.00.053.035 I llm_load_print_meta: vocab type       = BPE
0.00.053.036 I llm_load_print_meta: n_vocab          = 50304
0.00.053.036 I llm_load_print_meta: n_merges         = 50009
0.00.053.036 I llm_load_print_meta: vocab_only       = 0
0.00.053.036 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.036 I llm_load_print_meta: n_embd           = 2048
0.00.053.037 I llm_load_print_meta: n_layer          = 24
0.00.053.043 I llm_load_print_meta: n_head           = 16
0.00.053.044 I llm_load_print_meta: n_head_kv        = 16
0.00.053.044 I llm_load_print_meta: n_rot            = 32
0.00.053.044 I llm_load_print_meta: n_swa            = 0
0.00.053.045 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.045 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.045 I llm_load_print_meta: n_gqa            = 1
0.00.053.046 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.047 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.048 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.048 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.050 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.051 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.051 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.051 I llm_load_print_meta: n_ff             = 8192
0.00.053.052 I llm_load_print_meta: n_expert         = 0
0.00.053.052 I llm_load_print_meta: n_expert_used    = 0
0.00.053.052 I llm_load_print_meta: causal attn      = 1
0.00.053.052 I llm_load_print_meta: pooling type     = 0
0.00.053.052 I llm_load_print_meta: rope type        = 2
0.00.053.052 I llm_load_print_meta: rope scaling     = linear
0.00.053.053 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.053 I llm_load_print_meta: freq_scale_train = 1
0.00.053.053 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.054 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.054 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.054 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.054 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.054 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.054 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.067 I llm_load_print_meta: model type       = 1.4B
0.00.053.067 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.068 I llm_load_print_meta: model params     = 1.41 B
0.00.053.068 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.068 I llm_load_print_meta: general.name     = 1.4B
0.00.053.069 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.070 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.070 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.070 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.071 I llm_load_print_meta: LF token         = 128 ''
0.00.053.071 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.071 I llm_load_print_meta: max token length = 1024
0.00.055.321 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.321 I llm_load_tensors: offloading output layer to GPU
0.00.055.322 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.332 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.333 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.302 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.303 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.304 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.304 I llama_new_context_with_model: n_batch       = 2048
0.00.056.304 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.304 I llama_new_context_with_model: flash_attn    = 0
0.00.056.305 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.305 I llama_new_context_with_model: freq_scale    = 1
0.00.056.305 I ggml_metal_init: allocating
0.00.056.313 I ggml_metal_init: found device: Apple M4
0.00.056.316 I ggml_metal_init: picking default device: Apple M4
0.00.057.008 I ggml_metal_init: using embedded metal library
0.00.059.170 I ggml_metal_init: GPU name:   Apple M4
0.00.059.171 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.172 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.172 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.172 I ggml_metal_init: simdgroup reduction   = true
0.00.059.173 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.173 I ggml_metal_init: has bfloat            = true
0.00.059.173 I ggml_metal_init: use bfloat            = true
0.00.059.173 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.174 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.920 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.936 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.970 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.095 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.096 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.096 I llama_new_context_with_model: graph nodes  = 967
0.00.094.097 I llama_new_context_with_model: graph splits = 2
0.00.094.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.260 I main: llama threadpool init, n_threads = 4
0.00.686.296 I 
0.00.686.327 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.686.327 I 
0.00.686.569 I sampler seed: 1234
0.00.686.574 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.598 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.599 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.599 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.358.248 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.358.249 I llama_perf_context_print:        load time =     674.95 ms
0.01.358.250 I llama_perf_context_print: prompt eval time =      32.75 ms /     7 tokens (    4.68 ms per token,   213.71 tokens per second)
0.01.358.250 I llama_perf_context_print:        eval time =     635.80 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.358.252 I llama_perf_context_print:       total time =     671.99 ms /    70 tokens
0.01.358.406 I ggml_metal_free: deallocating

real	0m1.375s
user	0m0.108s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.243 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.028 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.030 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.031 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.031 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.031 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.032 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.039 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.039 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.040 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.040 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.040 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.041 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.041 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.787 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.820 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.603 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.605 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.605 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.606 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.606 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.607 I llama_model_loader: - type  f32:  194 tensors
0.00.023.607 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.607 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.575 I llm_load_vocab: special tokens cache size = 25
0.00.049.423 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.426 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.426 I llm_load_print_meta: arch             = gptneox
0.00.049.427 I llm_load_print_meta: vocab type       = BPE
0.00.049.427 I llm_load_print_meta: n_vocab          = 50304
0.00.049.427 I llm_load_print_meta: n_merges         = 50009
0.00.049.427 I llm_load_print_meta: vocab_only       = 0
0.00.049.427 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.428 I llm_load_print_meta: n_embd           = 2048
0.00.049.428 I llm_load_print_meta: n_layer          = 24
0.00.049.431 I llm_load_print_meta: n_head           = 16
0.00.049.432 I llm_load_print_meta: n_head_kv        = 16
0.00.049.432 I llm_load_print_meta: n_rot            = 32
0.00.049.432 I llm_load_print_meta: n_swa            = 0
0.00.049.434 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.434 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.435 I llm_load_print_meta: n_gqa            = 1
0.00.049.436 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.437 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.437 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.438 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.439 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.439 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.439 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.440 I llm_load_print_meta: n_ff             = 8192
0.00.049.440 I llm_load_print_meta: n_expert         = 0
0.00.049.440 I llm_load_print_meta: n_expert_used    = 0
0.00.049.440 I llm_load_print_meta: causal attn      = 1
0.00.049.440 I llm_load_print_meta: pooling type     = 0
0.00.049.440 I llm_load_print_meta: rope type        = 2
0.00.049.441 I llm_load_print_meta: rope scaling     = linear
0.00.049.441 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.441 I llm_load_print_meta: freq_scale_train = 1
0.00.049.442 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.442 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.442 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.442 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.442 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.443 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.443 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.455 I llm_load_print_meta: model type       = 1.4B
0.00.049.455 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.455 I llm_load_print_meta: model params     = 1.41 B
0.00.049.456 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.456 I llm_load_print_meta: general.name     = 1.4B
0.00.049.456 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.456 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.457 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.457 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.457 I llm_load_print_meta: LF token         = 128 ''
0.00.049.457 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.457 I llm_load_print_meta: max token length = 1024
0.00.051.360 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.361 I llm_load_tensors: offloading output layer to GPU
0.00.051.361 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.371 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.372 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.293 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.294 I llama_new_context_with_model: n_ctx         = 128
0.00.052.295 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.295 I llama_new_context_with_model: n_batch       = 128
0.00.052.295 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.295 I llama_new_context_with_model: flash_attn    = 0
0.00.052.295 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.296 I llama_new_context_with_model: freq_scale    = 1
0.00.052.296 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.297 I ggml_metal_init: allocating
0.00.052.303 I ggml_metal_init: found device: Apple M4
0.00.052.306 I ggml_metal_init: picking default device: Apple M4
0.00.052.862 I ggml_metal_init: using embedded metal library
0.00.054.806 I ggml_metal_init: GPU name:   Apple M4
0.00.054.807 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.808 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.808 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.808 I ggml_metal_init: simdgroup reduction   = true
0.00.054.809 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.809 I ggml_metal_init: has bfloat            = true
0.00.054.809 I ggml_metal_init: use bfloat            = true
0.00.054.809 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.317 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.320 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.335 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.226 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.227 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.227 I llama_new_context_with_model: graph nodes  = 967
0.00.065.227 I llama_new_context_with_model: graph splits = 2
0.00.065.239 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.495 I 
0.00.616.527 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.616.538 I perplexity: tokenizing the input ..
0.00.624.637 I perplexity: tokenization took 8.094 ms
0.00.624.650 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.747.572 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.748.990 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.749.008 I llama_perf_context_print:        load time =     607.25 ms
0.00.749.009 I llama_perf_context_print: prompt eval time =     122.69 ms /   128 tokens (    0.96 ms per token,  1043.25 tokens per second)
0.00.749.010 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.749.011 I llama_perf_context_print:       total time =     132.51 ms /   129 tokens
0.00.749.396 I ggml_metal_free: deallocating

real	0m0.765s
user	0m0.077s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.031 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.327 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.879 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.884 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.885 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.885 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.886 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.886 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.743 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.474 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.476 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.477 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.477 I llama_model_loader: - type  f32:  194 tensors
0.00.024.478 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.478 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.565 I llm_load_vocab: special tokens cache size = 25
0.00.050.522 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.525 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.525 I llm_load_print_meta: arch             = gptneox
0.00.050.526 I llm_load_print_meta: vocab type       = BPE
0.00.050.526 I llm_load_print_meta: n_vocab          = 50304
0.00.050.526 I llm_load_print_meta: n_merges         = 50009
0.00.050.526 I llm_load_print_meta: vocab_only       = 0
0.00.050.526 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.526 I llm_load_print_meta: n_embd           = 2048
0.00.050.527 I llm_load_print_meta: n_layer          = 24
0.00.050.529 I llm_load_print_meta: n_head           = 16
0.00.050.530 I llm_load_print_meta: n_head_kv        = 16
0.00.050.530 I llm_load_print_meta: n_rot            = 32
0.00.050.530 I llm_load_print_meta: n_swa            = 0
0.00.050.531 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.531 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.532 I llm_load_print_meta: n_gqa            = 1
0.00.050.532 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.533 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.533 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.534 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.534 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.534 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.534 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.535 I llm_load_print_meta: n_ff             = 8192
0.00.050.535 I llm_load_print_meta: n_expert         = 0
0.00.050.535 I llm_load_print_meta: n_expert_used    = 0
0.00.050.535 I llm_load_print_meta: causal attn      = 1
0.00.050.536 I llm_load_print_meta: pooling type     = 0
0.00.050.536 I llm_load_print_meta: rope type        = 2
0.00.050.536 I llm_load_print_meta: rope scaling     = linear
0.00.050.536 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.537 I llm_load_print_meta: freq_scale_train = 1
0.00.050.537 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.537 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.537 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.537 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.540 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.540 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.540 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.551 I llm_load_print_meta: model type       = 1.4B
0.00.050.551 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.551 I llm_load_print_meta: model params     = 1.41 B
0.00.050.552 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.552 I llm_load_print_meta: general.name     = 1.4B
0.00.050.552 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.552 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.552 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.553 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.553 I llm_load_print_meta: LF token         = 128 ''
0.00.050.553 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.553 I llm_load_print_meta: max token length = 1024
0.00.052.059 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.060 I llm_load_tensors: offloading output layer to GPU
0.00.052.060 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.069 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.070 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.909 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.910 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.910 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.910 I llama_new_context_with_model: n_batch       = 2048
0.00.052.911 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.911 I llama_new_context_with_model: flash_attn    = 0
0.00.052.911 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.911 I llama_new_context_with_model: freq_scale    = 1
0.00.052.912 I ggml_metal_init: allocating
0.00.052.917 I ggml_metal_init: found device: Apple M4
0.00.052.921 I ggml_metal_init: picking default device: Apple M4
0.00.053.460 I ggml_metal_init: using embedded metal library
0.00.055.355 I ggml_metal_init: GPU name:   Apple M4
0.00.055.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.357 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.357 I ggml_metal_init: simdgroup reduction   = true
0.00.055.358 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.358 I ggml_metal_init: has bfloat            = true
0.00.055.358 I ggml_metal_init: use bfloat            = true
0.00.055.358 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.397 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.403 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.422 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.339 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.340 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.341 I llama_new_context_with_model: graph nodes  = 967
0.00.083.341 I llama_new_context_with_model: graph splits = 2
0.00.083.354 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.686.142 I main: llama threadpool init, n_threads = 4
0.00.686.186 I 
0.00.686.210 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.686.210 I 
0.00.686.438 I sampler seed: 1234
0.00.686.442 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.686.486 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.686.509 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.686.510 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.409.257 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62555.07 tokens per second)
0.01.409.257 I llama_perf_context_print:        load time =     676.81 ms
0.01.409.258 I llama_perf_context_print: prompt eval time =      37.81 ms /     7 tokens (    5.40 ms per token,   185.15 tokens per second)
0.01.409.259 I llama_perf_context_print:        eval time =     682.02 ms /    63 runs   (   10.83 ms per token,    92.37 tokens per second)
0.01.409.259 I llama_perf_context_print:       total time =     723.12 ms /    70 tokens
0.01.409.437 I ggml_metal_free: deallocating

real	0m1.429s
user	0m0.107s
sys	0m0.147s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.832 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.859 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.861 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.867 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.867 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.879 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.880 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.880 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.881 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.883 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.883 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.883 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.638 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.679 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.516 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.517 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.517 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.518 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.518 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.518 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.519 I llama_model_loader: - type  f32:  194 tensors
0.00.023.519 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.520 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.398 I llm_load_vocab: special tokens cache size = 25
0.00.050.198 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.201 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.201 I llm_load_print_meta: arch             = gptneox
0.00.050.201 I llm_load_print_meta: vocab type       = BPE
0.00.050.202 I llm_load_print_meta: n_vocab          = 50304
0.00.050.202 I llm_load_print_meta: n_merges         = 50009
0.00.050.202 I llm_load_print_meta: vocab_only       = 0
0.00.050.202 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.202 I llm_load_print_meta: n_embd           = 2048
0.00.050.203 I llm_load_print_meta: n_layer          = 24
0.00.050.205 I llm_load_print_meta: n_head           = 16
0.00.050.206 I llm_load_print_meta: n_head_kv        = 16
0.00.050.206 I llm_load_print_meta: n_rot            = 32
0.00.050.206 I llm_load_print_meta: n_swa            = 0
0.00.050.206 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.207 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.210 I llm_load_print_meta: n_gqa            = 1
0.00.050.211 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.212 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.212 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.213 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.213 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.213 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.214 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.215 I llm_load_print_meta: n_ff             = 8192
0.00.050.215 I llm_load_print_meta: n_expert         = 0
0.00.050.215 I llm_load_print_meta: n_expert_used    = 0
0.00.050.216 I llm_load_print_meta: causal attn      = 1
0.00.050.216 I llm_load_print_meta: pooling type     = 0
0.00.050.216 I llm_load_print_meta: rope type        = 2
0.00.050.216 I llm_load_print_meta: rope scaling     = linear
0.00.050.216 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.217 I llm_load_print_meta: freq_scale_train = 1
0.00.050.217 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.217 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.217 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.217 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.218 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.218 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.218 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.229 I llm_load_print_meta: model type       = 1.4B
0.00.050.230 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.230 I llm_load_print_meta: model params     = 1.41 B
0.00.050.231 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.231 I llm_load_print_meta: general.name     = 1.4B
0.00.050.231 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.231 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.231 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.231 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.232 I llm_load_print_meta: LF token         = 128 ''
0.00.050.232 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.232 I llm_load_print_meta: max token length = 1024
0.00.052.233 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.233 I llm_load_tensors: offloading output layer to GPU
0.00.052.234 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.244 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.245 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.219 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.220 I llama_new_context_with_model: n_ctx         = 128
0.00.053.220 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.220 I llama_new_context_with_model: n_batch       = 128
0.00.053.221 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.221 I llama_new_context_with_model: flash_attn    = 0
0.00.053.221 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.221 I llama_new_context_with_model: freq_scale    = 1
0.00.053.222 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.222 I ggml_metal_init: allocating
0.00.053.225 I ggml_metal_init: found device: Apple M4
0.00.053.228 I ggml_metal_init: picking default device: Apple M4
0.00.053.777 I ggml_metal_init: using embedded metal library
0.00.055.714 I ggml_metal_init: GPU name:   Apple M4
0.00.055.715 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.716 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.716 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.716 I ggml_metal_init: simdgroup reduction   = true
0.00.055.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.717 I ggml_metal_init: has bfloat            = true
0.00.055.717 I ggml_metal_init: use bfloat            = true
0.00.055.717 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.718 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.697 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.699 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.712 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.660 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.661 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.662 I llama_new_context_with_model: graph nodes  = 967
0.00.066.662 I llama_new_context_with_model: graph splits = 2
0.00.066.674 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.569 I 
0.00.630.599 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.630.610 I perplexity: tokenizing the input ..
0.00.638.194 I perplexity: tokenization took 7.583 ms
0.00.638.206 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.761.079 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.762.361 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.762.376 I llama_perf_context_print:        load time =     621.74 ms
0.00.762.378 I llama_perf_context_print: prompt eval time =     122.65 ms /   128 tokens (    0.96 ms per token,  1043.60 tokens per second)
0.00.762.378 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.762.379 I llama_perf_context_print:       total time =     131.81 ms /   129 tokens
0.00.762.754 I ggml_metal_free: deallocating

real	0m0.776s
user	0m0.079s
sys	0m0.102s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.672 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.609 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.615 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.616 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.617 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.625 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.625 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.625 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.626 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.626 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.627 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.631 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.563 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.408 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.410 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.410 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.411 I llama_model_loader: - type  f32:  194 tensors
0.00.024.412 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.412 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.164 I llm_load_vocab: special tokens cache size = 25
0.00.051.051 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.053 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.054 I llm_load_print_meta: arch             = gptneox
0.00.051.054 I llm_load_print_meta: vocab type       = BPE
0.00.051.054 I llm_load_print_meta: n_vocab          = 50304
0.00.051.054 I llm_load_print_meta: n_merges         = 50009
0.00.051.054 I llm_load_print_meta: vocab_only       = 0
0.00.051.055 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.055 I llm_load_print_meta: n_embd           = 2048
0.00.051.055 I llm_load_print_meta: n_layer          = 24
0.00.051.058 I llm_load_print_meta: n_head           = 16
0.00.051.059 I llm_load_print_meta: n_head_kv        = 16
0.00.051.059 I llm_load_print_meta: n_rot            = 32
0.00.051.059 I llm_load_print_meta: n_swa            = 0
0.00.051.059 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.059 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.060 I llm_load_print_meta: n_gqa            = 1
0.00.051.061 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.062 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.062 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.062 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.063 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.063 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.063 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.064 I llm_load_print_meta: n_ff             = 8192
0.00.051.064 I llm_load_print_meta: n_expert         = 0
0.00.051.064 I llm_load_print_meta: n_expert_used    = 0
0.00.051.065 I llm_load_print_meta: causal attn      = 1
0.00.051.067 I llm_load_print_meta: pooling type     = 0
0.00.051.067 I llm_load_print_meta: rope type        = 2
0.00.051.067 I llm_load_print_meta: rope scaling     = linear
0.00.051.068 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.068 I llm_load_print_meta: freq_scale_train = 1
0.00.051.068 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.069 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.069 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.069 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.069 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.069 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.069 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.080 I llm_load_print_meta: model type       = 1.4B
0.00.051.081 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.081 I llm_load_print_meta: model params     = 1.41 B
0.00.051.081 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.082 I llm_load_print_meta: general.name     = 1.4B
0.00.051.082 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.082 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.082 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.082 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: LF token         = 128 ''
0.00.051.084 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.084 I llm_load_print_meta: max token length = 1024
0.00.052.655 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.655 I llm_load_tensors: offloading output layer to GPU
0.00.052.655 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.665 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.666 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.486 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.487 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.487 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.488 I llama_new_context_with_model: n_batch       = 2048
0.00.053.488 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.488 I llama_new_context_with_model: flash_attn    = 0
0.00.053.489 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.489 I llama_new_context_with_model: freq_scale    = 1
0.00.053.489 I ggml_metal_init: allocating
0.00.053.496 I ggml_metal_init: found device: Apple M4
0.00.053.498 I ggml_metal_init: picking default device: Apple M4
0.00.054.058 I ggml_metal_init: using embedded metal library
0.00.055.983 I ggml_metal_init: GPU name:   Apple M4
0.00.055.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.985 I ggml_metal_init: simdgroup reduction   = true
0.00.055.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.986 I ggml_metal_init: has bfloat            = true
0.00.055.986 I ggml_metal_init: use bfloat            = true
0.00.055.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.244 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.253 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.273 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.198 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.199 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.199 I llama_new_context_with_model: graph nodes  = 967
0.00.084.199 I llama_new_context_with_model: graph splits = 2
0.00.084.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.372 I main: llama threadpool init, n_threads = 4
0.00.769.408 I 
0.00.769.437 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.769.438 I 
0.00.769.686 I sampler seed: 1234
0.00.769.691 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.702 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.702 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.703 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.555.096 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58629.23 tokens per second)
0.01.555.097 I llama_perf_context_print:        load time =     760.70 ms
0.01.555.098 I llama_perf_context_print: prompt eval time =      40.52 ms /     7 tokens (    5.79 ms per token,   172.78 tokens per second)
0.01.555.099 I llama_perf_context_print:        eval time =     741.88 ms /    63 runs   (   11.78 ms per token,    84.92 tokens per second)
0.01.555.099 I llama_perf_context_print:       total time =     785.73 ms /    70 tokens
0.01.555.279 I ggml_metal_free: deallocating

real	0m1.571s
user	0m0.108s
sys	0m0.163s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.399 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.227 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.234 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.234 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.235 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.235 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.235 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.236 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.236 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.237 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.238 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.238 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.013 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.080 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.838 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.838 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.839 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.839 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.840 I llama_model_loader: - type  f32:  194 tensors
0.00.024.840 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.841 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.908 I llm_load_vocab: special tokens cache size = 25
0.00.050.620 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.622 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.623 I llm_load_print_meta: arch             = gptneox
0.00.050.623 I llm_load_print_meta: vocab type       = BPE
0.00.050.623 I llm_load_print_meta: n_vocab          = 50304
0.00.050.624 I llm_load_print_meta: n_merges         = 50009
0.00.050.624 I llm_load_print_meta: vocab_only       = 0
0.00.050.624 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.624 I llm_load_print_meta: n_embd           = 2048
0.00.050.624 I llm_load_print_meta: n_layer          = 24
0.00.050.628 I llm_load_print_meta: n_head           = 16
0.00.050.628 I llm_load_print_meta: n_head_kv        = 16
0.00.050.633 I llm_load_print_meta: n_rot            = 32
0.00.050.633 I llm_load_print_meta: n_swa            = 0
0.00.050.633 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.634 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.634 I llm_load_print_meta: n_gqa            = 1
0.00.050.635 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.636 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.636 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.637 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.638 I llm_load_print_meta: n_ff             = 8192
0.00.050.638 I llm_load_print_meta: n_expert         = 0
0.00.050.638 I llm_load_print_meta: n_expert_used    = 0
0.00.050.638 I llm_load_print_meta: causal attn      = 1
0.00.050.638 I llm_load_print_meta: pooling type     = 0
0.00.050.639 I llm_load_print_meta: rope type        = 2
0.00.050.639 I llm_load_print_meta: rope scaling     = linear
0.00.050.641 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.641 I llm_load_print_meta: freq_scale_train = 1
0.00.050.641 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.641 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.641 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.642 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.642 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.642 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.642 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.654 I llm_load_print_meta: model type       = 1.4B
0.00.050.654 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.655 I llm_load_print_meta: model params     = 1.41 B
0.00.050.655 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.655 I llm_load_print_meta: general.name     = 1.4B
0.00.050.655 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.657 I llm_load_print_meta: LF token         = 128 ''
0.00.050.658 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.658 I llm_load_print_meta: max token length = 1024
0.00.052.713 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.713 I llm_load_tensors: offloading output layer to GPU
0.00.052.713 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.723 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.724 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.710 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.711 I llama_new_context_with_model: n_ctx         = 128
0.00.053.711 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.712 I llama_new_context_with_model: n_batch       = 128
0.00.053.712 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.712 I llama_new_context_with_model: flash_attn    = 0
0.00.053.712 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.713 I llama_new_context_with_model: freq_scale    = 1
0.00.053.713 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.714 I ggml_metal_init: allocating
0.00.053.720 I ggml_metal_init: found device: Apple M4
0.00.053.723 I ggml_metal_init: picking default device: Apple M4
0.00.054.278 I ggml_metal_init: using embedded metal library
0.00.056.287 I ggml_metal_init: GPU name:   Apple M4
0.00.056.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.289 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.289 I ggml_metal_init: simdgroup reduction   = true
0.00.056.289 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.290 I ggml_metal_init: has bfloat            = true
0.00.056.290 I ggml_metal_init: use bfloat            = true
0.00.056.290 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.291 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.527 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.530 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.546 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.439 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.440 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.441 I llama_new_context_with_model: graph nodes  = 967
0.00.066.441 I llama_new_context_with_model: graph splits = 2
0.00.066.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.525 I 
0.00.701.550 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.701.561 I perplexity: tokenizing the input ..
0.00.708.949 I perplexity: tokenization took 7.387 ms
0.00.708.959 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.844.143 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.845.549 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.845.562 I llama_perf_context_print:        load time =     691.12 ms
0.00.845.562 I llama_perf_context_print: prompt eval time =     134.96 ms /   128 tokens (    1.05 ms per token,   948.41 tokens per second)
0.00.845.563 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.845.564 I llama_perf_context_print:       total time =     144.04 ms /   129 tokens
0.00.845.952 I ggml_metal_free: deallocating

real	0m0.860s
user	0m0.076s
sys	0m0.120s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.661 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.027 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.031 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.037 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.037 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.038 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.038 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.045 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.045 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.046 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.046 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.047 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.049 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.869 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.626 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.626 I llama_model_loader: - type  f32:  194 tensors
0.00.024.626 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.626 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.641 I llm_load_vocab: special tokens cache size = 25
0.00.050.671 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.673 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.674 I llm_load_print_meta: arch             = gptneox
0.00.050.674 I llm_load_print_meta: vocab type       = BPE
0.00.050.674 I llm_load_print_meta: n_vocab          = 50304
0.00.050.674 I llm_load_print_meta: n_merges         = 50009
0.00.050.675 I llm_load_print_meta: vocab_only       = 0
0.00.050.675 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.675 I llm_load_print_meta: n_embd           = 2048
0.00.050.675 I llm_load_print_meta: n_layer          = 24
0.00.050.678 I llm_load_print_meta: n_head           = 16
0.00.050.679 I llm_load_print_meta: n_head_kv        = 16
0.00.050.679 I llm_load_print_meta: n_rot            = 32
0.00.050.679 I llm_load_print_meta: n_swa            = 0
0.00.050.679 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.679 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.680 I llm_load_print_meta: n_gqa            = 1
0.00.050.681 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.682 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.682 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.682 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.683 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.683 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.683 I llm_load_print_meta: n_ff             = 8192
0.00.050.684 I llm_load_print_meta: n_expert         = 0
0.00.050.684 I llm_load_print_meta: n_expert_used    = 0
0.00.050.684 I llm_load_print_meta: causal attn      = 1
0.00.050.684 I llm_load_print_meta: pooling type     = 0
0.00.050.684 I llm_load_print_meta: rope type        = 2
0.00.050.686 I llm_load_print_meta: rope scaling     = linear
0.00.050.686 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.687 I llm_load_print_meta: freq_scale_train = 1
0.00.050.687 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.687 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.688 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.688 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.688 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.688 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.688 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.699 I llm_load_print_meta: model type       = 1.4B
0.00.050.699 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.700 I llm_load_print_meta: model params     = 1.41 B
0.00.050.703 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.703 I llm_load_print_meta: general.name     = 1.4B
0.00.050.704 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.704 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.704 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.704 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.705 I llm_load_print_meta: LF token         = 128 ''
0.00.050.705 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.705 I llm_load_print_meta: max token length = 1024
0.00.052.261 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.261 I llm_load_tensors: offloading output layer to GPU
0.00.052.261 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.271 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.272 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.133 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.134 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.134 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.134 I llama_new_context_with_model: n_batch       = 2048
0.00.053.134 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.134 I llama_new_context_with_model: flash_attn    = 0
0.00.053.135 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.135 I llama_new_context_with_model: freq_scale    = 1
0.00.053.135 I ggml_metal_init: allocating
0.00.053.138 I ggml_metal_init: found device: Apple M4
0.00.053.140 I ggml_metal_init: picking default device: Apple M4
0.00.053.700 I ggml_metal_init: using embedded metal library
0.00.055.592 I ggml_metal_init: GPU name:   Apple M4
0.00.055.593 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.594 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.594 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.594 I ggml_metal_init: simdgroup reduction   = true
0.00.055.594 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.595 I ggml_metal_init: has bfloat            = true
0.00.055.595 I ggml_metal_init: use bfloat            = true
0.00.055.595 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.596 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.487 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.491 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.511 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.523 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.525 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.525 I llama_new_context_with_model: graph nodes  = 967
0.00.084.525 I llama_new_context_with_model: graph splits = 2
0.00.084.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.504 I main: llama threadpool init, n_threads = 4
0.00.799.542 I 
0.00.799.585 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.799.586 I 
0.00.799.809 I sampler seed: 1234
0.00.799.814 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.858 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.859 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.859 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.632.703 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.632.704 I llama_perf_context_print:        load time =     789.84 ms
0.01.632.705 I llama_perf_context_print: prompt eval time =      36.58 ms /     7 tokens (    5.23 ms per token,   191.39 tokens per second)
0.01.632.710 I llama_perf_context_print:        eval time =     793.29 ms /    63 runs   (   12.59 ms per token,    79.42 tokens per second)
0.01.632.710 I llama_perf_context_print:       total time =     833.20 ms /    70 tokens
0.01.632.887 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.108s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.082 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.303 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.307 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.309 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.309 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.310 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.310 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.310 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.318 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.318 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.318 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.319 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.319 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.319 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.320 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.321 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.322 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.322 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.150 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.241 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.093 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.093 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.094 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.094 I llama_model_loader: - type  f32:  194 tensors
0.00.023.094 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.095 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.081 I llm_load_vocab: special tokens cache size = 25
0.00.048.794 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.800 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.800 I llm_load_print_meta: arch             = gptneox
0.00.048.801 I llm_load_print_meta: vocab type       = BPE
0.00.048.801 I llm_load_print_meta: n_vocab          = 50304
0.00.048.801 I llm_load_print_meta: n_merges         = 50009
0.00.048.801 I llm_load_print_meta: vocab_only       = 0
0.00.048.801 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.801 I llm_load_print_meta: n_embd           = 2048
0.00.048.802 I llm_load_print_meta: n_layer          = 24
0.00.048.805 I llm_load_print_meta: n_head           = 16
0.00.048.805 I llm_load_print_meta: n_head_kv        = 16
0.00.048.805 I llm_load_print_meta: n_rot            = 32
0.00.048.806 I llm_load_print_meta: n_swa            = 0
0.00.048.806 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.806 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.807 I llm_load_print_meta: n_gqa            = 1
0.00.048.807 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.808 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.809 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.809 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.809 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.809 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.809 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.810 I llm_load_print_meta: n_ff             = 8192
0.00.048.810 I llm_load_print_meta: n_expert         = 0
0.00.048.810 I llm_load_print_meta: n_expert_used    = 0
0.00.048.813 I llm_load_print_meta: causal attn      = 1
0.00.048.813 I llm_load_print_meta: pooling type     = 0
0.00.048.813 I llm_load_print_meta: rope type        = 2
0.00.048.813 I llm_load_print_meta: rope scaling     = linear
0.00.048.815 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.816 I llm_load_print_meta: freq_scale_train = 1
0.00.048.816 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.816 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.816 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.816 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.817 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.817 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.817 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.829 I llm_load_print_meta: model type       = 1.4B
0.00.048.829 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.830 I llm_load_print_meta: model params     = 1.41 B
0.00.048.830 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.830 I llm_load_print_meta: general.name     = 1.4B
0.00.048.830 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.831 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.831 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.831 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.831 I llm_load_print_meta: LF token         = 128 ''
0.00.048.831 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.831 I llm_load_print_meta: max token length = 1024
0.00.050.804 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.804 I llm_load_tensors: offloading output layer to GPU
0.00.050.804 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.814 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.815 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.826 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.827 I llama_new_context_with_model: n_ctx         = 128
0.00.051.827 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.827 I llama_new_context_with_model: n_batch       = 128
0.00.051.828 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.828 I llama_new_context_with_model: flash_attn    = 0
0.00.051.828 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.828 I llama_new_context_with_model: freq_scale    = 1
0.00.051.829 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.829 I ggml_metal_init: allocating
0.00.051.834 I ggml_metal_init: found device: Apple M4
0.00.051.836 I ggml_metal_init: picking default device: Apple M4
0.00.052.385 I ggml_metal_init: using embedded metal library
0.00.054.339 I ggml_metal_init: GPU name:   Apple M4
0.00.054.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.341 I ggml_metal_init: simdgroup reduction   = true
0.00.054.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.341 I ggml_metal_init: has bfloat            = true
0.00.054.342 I ggml_metal_init: use bfloat            = true
0.00.054.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.348 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.351 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.365 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.234 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.235 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.235 I llama_new_context_with_model: graph nodes  = 967
0.00.064.235 I llama_new_context_with_model: graph splits = 2
0.00.064.248 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.931 I 
0.00.732.969 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.732.995 I perplexity: tokenizing the input ..
0.00.740.680 I perplexity: tokenization took 7.683 ms
0.00.740.694 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.875.390 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.876.721 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.876.733 I llama_perf_context_print:        load time =     724.17 ms
0.00.876.735 I llama_perf_context_print: prompt eval time =     134.47 ms /   128 tokens (    1.05 ms per token,   951.92 tokens per second)
0.00.876.735 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.876.736 I llama_perf_context_print:       total time =     143.80 ms /   129 tokens
0.00.877.123 I ggml_metal_free: deallocating

real	0m0.889s
user	0m0.076s
sys	0m0.126s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.261 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.739 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.745 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.746 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.747 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.747 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.747 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.747 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.755 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.756 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.553 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.585 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.341 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.342 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.343 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.344 I llama_model_loader: - type  f32:  194 tensors
0.00.023.344 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.344 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.344 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.355 I llm_load_vocab: special tokens cache size = 25
0.00.049.108 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.111 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.111 I llm_load_print_meta: arch             = gptneox
0.00.049.111 I llm_load_print_meta: vocab type       = BPE
0.00.049.112 I llm_load_print_meta: n_vocab          = 50304
0.00.049.112 I llm_load_print_meta: n_merges         = 50009
0.00.049.112 I llm_load_print_meta: vocab_only       = 0
0.00.049.112 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.112 I llm_load_print_meta: n_embd           = 2048
0.00.049.112 I llm_load_print_meta: n_layer          = 24
0.00.049.116 I llm_load_print_meta: n_head           = 16
0.00.049.116 I llm_load_print_meta: n_head_kv        = 16
0.00.049.117 I llm_load_print_meta: n_rot            = 32
0.00.049.117 I llm_load_print_meta: n_swa            = 0
0.00.049.117 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.117 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.120 I llm_load_print_meta: n_gqa            = 1
0.00.049.121 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.122 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.122 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.123 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.123 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.123 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.124 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.125 I llm_load_print_meta: n_ff             = 8192
0.00.049.125 I llm_load_print_meta: n_expert         = 0
0.00.049.126 I llm_load_print_meta: n_expert_used    = 0
0.00.049.126 I llm_load_print_meta: causal attn      = 1
0.00.049.126 I llm_load_print_meta: pooling type     = 0
0.00.049.126 I llm_load_print_meta: rope type        = 2
0.00.049.126 I llm_load_print_meta: rope scaling     = linear
0.00.049.127 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.127 I llm_load_print_meta: freq_scale_train = 1
0.00.049.127 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.127 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.128 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.128 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.129 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.129 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.129 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.141 I llm_load_print_meta: model type       = 1.4B
0.00.049.141 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.142 I llm_load_print_meta: model params     = 1.41 B
0.00.049.142 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.142 I llm_load_print_meta: general.name     = 1.4B
0.00.049.143 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.143 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.143 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.143 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.143 I llm_load_print_meta: LF token         = 128 ''
0.00.049.143 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.144 I llm_load_print_meta: max token length = 1024
0.00.050.651 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.651 I llm_load_tensors: offloading output layer to GPU
0.00.050.651 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.660 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.661 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.508 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.508 I llama_new_context_with_model: n_ctx         = 2048
0.00.051.508 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.051.509 I llama_new_context_with_model: n_batch       = 2048
0.00.051.509 I llama_new_context_with_model: n_ubatch      = 512
0.00.051.509 I llama_new_context_with_model: flash_attn    = 0
0.00.051.509 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.510 I llama_new_context_with_model: freq_scale    = 1
0.00.051.510 I ggml_metal_init: allocating
0.00.051.513 I ggml_metal_init: found device: Apple M4
0.00.051.515 I ggml_metal_init: picking default device: Apple M4
0.00.052.080 I ggml_metal_init: using embedded metal library
0.00.054.038 I ggml_metal_init: GPU name:   Apple M4
0.00.054.039 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.040 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.040 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.040 I ggml_metal_init: simdgroup reduction   = true
0.00.054.041 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.041 I ggml_metal_init: has bfloat            = true
0.00.054.041 I ggml_metal_init: use bfloat            = true
0.00.054.041 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.042 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.429 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.080.434 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.080.452 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.405 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.081.406 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.081.406 I llama_new_context_with_model: graph nodes  = 967
0.00.081.406 I llama_new_context_with_model: graph splits = 2
0.00.081.421 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.513.092 I main: llama threadpool init, n_threads = 4
0.00.513.138 I 
0.00.513.165 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.513.165 I 
0.00.513.405 I sampler seed: 1234
0.00.513.409 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.513.452 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.513.452 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.513.452 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.192.996 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.192.996 I llama_perf_context_print:        load time =     503.82 ms
0.01.192.997 I llama_perf_context_print: prompt eval time =      35.79 ms /     7 tokens (    5.11 ms per token,   195.57 tokens per second)
0.01.193.001 I llama_perf_context_print:        eval time =     640.79 ms /    63 runs   (   10.17 ms per token,    98.32 tokens per second)
0.01.193.002 I llama_perf_context_print:       total time =     679.91 ms /    70 tokens
0.01.193.184 I ggml_metal_free: deallocating

real	0m1.211s
user	0m0.106s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.398 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.804 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.809 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.811 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.812 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.812 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.812 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.820 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.821 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.822 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.824 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.824 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.825 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.595 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.384 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.384 I llama_model_loader: - type  f32:  194 tensors
0.00.023.385 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.385 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.385 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.232 I llm_load_vocab: special tokens cache size = 25
0.00.049.073 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.075 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.075 I llm_load_print_meta: arch             = gptneox
0.00.049.076 I llm_load_print_meta: vocab type       = BPE
0.00.049.076 I llm_load_print_meta: n_vocab          = 50304
0.00.049.076 I llm_load_print_meta: n_merges         = 50009
0.00.049.077 I llm_load_print_meta: vocab_only       = 0
0.00.049.077 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.077 I llm_load_print_meta: n_embd           = 2048
0.00.049.077 I llm_load_print_meta: n_layer          = 24
0.00.049.080 I llm_load_print_meta: n_head           = 16
0.00.049.081 I llm_load_print_meta: n_head_kv        = 16
0.00.049.081 I llm_load_print_meta: n_rot            = 32
0.00.049.081 I llm_load_print_meta: n_swa            = 0
0.00.049.081 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.081 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.082 I llm_load_print_meta: n_gqa            = 1
0.00.049.083 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.084 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.085 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.085 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.085 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.087 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.087 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.088 I llm_load_print_meta: n_ff             = 8192
0.00.049.088 I llm_load_print_meta: n_expert         = 0
0.00.049.088 I llm_load_print_meta: n_expert_used    = 0
0.00.049.088 I llm_load_print_meta: causal attn      = 1
0.00.049.089 I llm_load_print_meta: pooling type     = 0
0.00.049.089 I llm_load_print_meta: rope type        = 2
0.00.049.089 I llm_load_print_meta: rope scaling     = linear
0.00.049.089 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.090 I llm_load_print_meta: freq_scale_train = 1
0.00.049.090 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.090 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.090 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.090 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.091 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.091 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.091 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.102 I llm_load_print_meta: model type       = 1.4B
0.00.049.103 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.103 I llm_load_print_meta: model params     = 1.41 B
0.00.049.104 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.104 I llm_load_print_meta: general.name     = 1.4B
0.00.049.104 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.105 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.106 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.106 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.106 I llm_load_print_meta: LF token         = 128 ''
0.00.049.106 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.106 I llm_load_print_meta: max token length = 1024
0.00.050.809 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.809 I llm_load_tensors: offloading output layer to GPU
0.00.050.809 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.815 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.815 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.736 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.737 I llama_new_context_with_model: n_ctx         = 128
0.00.051.737 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.737 I llama_new_context_with_model: n_batch       = 128
0.00.051.737 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.737 I llama_new_context_with_model: flash_attn    = 0
0.00.051.738 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.738 I llama_new_context_with_model: freq_scale    = 1
0.00.051.739 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.739 I ggml_metal_init: allocating
0.00.051.743 I ggml_metal_init: found device: Apple M4
0.00.051.745 I ggml_metal_init: picking default device: Apple M4
0.00.052.289 I ggml_metal_init: using embedded metal library
0.00.054.234 I ggml_metal_init: GPU name:   Apple M4
0.00.054.236 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.236 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.236 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.237 I ggml_metal_init: simdgroup reduction   = true
0.00.054.237 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.237 I ggml_metal_init: has bfloat            = true
0.00.054.237 I ggml_metal_init: use bfloat            = true
0.00.054.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.490 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.499 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.529 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.366 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.367 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.367 I llama_new_context_with_model: graph nodes  = 967
0.00.064.367 I llama_new_context_with_model: graph splits = 2
0.00.064.375 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.464.491 I 
0.00.464.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.464.570 I perplexity: tokenizing the input ..
0.00.472.972 I perplexity: tokenization took 8.402 ms
0.00.472.985 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.605.247 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.606.610 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.606.623 I llama_perf_context_print:        load time =     455.09 ms
0.00.606.625 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.44 tokens per second)
0.00.606.626 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.606.626 I llama_perf_context_print:       total time =     142.14 ms /   129 tokens
0.00.607.014 I ggml_metal_free: deallocating

real	0m0.621s
user	0m0.077s
sys	0m0.078s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.787 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.279 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.289 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.290 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.291 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.299 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.301 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.302 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.306 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.231 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.282 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.159 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.161 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.161 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.162 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.162 I llama_model_loader: - type  f32:  194 tensors
0.00.024.163 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.163 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.163 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.163 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.057 I llm_load_vocab: special tokens cache size = 25
0.00.051.166 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.168 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.169 I llm_load_print_meta: arch             = gptneox
0.00.051.169 I llm_load_print_meta: vocab type       = BPE
0.00.051.169 I llm_load_print_meta: n_vocab          = 50304
0.00.051.169 I llm_load_print_meta: n_merges         = 50009
0.00.051.169 I llm_load_print_meta: vocab_only       = 0
0.00.051.170 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.170 I llm_load_print_meta: n_embd           = 2048
0.00.051.170 I llm_load_print_meta: n_layer          = 24
0.00.051.173 I llm_load_print_meta: n_head           = 16
0.00.051.174 I llm_load_print_meta: n_head_kv        = 16
0.00.051.174 I llm_load_print_meta: n_rot            = 32
0.00.051.177 I llm_load_print_meta: n_swa            = 0
0.00.051.177 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.177 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.178 I llm_load_print_meta: n_gqa            = 1
0.00.051.179 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.179 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.180 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.180 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.180 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.180 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.181 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.181 I llm_load_print_meta: n_ff             = 8192
0.00.051.182 I llm_load_print_meta: n_expert         = 0
0.00.051.182 I llm_load_print_meta: n_expert_used    = 0
0.00.051.182 I llm_load_print_meta: causal attn      = 1
0.00.051.182 I llm_load_print_meta: pooling type     = 0
0.00.051.182 I llm_load_print_meta: rope type        = 2
0.00.051.182 I llm_load_print_meta: rope scaling     = linear
0.00.051.183 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.183 I llm_load_print_meta: freq_scale_train = 1
0.00.051.183 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.184 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.184 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.185 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.185 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.197 I llm_load_print_meta: model type       = 1.4B
0.00.051.197 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.198 I llm_load_print_meta: model params     = 1.41 B
0.00.051.198 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.198 I llm_load_print_meta: general.name     = 1.4B
0.00.051.199 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.199 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.200 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.200 I llm_load_print_meta: LF token         = 128 ''
0.00.051.201 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.201 I llm_load_print_meta: max token length = 1024
0.00.053.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.150 I llm_load_tensors: offloading output layer to GPU
0.00.053.150 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.160 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.161 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.045 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.046 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.046 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.046 I llama_new_context_with_model: n_batch       = 2048
0.00.054.046 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.047 I llama_new_context_with_model: flash_attn    = 0
0.00.054.047 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.047 I llama_new_context_with_model: freq_scale    = 1
0.00.054.048 I ggml_metal_init: allocating
0.00.054.051 I ggml_metal_init: found device: Apple M4
0.00.054.053 I ggml_metal_init: picking default device: Apple M4
0.00.054.612 I ggml_metal_init: using embedded metal library
0.00.056.539 I ggml_metal_init: GPU name:   Apple M4
0.00.056.540 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.541 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.541 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.541 I ggml_metal_init: simdgroup reduction   = true
0.00.056.541 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.541 I ggml_metal_init: has bfloat            = true
0.00.056.541 I ggml_metal_init: use bfloat            = true
0.00.056.542 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.543 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.392 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.397 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.415 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.424 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.426 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.426 I llama_new_context_with_model: graph nodes  = 967
0.00.085.426 I llama_new_context_with_model: graph splits = 2
0.00.085.439 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.556.317 I main: llama threadpool init, n_threads = 4
0.00.556.353 I 
0.00.556.378 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.556.378 I 
0.00.556.605 I sampler seed: 1234
0.00.556.609 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.556.663 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.556.665 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.556.665 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.298.144 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.01.298.145 I llama_perf_context_print:        load time =     547.53 ms
0.01.298.145 I llama_perf_context_print: prompt eval time =      35.65 ms /     7 tokens (    5.09 ms per token,   196.37 tokens per second)
0.01.298.146 I llama_perf_context_print:        eval time =     702.71 ms /    63 runs   (   11.15 ms per token,    89.65 tokens per second)
0.01.298.146 I llama_perf_context_print:       total time =     741.83 ms /    70 tokens
0.01.298.298 I ggml_metal_free: deallocating

real	0m1.320s
user	0m0.108s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.698 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.401 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.406 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.408 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.408 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.408 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.409 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.409 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.416 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.416 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.417 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.417 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.417 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.420 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.421 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.422 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.423 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.423 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.214 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.296 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.079 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.079 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.079 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.079 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.080 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.080 I llama_model_loader: - type  f32:  194 tensors
0.00.023.080 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.081 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.081 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.081 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.822 I llm_load_vocab: special tokens cache size = 25
0.00.048.858 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.860 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.861 I llm_load_print_meta: arch             = gptneox
0.00.048.861 I llm_load_print_meta: vocab type       = BPE
0.00.048.861 I llm_load_print_meta: n_vocab          = 50304
0.00.048.861 I llm_load_print_meta: n_merges         = 50009
0.00.048.862 I llm_load_print_meta: vocab_only       = 0
0.00.048.862 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.862 I llm_load_print_meta: n_embd           = 2048
0.00.048.862 I llm_load_print_meta: n_layer          = 24
0.00.048.865 I llm_load_print_meta: n_head           = 16
0.00.048.865 I llm_load_print_meta: n_head_kv        = 16
0.00.048.866 I llm_load_print_meta: n_rot            = 32
0.00.048.866 I llm_load_print_meta: n_swa            = 0
0.00.048.866 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.866 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.867 I llm_load_print_meta: n_gqa            = 1
0.00.048.870 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.870 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.871 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.871 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.871 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.871 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.872 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.872 I llm_load_print_meta: n_ff             = 8192
0.00.048.873 I llm_load_print_meta: n_expert         = 0
0.00.048.873 I llm_load_print_meta: n_expert_used    = 0
0.00.048.873 I llm_load_print_meta: causal attn      = 1
0.00.048.873 I llm_load_print_meta: pooling type     = 0
0.00.048.873 I llm_load_print_meta: rope type        = 2
0.00.048.875 I llm_load_print_meta: rope scaling     = linear
0.00.048.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.875 I llm_load_print_meta: freq_scale_train = 1
0.00.048.876 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.876 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.876 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.876 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.876 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.876 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.887 I llm_load_print_meta: model type       = 1.4B
0.00.048.888 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.048.888 I llm_load_print_meta: model params     = 1.41 B
0.00.048.889 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.048.889 I llm_load_print_meta: general.name     = 1.4B
0.00.048.889 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.889 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.889 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.889 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.890 I llm_load_print_meta: LF token         = 128 ''
0.00.048.890 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.891 I llm_load_print_meta: max token length = 1024
0.00.050.411 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.411 I llm_load_tensors: offloading output layer to GPU
0.00.050.411 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.420 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.421 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.250 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.251 I llama_new_context_with_model: n_ctx         = 128
0.00.051.251 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.251 I llama_new_context_with_model: n_batch       = 128
0.00.051.251 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.252 I llama_new_context_with_model: flash_attn    = 0
0.00.051.252 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.252 I llama_new_context_with_model: freq_scale    = 1
0.00.051.253 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.253 I ggml_metal_init: allocating
0.00.051.256 I ggml_metal_init: found device: Apple M4
0.00.051.258 I ggml_metal_init: picking default device: Apple M4
0.00.051.794 I ggml_metal_init: using embedded metal library
0.00.053.714 I ggml_metal_init: GPU name:   Apple M4
0.00.053.715 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.716 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.716 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.716 I ggml_metal_init: simdgroup reduction   = true
0.00.053.716 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.717 I ggml_metal_init: has bfloat            = true
0.00.053.717 I ggml_metal_init: use bfloat            = true
0.00.053.717 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.654 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.658 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.671 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.513 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.514 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.514 I llama_new_context_with_model: graph nodes  = 967
0.00.063.515 I llama_new_context_with_model: graph splits = 2
0.00.063.527 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.828 I 
0.00.505.908 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.505.935 I perplexity: tokenizing the input ..
0.00.513.603 I perplexity: tokenization took 7.667 ms
0.00.513.614 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.732 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.647.068 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.647.086 I llama_perf_context_print:        load time =     497.12 ms
0.00.647.086 I llama_perf_context_print: prompt eval time =     131.89 ms /   128 tokens (    1.03 ms per token,   970.48 tokens per second)
0.00.647.090 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.647.090 I llama_perf_context_print:       total time =     141.26 ms /   129 tokens
0.00.647.534 I ggml_metal_free: deallocating

real	0m0.660s
user	0m0.076s
sys	0m0.095s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.009.696 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.191 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.196 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.198 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.199 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.200 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.209 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.209 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.210 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.210 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.213 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.215 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.215 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.158 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.815 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.816 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.816 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.817 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.817 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.818 I llama_model_loader: - type  f32:  194 tensors
0.00.024.818 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.818 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.819 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.809 I llm_load_vocab: special tokens cache size = 25
0.00.050.519 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.521 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.522 I llm_load_print_meta: arch             = gptneox
0.00.050.522 I llm_load_print_meta: vocab type       = BPE
0.00.050.522 I llm_load_print_meta: n_vocab          = 50304
0.00.050.522 I llm_load_print_meta: n_merges         = 50009
0.00.050.522 I llm_load_print_meta: vocab_only       = 0
0.00.050.523 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.523 I llm_load_print_meta: n_embd           = 2048
0.00.050.523 I llm_load_print_meta: n_layer          = 24
0.00.050.526 I llm_load_print_meta: n_head           = 16
0.00.050.527 I llm_load_print_meta: n_head_kv        = 16
0.00.050.527 I llm_load_print_meta: n_rot            = 32
0.00.050.527 I llm_load_print_meta: n_swa            = 0
0.00.050.527 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.527 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.528 I llm_load_print_meta: n_gqa            = 1
0.00.050.529 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.529 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.530 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.531 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.531 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.531 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.534 I llm_load_print_meta: n_ff             = 8192
0.00.050.534 I llm_load_print_meta: n_expert         = 0
0.00.050.536 I llm_load_print_meta: n_expert_used    = 0
0.00.050.537 I llm_load_print_meta: causal attn      = 1
0.00.050.537 I llm_load_print_meta: pooling type     = 0
0.00.050.537 I llm_load_print_meta: rope type        = 2
0.00.050.537 I llm_load_print_meta: rope scaling     = linear
0.00.050.538 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.538 I llm_load_print_meta: freq_scale_train = 1
0.00.050.538 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.538 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.539 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.539 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.539 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.539 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.539 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.551 I llm_load_print_meta: model type       = 1.4B
0.00.050.552 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.552 I llm_load_print_meta: model params     = 1.41 B
0.00.050.552 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.552 I llm_load_print_meta: general.name     = 1.4B
0.00.050.553 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.553 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.553 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.553 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.553 I llm_load_print_meta: LF token         = 128 ''
0.00.050.554 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.554 I llm_load_print_meta: max token length = 1024
0.00.052.488 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.488 I llm_load_tensors: offloading output layer to GPU
0.00.052.489 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.498 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.500 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.375 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.376 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.376 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.376 I llama_new_context_with_model: n_batch       = 2048
0.00.053.377 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.377 I llama_new_context_with_model: flash_attn    = 0
0.00.053.377 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.377 I llama_new_context_with_model: freq_scale    = 1
0.00.053.378 I ggml_metal_init: allocating
0.00.053.384 I ggml_metal_init: found device: Apple M4
0.00.053.386 I ggml_metal_init: picking default device: Apple M4
0.00.053.928 I ggml_metal_init: using embedded metal library
0.00.055.861 I ggml_metal_init: GPU name:   Apple M4
0.00.055.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.863 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.864 I ggml_metal_init: simdgroup reduction   = true
0.00.055.865 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.865 I ggml_metal_init: has bfloat            = true
0.00.055.865 I ggml_metal_init: use bfloat            = true
0.00.055.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.866 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.603 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.611 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.628 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.695 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.696 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.697 I llama_new_context_with_model: graph nodes  = 967
0.00.085.697 I llama_new_context_with_model: graph splits = 2
0.00.085.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.630.416 I main: llama threadpool init, n_threads = 4
0.00.630.459 I 
0.00.630.482 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.630.482 I 
0.00.630.722 I sampler seed: 1234
0.00.630.726 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.770 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.770 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.380.706 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57304.28 tokens per second)
0.01.380.707 I llama_perf_context_print:        load time =     620.72 ms
0.01.380.707 I llama_perf_context_print: prompt eval time =      36.49 ms /     7 tokens (    5.21 ms per token,   191.81 tokens per second)
0.01.380.708 I llama_perf_context_print:        eval time =     710.44 ms /    63 runs   (   11.28 ms per token,    88.68 tokens per second)
0.01.380.708 I llama_perf_context_print:       total time =     750.29 ms /    70 tokens
0.01.380.910 I ggml_metal_free: deallocating

real	0m1.400s
user	0m0.107s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.954 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.749 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.754 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.755 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.756 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.756 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.757 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.759 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.766 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.766 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.767 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.767 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.767 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.769 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.770 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.653 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.733 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.596 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.597 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.597 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.598 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.598 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.598 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.599 I llama_model_loader: - type  f32:  194 tensors
0.00.024.599 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.599 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.600 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.288 I llm_load_vocab: special tokens cache size = 25
0.00.051.318 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.320 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.321 I llm_load_print_meta: arch             = gptneox
0.00.051.321 I llm_load_print_meta: vocab type       = BPE
0.00.051.321 I llm_load_print_meta: n_vocab          = 50304
0.00.051.322 I llm_load_print_meta: n_merges         = 50009
0.00.051.322 I llm_load_print_meta: vocab_only       = 0
0.00.051.322 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.322 I llm_load_print_meta: n_embd           = 2048
0.00.051.322 I llm_load_print_meta: n_layer          = 24
0.00.051.325 I llm_load_print_meta: n_head           = 16
0.00.051.326 I llm_load_print_meta: n_head_kv        = 16
0.00.051.326 I llm_load_print_meta: n_rot            = 32
0.00.051.326 I llm_load_print_meta: n_swa            = 0
0.00.051.327 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.327 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.329 I llm_load_print_meta: n_gqa            = 1
0.00.051.330 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.331 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.331 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.332 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.332 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.332 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.333 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.333 I llm_load_print_meta: n_ff             = 8192
0.00.051.333 I llm_load_print_meta: n_expert         = 0
0.00.051.334 I llm_load_print_meta: n_expert_used    = 0
0.00.051.335 I llm_load_print_meta: causal attn      = 1
0.00.051.335 I llm_load_print_meta: pooling type     = 0
0.00.051.335 I llm_load_print_meta: rope type        = 2
0.00.051.336 I llm_load_print_meta: rope scaling     = linear
0.00.051.336 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.336 I llm_load_print_meta: freq_scale_train = 1
0.00.051.336 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.337 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.337 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.337 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.337 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.337 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.337 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.349 I llm_load_print_meta: model type       = 1.4B
0.00.051.349 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.350 I llm_load_print_meta: model params     = 1.41 B
0.00.051.350 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.350 I llm_load_print_meta: general.name     = 1.4B
0.00.051.351 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.351 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.351 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.351 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.351 I llm_load_print_meta: LF token         = 128 ''
0.00.051.351 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.351 I llm_load_print_meta: max token length = 1024
0.00.053.278 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.278 I llm_load_tensors: offloading output layer to GPU
0.00.053.279 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.288 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.290 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.155 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.156 I llama_new_context_with_model: n_ctx         = 128
0.00.054.156 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.156 I llama_new_context_with_model: n_batch       = 128
0.00.054.156 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.157 I llama_new_context_with_model: flash_attn    = 0
0.00.054.157 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.157 I llama_new_context_with_model: freq_scale    = 1
0.00.054.157 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.158 I ggml_metal_init: allocating
0.00.054.161 I ggml_metal_init: found device: Apple M4
0.00.054.163 I ggml_metal_init: picking default device: Apple M4
0.00.054.722 I ggml_metal_init: using embedded metal library
0.00.056.691 I ggml_metal_init: GPU name:   Apple M4
0.00.056.693 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.693 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.694 I ggml_metal_init: simdgroup reduction   = true
0.00.056.694 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.694 I ggml_metal_init: has bfloat            = true
0.00.056.694 I ggml_metal_init: use bfloat            = true
0.00.056.694 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.562 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.566 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.580 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.496 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.497 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.498 I llama_new_context_with_model: graph nodes  = 967
0.00.067.498 I llama_new_context_with_model: graph splits = 2
0.00.067.510 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.577.795 I 
0.00.577.828 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.577.847 I perplexity: tokenizing the input ..
0.00.586.087 I perplexity: tokenization took 8.238 ms
0.00.586.103 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.720.605 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.721.985 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.722.001 I llama_perf_context_print:        load time =     567.84 ms
0.00.722.002 I llama_perf_context_print: prompt eval time =     134.27 ms /   128 tokens (    1.05 ms per token,   953.27 tokens per second)
0.00.722.003 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.722.003 I llama_perf_context_print:       total time =     144.21 ms /   129 tokens
0.00.722.440 I ggml_metal_free: deallocating

real	0m0.738s
user	0m0.079s
sys	0m0.112s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.699 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.704 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.705 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.706 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.711 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.712 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.720 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.720 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.721 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.721 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.721 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.722 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.722 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.724 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.724 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.725 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.628 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.552 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.553 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.553 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.554 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.554 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.554 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.555 I llama_model_loader: - type  f32:  194 tensors
0.00.024.555 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.556 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.585 I llm_load_vocab: special tokens cache size = 25
0.00.051.673 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.676 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.676 I llm_load_print_meta: arch             = gptneox
0.00.051.677 I llm_load_print_meta: vocab type       = BPE
0.00.051.677 I llm_load_print_meta: n_vocab          = 50304
0.00.051.677 I llm_load_print_meta: n_merges         = 50009
0.00.051.677 I llm_load_print_meta: vocab_only       = 0
0.00.051.677 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.678 I llm_load_print_meta: n_embd           = 2048
0.00.051.678 I llm_load_print_meta: n_layer          = 24
0.00.051.681 I llm_load_print_meta: n_head           = 16
0.00.051.681 I llm_load_print_meta: n_head_kv        = 16
0.00.051.682 I llm_load_print_meta: n_rot            = 32
0.00.051.684 I llm_load_print_meta: n_swa            = 0
0.00.051.684 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.685 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.685 I llm_load_print_meta: n_gqa            = 1
0.00.051.686 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.687 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.689 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.689 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.689 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.690 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.690 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.690 I llm_load_print_meta: n_ff             = 8192
0.00.051.691 I llm_load_print_meta: n_expert         = 0
0.00.051.691 I llm_load_print_meta: n_expert_used    = 0
0.00.051.692 I llm_load_print_meta: causal attn      = 1
0.00.051.692 I llm_load_print_meta: pooling type     = 0
0.00.051.693 I llm_load_print_meta: rope type        = 2
0.00.051.693 I llm_load_print_meta: rope scaling     = linear
0.00.051.693 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.694 I llm_load_print_meta: freq_scale_train = 1
0.00.051.694 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.694 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.694 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.694 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.694 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.695 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.706 I llm_load_print_meta: model type       = 1.4B
0.00.051.706 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.707 I llm_load_print_meta: model params     = 1.41 B
0.00.051.708 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.708 I llm_load_print_meta: general.name     = 1.4B
0.00.051.708 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.708 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.709 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.709 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.710 I llm_load_print_meta: LF token         = 128 ''
0.00.051.710 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.711 I llm_load_print_meta: max token length = 1024
0.00.053.695 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.695 I llm_load_tensors: offloading output layer to GPU
0.00.053.695 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.705 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.707 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.563 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.564 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.564 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.564 I llama_new_context_with_model: n_batch       = 2048
0.00.054.564 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.564 I llama_new_context_with_model: flash_attn    = 0
0.00.054.565 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.565 I llama_new_context_with_model: freq_scale    = 1
0.00.054.565 I ggml_metal_init: allocating
0.00.054.568 I ggml_metal_init: found device: Apple M4
0.00.054.570 I ggml_metal_init: picking default device: Apple M4
0.00.055.092 I ggml_metal_init: using embedded metal library
0.00.056.993 I ggml_metal_init: GPU name:   Apple M4
0.00.056.995 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.995 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.995 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.996 I ggml_metal_init: simdgroup reduction   = true
0.00.056.996 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.996 I ggml_metal_init: has bfloat            = true
0.00.056.996 I ggml_metal_init: use bfloat            = true
0.00.056.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.998 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.110 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.120 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.139 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.063 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.064 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.065 I llama_new_context_with_model: graph nodes  = 967
0.00.085.065 I llama_new_context_with_model: graph splits = 2
0.00.085.078 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.703.695 I main: llama threadpool init, n_threads = 4
0.00.703.733 I 
0.00.703.761 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.703.761 I 
0.00.703.980 I sampler seed: 1234
0.00.703.984 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.703.995 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.703.996 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.703.996 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.540.335 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 63963.96 tokens per second)
0.01.540.335 I llama_perf_context_print:        load time =     694.93 ms
0.01.540.336 I llama_perf_context_print: prompt eval time =      38.73 ms /     7 tokens (    5.53 ms per token,   180.74 tokens per second)
0.01.540.337 I llama_perf_context_print:        eval time =     794.82 ms /    63 runs   (   12.62 ms per token,    79.26 tokens per second)
0.01.540.337 I llama_perf_context_print:       total time =     836.64 ms /    70 tokens
0.01.540.519 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.108s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.635 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.470 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.475 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.476 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.477 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.477 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.485 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.485 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.486 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.486 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.487 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.487 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.487 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.489 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.489 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.490 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.312 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.357 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.240 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.241 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.242 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.242 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.242 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.243 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.243 I llama_model_loader: - type  f32:  194 tensors
0.00.023.244 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.244 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.976 I llm_load_vocab: special tokens cache size = 25
0.00.049.882 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.884 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.885 I llm_load_print_meta: arch             = gptneox
0.00.049.885 I llm_load_print_meta: vocab type       = BPE
0.00.049.885 I llm_load_print_meta: n_vocab          = 50304
0.00.049.886 I llm_load_print_meta: n_merges         = 50009
0.00.049.886 I llm_load_print_meta: vocab_only       = 0
0.00.049.886 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.886 I llm_load_print_meta: n_embd           = 2048
0.00.049.886 I llm_load_print_meta: n_layer          = 24
0.00.049.889 I llm_load_print_meta: n_head           = 16
0.00.049.890 I llm_load_print_meta: n_head_kv        = 16
0.00.049.890 I llm_load_print_meta: n_rot            = 32
0.00.049.891 I llm_load_print_meta: n_swa            = 0
0.00.049.891 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.891 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.892 I llm_load_print_meta: n_gqa            = 1
0.00.049.893 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.893 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.894 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.894 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.895 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.895 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.895 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.896 I llm_load_print_meta: n_ff             = 8192
0.00.049.896 I llm_load_print_meta: n_expert         = 0
0.00.049.896 I llm_load_print_meta: n_expert_used    = 0
0.00.049.896 I llm_load_print_meta: causal attn      = 1
0.00.049.896 I llm_load_print_meta: pooling type     = 0
0.00.049.896 I llm_load_print_meta: rope type        = 2
0.00.049.897 I llm_load_print_meta: rope scaling     = linear
0.00.049.897 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.898 I llm_load_print_meta: freq_scale_train = 1
0.00.049.898 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.898 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.898 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.898 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.898 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.899 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.899 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.910 I llm_load_print_meta: model type       = 1.4B
0.00.049.911 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.911 I llm_load_print_meta: model params     = 1.41 B
0.00.049.912 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.912 I llm_load_print_meta: general.name     = 1.4B
0.00.049.912 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.914 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.914 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: LF token         = 128 ''
0.00.049.915 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: max token length = 1024
0.00.051.856 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.856 I llm_load_tensors: offloading output layer to GPU
0.00.051.857 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.867 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.868 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.760 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.761 I llama_new_context_with_model: n_ctx         = 128
0.00.052.761 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.761 I llama_new_context_with_model: n_batch       = 128
0.00.052.761 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.761 I llama_new_context_with_model: flash_attn    = 0
0.00.052.762 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.762 I llama_new_context_with_model: freq_scale    = 1
0.00.052.762 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.763 I ggml_metal_init: allocating
0.00.052.769 I ggml_metal_init: found device: Apple M4
0.00.052.771 I ggml_metal_init: picking default device: Apple M4
0.00.053.326 I ggml_metal_init: using embedded metal library
0.00.055.276 I ggml_metal_init: GPU name:   Apple M4
0.00.055.277 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.278 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.278 I ggml_metal_init: simdgroup reduction   = true
0.00.055.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.279 I ggml_metal_init: has bfloat            = true
0.00.055.279 I ggml_metal_init: use bfloat            = true
0.00.055.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.280 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.037 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.042 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.058 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.969 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.970 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.970 I llama_new_context_with_model: graph nodes  = 967
0.00.065.970 I llama_new_context_with_model: graph splits = 2
0.00.065.983 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.495 I 
0.00.645.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.645.567 I perplexity: tokenizing the input ..
0.00.653.619 I perplexity: tokenization took 8.05 ms
0.00.653.633 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.494 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.795.843 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.795.867 I llama_perf_context_print:        load time =     636.86 ms
0.00.795.868 I llama_perf_context_print: prompt eval time =     140.62 ms /   128 tokens (    1.10 ms per token,   910.27 tokens per second)
0.00.795.869 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.870 I llama_perf_context_print:       total time =     150.37 ms /   129 tokens
0.00.796.300 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.078s
sys	0m0.118s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.770 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.139 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.144 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.150 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.150 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.151 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.151 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.152 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.160 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.161 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.161 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.161 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.163 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.163 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.164 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.124 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.937 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.938 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.938 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.939 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.939 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.940 I llama_model_loader: - type  f32:  194 tensors
0.00.024.940 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.763 I llm_load_vocab: special tokens cache size = 25
0.00.051.675 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.678 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.678 I llm_load_print_meta: arch             = gptneox
0.00.051.679 I llm_load_print_meta: vocab type       = BPE
0.00.051.679 I llm_load_print_meta: n_vocab          = 50304
0.00.051.679 I llm_load_print_meta: n_merges         = 50009
0.00.051.679 I llm_load_print_meta: vocab_only       = 0
0.00.051.679 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.680 I llm_load_print_meta: n_embd           = 2048
0.00.051.680 I llm_load_print_meta: n_layer          = 24
0.00.051.683 I llm_load_print_meta: n_head           = 16
0.00.051.683 I llm_load_print_meta: n_head_kv        = 16
0.00.051.684 I llm_load_print_meta: n_rot            = 32
0.00.051.684 I llm_load_print_meta: n_swa            = 0
0.00.051.684 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.684 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.685 I llm_load_print_meta: n_gqa            = 1
0.00.051.686 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.687 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.687 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.688 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.688 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.688 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.688 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.689 I llm_load_print_meta: n_ff             = 8192
0.00.051.689 I llm_load_print_meta: n_expert         = 0
0.00.051.689 I llm_load_print_meta: n_expert_used    = 0
0.00.051.689 I llm_load_print_meta: causal attn      = 1
0.00.051.689 I llm_load_print_meta: pooling type     = 0
0.00.051.690 I llm_load_print_meta: rope type        = 2
0.00.051.690 I llm_load_print_meta: rope scaling     = linear
0.00.051.690 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.690 I llm_load_print_meta: freq_scale_train = 1
0.00.051.691 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.691 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.692 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.692 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.692 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.694 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.694 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.705 I llm_load_print_meta: model type       = 1.4B
0.00.051.706 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.706 I llm_load_print_meta: model params     = 1.41 B
0.00.051.706 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.706 I llm_load_print_meta: general.name     = 1.4B
0.00.051.707 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.707 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.707 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.707 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.708 I llm_load_print_meta: LF token         = 128 ''
0.00.051.708 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.708 I llm_load_print_meta: max token length = 1024
0.00.053.241 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.241 I llm_load_tensors: offloading output layer to GPU
0.00.053.241 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.250 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.251 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.084 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.085 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.086 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.086 I llama_new_context_with_model: n_batch       = 2048
0.00.054.086 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.086 I llama_new_context_with_model: flash_attn    = 0
0.00.054.087 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.087 I llama_new_context_with_model: freq_scale    = 1
0.00.054.087 I ggml_metal_init: allocating
0.00.054.093 I ggml_metal_init: found device: Apple M4
0.00.054.095 I ggml_metal_init: picking default device: Apple M4
0.00.054.647 I ggml_metal_init: using embedded metal library
0.00.056.566 I ggml_metal_init: GPU name:   Apple M4
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.568 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.569 I ggml_metal_init: simdgroup reduction   = true
0.00.056.569 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.569 I ggml_metal_init: has bfloat            = true
0.00.056.569 I ggml_metal_init: use bfloat            = true
0.00.056.570 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.599 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.604 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.622 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.655 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.657 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.657 I llama_new_context_with_model: graph nodes  = 967
0.00.085.657 I llama_new_context_with_model: graph splits = 2
0.00.085.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.212 I main: llama threadpool init, n_threads = 4
0.00.761.253 I 
0.00.761.285 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.761.285 I 
0.00.761.516 I sampler seed: 1234
0.00.761.520 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.541 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.541 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.541 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.624.605 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 57028.11 tokens per second)
0.01.624.605 I llama_perf_context_print:        load time =     751.44 ms
0.01.624.606 I llama_perf_context_print: prompt eval time =      38.48 ms /     7 tokens (    5.50 ms per token,   181.89 tokens per second)
0.01.624.607 I llama_perf_context_print:        eval time =     821.48 ms /    63 runs   (   13.04 ms per token,    76.69 tokens per second)
0.01.624.608 I llama_perf_context_print:       total time =     863.39 ms /    70 tokens
0.01.624.788 I ggml_metal_free: deallocating

real	0m1.643s
user	0m0.109s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4242 (642330ac) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.917 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.506 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.510 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.512 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.513 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.513 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.513 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.521 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.521 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.521 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.522 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.522 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.522 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.523 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.525 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.525 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.187 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.925 I llama_model_loader: - type  f32:  194 tensors
0.00.023.926 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.797 I llm_load_vocab: special tokens cache size = 25
0.00.049.644 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.647 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.647 I llm_load_print_meta: arch             = gptneox
0.00.049.647 I llm_load_print_meta: vocab type       = BPE
0.00.049.648 I llm_load_print_meta: n_vocab          = 50304
0.00.049.648 I llm_load_print_meta: n_merges         = 50009
0.00.049.648 I llm_load_print_meta: vocab_only       = 0
0.00.049.648 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.648 I llm_load_print_meta: n_embd           = 2048
0.00.049.648 I llm_load_print_meta: n_layer          = 24
0.00.049.651 I llm_load_print_meta: n_head           = 16
0.00.049.652 I llm_load_print_meta: n_head_kv        = 16
0.00.049.652 I llm_load_print_meta: n_rot            = 32
0.00.049.652 I llm_load_print_meta: n_swa            = 0
0.00.049.652 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.652 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.654 I llm_load_print_meta: n_gqa            = 1
0.00.049.655 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.656 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.656 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.657 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.657 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.657 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.657 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.659 I llm_load_print_meta: n_ff             = 8192
0.00.049.659 I llm_load_print_meta: n_expert         = 0
0.00.049.659 I llm_load_print_meta: n_expert_used    = 0
0.00.049.659 I llm_load_print_meta: causal attn      = 1
0.00.049.659 I llm_load_print_meta: pooling type     = 0
0.00.049.659 I llm_load_print_meta: rope type        = 2
0.00.049.659 I llm_load_print_meta: rope scaling     = linear
0.00.049.660 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.660 I llm_load_print_meta: freq_scale_train = 1
0.00.049.660 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.661 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.661 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.661 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.661 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.661 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.661 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.673 I llm_load_print_meta: model type       = 1.4B
0.00.049.673 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.673 I llm_load_print_meta: model params     = 1.41 B
0.00.049.674 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.674 I llm_load_print_meta: general.name     = 1.4B
0.00.049.674 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.674 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.674 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.675 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.675 I llm_load_print_meta: LF token         = 128 ''
0.00.049.675 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.675 I llm_load_print_meta: max token length = 1024
0.00.051.647 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.647 I llm_load_tensors: offloading output layer to GPU
0.00.051.647 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.657 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.659 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.511 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.511 I llama_new_context_with_model: n_ctx         = 128
0.00.052.512 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.512 I llama_new_context_with_model: n_batch       = 128
0.00.052.512 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.512 I llama_new_context_with_model: flash_attn    = 0
0.00.052.513 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.513 I llama_new_context_with_model: freq_scale    = 1
0.00.052.513 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.514 I ggml_metal_init: allocating
0.00.052.516 I ggml_metal_init: found device: Apple M4
0.00.052.518 I ggml_metal_init: picking default device: Apple M4
0.00.053.042 I ggml_metal_init: using embedded metal library
0.00.054.949 I ggml_metal_init: GPU name:   Apple M4
0.00.054.951 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.951 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.951 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.952 I ggml_metal_init: simdgroup reduction   = true
0.00.054.952 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.952 I ggml_metal_init: has bfloat            = true
0.00.054.952 I ggml_metal_init: use bfloat            = true
0.00.054.953 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.869 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.872 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.888 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.793 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.794 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.794 I llama_new_context_with_model: graph nodes  = 967
0.00.064.794 I llama_new_context_with_model: graph splits = 2
0.00.064.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.234.130 I 
0.00.234.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.234.171 I perplexity: tokenizing the input ..
0.00.241.765 I perplexity: tokenization took 7.593 ms
0.00.241.777 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.382.051 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.383.429 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.383.453 I llama_perf_context_print:        load time =     224.21 ms
0.00.383.454 I llama_perf_context_print: prompt eval time =     140.04 ms /   128 tokens (    1.09 ms per token,   914.00 tokens per second)
0.00.383.455 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.383.455 I llama_perf_context_print:       total time =     149.32 ms /   129 tokens
0.00.383.841 I ggml_metal_free: deallocating

real	0m0.400s
user	0m0.076s
sys	0m0.051s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4242 (642330ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141c0a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141c0ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141c0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141c0b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141c0be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141c0c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141c0c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141c0cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141c0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141c0da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141c0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141c0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141c0ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141c0f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141c0fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141c10610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141c10d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141c11450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141c11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141c12340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141c12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141c13180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141c138a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141c14140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141c14860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141c14b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141c15130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141c15da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141c162e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141c165a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141c16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141c16d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141c17590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141c17ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141c17d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141c18230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141c186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141c18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141c19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141c194b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141c19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141c19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141c1a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141c1a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141c1a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141c1b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141c1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141c1bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141c1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141c1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141c1d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141c1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141c1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141c1e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141c1eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141c1f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141c1f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141c1f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141c1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141c20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141c20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141c20ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141c21180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141c21620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141c21ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141c21f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141c22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141c228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141c22d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141c231e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141c23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141c23b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141c23fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141c24460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141c24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141c24da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141c25240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141c256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141c25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141c26020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141c264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141c26960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141c26e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141c272a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141c27740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141c27be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141c28080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141c28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141c289c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141c28e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141c29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141c297a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141c29c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141c2a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141c2a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141c2aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141c2aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141c1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141c2b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141c2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141c2be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141c2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141c2c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141c2cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141c2d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141c2d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141c2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141c2deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141c2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141c2e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141c2ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141c2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141c2f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141c2fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141c2ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141c303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141c30850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141c30cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141c31190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141c31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141c31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141c31f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141c32410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141c328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141c32d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141c331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141c33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141c33b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141c33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141c34470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141c34910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141c34db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141c35250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141c356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141c35b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141c36030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141c364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141c36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141c36e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141c372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141c37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141c37bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141c38090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141c38530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141c389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141c38e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141c39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141c397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141c39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141c3a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141c3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141c3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141c3aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141c3b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141c3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141c3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141c3c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141c3c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141c3cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141c3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141c3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141c3df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141c3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141c3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141c3f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141c3f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141c3faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141c402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141c407f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141c40d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141c41290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141c417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141c41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141c42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141c427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141c42d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141c43270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141c437c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141c43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141c44260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141c447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141c44d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141c45250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141c457a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141c45cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141c46240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141c46790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141c46ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141c47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141c47780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141c47cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141c48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141c48770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141c48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141c49210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141c49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141c49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141c4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141c4a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141c4aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141c4b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141c4b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141c4bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141c4c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141c4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141c4cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141c4d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141c4d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141c4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141c4e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141c4e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141c4ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141c4f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141c4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141c4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141c501a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141c506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141c50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141c51190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141c516e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141c51c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141c52180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141c526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141c52c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141c530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141c53560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141c53a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141c53ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141c54340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141c547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141c54c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141c55120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141c555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141c55a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141c55f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141c563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141c56840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141c56d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141c574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141c57bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141c582f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141c58a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141c58cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141c592e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141c598f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.138.955 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141c49f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141c4a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141c4a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141c4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141c4b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141c4b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141c4b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141c4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141c4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141c4c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141c4cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141c4d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141c4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141c4e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141c4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141c4f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141c4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141c4fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141c50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141c50f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141c515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141c51ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141c523d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141c52ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141c531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141c53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141c53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141c53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141c54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141c547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141c54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141c550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141c55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141c557f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141c55c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141c560d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141c56540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141c569b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141c56e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141c57290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141c57700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141c57b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141c57fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141c58450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141c588c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141c58d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141c591a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141c59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141c59a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141c0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141c0c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141c0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141c0be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141c09ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141c0a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141c17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141c180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141c18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141c189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141c18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141c192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141c19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141c19b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141c19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141c1a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141c1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141b04d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141b05040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141b054b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141b05920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141b05d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141b06200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141b06670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141b06ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141b06f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141b073c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141b07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141b07ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141b08110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141b08580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141b089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141b08e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141b092d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141b09740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141b09bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141b0a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141b0a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141b0a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141b0ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141b0b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141b0b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141b0bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141b0bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141b0c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141b0c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141b0cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141b0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141b0d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141b0d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141b0de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141b0e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141b0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141b0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141b0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141b0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141b0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141b0fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141b101c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141b10630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141b10aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141b10f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141b11380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141b117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141b11c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141b120d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141b12540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141b129b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141b12e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141b13290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141b13700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141b13b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141b13fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141b14450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141b148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141b14d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141b151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141b15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141b15a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141b15ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141b16360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141b167d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141b16c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141b170b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141b17520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141b17990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141b17e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141b18270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141b186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141b18b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141b18fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141b19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141b198a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141b19d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141b1a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141b1a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141b1aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141b1aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141b1b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141b1b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141b1bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141b1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141b1c500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141b1c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141b1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141b1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141b1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141b1db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141b1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141b1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141b1e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141b1ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141b1f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141b1f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141b1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141b1ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141b20430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141b208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141b213f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141b216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141b21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141b21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141b22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141b226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141b22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141b22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141b23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141b23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141b23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141b24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141b245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141b24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141b24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141b25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141b25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141b25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141b26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141b264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141b26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141b26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141b27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141b276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141b27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141b27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141b283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141b28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141b28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141b29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141b295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141b29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141b29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141b2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141b2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141b2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141b2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141b2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141b2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141b2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141b2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141b2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141b2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141b2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141b2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141b2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141b2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141b2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141b2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141b2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141b2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141b2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141b2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141b2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141b30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141b304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141b30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141b30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141b311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141b31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141b31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141b31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141b323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141b32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141b32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141b33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141b33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141b339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141b33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141b342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141b34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141b35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141b35990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141b360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141b367d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141b36a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141b36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141b371c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x141c0bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x141c09d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x141c0b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x141c0c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x141c0b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x141c49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x141c49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x141c4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141c4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141c4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141c4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141c4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141c4bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141c4c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141c4cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141c4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141c4dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141c4e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141c4eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141c4f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141c4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141c50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141c50940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141c51030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141c51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141c51b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141c52000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141c52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141c528e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141c52d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x141c531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141c53630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141c53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141c53d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141c541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141c54640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141c54ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141c54f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141c55390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141c55800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x141c55c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141c560e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141c56550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x141c569c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x141c56e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x141c572a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x141c57710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x141c57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x141c57ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x141c58460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x141c588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x141c58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141c591b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141c59620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141c59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141c17c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141c180e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141c18550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141c189c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141c18e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x141c192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141c19710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141c19b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x141c19ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141c1a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141c1a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141c1ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141c1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141c1b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141c1ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141c1bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141c1c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141c1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141c1cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141c1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141c1d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141c1d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141c1de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141c1e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141c1e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141c1eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141c1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141c1f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141c1f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141c1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141c20190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141c20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141c20a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141c20ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141c21350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141c217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x141c21c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141c220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141c22510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x141c22980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141c22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141c23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141c236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141c23b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x141c23fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141c24420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141c24890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x141c24d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141c25170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x141c255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x141c25a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x141c25ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x141c26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x141c267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x141c26c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x141c27080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x141c274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x141c27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x141c27dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141c28240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x141c286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141c28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141c28f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x141c29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141c29870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141c29ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141c2a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141c2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141c2aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141c2aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141c2b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x141c2b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141c2bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141c2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141c2c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141c2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141c2cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141c2d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141c2d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141c2db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141c2df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141c2e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141c2e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141c2ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x141c2f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141c2f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141c2fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141c2fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141c302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141c30760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141c30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141c31040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141c314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141c31920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141c31d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x141c32200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141c32670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141c32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141c32f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x141c333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141c33830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141c33ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141c34110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141c34580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x141c349f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x141c34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x141c352d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x141c35740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x141c35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141c36020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x141c36490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141c36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141c37080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141c374f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141c37960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141c37dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141c38240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141c386b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141c38b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141c38f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141c39400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141c39870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141c39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141c3a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x141c3a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141c3aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141c3aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x141c3b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141c3b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141c3bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x141c3c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141c3c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141c3c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x141c3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141c3d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141c3d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141c3db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141c3df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141c3e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x141c3e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x141c3ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141c3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141c3f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x141c3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141c3fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141c402f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x141c40760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141c40bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x141c41040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x141c414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x141c41920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x141c41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x141c42200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x141c42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x141c42ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x141c42f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x141c433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141c43830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141c43ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141c44110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141c44580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141c449f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141c44e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141c452d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141c45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141c45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141c46020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141c46490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141c46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141c46d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141c471e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x141c47650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141c47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141c47f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141c483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141c48810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141c48c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141c490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141c16480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141c168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141c16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141c171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141c0d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141c0df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x141c0e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141c0ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x141c0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x141c0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141c0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141c10130 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.786s
user	0m0.288s
sys	0m0.295s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4242 (642330ac)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11ce0dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11ce0e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11ce0e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11ce0ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11ce0f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11ce0faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11ce100a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11ce10650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11ce10c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11ce11100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11ce11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11ce11b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11ce12620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11ce12dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11ce135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11ce13d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11ce14420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11ce14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11ce15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11ce15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11ce16150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11ce16870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11ce16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11ce17830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11ce17f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11ce18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11ce18820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11ce19490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11ce199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11ce19c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11ce1a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11ce1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11ce1ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11ce1b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11ce1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11ce1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11ce1bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11ce1c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11ce1c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11ce1cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11ce1d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11ce1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11ce1d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11ce1de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11ce1e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11ce1e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11ce1ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11ce1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11ce1fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11ce20240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11ce20850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11ce20e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11ce21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11ce21a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11ce22270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11ce22710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11ce22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11ce22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11ce23480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11ce23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11ce23f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11ce243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11ce24870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11ce24d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11ce251b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11ce25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11ce25af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11ce25f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11ce26430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11ce268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11ce26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11ce27210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11ce276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11ce27b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11ce27ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11ce28490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11ce28930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ce28dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ce29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ce29710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ce29bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ce2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ce2a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ce2a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ce2ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ce2b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ce2b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ce2bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ce2c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ce2c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ce2c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ce2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ce2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ce2d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ce2dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ce2e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ce2e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ce1f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ce2ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ce2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ce2f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ce2f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ce2fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ce30320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ce307c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ce30c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ce31100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ce315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ce31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ce31ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ce32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ce32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ce32cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ce33160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ce33600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ce33aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ce33f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ce343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ce34880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ce34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ce351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ce35660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ce35b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ce35fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ce36440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ce368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ce36d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ce37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ce376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ce37b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ce38000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ce384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ce38940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ce38de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ce39280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ce39720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ce39bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ce3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ce3a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ce3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ce3ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ce3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ce3b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ce3bc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ce3c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ce3c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ce3ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ce3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ce3d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ce3d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ce3dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ce3e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ce3e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ce3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ce3f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ce3f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ce3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ce3fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ce403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ce409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ce40ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ce41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ce41c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ce42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ce428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ce42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ce431e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ce43990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ce43ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ce44430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ce44980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ce44ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ce45420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ce45970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ce45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ce46410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ce46960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ce46eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ce47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ce47950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ce47ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ce483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ce48940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ce48e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ce493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ce49930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ce49e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ce4a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ce4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ce4ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ce4b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ce4b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ce4be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ce4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ce4c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ce4ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ce4d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ce4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ce4de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ce4e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ce4e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ce4ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ce4f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ce4f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ce4fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ce50370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ce508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ce50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ce51360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ce518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ce51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ce52350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ce528a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ce52df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ce53340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ce53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ce53de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ce54330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ce54880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ce54dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ce55320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ce55870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ce55dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ce56310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ce567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ce56c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ce570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ce57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ce57a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ce57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ce58370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ce58810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ce58cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ce59150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ce595f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ce59a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ce59f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ce5a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ce5aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ce5b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ce5b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ce5c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ce5c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ce5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ce5cfe0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.397 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e0053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e0069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e0072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e007e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e008940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e0090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e009900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e00a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e00a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e00ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e00b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e00bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e00c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e00cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e00d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e00d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e00e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e00e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e00e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e00eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e00ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e00f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e00f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e00fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e0101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e0111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e0123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e0130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e0139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e0142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e0158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e0161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e016ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e0170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e017510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e017980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e017df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e018260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e0186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e018b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e018fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e019420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e019890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e019d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e01a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e01a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e01aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e01aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e01b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e01b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e01bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e01c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e01c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e01c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e01cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e01d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e01d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e01db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e01df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e01e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e01e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e01ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e01f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e01f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e01fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e01fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e020310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e020780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e020bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e021060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e0214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e021940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e021db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e022220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e022690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e022b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e022f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e0233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e023850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e023cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e024130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e0245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e024a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e024e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e0252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e025760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e025bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e026040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e0264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e026920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e026d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e027200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e027670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e027ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e027f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e0283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e028830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e028ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e029110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e029580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e0299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e029e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e02a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e02a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e02abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e02b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e02b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e02b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e02bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e02c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e02c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e02cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e02cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e02d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e02d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e02dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e02e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e02e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e02e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e02ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e02f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e02f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e02fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e030000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e030470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e0308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e030d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e0311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e031630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e031aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e031f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e032380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e0327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e032c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e0330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e033540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e0339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e033e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e034290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e034700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e034b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e034fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e035450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e035fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e0362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e036560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e0369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e036e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e0372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e037720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e037b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e038000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e038470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e0388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e038d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e0391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e039630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e039aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e039f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e03a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e03a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e03ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e03b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e03b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e03b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e03be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e03c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e03c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e03cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e03cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e03d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e03d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e03dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e03e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e03e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e03ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e03eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e03f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e03f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e03fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e0400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e040520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e040990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e040e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e041270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e0416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e041b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e041fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e042430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e0428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e042d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e043180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e0435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e043a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e043ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e044340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e0447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e044c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e045090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e045500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e045970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e045de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e046250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e0466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e046b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e046fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e047410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e047880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e047cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e048160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e0485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e048a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e048eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e049320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e049e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e04a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e04aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e04b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e04b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e04b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e04bdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e004ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e004f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e0053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e005830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e005ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e006110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e006580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e0069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e006e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e0072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e007740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e007d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e008610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e008d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e009570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e009c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e00a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e00aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e00b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e00bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e00c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e00c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e00cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e00d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e00dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e00e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e00e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e00eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e00ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e00f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e00f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e00fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e0100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e0103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e010810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e010c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e0110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e011560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e0119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e011e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e0122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e012720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e012b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e013000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e013470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e0138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e013d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e0141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e014630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e014aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e014f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e015380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e0157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e015c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e0160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e016540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e0169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e016e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e017290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e017700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e017b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e017fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e018450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e0188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e018d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e0191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e019610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e019a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11cf07400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11cf076c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11cf07c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11cf08190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11cf086a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11cf08bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11cf090c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11cf095d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11cf09ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11cf09ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11cf0a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11cf0aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11cf0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11cf0b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11cf0b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11cf0be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11cf0c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11cf0c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11cf0cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11cf0d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11cf0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11cf0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11cf0e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11cf0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11cf0ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11cf0f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11cf0f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11cf0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11cf10020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11cf10530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11cf10a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11cf10f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11cf11460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11cf11970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11cf11e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11cf12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11cf128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11cf12db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11cf132c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11cf137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11cf13ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11cf141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11cf146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11cf14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11cf15100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11cf15610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11cf15b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11cf16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11cf16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11cf16a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11cf16f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11cf17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11cf17980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11cf17e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11cf183a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11cf188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11cf18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11cf192d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11cf197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11cf19cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11cf1a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11cf1a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11cf1ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11cf1b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11cf1b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11cf1bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11cf1c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11cf1c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11cf1ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11cf1cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11cf1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11cf1d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11cf1dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11cf1e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11cf1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11cf1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11cf1f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11cf1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11cf1fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11cf20230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11cf20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11cf20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11cf21160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11cf21670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11cf21b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11cf22090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11cf22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11cf22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11cf231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11cf23750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11cf23d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11cf24370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11cf24980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11cf24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11cf255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11cf25d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11cf26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11cf266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11cf26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11cf27320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11cf27870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11cf27dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11cf28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11cf28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11cf28db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11cf29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11cf29850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11cf29da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11cf2a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11cf2a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11cf2ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11cf2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11cf2b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11cf2bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11cf2c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11cf2c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11cf2cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11cf2d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11cf2d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11cf2dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11cf2e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11cf2e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11cf2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11cf2f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11cf2f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11cf2fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11cf30290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11cf307e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11cf30d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11cf31280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11cf317d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11cf31d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11cf32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11cf327c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11cf32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11cf33260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11cf337b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11cf33d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11cf34250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11cf347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11cf34cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11cf35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11cf35790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11cf35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11cf36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11cf36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11cf36cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11cf37220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11cf37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11cf37cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11cf38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11cf38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11cf38cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11cf39200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11cf39750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11cf39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11cf3a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11cf3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11cf3aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11cf3af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11cf3b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11cf3b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11cf3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11cf3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11cf3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11cf3cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11cf3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11cf3d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11cf3d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11cf3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11cf3e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11cf3ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11cf3f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11cf3fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11cf3fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11cf40360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11cf40970 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.923s
user	0m0.239s
sys	0m0.140s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.55 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.13 sec
        1.15 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.24 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.36 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.60 sec*proc (2 tests)

Total Test time (real) =   0.60 sec
        0.61 real         0.15 user         0.04 sys
```
