Requirement already satisfied: numpy~=1.24.4 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 1)) (1.24.4)
Requirement already satisfied: sentencepiece~=0.1.98 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 2)) (0.1.99)
Requirement already satisfied: transformers<5.0.0,>=4.35.2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.38.1)
Requirement already satisfied: gguf>=0.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 4)) (0.8.0)
Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 5)) (4.25.3)
Requirement already satisfied: torch~=2.1.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.2)
Requirement already satisfied: einops~=0.7.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from -r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 3)) (0.7.0)
Requirement already satisfied: filelock in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)
Requirement already satisfied: pyyaml>=5.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.3)
Requirement already satisfied: safetensors>=0.4.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.2)
Requirement already satisfied: regex!=2019.12.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.12.25)
Requirement already satisfied: tqdm>=4.27 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.2)
Requirement already satisfied: tokenizers<0.19,>=0.14 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.2)
Requirement already satisfied: packaging>=20.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)
Requirement already satisfied: requests in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)
Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.4.5.107)
Requirement already satisfied: networkx in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)
Requirement already satisfied: typing-extensions in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.9.0)
Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.18.1)
Requirement already satisfied: jinja2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.3)
Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (10.3.2.106)
Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.0.106)
Requirement already satisfied: triton==2.1.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)
Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: sympy in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)
Requirement already satisfied: fsspec in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2024.2.0)
Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (11.0.2.54)
Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (8.9.2.26)
Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.105)
Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.1.3.1)
Requirement already satisfied: nvidia-nvjitlink-cu12 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (12.3.101)
Requirement already satisfied: MarkupSafe>=2.0 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from jinja2->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.5)
Requirement already satisfied: urllib3<3,>=1.21.1 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.2.1)
Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)
Requirement already satisfied: certifi>=2017.4.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (2024.2.2)
Requirement already satisfied: idna<4,>=2.5 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.35.2->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)
Requirement already satisfied: mpmath>=0.19 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from sympy->torch~=2.1.1->-r /home/ggml/work/llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)
Obtaining file:///home/ggml/work/llama.cpp/gguf-py
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Checking if build backend supports build_editable: started
  Checking if build backend supports build_editable: finished with status 'done'
  Getting requirements to build editable: started
  Getting requirements to build editable: finished with status 'done'
  Preparing editable metadata (pyproject.toml): started
  Preparing editable metadata (pyproject.toml): finished with status 'done'
Requirement already satisfied: numpy>=1.17 in /mnt/llama.cpp/venv/lib/python3.10/site-packages (from gguf==0.8.0) (1.24.4)
Building wheels for collected packages: gguf
  Building editable for gguf (pyproject.toml): started
  Building editable for gguf (pyproject.toml): finished with status 'done'
  Created wheel for gguf: filename=gguf-0.8.0-py3-none-any.whl size=3229 sha256=e3aafedf1f270ac30d9f1b3b317301ce657a1cae1c7f5a947ba0148604ce0003
  Stored in directory: /tmp/pip-ephem-wheel-cache-36prlwx4/wheels/a3/4c/52/c5934ad001d1a70ca5434f11ddc622cad9c0a484e9bf6feda3
Successfully built gguf
Installing collected packages: gguf
  Attempting uninstall: gguf
    Found existing installation: gguf 0.8.0
    Uninstalling gguf-0.8.0:
      Successfully uninstalled gguf-0.8.0
Successfully installed gguf-0.8.0
+ tee /home/ggml/results/llama.cpp/64/bfe50b684eb2a736ef546fb71d9c902fdf4594/ggml-4-x86-cuda-v100/ctest_debug.log
+ gg_run_ctest_debug
+ cd /home/ggml/work/llama.cpp
+ rm -rf build-ci-debug
+ mkdir build-ci-debug
+ cd build-ci-debug
+ set -e
+ tee -a /home/ggml/results/llama.cpp/64/bfe50b684eb2a736ef546fb71d9c902fdf4594/ggml-4-x86-cuda-v100/ctest_debug-cmake.log
+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON -DLLAMA_CUBLAS=1 ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- Found CUDAToolkit: /usr/local/cuda-12.2/include (found version "12.2.140") 
-- cuBLAS found
-- The CUDA compiler identification is NVIDIA 12.2.140
-- Detecting CUDA compiler ABI info
-- Detecting CUDA compiler ABI info - done
-- Check for working CUDA compiler: /usr/local/cuda-12.2/bin/nvcc - skipped
-- Detecting CUDA compile features
-- Detecting CUDA compile features - done
-- Using CUDA architectures: 52;61;70
-- CUDA host compiler is GNU 11.4.0

-- ccache found, compilation results will be cached. Disable with LLAMA_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: x86_64
-- x86 detected
-- Configuring done (3.2s)
-- Generating done (0.2s)
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m3.437s
user	0m2.589s
sys	0m0.844s
+ tee -a /home/ggml/results/llama.cpp/64/bfe50b684eb2a736ef546fb71d9c902fdf4594/ggml-4-x86-cuda-v100/ctest_debug-make.log
+ make -j
[  1%] Generating build details from Git
[  1%] Building C object CMakeFiles/ggml.dir/ggml.c.o
[  2%] Building C object CMakeFiles/ggml.dir/ggml-alloc.c.o
[  3%] Building C object CMakeFiles/ggml.dir/ggml-backend.c.o
[  4%] Building C object CMakeFiles/ggml.dir/ggml-quants.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  5%] Building CUDA object CMakeFiles/ggml.dir/ggml-cuda.cu.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Built target build_info
/home/ggml/work/llama.cpp/ggml-cuda.cu: In destructor ‘virtual ggml_cuda_pool_leg::~ggml_cuda_pool_leg()’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:394:19: error: comparison of integer expressions of different signedness: ‘int’ and ‘const size_t’ {aka ‘const long unsigned int’} [-Werror=sign-compare]
  394 |         for (int i = 0; i < MAX_CUDA_BUFFERS; ++i) {
      |                 ~~^~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/ggml-cuda.cu: In member function ‘virtual void* ggml_cuda_pool_leg::alloc(size_t, size_t*)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:411:19: error: comparison of integer expressions of different signedness: ‘int’ and ‘const size_t’ {aka ‘const long unsigned int’} [-Werror=sign-compare]
  411 |         for (int i = 0; i < MAX_CUDA_BUFFERS; ++i) {
      |                 ~~^~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/ggml-cuda.cu: In member function ‘virtual void ggml_cuda_pool_leg::free(void*, size_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:457:19: error: comparison of integer expressions of different signedness: ‘int’ and ‘const size_t’ {aka ‘const long unsigned int’} [-Werror=sign-compare]
  457 |         for (int i = 0; i < MAX_CUDA_BUFFERS; ++i) {
      |                 ~~^~~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_get_rows(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8701:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8700 | static void ggml_cuda_op_get_rows(
      |                                   
 8701 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_acc(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8785:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8784 | static void ggml_cuda_op_acc(
      |                              
 8785 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_gelu(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8821:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8820 | static void ggml_cuda_op_gelu(
      |                               
 8821 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_silu(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8836:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8835 | static void ggml_cuda_op_silu(
      |                               
 8836 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_gelu_quick(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8851:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8850 | static void ggml_cuda_op_gelu_quick(
      |                                     
 8851 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_tanh(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8866:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8865 | static void ggml_cuda_op_tanh(
      |                               
 8866 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_relu(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8881:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8880 | static void ggml_cuda_op_relu(
      |                               
 8881 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_hardsigmoid(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8896:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8895 | static void ggml_cuda_op_hardsigmoid(
      |                                      
 8896 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_hardswish(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8911:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8910 | static void ggml_cuda_op_hardswish(
      |                                    
 8911 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_leaky_relu(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8926:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8925 | static void ggml_cuda_op_leaky_relu(
      |                                     
 8926 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_sqr(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8944:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8943 | static void ggml_cuda_op_sqr(
      |                              
 8944 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_norm(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8959:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8958 | static void ggml_cuda_op_norm(
      |                               
 8959 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_group_norm(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8980:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8979 | static void ggml_cuda_op_group_norm(
      |                                     
 8980 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_concat(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8997:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8996 | static void ggml_cuda_op_concat(
      |                                 
 8997 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_upscale(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9014:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9013 | static void ggml_cuda_op_upscale(
      |                                  
 9014 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_pad(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9032:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9031 | static void ggml_cuda_op_pad(
      |                              
 9032 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_arange(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9050:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9049 | static void ggml_cuda_op_arange(
      |                                 
 9050 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_timestep_embedding(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9075:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9074 | static void ggml_cuda_op_timestep_embedding(
      |                                             
 9075 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_rms_norm(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9093:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9092 | static void ggml_cuda_op_rms_norm(
      |                                   
 9093 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_dequantize_mul_mat_vec(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const char*, const float*, const char*, float*, int64_t, int64_t, int64_t, int64_t, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9282:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9281 | static void ggml_cuda_op_dequantize_mul_mat_vec(
      |                                                 
 9282 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_rope(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9459:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9458 | static void ggml_cuda_op_rope(
      |                               
 9459 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_alibi(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9540:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9539 | static void ggml_cuda_op_alibi(
      |                                
 9540 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_pool2d(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9572:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9571 | static void ggml_cuda_op_pool2d(
      |                                 
 9572 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_im2col(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9606:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9605 | static void ggml_cuda_op_im2col(
      |                                 
 9606 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_sum_rows(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9648:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9647 | static void ggml_cuda_op_sum_rows(
      |                                   
 9648 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_argsort(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9666:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9665 | static void ggml_cuda_op_argsort(
      |                                  
 9666 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_diag_mask_inf(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9686:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9685 | static void ggml_cuda_op_diag_mask_inf(
      |                                        
 9686 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_soft_max(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9707:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9706 | static void ggml_cuda_op_soft_max(
      |                                   
 9707 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_scale(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9740:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9739 | static void ggml_cuda_op_scale(
      |                                
 9740 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_op_clamp(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9759:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 9758 | static void ggml_cuda_op_clamp(
      |                                
 9759 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_set_peer_access(int, int)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:9807:63: error: unused parameter ‘main_device’ [-Werror=unused-parameter]
 9807 | static void ggml_cuda_set_peer_access(const int n_tokens, int main_device) {
      |                                                           ~~~~^~~~~~~~~~~
/home/ggml/work/llama.cpp/ggml-cuda.cu: In function ‘void ggml_cuda_nop(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*)’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:10941:54: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
10941 | static void ggml_cuda_nop(ggml_backend_cuda_context & ctx, const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {
      |                           ~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~
/home/ggml/work/llama.cpp/ggml-cuda.cu: In instantiation of ‘void ggml_cuda_op_bin_bcast(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t) [with op = bin_bcast_cuda<op_repeat>; cudaStream_t = CUstream_st*]’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8770:54:   required from here
/home/ggml/work/llama.cpp/ggml-cuda.cu:8746:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
 8745 | static void ggml_cuda_op_bin_bcast(
      |                        ~~~~~~~~~~~~
 8746 |     ggml_backend_cuda_context & ctx,
      | ^  
/home/ggml/work/llama.cpp/ggml-cuda.cu: In instantiation of ‘void ggml_cuda_op_bin_bcast(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t) [with op = bin_bcast_cuda<op_add>; cudaStream_t = CUstream_st*]’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8781:51:   required from here
/home/ggml/work/llama.cpp/ggml-cuda.cu:8746:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
/home/ggml/work/llama.cpp/ggml-cuda.cu: In instantiation of ‘void ggml_cuda_op_bin_bcast(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t) [with op = bin_bcast_cuda<op_mul>; cudaStream_t = CUstream_st*]’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8809:51:   required from here
/home/ggml/work/llama.cpp/ggml-cuda.cu:8746:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
/home/ggml/work/llama.cpp/ggml-cuda.cu: In instantiation of ‘void ggml_cuda_op_bin_bcast(ggml_backend_cuda_context&, const ggml_tensor*, const ggml_tensor*, ggml_tensor*, const float*, const float*, float*, cudaStream_t) [with op = bin_bcast_cuda<op_div>; cudaStream_t = CUstream_st*]’:
/home/ggml/work/llama.cpp/ggml-cuda.cu:8817:51:   required from here
/home/ggml/work/llama.cpp/ggml-cuda.cu:8746:1: error: unused parameter ‘ctx’ [-Werror=unused-parameter]
cc1plus: all warnings being treated as errors
make[2]: *** [CMakeFiles/ggml.dir/build.make:133: CMakeFiles/ggml.dir/ggml-cuda.cu.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:758: CMakeFiles/ggml.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	1m38.488s
user	1m36.454s
sys	0m1.960s
+ cur=2
+ echo 2
+ set +x
cat: /home/ggml/results/llama.cpp/64/bfe50b684eb2a736ef546fb71d9c902fdf4594/ggml-4-x86-cuda-v100/ctest_debug-ctest.log: No such file or directory
