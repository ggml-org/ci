Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:298 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD
-- Performing Test GGML_COMPILER_SUPPORT_DOTPROD - Success
-- ARM feature DOTPROD enabled
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8
-- Performing Test GGML_COMPILER_SUPPORT_MATMUL_INT8 - Success
-- ARM feature MATMUL_INT8 enabled
-- Adding CPU backend variant ggml-cpu: -march=armv8.2a+dotprod+i8mm __ARM_FEATURE_DOTPROD;__ARM_FEATURE_MATMUL_INT8
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.657s
user	0m0.699s
sys	0m0.998s
++ nproc
+ make -j10
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Built target sha1
[  6%] Built target sha256
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 13%] Linking CXX shared library libggml-cpu.dylib
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 14%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 14%] Built target ggml-blas
[ 14%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 17%] Linking CXX shared library libggml.dylib
[ 17%] Built target ggml
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 23%] Linking CXX shared library libllama.dylib
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Built target llama-gguf
[ 23%] Built target llama-gguf-hash
[ 23%] Built target llama
[ 23%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 24%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 25%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Linking C executable ../bin/test-c
[ 28%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Built target llava
[ 32%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-run
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX static library libllava_static.a
[ 33%] Linking CXX static library libcommon.a
[ 34%] Linking CXX shared library libllava_shared.dylib
[ 34%] Built target llama-simple
[ 34%] Built target llama-simple-chat
[ 34%] Built target test-c
[ 34%] Built target llama-run
[ 34%] Built target llama-quantize-stats
[ 34%] Built target llava_static
[ 34%] Built target common
[ 34%] Built target llava_shared
[ 34%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-0
[ 40%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 41%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 44%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 45%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-log
[ 47%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-tokenizer-0
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-log
[ 49%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Built target test-arg-parser
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Built target test-chat-template
[ 61%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 64%] Built target test-autorelease
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Linking CXX executable ../bin/test-rope
[ 67%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Built target test-quantize-perf
[ 68%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Built target llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Built target test-rope
[ 72%] Built target llama-eval-callback
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-embedding
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gritlm
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-imatrix
[ 75%] Linking CXX executable ../../bin/llama-bench
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookahead
[ 77%] Built target llama-gguf-split
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Built target llama-infill
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 81%] Built target llama-bench
[ 81%] Built target llama-lookahead
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup-merge
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 84%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 85%] Generating loading.html.hpp
[ 86%] Linking CXX executable ../../bin/llama-perplexity
[ 86%] Built target llama-cli
[ 86%] Built target llama-lookup-stats
[ 86%] Built target llama-parallel
[ 87%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-quantize
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-retrieval
[ 88%] Generating index.html.hpp
[ 88%] Built target llama-passkey
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-perplexity
[ 91%] Linking CXX executable ../../bin/llama-tokenize
[ 92%] Linking CXX executable ../../bin/llama-speculative-simple
[ 92%] Built target llama-quantize
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Built target llama-retrieval
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Built target llama-save-load-state
[ 96%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 96%] Built target llama-speculative
[ 96%] Linking CXX executable ../../bin/llama-cvector-generator
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-llava-cli
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-speculative-simple
[ 96%] Built target llama-gen-docs
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 97%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 97%] Built target llama-tokenize
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.480s
user	0m5.223s
sys	0m8.632s

main: quantize time =  5467.70 ms
main:    total time =  5467.70 ms

main: quantize time =  2022.63 ms
main:    total time =  2022.63 ms

main: quantize time =  1855.47 ms
main:    total time =  1855.47 ms

main: quantize time =  3470.45 ms
main:    total time =  3470.45 ms

main: quantize time =  1380.45 ms
main:    total time =  1380.45 ms

main: quantize time =  4985.97 ms
main:    total time =  4985.97 ms

main: quantize time =  5534.10 ms
main:    total time =  5534.10 ms

main: quantize time =  6926.59 ms
main:    total time =  6926.59 ms

main: quantize time =  5908.01 ms
main:    total time =  5908.01 ms

main: quantize time =  4578.62 ms
main:    total time =  4578.62 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.111 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.224 I main: llama backend init
0.00.000.230 I main: load the model and apply lora adapter, if any
0.00.039.292 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.050.255 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.050.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.050.270 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.050.271 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.050.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.050.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.050.273 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.050.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.050.276 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.050.277 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.050.278 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.050.281 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.050.282 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.050.283 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.050.288 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.050.289 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.050.289 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.057.226 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.059.834 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.068.796 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.068.799 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.068.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.068.800 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.068.801 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.068.802 I llama_model_loader: - type  f32:  194 tensors
0.00.068.802 I llama_model_loader: - type  f16:   98 tensors
0.00.101.784 I llm_load_vocab: special tokens cache size = 25
0.00.108.708 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.108.710 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.108.711 I llm_load_print_meta: arch             = gptneox
0.00.108.711 I llm_load_print_meta: vocab type       = BPE
0.00.108.711 I llm_load_print_meta: n_vocab          = 50304
0.00.108.711 I llm_load_print_meta: n_merges         = 50009
0.00.108.712 I llm_load_print_meta: vocab_only       = 0
0.00.108.712 I llm_load_print_meta: n_ctx_train      = 2048
0.00.108.712 I llm_load_print_meta: n_embd           = 2048
0.00.108.712 I llm_load_print_meta: n_layer          = 24
0.00.108.735 I llm_load_print_meta: n_head           = 16
0.00.108.736 I llm_load_print_meta: n_head_kv        = 16
0.00.108.736 I llm_load_print_meta: n_rot            = 32
0.00.108.737 I llm_load_print_meta: n_swa            = 0
0.00.108.737 I llm_load_print_meta: n_embd_head_k    = 128
0.00.108.737 I llm_load_print_meta: n_embd_head_v    = 128
0.00.108.738 I llm_load_print_meta: n_gqa            = 1
0.00.108.738 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.108.739 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.108.740 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.108.740 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.108.740 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.108.740 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.108.741 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.108.741 I llm_load_print_meta: n_ff             = 8192
0.00.108.741 I llm_load_print_meta: n_expert         = 0
0.00.108.742 I llm_load_print_meta: n_expert_used    = 0
0.00.108.742 I llm_load_print_meta: causal attn      = 1
0.00.108.742 I llm_load_print_meta: pooling type     = 0
0.00.108.743 I llm_load_print_meta: rope type        = 2
0.00.108.744 I llm_load_print_meta: rope scaling     = linear
0.00.108.744 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.108.744 I llm_load_print_meta: freq_scale_train = 1
0.00.108.744 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.108.745 I llm_load_print_meta: rope_finetuned   = unknown
0.00.108.745 I llm_load_print_meta: ssm_d_conv       = 0
0.00.108.745 I llm_load_print_meta: ssm_d_inner      = 0
0.00.108.745 I llm_load_print_meta: ssm_d_state      = 0
0.00.108.747 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.108.747 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.108.757 I llm_load_print_meta: model type       = 1.4B
0.00.108.757 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.108.758 I llm_load_print_meta: model params     = 1.41 B
0.00.108.759 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.108.759 I llm_load_print_meta: general.name     = 1.4B
0.00.108.759 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.108.759 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.108.759 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.108.760 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.108.760 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.108.761 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.108.762 I llm_load_print_meta: max token length = 1024
0.00.111.285 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.111.286 I llm_load_tensors: offloading output layer to GPU
0.00.111.286 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.111.304 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.111.305 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.112.273 I llama_new_context_with_model: n_seq_max     = 1
0.00.112.274 I llama_new_context_with_model: n_ctx         = 2048
0.00.112.274 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.112.274 I llama_new_context_with_model: n_batch       = 2048
0.00.112.274 I llama_new_context_with_model: n_ubatch      = 512
0.00.112.275 I llama_new_context_with_model: flash_attn    = 0
0.00.112.275 I llama_new_context_with_model: freq_base     = 10000.0
0.00.112.275 I llama_new_context_with_model: freq_scale    = 1
0.00.112.276 I ggml_metal_init: allocating
0.00.112.279 I ggml_metal_init: found device: Apple M4
0.00.112.281 I ggml_metal_init: picking default device: Apple M4
0.00.112.975 I ggml_metal_init: using embedded metal library
0.00.124.820 I ggml_metal_init: GPU name:   Apple M4
0.00.124.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.124.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.124.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.124.823 I ggml_metal_init: simdgroup reduction   = true
0.00.124.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.124.824 I ggml_metal_init: has bfloat            = true
0.00.124.824 I ggml_metal_init: use bfloat            = true
0.00.124.824 I ggml_metal_init: hasUnifiedMemory      = true
0.00.124.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.170.840 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.170.848 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.170.869 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.171.882 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.171.885 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.171.885 I llama_new_context_with_model: graph nodes  = 967
0.00.171.885 I llama_new_context_with_model: graph splits = 2
0.00.171.909 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.249.824 I main: llama threadpool init, n_threads = 4
0.00.249.857 I 
0.00.249.897 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.249.898 I 
0.00.249.981 I sampler seed: 1234
0.00.249.985 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.250.009 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.250.011 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.250.011 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.103.643 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56082.15 tokens per second)
0.02.103.644 I llama_perf_context_print:        load time =     210.52 ms
0.02.103.645 I llama_perf_context_print: prompt eval time =      53.54 ms /     7 tokens (    7.65 ms per token,   130.76 tokens per second)
0.02.103.646 I llama_perf_context_print:        eval time =    1797.10 ms /    63 runs   (   28.53 ms per token,    35.06 tokens per second)
0.02.103.647 I llama_perf_context_print:       total time =    1853.82 ms /    70 tokens
0.02.103.823 I ggml_metal_free: deallocating

real	0m2.411s
user	0m0.145s
sys	0m0.100s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.848 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.412 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.417 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.418 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.424 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.426 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.426 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.426 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.427 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.427 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.427 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.429 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.430 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.430 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.230 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.298 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.203 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.205 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.205 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.205 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.206 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.206 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.207 I llama_model_loader: - type  f32:  194 tensors
0.00.025.207 I llama_model_loader: - type q8_0:   98 tensors
0.00.046.473 I llm_load_vocab: special tokens cache size = 25
0.00.052.554 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.559 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.560 I llm_load_print_meta: arch             = gptneox
0.00.052.560 I llm_load_print_meta: vocab type       = BPE
0.00.052.560 I llm_load_print_meta: n_vocab          = 50304
0.00.052.562 I llm_load_print_meta: n_merges         = 50009
0.00.052.563 I llm_load_print_meta: vocab_only       = 0
0.00.052.563 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.563 I llm_load_print_meta: n_embd           = 2048
0.00.052.563 I llm_load_print_meta: n_layer          = 24
0.00.052.577 I llm_load_print_meta: n_head           = 16
0.00.052.578 I llm_load_print_meta: n_head_kv        = 16
0.00.052.578 I llm_load_print_meta: n_rot            = 32
0.00.052.578 I llm_load_print_meta: n_swa            = 0
0.00.052.578 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.579 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.579 I llm_load_print_meta: n_gqa            = 1
0.00.052.580 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.580 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.583 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.583 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.583 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.584 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.584 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.585 I llm_load_print_meta: n_ff             = 8192
0.00.052.585 I llm_load_print_meta: n_expert         = 0
0.00.052.585 I llm_load_print_meta: n_expert_used    = 0
0.00.052.585 I llm_load_print_meta: causal attn      = 1
0.00.052.585 I llm_load_print_meta: pooling type     = 0
0.00.052.585 I llm_load_print_meta: rope type        = 2
0.00.052.585 I llm_load_print_meta: rope scaling     = linear
0.00.052.586 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.591 I llm_load_print_meta: freq_scale_train = 1
0.00.052.593 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.593 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.593 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.593 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.594 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.594 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.594 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.601 I llm_load_print_meta: model type       = 1.4B
0.00.052.601 I llm_load_print_meta: model ftype      = Q8_0
0.00.052.602 I llm_load_print_meta: model params     = 1.41 B
0.00.052.602 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.052.603 I llm_load_print_meta: general.name     = 1.4B
0.00.052.603 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.603 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.603 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.603 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.604 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.604 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.604 I llm_load_print_meta: max token length = 1024
0.00.054.644 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.644 I llm_load_tensors: offloading output layer to GPU
0.00.054.644 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.650 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.054.651 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.055.604 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.604 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.605 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.605 I llama_new_context_with_model: n_batch       = 2048
0.00.055.605 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.605 I llama_new_context_with_model: flash_attn    = 0
0.00.055.606 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.606 I llama_new_context_with_model: freq_scale    = 1
0.00.055.606 I ggml_metal_init: allocating
0.00.055.609 I ggml_metal_init: found device: Apple M4
0.00.055.611 I ggml_metal_init: picking default device: Apple M4
0.00.056.309 I ggml_metal_init: using embedded metal library
0.00.058.839 I ggml_metal_init: GPU name:   Apple M4
0.00.058.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.841 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.842 I ggml_metal_init: simdgroup reduction   = true
0.00.058.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.842 I ggml_metal_init: has bfloat            = true
0.00.058.842 I ggml_metal_init: use bfloat            = true
0.00.058.843 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.845 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.092.296 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.092.305 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.092.331 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.093.465 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.093.467 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.093.467 I llama_new_context_with_model: graph nodes  = 967
0.00.093.468 I llama_new_context_with_model: graph splits = 2
0.00.093.485 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.128.972 I main: llama threadpool init, n_threads = 4
0.01.129.008 I 
0.01.129.039 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.129.040 I 
0.01.129.295 I sampler seed: 1234
0.01.129.301 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.129.315 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.129.316 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.129.316 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.223.534 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48497.27 tokens per second)
0.02.223.535 I llama_perf_context_print:        load time =    1119.12 ms
0.02.223.536 I llama_perf_context_print: prompt eval time =      43.67 ms /     7 tokens (    6.24 ms per token,   160.29 tokens per second)
0.02.223.536 I llama_perf_context_print:        eval time =    1047.79 ms /    63 runs   (   16.63 ms per token,    60.13 tokens per second)
0.02.223.537 I llama_perf_context_print:       total time =    1094.56 ms /    70 tokens
0.02.223.736 I ggml_metal_free: deallocating

real	0m2.242s
user	0m0.112s
sys	0m0.221s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.011.055 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.547 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.554 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.555 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.555 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.555 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.556 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.559 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.559 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.560 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.560 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.560 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.561 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.561 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.563 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.396 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.435 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.435 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.436 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.437 I llama_model_loader: - type  f32:  194 tensors
0.00.026.437 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.438 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.005 I llm_load_vocab: special tokens cache size = 25
0.00.052.996 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.999 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.999 I llm_load_print_meta: arch             = gptneox
0.00.052.999 I llm_load_print_meta: vocab type       = BPE
0.00.053.000 I llm_load_print_meta: n_vocab          = 50304
0.00.053.000 I llm_load_print_meta: n_merges         = 50009
0.00.053.000 I llm_load_print_meta: vocab_only       = 0
0.00.053.000 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.000 I llm_load_print_meta: n_embd           = 2048
0.00.053.001 I llm_load_print_meta: n_layer          = 24
0.00.053.019 I llm_load_print_meta: n_head           = 16
0.00.053.020 I llm_load_print_meta: n_head_kv        = 16
0.00.053.020 I llm_load_print_meta: n_rot            = 32
0.00.053.020 I llm_load_print_meta: n_swa            = 0
0.00.053.020 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.020 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.021 I llm_load_print_meta: n_gqa            = 1
0.00.053.022 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.022 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.023 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.023 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.024 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.024 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.026 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.027 I llm_load_print_meta: n_ff             = 8192
0.00.053.027 I llm_load_print_meta: n_expert         = 0
0.00.053.027 I llm_load_print_meta: n_expert_used    = 0
0.00.053.028 I llm_load_print_meta: causal attn      = 1
0.00.053.028 I llm_load_print_meta: pooling type     = 0
0.00.053.028 I llm_load_print_meta: rope type        = 2
0.00.053.028 I llm_load_print_meta: rope scaling     = linear
0.00.053.028 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.029 I llm_load_print_meta: freq_scale_train = 1
0.00.053.029 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.029 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.029 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.029 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.029 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.029 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.030 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.040 I llm_load_print_meta: model type       = 1.4B
0.00.053.041 I llm_load_print_meta: model ftype      = Q4_0
0.00.053.041 I llm_load_print_meta: model params     = 1.41 B
0.00.053.041 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.053.042 I llm_load_print_meta: general.name     = 1.4B
0.00.053.042 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.042 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.042 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.042 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.046 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.047 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.047 I llm_load_print_meta: max token length = 1024
0.00.055.330 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.330 I llm_load_tensors: offloading output layer to GPU
0.00.055.331 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.342 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.055.343 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.056.325 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.326 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.326 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.326 I llama_new_context_with_model: n_batch       = 2048
0.00.056.326 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.327 I llama_new_context_with_model: flash_attn    = 0
0.00.056.327 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.327 I llama_new_context_with_model: freq_scale    = 1
0.00.056.328 I ggml_metal_init: allocating
0.00.056.335 I ggml_metal_init: found device: Apple M4
0.00.056.338 I ggml_metal_init: picking default device: Apple M4
0.00.057.089 I ggml_metal_init: using embedded metal library
0.00.059.644 I ggml_metal_init: GPU name:   Apple M4
0.00.059.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.646 I ggml_metal_init: simdgroup reduction   = true
0.00.059.646 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.647 I ggml_metal_init: has bfloat            = true
0.00.059.647 I ggml_metal_init: use bfloat            = true
0.00.059.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.524 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.532 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.558 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.722 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.724 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.725 I llama_new_context_with_model: graph nodes  = 967
0.00.094.725 I llama_new_context_with_model: graph splits = 2
0.00.094.743 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.235 I main: llama threadpool init, n_threads = 4
0.00.664.279 I 
0.00.664.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.307 I 
0.00.664.547 I sampler seed: 1234
0.00.664.553 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.664.565 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.664.565 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.664.565 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.340.717 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58580.86 tokens per second)
0.01.340.719 I llama_perf_context_print:        load time =     653.17 ms
0.01.340.721 I llama_perf_context_print: prompt eval time =      43.35 ms /     7 tokens (    6.19 ms per token,   161.49 tokens per second)
0.01.340.721 I llama_perf_context_print:        eval time =     629.79 ms /    63 runs   (   10.00 ms per token,   100.03 tokens per second)
0.01.340.722 I llama_perf_context_print:       total time =     676.49 ms /    70 tokens
0.01.340.913 I ggml_metal_free: deallocating

real	0m1.360s
user	0m0.109s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.008.662 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.866 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.867 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.868 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.868 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.869 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.869 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.870 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.870 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.870 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.871 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.871 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.872 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.873 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.873 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.747 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.764 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.573 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.574 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.574 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.575 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.575 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.576 I llama_model_loader: - type  f32:  194 tensors
0.00.024.576 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.576 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.675 I llm_load_vocab: special tokens cache size = 25
0.00.051.635 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.637 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.638 I llm_load_print_meta: arch             = gptneox
0.00.051.638 I llm_load_print_meta: vocab type       = BPE
0.00.051.639 I llm_load_print_meta: n_vocab          = 50304
0.00.051.639 I llm_load_print_meta: n_merges         = 50009
0.00.051.639 I llm_load_print_meta: vocab_only       = 0
0.00.051.639 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.639 I llm_load_print_meta: n_embd           = 2048
0.00.051.639 I llm_load_print_meta: n_layer          = 24
0.00.051.654 I llm_load_print_meta: n_head           = 16
0.00.051.655 I llm_load_print_meta: n_head_kv        = 16
0.00.051.656 I llm_load_print_meta: n_rot            = 32
0.00.051.656 I llm_load_print_meta: n_swa            = 0
0.00.051.656 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.656 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.657 I llm_load_print_meta: n_gqa            = 1
0.00.051.658 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.658 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.659 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.659 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.659 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.660 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.660 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.660 I llm_load_print_meta: n_ff             = 8192
0.00.051.660 I llm_load_print_meta: n_expert         = 0
0.00.051.661 I llm_load_print_meta: n_expert_used    = 0
0.00.051.661 I llm_load_print_meta: causal attn      = 1
0.00.051.661 I llm_load_print_meta: pooling type     = 0
0.00.051.661 I llm_load_print_meta: rope type        = 2
0.00.051.661 I llm_load_print_meta: rope scaling     = linear
0.00.051.662 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.662 I llm_load_print_meta: freq_scale_train = 1
0.00.051.662 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.662 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.662 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.662 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.662 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.663 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.663 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.672 I llm_load_print_meta: model type       = 1.4B
0.00.051.673 I llm_load_print_meta: model ftype      = Q4_1
0.00.051.673 I llm_load_print_meta: model params     = 1.41 B
0.00.051.673 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.051.674 I llm_load_print_meta: general.name     = 1.4B
0.00.051.674 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.675 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.675 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.675 I llm_load_print_meta: max token length = 1024
0.00.053.703 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.703 I llm_load_tensors: offloading output layer to GPU
0.00.053.703 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.714 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.715 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.054.663 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.664 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.664 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.665 I llama_new_context_with_model: n_batch       = 2048
0.00.054.665 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.665 I llama_new_context_with_model: flash_attn    = 0
0.00.054.665 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.666 I llama_new_context_with_model: freq_scale    = 1
0.00.054.666 I ggml_metal_init: allocating
0.00.054.669 I ggml_metal_init: found device: Apple M4
0.00.054.671 I ggml_metal_init: picking default device: Apple M4
0.00.055.266 I ggml_metal_init: using embedded metal library
0.00.057.590 I ggml_metal_init: GPU name:   Apple M4
0.00.057.591 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.591 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.592 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.592 I ggml_metal_init: simdgroup reduction   = true
0.00.057.592 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.592 I ggml_metal_init: has bfloat            = true
0.00.057.592 I ggml_metal_init: use bfloat            = true
0.00.057.593 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.593 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.391 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.396 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.416 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.409 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.411 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.411 I llama_new_context_with_model: graph nodes  = 967
0.00.089.412 I llama_new_context_with_model: graph splits = 2
0.00.089.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.852 I main: llama threadpool init, n_threads = 4
0.00.645.900 I 
0.00.645.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.938 I 
0.00.646.175 I sampler seed: 1234
0.00.646.179 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.646.222 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.646.223 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.646.223 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.377.172 I llama_perf_sampler_print:    sampling time =       1.11 ms /    71 runs   (    0.02 ms per token, 64021.64 tokens per second)
0.01.377.173 I llama_perf_context_print:        load time =     637.19 ms
0.01.377.174 I llama_perf_context_print: prompt eval time =      45.54 ms /     7 tokens (    6.51 ms per token,   153.71 tokens per second)
0.01.377.175 I llama_perf_context_print:        eval time =     682.51 ms /    63 runs   (   10.83 ms per token,    92.31 tokens per second)
0.01.377.175 I llama_perf_context_print:       total time =     731.32 ms /    70 tokens
0.01.377.360 I ggml_metal_free: deallocating

real	0m1.393s
user	0m0.110s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.791 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.454 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.458 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.465 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.465 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.466 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.467 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.473 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.271 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.272 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.273 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.273 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.274 I llama_model_loader: - type  f32:  194 tensors
0.00.026.274 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.275 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.161 I llm_load_vocab: special tokens cache size = 25
0.00.053.312 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.315 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.315 I llm_load_print_meta: arch             = gptneox
0.00.053.315 I llm_load_print_meta: vocab type       = BPE
0.00.053.316 I llm_load_print_meta: n_vocab          = 50304
0.00.053.316 I llm_load_print_meta: n_merges         = 50009
0.00.053.316 I llm_load_print_meta: vocab_only       = 0
0.00.053.316 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.316 I llm_load_print_meta: n_embd           = 2048
0.00.053.317 I llm_load_print_meta: n_layer          = 24
0.00.053.331 I llm_load_print_meta: n_head           = 16
0.00.053.332 I llm_load_print_meta: n_head_kv        = 16
0.00.053.332 I llm_load_print_meta: n_rot            = 32
0.00.053.332 I llm_load_print_meta: n_swa            = 0
0.00.053.332 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.333 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.333 I llm_load_print_meta: n_gqa            = 1
0.00.053.334 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.335 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.335 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.336 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.336 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.337 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.338 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.338 I llm_load_print_meta: n_ff             = 8192
0.00.053.339 I llm_load_print_meta: n_expert         = 0
0.00.053.339 I llm_load_print_meta: n_expert_used    = 0
0.00.053.339 I llm_load_print_meta: causal attn      = 1
0.00.053.340 I llm_load_print_meta: pooling type     = 0
0.00.053.340 I llm_load_print_meta: rope type        = 2
0.00.053.340 I llm_load_print_meta: rope scaling     = linear
0.00.053.340 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.341 I llm_load_print_meta: freq_scale_train = 1
0.00.053.341 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.342 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.342 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.342 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.342 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.342 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.342 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.352 I llm_load_print_meta: model type       = 1.4B
0.00.053.352 I llm_load_print_meta: model ftype      = Q5_0
0.00.053.352 I llm_load_print_meta: model params     = 1.41 B
0.00.053.353 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.053.353 I llm_load_print_meta: general.name     = 1.4B
0.00.053.353 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.353 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.353 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.354 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.355 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.355 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.355 I llm_load_print_meta: max token length = 1024
0.00.055.308 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.308 I llm_load_tensors: offloading output layer to GPU
0.00.055.308 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.319 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.055.320 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.056.198 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.199 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.199 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.199 I llama_new_context_with_model: n_batch       = 2048
0.00.056.199 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.199 I llama_new_context_with_model: flash_attn    = 0
0.00.056.200 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.200 I llama_new_context_with_model: freq_scale    = 1
0.00.056.201 I ggml_metal_init: allocating
0.00.056.206 I ggml_metal_init: found device: Apple M4
0.00.056.209 I ggml_metal_init: picking default device: Apple M4
0.00.056.796 I ggml_metal_init: using embedded metal library
0.00.059.135 I ggml_metal_init: GPU name:   Apple M4
0.00.059.136 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.137 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.137 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.137 I ggml_metal_init: simdgroup reduction   = true
0.00.059.138 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.138 I ggml_metal_init: has bfloat            = true
0.00.059.138 I ggml_metal_init: use bfloat            = true
0.00.059.138 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.139 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.087.417 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.423 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.449 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.520 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.521 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.522 I llama_new_context_with_model: graph nodes  = 967
0.00.088.522 I llama_new_context_with_model: graph splits = 2
0.00.088.536 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.195 I main: llama threadpool init, n_threads = 4
0.00.767.242 I 
0.00.767.271 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.271 I 
0.00.767.499 I sampler seed: 1234
0.00.767.504 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.515 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.517 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.517 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.554.958 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.554.959 I llama_perf_context_print:        load time =     756.40 ms
0.01.554.960 I llama_perf_context_print: prompt eval time =      43.11 ms /     7 tokens (    6.16 ms per token,   162.37 tokens per second)
0.01.554.966 I llama_perf_context_print:        eval time =     741.31 ms /    63 runs   (   11.77 ms per token,    84.98 tokens per second)
0.01.554.967 I llama_perf_context_print:       total time =     787.77 ms /    70 tokens
0.01.555.172 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.009.448 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.410 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.412 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.412 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.414 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.415 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.415 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.416 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.416 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.419 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.419 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.420 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.236 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.087 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.088 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.088 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.089 I llama_model_loader: - type  f32:  194 tensors
0.00.024.089 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.089 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.332 I llm_load_vocab: special tokens cache size = 25
0.00.050.123 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.125 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.126 I llm_load_print_meta: arch             = gptneox
0.00.050.126 I llm_load_print_meta: vocab type       = BPE
0.00.050.126 I llm_load_print_meta: n_vocab          = 50304
0.00.050.126 I llm_load_print_meta: n_merges         = 50009
0.00.050.127 I llm_load_print_meta: vocab_only       = 0
0.00.050.127 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.127 I llm_load_print_meta: n_embd           = 2048
0.00.050.127 I llm_load_print_meta: n_layer          = 24
0.00.050.142 I llm_load_print_meta: n_head           = 16
0.00.050.143 I llm_load_print_meta: n_head_kv        = 16
0.00.050.143 I llm_load_print_meta: n_rot            = 32
0.00.050.143 I llm_load_print_meta: n_swa            = 0
0.00.050.144 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.144 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.145 I llm_load_print_meta: n_gqa            = 1
0.00.050.145 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.146 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.147 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.147 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.147 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.147 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.149 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.150 I llm_load_print_meta: n_ff             = 8192
0.00.050.150 I llm_load_print_meta: n_expert         = 0
0.00.050.150 I llm_load_print_meta: n_expert_used    = 0
0.00.050.152 I llm_load_print_meta: causal attn      = 1
0.00.050.153 I llm_load_print_meta: pooling type     = 0
0.00.050.153 I llm_load_print_meta: rope type        = 2
0.00.050.153 I llm_load_print_meta: rope scaling     = linear
0.00.050.154 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.154 I llm_load_print_meta: freq_scale_train = 1
0.00.050.154 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.154 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.155 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.155 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.155 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.155 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.156 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.165 I llm_load_print_meta: model type       = 1.4B
0.00.050.165 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.166 I llm_load_print_meta: model params     = 1.41 B
0.00.050.166 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.166 I llm_load_print_meta: general.name     = 1.4B
0.00.050.167 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.167 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.167 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.168 I llm_load_print_meta: max token length = 1024
0.00.052.136 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.137 I llm_load_tensors: offloading output layer to GPU
0.00.052.137 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.147 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.148 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.114 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.115 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.115 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.115 I llama_new_context_with_model: n_batch       = 2048
0.00.053.115 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.115 I llama_new_context_with_model: flash_attn    = 0
0.00.053.116 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.116 I llama_new_context_with_model: freq_scale    = 1
0.00.053.116 I ggml_metal_init: allocating
0.00.053.120 I ggml_metal_init: found device: Apple M4
0.00.053.122 I ggml_metal_init: picking default device: Apple M4
0.00.053.711 I ggml_metal_init: using embedded metal library
0.00.055.988 I ggml_metal_init: GPU name:   Apple M4
0.00.055.989 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.989 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.990 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.990 I ggml_metal_init: simdgroup reduction   = true
0.00.055.992 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.992 I ggml_metal_init: has bfloat            = true
0.00.055.992 I ggml_metal_init: use bfloat            = true
0.00.055.992 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.997 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.129 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.137 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.160 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.129 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.130 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.130 I llama_new_context_with_model: graph nodes  = 967
0.00.086.130 I llama_new_context_with_model: graph splits = 2
0.00.086.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.082 I main: llama threadpool init, n_threads = 4
0.00.712.122 I 
0.00.712.155 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.156 I 
0.00.712.442 I sampler seed: 1234
0.00.712.446 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.484 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.486 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.486 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.549.855 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62117.24 tokens per second)
0.01.549.855 I llama_perf_context_print:        load time =     702.63 ms
0.01.549.856 I llama_perf_context_print: prompt eval time =      42.30 ms /     7 tokens (    6.04 ms per token,   165.48 tokens per second)
0.01.549.857 I llama_perf_context_print:        eval time =     792.22 ms /    63 runs   (   12.57 ms per token,    79.52 tokens per second)
0.01.549.861 I llama_perf_context_print:       total time =     837.77 ms /    70 tokens
0.01.550.057 I ggml_metal_free: deallocating

real	0m1.572s
user	0m0.108s
sys	0m0.168s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.029 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.009.583 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.042 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.047 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.048 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.049 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.049 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.049 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.049 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.050 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.053 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.053 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.053 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.054 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.054 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.054 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.057 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.057 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.058 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.931 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.034 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.933 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.934 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.934 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.935 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.935 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.935 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.936 I llama_model_loader: - type  f32:  194 tensors
0.00.023.936 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.936 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.937 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.799 I llm_load_vocab: special tokens cache size = 25
0.00.050.869 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.872 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.872 I llm_load_print_meta: arch             = gptneox
0.00.050.873 I llm_load_print_meta: vocab type       = BPE
0.00.050.873 I llm_load_print_meta: n_vocab          = 50304
0.00.050.873 I llm_load_print_meta: n_merges         = 50009
0.00.050.873 I llm_load_print_meta: vocab_only       = 0
0.00.050.873 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.873 I llm_load_print_meta: n_embd           = 2048
0.00.050.874 I llm_load_print_meta: n_layer          = 24
0.00.050.888 I llm_load_print_meta: n_head           = 16
0.00.050.889 I llm_load_print_meta: n_head_kv        = 16
0.00.050.889 I llm_load_print_meta: n_rot            = 32
0.00.050.889 I llm_load_print_meta: n_swa            = 0
0.00.050.889 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.890 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.890 I llm_load_print_meta: n_gqa            = 1
0.00.050.891 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.892 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.892 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.893 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.895 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.895 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.895 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.896 I llm_load_print_meta: n_ff             = 8192
0.00.050.896 I llm_load_print_meta: n_expert         = 0
0.00.050.896 I llm_load_print_meta: n_expert_used    = 0
0.00.050.896 I llm_load_print_meta: causal attn      = 1
0.00.050.896 I llm_load_print_meta: pooling type     = 0
0.00.050.896 I llm_load_print_meta: rope type        = 2
0.00.050.897 I llm_load_print_meta: rope scaling     = linear
0.00.050.898 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.898 I llm_load_print_meta: freq_scale_train = 1
0.00.050.898 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.899 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.899 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.899 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.899 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.899 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.899 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.909 I llm_load_print_meta: model type       = 1.4B
0.00.050.909 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.910 I llm_load_print_meta: model params     = 1.41 B
0.00.050.910 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.910 I llm_load_print_meta: general.name     = 1.4B
0.00.050.910 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.911 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.911 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.912 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.913 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.913 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.913 I llm_load_print_meta: max token length = 1024
0.00.052.782 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.782 I llm_load_tensors: offloading output layer to GPU
0.00.052.782 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.792 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.794 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.688 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.688 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.689 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.689 I llama_new_context_with_model: n_batch       = 2048
0.00.053.689 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.689 I llama_new_context_with_model: flash_attn    = 0
0.00.053.690 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.690 I llama_new_context_with_model: freq_scale    = 1
0.00.053.690 I ggml_metal_init: allocating
0.00.053.696 I ggml_metal_init: found device: Apple M4
0.00.053.699 I ggml_metal_init: picking default device: Apple M4
0.00.054.295 I ggml_metal_init: using embedded metal library
0.00.056.653 I ggml_metal_init: GPU name:   Apple M4
0.00.056.654 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.655 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.655 I ggml_metal_init: simdgroup reduction   = true
0.00.056.655 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.655 I ggml_metal_init: has bfloat            = true
0.00.056.656 I ggml_metal_init: use bfloat            = true
0.00.056.656 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.657 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.352 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.360 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.380 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.367 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.368 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.369 I llama_new_context_with_model: graph nodes  = 967
0.00.087.369 I llama_new_context_with_model: graph splits = 2
0.00.087.383 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.437.501 I main: llama threadpool init, n_threads = 4
0.00.437.542 I 
0.00.437.580 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.437.582 I 
0.00.437.815 I sampler seed: 1234
0.00.437.822 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.437.835 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.437.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.437.835 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.112.037 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.112.039 I llama_perf_context_print:        load time =     427.91 ms
0.01.112.039 I llama_perf_context_print: prompt eval time =      35.86 ms /     7 tokens (    5.12 ms per token,   195.20 tokens per second)
0.01.112.040 I llama_perf_context_print:        eval time =     635.81 ms /    63 runs   (   10.09 ms per token,    99.09 tokens per second)
0.01.112.040 I llama_perf_context_print:       total time =     674.54 ms /    70 tokens
0.01.112.260 I ggml_metal_free: deallocating

real	0m1.131s
user	0m0.109s
sys	0m0.110s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.252 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.256 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.257 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.265 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.269 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.269 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.270 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.270 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.271 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.272 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.273 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.273 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.110 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.862 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.862 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.863 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.863 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.863 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.864 I llama_model_loader: - type  f32:  194 tensors
0.00.022.864 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.864 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.865 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.865 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.763 I llm_load_vocab: special tokens cache size = 25
0.00.049.812 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.816 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.816 I llm_load_print_meta: arch             = gptneox
0.00.049.817 I llm_load_print_meta: vocab type       = BPE
0.00.049.817 I llm_load_print_meta: n_vocab          = 50304
0.00.049.817 I llm_load_print_meta: n_merges         = 50009
0.00.049.817 I llm_load_print_meta: vocab_only       = 0
0.00.049.817 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.818 I llm_load_print_meta: n_embd           = 2048
0.00.049.818 I llm_load_print_meta: n_layer          = 24
0.00.049.831 I llm_load_print_meta: n_head           = 16
0.00.049.833 I llm_load_print_meta: n_head_kv        = 16
0.00.049.833 I llm_load_print_meta: n_rot            = 32
0.00.049.833 I llm_load_print_meta: n_swa            = 0
0.00.049.833 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.833 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.834 I llm_load_print_meta: n_gqa            = 1
0.00.049.835 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.836 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.836 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.836 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.837 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.837 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.837 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.838 I llm_load_print_meta: n_ff             = 8192
0.00.049.838 I llm_load_print_meta: n_expert         = 0
0.00.049.838 I llm_load_print_meta: n_expert_used    = 0
0.00.049.838 I llm_load_print_meta: causal attn      = 1
0.00.049.840 I llm_load_print_meta: pooling type     = 0
0.00.049.841 I llm_load_print_meta: rope type        = 2
0.00.049.841 I llm_load_print_meta: rope scaling     = linear
0.00.049.841 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.842 I llm_load_print_meta: freq_scale_train = 1
0.00.049.842 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.842 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.842 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.842 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.842 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.842 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.844 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.853 I llm_load_print_meta: model type       = 1.4B
0.00.049.854 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.854 I llm_load_print_meta: model params     = 1.41 B
0.00.049.855 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.856 I llm_load_print_meta: general.name     = 1.4B
0.00.049.856 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.856 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.856 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.856 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.857 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.857 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.857 I llm_load_print_meta: max token length = 1024
0.00.051.482 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.482 I llm_load_tensors: offloading output layer to GPU
0.00.051.482 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.492 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.493 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.318 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.319 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.319 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.320 I llama_new_context_with_model: n_batch       = 2048
0.00.052.320 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.320 I llama_new_context_with_model: flash_attn    = 0
0.00.052.320 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.321 I llama_new_context_with_model: freq_scale    = 1
0.00.052.321 I ggml_metal_init: allocating
0.00.052.327 I ggml_metal_init: found device: Apple M4
0.00.052.329 I ggml_metal_init: picking default device: Apple M4
0.00.052.883 I ggml_metal_init: using embedded metal library
0.00.055.193 I ggml_metal_init: GPU name:   Apple M4
0.00.055.195 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.195 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.195 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.196 I ggml_metal_init: simdgroup reduction   = true
0.00.055.196 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.196 I ggml_metal_init: has bfloat            = true
0.00.055.196 I ggml_metal_init: use bfloat            = true
0.00.055.196 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.197 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.805 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.811 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.832 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.837 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.839 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.839 I llama_new_context_with_model: graph nodes  = 967
0.00.084.840 I llama_new_context_with_model: graph splits = 2
0.00.084.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.079 I main: llama threadpool init, n_threads = 4
0.00.539.126 I 
0.00.539.160 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.539.160 I 
0.00.539.399 I sampler seed: 1234
0.00.539.403 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.539.453 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.539.455 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.539.455 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.285.394 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58484.35 tokens per second)
0.01.285.394 I llama_perf_context_print:        load time =     530.31 ms
0.01.285.395 I llama_perf_context_print: prompt eval time =      40.51 ms /     7 tokens (    5.79 ms per token,   172.78 tokens per second)
0.01.285.396 I llama_perf_context_print:        eval time =     702.50 ms /    63 runs   (   11.15 ms per token,    89.68 tokens per second)
0.01.285.398 I llama_perf_context_print:       total time =     746.32 ms /    70 tokens
0.01.285.594 I ggml_metal_free: deallocating

real	0m1.302s
user	0m0.110s
sys	0m0.126s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.011.038 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.550 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.555 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.557 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.558 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.558 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.559 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.560 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.561 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.561 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.562 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.563 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.564 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.564 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.418 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.398 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.400 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.402 I llama_model_loader: - type  f32:  194 tensors
0.00.025.402 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.402 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.402 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.342 I llm_load_vocab: special tokens cache size = 25
0.00.052.326 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.328 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.329 I llm_load_print_meta: arch             = gptneox
0.00.052.329 I llm_load_print_meta: vocab type       = BPE
0.00.052.329 I llm_load_print_meta: n_vocab          = 50304
0.00.052.330 I llm_load_print_meta: n_merges         = 50009
0.00.052.330 I llm_load_print_meta: vocab_only       = 0
0.00.052.330 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.330 I llm_load_print_meta: n_embd           = 2048
0.00.052.330 I llm_load_print_meta: n_layer          = 24
0.00.052.345 I llm_load_print_meta: n_head           = 16
0.00.052.346 I llm_load_print_meta: n_head_kv        = 16
0.00.052.346 I llm_load_print_meta: n_rot            = 32
0.00.052.346 I llm_load_print_meta: n_swa            = 0
0.00.052.346 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.346 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.347 I llm_load_print_meta: n_gqa            = 1
0.00.052.348 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.350 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.351 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.351 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.351 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.351 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.351 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.352 I llm_load_print_meta: n_ff             = 8192
0.00.052.352 I llm_load_print_meta: n_expert         = 0
0.00.052.354 I llm_load_print_meta: n_expert_used    = 0
0.00.052.354 I llm_load_print_meta: causal attn      = 1
0.00.052.354 I llm_load_print_meta: pooling type     = 0
0.00.052.354 I llm_load_print_meta: rope type        = 2
0.00.052.354 I llm_load_print_meta: rope scaling     = linear
0.00.052.355 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.355 I llm_load_print_meta: freq_scale_train = 1
0.00.052.355 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.355 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.355 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.356 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.356 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.356 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.356 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.365 I llm_load_print_meta: model type       = 1.4B
0.00.052.366 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.366 I llm_load_print_meta: model params     = 1.41 B
0.00.052.367 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.367 I llm_load_print_meta: general.name     = 1.4B
0.00.052.367 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.367 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.367 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.367 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.368 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.369 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.369 I llm_load_print_meta: max token length = 1024
0.00.054.333 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.334 I llm_load_tensors: offloading output layer to GPU
0.00.054.334 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.344 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.345 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.224 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.225 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.225 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.225 I llama_new_context_with_model: n_batch       = 2048
0.00.055.225 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.226 I llama_new_context_with_model: flash_attn    = 0
0.00.055.226 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.226 I llama_new_context_with_model: freq_scale    = 1
0.00.055.227 I ggml_metal_init: allocating
0.00.055.233 I ggml_metal_init: found device: Apple M4
0.00.055.235 I ggml_metal_init: picking default device: Apple M4
0.00.055.806 I ggml_metal_init: using embedded metal library
0.00.058.169 I ggml_metal_init: GPU name:   Apple M4
0.00.058.170 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.171 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.171 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.171 I ggml_metal_init: simdgroup reduction   = true
0.00.058.171 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.171 I ggml_metal_init: has bfloat            = true
0.00.058.172 I ggml_metal_init: use bfloat            = true
0.00.058.172 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.173 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.546 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.554 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.575 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.569 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.571 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.571 I llama_new_context_with_model: graph nodes  = 967
0.00.087.571 I llama_new_context_with_model: graph splits = 2
0.00.087.585 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.614.526 I main: llama threadpool init, n_threads = 4
0.00.614.620 I 
0.00.614.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.659 I 
0.00.614.899 I sampler seed: 1234
0.00.614.904 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.938 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.940 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.940 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.373.416 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55209.95 tokens per second)
0.01.373.417 I llama_perf_context_print:        load time =     603.48 ms
0.01.373.418 I llama_perf_context_print: prompt eval time =      47.14 ms /     7 tokens (    6.73 ms per token,   148.50 tokens per second)
0.01.373.418 I llama_perf_context_print:        eval time =     708.23 ms /    63 runs   (   11.24 ms per token,    88.95 tokens per second)
0.01.373.419 I llama_perf_context_print:       total time =     758.90 ms /    70 tokens
0.01.373.582 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.110s
sys	0m0.135s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.012.812 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.159 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.019.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.169 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.169 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.171 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.172 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.175 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.050 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.950 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.951 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.952 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.952 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.952 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.953 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.953 I llama_model_loader: - type  f32:  194 tensors
0.00.027.954 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.954 I llama_model_loader: - type q6_K:   37 tensors
0.00.049.030 I llm_load_vocab: special tokens cache size = 25
0.00.055.174 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.177 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.177 I llm_load_print_meta: arch             = gptneox
0.00.055.177 I llm_load_print_meta: vocab type       = BPE
0.00.055.178 I llm_load_print_meta: n_vocab          = 50304
0.00.055.178 I llm_load_print_meta: n_merges         = 50009
0.00.055.178 I llm_load_print_meta: vocab_only       = 0
0.00.055.178 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.178 I llm_load_print_meta: n_embd           = 2048
0.00.055.179 I llm_load_print_meta: n_layer          = 24
0.00.055.193 I llm_load_print_meta: n_head           = 16
0.00.055.194 I llm_load_print_meta: n_head_kv        = 16
0.00.055.194 I llm_load_print_meta: n_rot            = 32
0.00.055.194 I llm_load_print_meta: n_swa            = 0
0.00.055.194 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.194 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.195 I llm_load_print_meta: n_gqa            = 1
0.00.055.196 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.196 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.197 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.197 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.197 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.197 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.198 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.198 I llm_load_print_meta: n_ff             = 8192
0.00.055.199 I llm_load_print_meta: n_expert         = 0
0.00.055.199 I llm_load_print_meta: n_expert_used    = 0
0.00.055.200 I llm_load_print_meta: causal attn      = 1
0.00.055.202 I llm_load_print_meta: pooling type     = 0
0.00.055.202 I llm_load_print_meta: rope type        = 2
0.00.055.202 I llm_load_print_meta: rope scaling     = linear
0.00.055.202 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.203 I llm_load_print_meta: freq_scale_train = 1
0.00.055.203 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.203 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.203 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.203 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.203 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.204 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.213 I llm_load_print_meta: model type       = 1.4B
0.00.055.213 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.055.214 I llm_load_print_meta: model params     = 1.41 B
0.00.055.214 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.055.214 I llm_load_print_meta: general.name     = 1.4B
0.00.055.215 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.215 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.215 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.215 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.215 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.216 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.216 I llm_load_print_meta: max token length = 1024
0.00.057.235 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.236 I llm_load_tensors: offloading output layer to GPU
0.00.057.236 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.246 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.057.248 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.058.204 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.205 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.205 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.205 I llama_new_context_with_model: n_batch       = 2048
0.00.058.205 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.205 I llama_new_context_with_model: flash_attn    = 0
0.00.058.206 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.206 I llama_new_context_with_model: freq_scale    = 1
0.00.058.207 I ggml_metal_init: allocating
0.00.058.210 I ggml_metal_init: found device: Apple M4
0.00.058.212 I ggml_metal_init: picking default device: Apple M4
0.00.058.807 I ggml_metal_init: using embedded metal library
0.00.061.126 I ggml_metal_init: GPU name:   Apple M4
0.00.061.128 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.061.129 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.061.130 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.061.130 I ggml_metal_init: simdgroup reduction   = true
0.00.061.130 I ggml_metal_init: simdgroup matrix mul. = true
0.00.061.130 I ggml_metal_init: has bfloat            = true
0.00.061.130 I ggml_metal_init: use bfloat            = true
0.00.061.131 I ggml_metal_init: hasUnifiedMemory      = true
0.00.061.131 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.212 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.217 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.235 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.094.353 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.094.355 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.094.355 I llama_new_context_with_model: graph nodes  = 967
0.00.094.355 I llama_new_context_with_model: graph splits = 2
0.00.094.369 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.698.830 I main: llama threadpool init, n_threads = 4
0.00.698.866 I 
0.00.698.895 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.698.895 I 
0.00.699.128 I sampler seed: 1234
0.00.699.135 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.699.174 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.699.176 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.699.176 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.547.711 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59714.05 tokens per second)
0.01.547.711 I llama_perf_context_print:        load time =     686.01 ms
0.01.547.712 I llama_perf_context_print: prompt eval time =      51.60 ms /     7 tokens (    7.37 ms per token,   135.65 tokens per second)
0.01.547.713 I llama_perf_context_print:        eval time =     793.99 ms /    63 runs   (   12.60 ms per token,    79.35 tokens per second)
0.01.547.713 I llama_perf_context_print:       total time =     848.88 ms /    70 tokens
0.01.547.914 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.110s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.556 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.304 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.309 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.321 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.323 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.323 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.323 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.324 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.324 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.325 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.327 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.327 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.328 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.328 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.329 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.331 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.331 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.119 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.982 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.983 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.983 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.984 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.984 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.984 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.985 I llama_model_loader: - type  f32:  194 tensors
0.00.023.985 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.870 I llm_load_vocab: special tokens cache size = 25
0.00.050.984 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.987 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.988 I llm_load_print_meta: arch             = gptneox
0.00.050.988 I llm_load_print_meta: vocab type       = BPE
0.00.050.989 I llm_load_print_meta: n_vocab          = 50304
0.00.050.989 I llm_load_print_meta: n_merges         = 50009
0.00.050.989 I llm_load_print_meta: vocab_only       = 0
0.00.050.989 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.989 I llm_load_print_meta: n_embd           = 2048
0.00.050.989 I llm_load_print_meta: n_layer          = 24
0.00.051.004 I llm_load_print_meta: n_head           = 16
0.00.051.006 I llm_load_print_meta: n_head_kv        = 16
0.00.051.006 I llm_load_print_meta: n_rot            = 32
0.00.051.006 I llm_load_print_meta: n_swa            = 0
0.00.051.006 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.006 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.007 I llm_load_print_meta: n_gqa            = 1
0.00.051.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.009 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.009 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.009 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.010 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.010 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.010 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.010 I llm_load_print_meta: n_ff             = 8192
0.00.051.011 I llm_load_print_meta: n_expert         = 0
0.00.051.011 I llm_load_print_meta: n_expert_used    = 0
0.00.051.011 I llm_load_print_meta: causal attn      = 1
0.00.051.011 I llm_load_print_meta: pooling type     = 0
0.00.051.011 I llm_load_print_meta: rope type        = 2
0.00.051.014 I llm_load_print_meta: rope scaling     = linear
0.00.051.014 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.014 I llm_load_print_meta: freq_scale_train = 1
0.00.051.016 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.017 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.017 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.017 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.027 I llm_load_print_meta: model type       = 1.4B
0.00.051.027 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.028 I llm_load_print_meta: model params     = 1.41 B
0.00.051.028 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.028 I llm_load_print_meta: general.name     = 1.4B
0.00.051.028 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.029 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.029 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.029 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.029 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.030 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.030 I llm_load_print_meta: max token length = 1024
0.00.053.077 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.077 I llm_load_tensors: offloading output layer to GPU
0.00.053.078 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.089 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.090 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.025 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.026 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.026 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.027 I llama_new_context_with_model: n_batch       = 2048
0.00.054.027 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.027 I llama_new_context_with_model: flash_attn    = 0
0.00.054.027 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.028 I llama_new_context_with_model: freq_scale    = 1
0.00.054.028 I ggml_metal_init: allocating
0.00.054.032 I ggml_metal_init: found device: Apple M4
0.00.054.035 I ggml_metal_init: picking default device: Apple M4
0.00.054.665 I ggml_metal_init: using embedded metal library
0.00.057.006 I ggml_metal_init: GPU name:   Apple M4
0.00.057.008 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.008 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.009 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.009 I ggml_metal_init: simdgroup reduction   = true
0.00.057.009 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.009 I ggml_metal_init: has bfloat            = true
0.00.057.009 I ggml_metal_init: use bfloat            = true
0.00.057.010 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.010 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.611 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.620 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.639 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.602 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.604 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.604 I llama_new_context_with_model: graph nodes  = 967
0.00.087.604 I llama_new_context_with_model: graph splits = 2
0.00.087.619 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.678 I main: llama threadpool init, n_threads = 4
0.00.759.717 I 
0.00.759.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.759.771 I 
0.00.759.992 I sampler seed: 1234
0.00.759.996 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.008 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.008 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.008 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.640.406 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60580.20 tokens per second)
0.01.640.407 I llama_perf_context_print:        load time =     751.12 ms
0.01.640.407 I llama_perf_context_print: prompt eval time =      54.51 ms /     7 tokens (    7.79 ms per token,   128.42 tokens per second)
0.01.640.408 I llama_perf_context_print:        eval time =     822.97 ms /    63 runs   (   13.06 ms per token,    76.55 tokens per second)
0.01.640.408 I llama_perf_context_print:       total time =     880.73 ms /    70 tokens
0.01.640.591 I ggml_metal_free: deallocating

real	0m1.654s
user	0m0.109s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.594 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.168 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.999 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.005 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.007 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.012 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.012 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.013 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.013 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.016 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.016 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.016 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.017 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.017 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.018 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.018 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.020 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.020 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.020 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.638 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.638 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.639 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.639 I llama_model_loader: - type  f32:  194 tensors
0.00.051.640 I llama_model_loader: - type  f16:   98 tensors
0.00.079.586 I llm_load_vocab: special tokens cache size = 25
0.00.085.914 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.916 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.917 I llm_load_print_meta: arch             = gptneox
0.00.085.917 I llm_load_print_meta: vocab type       = BPE
0.00.085.917 I llm_load_print_meta: n_vocab          = 50304
0.00.085.917 I llm_load_print_meta: n_merges         = 50009
0.00.085.917 I llm_load_print_meta: vocab_only       = 0
0.00.085.918 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.918 I llm_load_print_meta: n_embd           = 2048
0.00.085.918 I llm_load_print_meta: n_layer          = 24
0.00.085.933 I llm_load_print_meta: n_head           = 16
0.00.085.934 I llm_load_print_meta: n_head_kv        = 16
0.00.085.934 I llm_load_print_meta: n_rot            = 32
0.00.085.934 I llm_load_print_meta: n_swa            = 0
0.00.085.934 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.934 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.936 I llm_load_print_meta: n_gqa            = 1
0.00.085.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.938 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.938 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.939 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.939 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.939 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.940 I llm_load_print_meta: n_ff             = 8192
0.00.085.941 I llm_load_print_meta: n_expert         = 0
0.00.085.941 I llm_load_print_meta: n_expert_used    = 0
0.00.085.941 I llm_load_print_meta: causal attn      = 1
0.00.085.941 I llm_load_print_meta: pooling type     = 0
0.00.085.941 I llm_load_print_meta: rope type        = 2
0.00.085.941 I llm_load_print_meta: rope scaling     = linear
0.00.085.941 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.942 I llm_load_print_meta: freq_scale_train = 1
0.00.085.946 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.946 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.947 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.947 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.947 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.947 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.957 I llm_load_print_meta: model type       = 1.4B
0.00.085.958 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.085.958 I llm_load_print_meta: model params     = 1.41 B
0.00.085.958 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.085.959 I llm_load_print_meta: general.name     = 1.4B
0.00.085.959 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.959 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.959 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.959 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.960 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.960 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.960 I llm_load_print_meta: max token length = 1024
0.00.088.487 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.088.487 I llm_load_tensors: offloading output layer to GPU
0.00.088.488 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.088.499 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.088.500 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.089.411 I llama_new_context_with_model: n_seq_max     = 1
0.00.089.412 I llama_new_context_with_model: n_ctx         = 128
0.00.089.412 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.089.412 I llama_new_context_with_model: n_batch       = 128
0.00.089.412 I llama_new_context_with_model: n_ubatch      = 128
0.00.089.413 I llama_new_context_with_model: flash_attn    = 0
0.00.089.413 I llama_new_context_with_model: freq_base     = 10000.0
0.00.089.413 I llama_new_context_with_model: freq_scale    = 1
0.00.089.414 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.089.414 I ggml_metal_init: allocating
0.00.089.417 I ggml_metal_init: found device: Apple M4
0.00.089.419 I ggml_metal_init: picking default device: Apple M4
0.00.089.994 I ggml_metal_init: using embedded metal library
0.00.092.477 I ggml_metal_init: GPU name:   Apple M4
0.00.092.479 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.092.479 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.092.480 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.092.480 I ggml_metal_init: simdgroup reduction   = true
0.00.092.480 I ggml_metal_init: simdgroup matrix mul. = true
0.00.092.480 I ggml_metal_init: has bfloat            = true
0.00.092.480 I ggml_metal_init: use bfloat            = true
0.00.092.481 I ggml_metal_init: hasUnifiedMemory      = true
0.00.092.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.817 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.102.820 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.102.834 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.654 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.103.655 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.103.655 I llama_new_context_with_model: graph nodes  = 967
0.00.103.656 I llama_new_context_with_model: graph splits = 2
0.00.103.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.334.441 I 
0.01.334.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.334.500 I perplexity: tokenizing the input ..
0.01.343.356 I perplexity: tokenization took 8.854 ms
0.01.343.376 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.462.516 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.463.915 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.463.928 I llama_perf_context_print:        load time =    1313.26 ms
0.01.463.929 I llama_perf_context_print: prompt eval time =     118.83 ms /   128 tokens (    0.93 ms per token,  1077.21 tokens per second)
0.01.463.930 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.463.930 I llama_perf_context_print:       total time =     129.49 ms /   129 tokens
0.01.464.292 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.111s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.249 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.553 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.015.558 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.560 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.561 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.561 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.562 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.562 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.563 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.563 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.564 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.564 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.566 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.567 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.568 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.448 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.381 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.382 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.382 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.383 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.383 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.383 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.024.384 I llama_model_loader: - type  f32:  194 tensors
0.00.024.384 I llama_model_loader: - type q8_0:   98 tensors
0.00.044.951 I llm_load_vocab: special tokens cache size = 25
0.00.050.962 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.966 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.966 I llm_load_print_meta: arch             = gptneox
0.00.050.967 I llm_load_print_meta: vocab type       = BPE
0.00.050.967 I llm_load_print_meta: n_vocab          = 50304
0.00.050.967 I llm_load_print_meta: n_merges         = 50009
0.00.050.967 I llm_load_print_meta: vocab_only       = 0
0.00.050.967 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.967 I llm_load_print_meta: n_embd           = 2048
0.00.050.968 I llm_load_print_meta: n_layer          = 24
0.00.050.978 I llm_load_print_meta: n_head           = 16
0.00.050.979 I llm_load_print_meta: n_head_kv        = 16
0.00.050.979 I llm_load_print_meta: n_rot            = 32
0.00.050.979 I llm_load_print_meta: n_swa            = 0
0.00.050.979 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.979 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.980 I llm_load_print_meta: n_gqa            = 1
0.00.050.980 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.981 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.982 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.982 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.982 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.984 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.985 I llm_load_print_meta: n_ff             = 8192
0.00.050.985 I llm_load_print_meta: n_expert         = 0
0.00.050.985 I llm_load_print_meta: n_expert_used    = 0
0.00.050.985 I llm_load_print_meta: causal attn      = 1
0.00.050.985 I llm_load_print_meta: pooling type     = 0
0.00.050.986 I llm_load_print_meta: rope type        = 2
0.00.050.986 I llm_load_print_meta: rope scaling     = linear
0.00.050.986 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.986 I llm_load_print_meta: freq_scale_train = 1
0.00.050.986 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.987 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.987 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.987 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.987 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.987 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.987 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.991 I llm_load_print_meta: model type       = 1.4B
0.00.050.991 I llm_load_print_meta: model ftype      = Q8_0
0.00.050.992 I llm_load_print_meta: model params     = 1.41 B
0.00.050.992 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.050.992 I llm_load_print_meta: general.name     = 1.4B
0.00.050.993 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.993 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.994 I llm_load_print_meta: max token length = 1024
0.00.052.801 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.801 I llm_load_tensors: offloading output layer to GPU
0.00.052.802 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.808 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.052.808 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.053.725 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.726 I llama_new_context_with_model: n_ctx         = 128
0.00.053.726 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.726 I llama_new_context_with_model: n_batch       = 128
0.00.053.727 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.727 I llama_new_context_with_model: flash_attn    = 0
0.00.053.727 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.730 I llama_new_context_with_model: freq_scale    = 1
0.00.053.731 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.731 I ggml_metal_init: allocating
0.00.053.736 I ggml_metal_init: found device: Apple M4
0.00.053.739 I ggml_metal_init: picking default device: Apple M4
0.00.054.344 I ggml_metal_init: using embedded metal library
0.00.056.927 I ggml_metal_init: GPU name:   Apple M4
0.00.056.928 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.929 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.929 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.929 I ggml_metal_init: simdgroup reduction   = true
0.00.056.929 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.930 I ggml_metal_init: has bfloat            = true
0.00.056.930 I ggml_metal_init: use bfloat            = true
0.00.056.930 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.931 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.208 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.211 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.227 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.156 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.158 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.158 I llama_new_context_with_model: graph nodes  = 967
0.00.068.158 I llama_new_context_with_model: graph splits = 2
0.00.068.166 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.944.323 I 
0.00.944.353 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.944.362 I perplexity: tokenizing the input ..
0.00.952.097 I perplexity: tokenization took 7.734 ms
0.00.952.112 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.075.601 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.076.728 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.076.749 I llama_perf_context_print:        load time =     935.07 ms
0.01.076.751 I llama_perf_context_print: prompt eval time =     123.24 ms /   128 tokens (    0.96 ms per token,  1038.62 tokens per second)
0.01.076.752 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.076.752 I llama_perf_context_print:       total time =     132.42 ms /   129 tokens
0.01.077.074 I ggml_metal_free: deallocating

real	0m1.091s
user	0m0.079s
sys	0m0.155s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.097 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.772 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.605 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.610 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.612 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.613 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.613 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.613 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.613 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.614 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.615 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.615 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.616 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.616 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.616 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.617 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.618 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.619 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.619 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.722 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.946 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.032 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.033.036 I llama_model_loader: - type  f32:  194 tensors
0.00.033.036 I llama_model_loader: - type q4_0:   97 tensors
0.00.033.036 I llama_model_loader: - type q6_K:    1 tensors
0.00.062.563 I llm_load_vocab: special tokens cache size = 25
0.00.071.944 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.948 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.948 I llm_load_print_meta: arch             = gptneox
0.00.071.948 I llm_load_print_meta: vocab type       = BPE
0.00.071.949 I llm_load_print_meta: n_vocab          = 50304
0.00.071.949 I llm_load_print_meta: n_merges         = 50009
0.00.071.949 I llm_load_print_meta: vocab_only       = 0
0.00.071.949 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.950 I llm_load_print_meta: n_embd           = 2048
0.00.071.950 I llm_load_print_meta: n_layer          = 24
0.00.071.965 I llm_load_print_meta: n_head           = 16
0.00.071.966 I llm_load_print_meta: n_head_kv        = 16
0.00.071.970 I llm_load_print_meta: n_rot            = 32
0.00.071.970 I llm_load_print_meta: n_swa            = 0
0.00.071.970 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.970 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.971 I llm_load_print_meta: n_gqa            = 1
0.00.071.972 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.973 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.974 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.974 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.974 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.977 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.977 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.978 I llm_load_print_meta: n_ff             = 8192
0.00.071.978 I llm_load_print_meta: n_expert         = 0
0.00.071.978 I llm_load_print_meta: n_expert_used    = 0
0.00.071.978 I llm_load_print_meta: causal attn      = 1
0.00.071.979 I llm_load_print_meta: pooling type     = 0
0.00.071.979 I llm_load_print_meta: rope type        = 2
0.00.071.979 I llm_load_print_meta: rope scaling     = linear
0.00.071.980 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.980 I llm_load_print_meta: freq_scale_train = 1
0.00.071.980 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.981 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.981 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.981 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.982 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.982 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.982 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.993 I llm_load_print_meta: model type       = 1.4B
0.00.071.993 I llm_load_print_meta: model ftype      = Q4_0
0.00.071.994 I llm_load_print_meta: model params     = 1.41 B
0.00.071.995 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.071.995 I llm_load_print_meta: general.name     = 1.4B
0.00.071.995 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.995 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.996 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.996 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.996 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.071.997 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.997 I llm_load_print_meta: max token length = 1024
0.00.074.739 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.740 I llm_load_tensors: offloading output layer to GPU
0.00.074.740 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.751 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.074.753 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.076.066 I llama_new_context_with_model: n_seq_max     = 1
0.00.076.067 I llama_new_context_with_model: n_ctx         = 128
0.00.076.067 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.076.068 I llama_new_context_with_model: n_batch       = 128
0.00.076.068 I llama_new_context_with_model: n_ubatch      = 128
0.00.076.068 I llama_new_context_with_model: flash_attn    = 0
0.00.076.069 I llama_new_context_with_model: freq_base     = 10000.0
0.00.076.069 I llama_new_context_with_model: freq_scale    = 1
0.00.076.070 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.076.070 I ggml_metal_init: allocating
0.00.076.075 I ggml_metal_init: found device: Apple M4
0.00.076.079 I ggml_metal_init: picking default device: Apple M4
0.00.076.859 I ggml_metal_init: using embedded metal library
0.00.080.388 I ggml_metal_init: GPU name:   Apple M4
0.00.080.390 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.391 I ggml_metal_init: simdgroup reduction   = true
0.00.080.391 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.392 I ggml_metal_init: has bfloat            = true
0.00.080.392 I ggml_metal_init: use bfloat            = true
0.00.080.392 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.394 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.684 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.094.689 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.094.705 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.824 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.095.826 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.095.826 I llama_new_context_with_model: graph nodes  = 967
0.00.095.826 I llama_new_context_with_model: graph splits = 2
0.00.095.840 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.675.548 I 
0.00.675.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.675.667 I perplexity: tokenizing the input ..
0.00.690.797 I perplexity: tokenization took 15.124 ms
0.00.690.826 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.826.238 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.827.408 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.827.429 I llama_perf_context_print:        load time =     664.76 ms
0.00.827.431 I llama_perf_context_print: prompt eval time =     134.46 ms /   128 tokens (    1.05 ms per token,   951.98 tokens per second)
0.00.827.432 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.827.435 I llama_perf_context_print:       total time =     151.89 ms /   129 tokens
0.00.827.918 I ggml_metal_free: deallocating

real	0m0.864s
user	0m0.108s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.619 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.862 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.866 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.869 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.870 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.870 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.871 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.871 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.872 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.874 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.874 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.874 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.874 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.877 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.877 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.878 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.701 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.535 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.536 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.536 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.537 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.537 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.538 I llama_model_loader: - type  f32:  194 tensors
0.00.026.538 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.538 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.109 I llm_load_vocab: special tokens cache size = 25
0.00.053.174 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.176 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.177 I llm_load_print_meta: arch             = gptneox
0.00.053.177 I llm_load_print_meta: vocab type       = BPE
0.00.053.177 I llm_load_print_meta: n_vocab          = 50304
0.00.053.177 I llm_load_print_meta: n_merges         = 50009
0.00.053.178 I llm_load_print_meta: vocab_only       = 0
0.00.053.178 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.178 I llm_load_print_meta: n_embd           = 2048
0.00.053.178 I llm_load_print_meta: n_layer          = 24
0.00.053.192 I llm_load_print_meta: n_head           = 16
0.00.053.193 I llm_load_print_meta: n_head_kv        = 16
0.00.053.193 I llm_load_print_meta: n_rot            = 32
0.00.053.193 I llm_load_print_meta: n_swa            = 0
0.00.053.196 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.196 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.197 I llm_load_print_meta: n_gqa            = 1
0.00.053.198 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.198 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.199 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.199 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.200 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.200 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.200 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.200 I llm_load_print_meta: n_ff             = 8192
0.00.053.201 I llm_load_print_meta: n_expert         = 0
0.00.053.201 I llm_load_print_meta: n_expert_used    = 0
0.00.053.201 I llm_load_print_meta: causal attn      = 1
0.00.053.201 I llm_load_print_meta: pooling type     = 0
0.00.053.201 I llm_load_print_meta: rope type        = 2
0.00.053.202 I llm_load_print_meta: rope scaling     = linear
0.00.053.202 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.203 I llm_load_print_meta: freq_scale_train = 1
0.00.053.203 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.203 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.203 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.203 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.203 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.203 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.204 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.213 I llm_load_print_meta: model type       = 1.4B
0.00.053.213 I llm_load_print_meta: model ftype      = Q4_1
0.00.053.214 I llm_load_print_meta: model params     = 1.41 B
0.00.053.214 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.053.214 I llm_load_print_meta: general.name     = 1.4B
0.00.053.215 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.215 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.215 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.215 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.216 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.216 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.216 I llm_load_print_meta: max token length = 1024
0.00.055.163 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.163 I llm_load_tensors: offloading output layer to GPU
0.00.055.163 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.173 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.055.174 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.056.058 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.058 I llama_new_context_with_model: n_ctx         = 128
0.00.056.058 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.056.059 I llama_new_context_with_model: n_batch       = 128
0.00.056.059 I llama_new_context_with_model: n_ubatch      = 128
0.00.056.059 I llama_new_context_with_model: flash_attn    = 0
0.00.056.060 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.060 I llama_new_context_with_model: freq_scale    = 1
0.00.056.060 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.056.061 I ggml_metal_init: allocating
0.00.056.067 I ggml_metal_init: found device: Apple M4
0.00.056.069 I ggml_metal_init: picking default device: Apple M4
0.00.056.635 I ggml_metal_init: using embedded metal library
0.00.058.951 I ggml_metal_init: GPU name:   Apple M4
0.00.058.952 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.952 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.953 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.953 I ggml_metal_init: simdgroup reduction   = true
0.00.058.953 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.953 I ggml_metal_init: has bfloat            = true
0.00.058.953 I ggml_metal_init: use bfloat            = true
0.00.058.954 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.954 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.657 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.662 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.675 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.560 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.561 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.561 I llama_new_context_with_model: graph nodes  = 967
0.00.070.561 I llama_new_context_with_model: graph splits = 2
0.00.070.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.870.399 I 
0.00.870.541 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.870.561 I perplexity: tokenizing the input ..
0.00.888.103 I perplexity: tokenization took 17.538 ms
0.00.888.138 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.030.173 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.01.033.676 I Final estimate: PPL = 10.5507 +/- 3.34263

0.01.033.713 I llama_perf_context_print:        load time =     861.76 ms
0.01.033.720 I llama_perf_context_print: prompt eval time =     141.07 ms /   128 tokens (    1.10 ms per token,   907.38 tokens per second)
0.01.033.724 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.033.730 I llama_perf_context_print:       total time =     163.32 ms /   129 tokens
0.01.035.189 I ggml_metal_free: deallocating

real	0m1.066s
user	0m0.108s
sys	0m0.115s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.194 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.439 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.314 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.037.322 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.325 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.326 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.327 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.328 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.330 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.334 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.335 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.335 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.336 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.336 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.437 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.031 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.033 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.033 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.034 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.053.035 I llama_model_loader: - type  f32:  194 tensors
0.00.053.036 I llama_model_loader: - type q5_0:   97 tensors
0.00.053.036 I llama_model_loader: - type q6_K:    1 tensors
0.00.079.284 I llm_load_vocab: special tokens cache size = 25
0.00.085.573 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.085.575 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.085.576 I llm_load_print_meta: arch             = gptneox
0.00.085.576 I llm_load_print_meta: vocab type       = BPE
0.00.085.576 I llm_load_print_meta: n_vocab          = 50304
0.00.085.576 I llm_load_print_meta: n_merges         = 50009
0.00.085.577 I llm_load_print_meta: vocab_only       = 0
0.00.085.577 I llm_load_print_meta: n_ctx_train      = 2048
0.00.085.577 I llm_load_print_meta: n_embd           = 2048
0.00.085.577 I llm_load_print_meta: n_layer          = 24
0.00.085.591 I llm_load_print_meta: n_head           = 16
0.00.085.593 I llm_load_print_meta: n_head_kv        = 16
0.00.085.593 I llm_load_print_meta: n_rot            = 32
0.00.085.593 I llm_load_print_meta: n_swa            = 0
0.00.085.593 I llm_load_print_meta: n_embd_head_k    = 128
0.00.085.593 I llm_load_print_meta: n_embd_head_v    = 128
0.00.085.594 I llm_load_print_meta: n_gqa            = 1
0.00.085.594 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.085.595 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.085.595 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.085.595 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.085.596 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.085.596 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.085.596 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.085.596 I llm_load_print_meta: n_ff             = 8192
0.00.085.597 I llm_load_print_meta: n_expert         = 0
0.00.085.597 I llm_load_print_meta: n_expert_used    = 0
0.00.085.597 I llm_load_print_meta: causal attn      = 1
0.00.085.597 I llm_load_print_meta: pooling type     = 0
0.00.085.597 I llm_load_print_meta: rope type        = 2
0.00.085.597 I llm_load_print_meta: rope scaling     = linear
0.00.085.598 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.085.598 I llm_load_print_meta: freq_scale_train = 1
0.00.085.598 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.085.598 I llm_load_print_meta: rope_finetuned   = unknown
0.00.085.598 I llm_load_print_meta: ssm_d_conv       = 0
0.00.085.598 I llm_load_print_meta: ssm_d_inner      = 0
0.00.085.598 I llm_load_print_meta: ssm_d_state      = 0
0.00.085.599 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.085.599 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.085.608 I llm_load_print_meta: model type       = 1.4B
0.00.085.608 I llm_load_print_meta: model ftype      = Q5_0
0.00.085.609 I llm_load_print_meta: model params     = 1.41 B
0.00.085.609 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.085.610 I llm_load_print_meta: general.name     = 1.4B
0.00.085.610 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.085.610 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.085.612 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.085.612 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.085.612 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.085.612 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.085.613 I llm_load_print_meta: max token length = 1024
0.00.087.639 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.087.640 I llm_load_tensors: offloading output layer to GPU
0.00.087.640 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.087.651 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.087.652 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.088.537 I llama_new_context_with_model: n_seq_max     = 1
0.00.088.538 I llama_new_context_with_model: n_ctx         = 128
0.00.088.538 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.088.538 I llama_new_context_with_model: n_batch       = 128
0.00.088.538 I llama_new_context_with_model: n_ubatch      = 128
0.00.088.539 I llama_new_context_with_model: flash_attn    = 0
0.00.088.539 I llama_new_context_with_model: freq_base     = 10000.0
0.00.088.539 I llama_new_context_with_model: freq_scale    = 1
0.00.088.540 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.088.540 I ggml_metal_init: allocating
0.00.088.543 I ggml_metal_init: found device: Apple M4
0.00.088.545 I ggml_metal_init: picking default device: Apple M4
0.00.089.102 I ggml_metal_init: using embedded metal library
0.00.091.551 I ggml_metal_init: GPU name:   Apple M4
0.00.091.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.091.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.091.554 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.091.554 I ggml_metal_init: simdgroup reduction   = true
0.00.091.554 I ggml_metal_init: simdgroup matrix mul. = true
0.00.091.554 I ggml_metal_init: has bfloat            = true
0.00.091.555 I ggml_metal_init: use bfloat            = true
0.00.091.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.091.556 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.101.316 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.318 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.331 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.220 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.102.221 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.102.222 I llama_new_context_with_model: graph nodes  = 967
0.00.102.222 I llama_new_context_with_model: graph splits = 2
0.00.102.234 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.772.761 I 
0.00.772.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.772.855 I perplexity: tokenizing the input ..
0.00.788.218 I perplexity: tokenization took 15.36 ms
0.00.788.237 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.937.084 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.938.327 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.938.349 I llama_perf_context_print:        load time =     750.31 ms
0.00.938.350 I llama_perf_context_print: prompt eval time =     147.94 ms /   128 tokens (    1.16 ms per token,   865.23 tokens per second)
0.00.938.351 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.938.351 I llama_perf_context_print:       total time =     165.59 ms /   129 tokens
0.00.938.893 I ggml_metal_free: deallocating

real	0m0.979s
user	0m0.112s
sys	0m0.121s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.543 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.071 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.074 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.076 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.079 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.080 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.081 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.082 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.082 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.084 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.084 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.086 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.086 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.087 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.987 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.852 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.854 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.855 I llama_model_loader: - type  f32:  194 tensors
0.00.024.855 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.855 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.961 I llm_load_vocab: special tokens cache size = 25
0.00.050.926 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.928 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.929 I llm_load_print_meta: arch             = gptneox
0.00.050.929 I llm_load_print_meta: vocab type       = BPE
0.00.050.929 I llm_load_print_meta: n_vocab          = 50304
0.00.050.930 I llm_load_print_meta: n_merges         = 50009
0.00.050.930 I llm_load_print_meta: vocab_only       = 0
0.00.050.930 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.930 I llm_load_print_meta: n_embd           = 2048
0.00.050.930 I llm_load_print_meta: n_layer          = 24
0.00.050.944 I llm_load_print_meta: n_head           = 16
0.00.050.945 I llm_load_print_meta: n_head_kv        = 16
0.00.050.945 I llm_load_print_meta: n_rot            = 32
0.00.050.946 I llm_load_print_meta: n_swa            = 0
0.00.050.946 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.946 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.947 I llm_load_print_meta: n_gqa            = 1
0.00.050.948 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.949 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.950 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.950 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.950 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.950 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.950 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.951 I llm_load_print_meta: n_ff             = 8192
0.00.050.951 I llm_load_print_meta: n_expert         = 0
0.00.050.951 I llm_load_print_meta: n_expert_used    = 0
0.00.050.951 I llm_load_print_meta: causal attn      = 1
0.00.050.952 I llm_load_print_meta: pooling type     = 0
0.00.050.952 I llm_load_print_meta: rope type        = 2
0.00.050.952 I llm_load_print_meta: rope scaling     = linear
0.00.050.952 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.952 I llm_load_print_meta: freq_scale_train = 1
0.00.050.953 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.953 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.953 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.953 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.953 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.955 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.955 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.964 I llm_load_print_meta: model type       = 1.4B
0.00.050.964 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.965 I llm_load_print_meta: model params     = 1.41 B
0.00.050.965 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.965 I llm_load_print_meta: general.name     = 1.4B
0.00.050.966 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.966 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.966 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.966 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.966 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.967 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.967 I llm_load_print_meta: max token length = 1024
0.00.052.930 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.931 I llm_load_tensors: offloading output layer to GPU
0.00.052.931 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.941 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.942 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.918 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.919 I llama_new_context_with_model: n_ctx         = 128
0.00.053.919 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.919 I llama_new_context_with_model: n_batch       = 128
0.00.053.920 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.920 I llama_new_context_with_model: flash_attn    = 0
0.00.053.920 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.920 I llama_new_context_with_model: freq_scale    = 1
0.00.053.921 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.921 I ggml_metal_init: allocating
0.00.053.924 I ggml_metal_init: found device: Apple M4
0.00.053.926 I ggml_metal_init: picking default device: Apple M4
0.00.054.486 I ggml_metal_init: using embedded metal library
0.00.056.779 I ggml_metal_init: GPU name:   Apple M4
0.00.056.780 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.780 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.781 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.781 I ggml_metal_init: simdgroup reduction   = true
0.00.056.781 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.781 I ggml_metal_init: has bfloat            = true
0.00.056.781 I ggml_metal_init: use bfloat            = true
0.00.056.782 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.782 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.470 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.472 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.485 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.431 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.432 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.433 I llama_new_context_with_model: graph nodes  = 967
0.00.068.433 I llama_new_context_with_model: graph splits = 2
0.00.068.445 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.267 I 
0.00.739.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.308 I perplexity: tokenizing the input ..
0.00.747.030 I perplexity: tokenization took 7.72 ms
0.00.747.041 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.881.488 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.882.959 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.882.976 I llama_perf_context_print:        load time =     730.72 ms
0.00.882.977 I llama_perf_context_print: prompt eval time =     134.22 ms /   128 tokens (    1.05 ms per token,   953.67 tokens per second)
0.00.882.978 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.882.978 I llama_perf_context_print:       total time =     143.71 ms /   129 tokens
0.00.883.405 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.076s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.311 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.893 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.893 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.894 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.897 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.897 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.897 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.898 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.898 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.898 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.901 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.902 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.902 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.094 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.131 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.302 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.302 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.302 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.303 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.303 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.303 I llama_model_loader: - type  f32:  194 tensors
0.00.026.304 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.304 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.304 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.086 I llm_load_vocab: special tokens cache size = 25
0.00.051.993 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.995 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.996 I llm_load_print_meta: arch             = gptneox
0.00.051.996 I llm_load_print_meta: vocab type       = BPE
0.00.051.996 I llm_load_print_meta: n_vocab          = 50304
0.00.051.996 I llm_load_print_meta: n_merges         = 50009
0.00.051.997 I llm_load_print_meta: vocab_only       = 0
0.00.051.997 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.997 I llm_load_print_meta: n_embd           = 2048
0.00.051.997 I llm_load_print_meta: n_layer          = 24
0.00.052.007 I llm_load_print_meta: n_head           = 16
0.00.052.007 I llm_load_print_meta: n_head_kv        = 16
0.00.052.008 I llm_load_print_meta: n_rot            = 32
0.00.052.008 I llm_load_print_meta: n_swa            = 0
0.00.052.009 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.010 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.011 I llm_load_print_meta: n_gqa            = 1
0.00.052.012 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.013 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.013 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.014 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.016 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.016 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.016 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.017 I llm_load_print_meta: n_ff             = 8192
0.00.052.017 I llm_load_print_meta: n_expert         = 0
0.00.052.020 I llm_load_print_meta: n_expert_used    = 0
0.00.052.021 I llm_load_print_meta: causal attn      = 1
0.00.052.021 I llm_load_print_meta: pooling type     = 0
0.00.052.021 I llm_load_print_meta: rope type        = 2
0.00.052.021 I llm_load_print_meta: rope scaling     = linear
0.00.052.021 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.022 I llm_load_print_meta: freq_scale_train = 1
0.00.052.022 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.022 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.022 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.022 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.022 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.023 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.023 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.027 I llm_load_print_meta: model type       = 1.4B
0.00.052.028 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.028 I llm_load_print_meta: model params     = 1.41 B
0.00.052.029 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.029 I llm_load_print_meta: general.name     = 1.4B
0.00.052.029 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.029 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.029 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.029 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.030 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.030 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.030 I llm_load_print_meta: max token length = 1024
0.00.053.756 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.756 I llm_load_tensors: offloading output layer to GPU
0.00.053.756 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.761 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.762 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.663 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.663 I llama_new_context_with_model: n_ctx         = 128
0.00.054.664 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.664 I llama_new_context_with_model: n_batch       = 128
0.00.054.664 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.664 I llama_new_context_with_model: flash_attn    = 0
0.00.054.665 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.665 I llama_new_context_with_model: freq_scale    = 1
0.00.054.665 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.665 I ggml_metal_init: allocating
0.00.054.671 I ggml_metal_init: found device: Apple M4
0.00.054.674 I ggml_metal_init: picking default device: Apple M4
0.00.055.237 I ggml_metal_init: using embedded metal library
0.00.057.878 I ggml_metal_init: GPU name:   Apple M4
0.00.057.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.881 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.882 I ggml_metal_init: simdgroup reduction   = true
0.00.057.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.883 I ggml_metal_init: has bfloat            = true
0.00.057.883 I ggml_metal_init: use bfloat            = true
0.00.057.883 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.887 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.525 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.540 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.382 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.383 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.383 I llama_new_context_with_model: graph nodes  = 967
0.00.069.383 I llama_new_context_with_model: graph splits = 2
0.00.069.391 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.404.202 I 
0.00.404.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.404.243 I perplexity: tokenizing the input ..
0.00.411.791 I perplexity: tokenization took 7.547 ms
0.00.411.805 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.544.389 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.545.566 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.545.581 I llama_perf_context_print:        load time =     394.89 ms
0.00.545.582 I llama_perf_context_print: prompt eval time =     132.36 ms /   128 tokens (    1.03 ms per token,   967.08 tokens per second)
0.00.545.583 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.545.584 I llama_perf_context_print:       total time =     141.38 ms /   129 tokens
0.00.545.983 I ggml_metal_free: deallocating

real	0m0.566s
user	0m0.080s
sys	0m0.081s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.620 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.368 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.373 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.375 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.375 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.378 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.378 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.381 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.236 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.080 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.082 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.082 I llama_model_loader: - type  f32:  194 tensors
0.00.023.083 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.083 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.083 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.083 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.000 I llm_load_vocab: special tokens cache size = 25
0.00.049.014 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.017 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.017 I llm_load_print_meta: arch             = gptneox
0.00.049.017 I llm_load_print_meta: vocab type       = BPE
0.00.049.017 I llm_load_print_meta: n_vocab          = 50304
0.00.049.018 I llm_load_print_meta: n_merges         = 50009
0.00.049.018 I llm_load_print_meta: vocab_only       = 0
0.00.049.018 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.018 I llm_load_print_meta: n_embd           = 2048
0.00.049.018 I llm_load_print_meta: n_layer          = 24
0.00.049.032 I llm_load_print_meta: n_head           = 16
0.00.049.033 I llm_load_print_meta: n_head_kv        = 16
0.00.049.033 I llm_load_print_meta: n_rot            = 32
0.00.049.033 I llm_load_print_meta: n_swa            = 0
0.00.049.034 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.034 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.035 I llm_load_print_meta: n_gqa            = 1
0.00.049.035 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.036 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.037 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.037 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.037 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.037 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.037 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.038 I llm_load_print_meta: n_ff             = 8192
0.00.049.038 I llm_load_print_meta: n_expert         = 0
0.00.049.038 I llm_load_print_meta: n_expert_used    = 0
0.00.049.038 I llm_load_print_meta: causal attn      = 1
0.00.049.039 I llm_load_print_meta: pooling type     = 0
0.00.049.039 I llm_load_print_meta: rope type        = 2
0.00.049.039 I llm_load_print_meta: rope scaling     = linear
0.00.049.039 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.040 I llm_load_print_meta: freq_scale_train = 1
0.00.049.040 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.040 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.040 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.040 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.042 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.042 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.042 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.051 I llm_load_print_meta: model type       = 1.4B
0.00.049.051 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.052 I llm_load_print_meta: model params     = 1.41 B
0.00.049.052 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.052 I llm_load_print_meta: general.name     = 1.4B
0.00.049.053 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.053 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.053 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.053 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.053 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.054 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.054 I llm_load_print_meta: max token length = 1024
0.00.050.959 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.959 I llm_load_tensors: offloading output layer to GPU
0.00.050.959 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.970 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.050.971 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.839 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.840 I llama_new_context_with_model: n_ctx         = 128
0.00.051.840 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.840 I llama_new_context_with_model: n_batch       = 128
0.00.051.840 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.841 I llama_new_context_with_model: flash_attn    = 0
0.00.051.841 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.841 I llama_new_context_with_model: freq_scale    = 1
0.00.051.842 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.842 I ggml_metal_init: allocating
0.00.051.845 I ggml_metal_init: found device: Apple M4
0.00.051.847 I ggml_metal_init: picking default device: Apple M4
0.00.052.402 I ggml_metal_init: using embedded metal library
0.00.054.697 I ggml_metal_init: GPU name:   Apple M4
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.700 I ggml_metal_init: simdgroup reduction   = true
0.00.054.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.700 I ggml_metal_init: has bfloat            = true
0.00.054.700 I ggml_metal_init: use bfloat            = true
0.00.054.700 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.596 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.598 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.611 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.472 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.473 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.473 I llama_new_context_with_model: graph nodes  = 967
0.00.066.474 I llama_new_context_with_model: graph splits = 2
0.00.066.486 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.479.990 I 
0.00.480.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.480.059 I perplexity: tokenizing the input ..
0.00.487.674 I perplexity: tokenization took 7.613 ms
0.00.487.685 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.619.957 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.621.177 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.621.200 I llama_perf_context_print:        load time =     471.36 ms
0.00.621.201 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.45 tokens per second)
0.00.621.202 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.621.202 I llama_perf_context_print:       total time =     141.21 ms /   129 tokens
0.00.621.681 I ggml_metal_free: deallocating

real	0m0.635s
user	0m0.077s
sys	0m0.085s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.080 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.756 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.460 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.349 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.411 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.369 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.370 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.370 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.370 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.371 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.371 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.372 I llama_model_loader: - type  f32:  194 tensors
0.00.023.372 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.372 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.373 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.131 I llm_load_vocab: special tokens cache size = 25
0.00.050.092 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.095 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.095 I llm_load_print_meta: arch             = gptneox
0.00.050.095 I llm_load_print_meta: vocab type       = BPE
0.00.050.096 I llm_load_print_meta: n_vocab          = 50304
0.00.050.096 I llm_load_print_meta: n_merges         = 50009
0.00.050.096 I llm_load_print_meta: vocab_only       = 0
0.00.050.096 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.096 I llm_load_print_meta: n_embd           = 2048
0.00.050.096 I llm_load_print_meta: n_layer          = 24
0.00.050.111 I llm_load_print_meta: n_head           = 16
0.00.050.114 I llm_load_print_meta: n_head_kv        = 16
0.00.050.114 I llm_load_print_meta: n_rot            = 32
0.00.050.114 I llm_load_print_meta: n_swa            = 0
0.00.050.114 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.115 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.115 I llm_load_print_meta: n_gqa            = 1
0.00.050.116 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.117 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.117 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.118 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.118 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.118 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.118 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.119 I llm_load_print_meta: n_ff             = 8192
0.00.050.119 I llm_load_print_meta: n_expert         = 0
0.00.050.119 I llm_load_print_meta: n_expert_used    = 0
0.00.050.119 I llm_load_print_meta: causal attn      = 1
0.00.050.119 I llm_load_print_meta: pooling type     = 0
0.00.050.119 I llm_load_print_meta: rope type        = 2
0.00.050.119 I llm_load_print_meta: rope scaling     = linear
0.00.050.120 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.120 I llm_load_print_meta: freq_scale_train = 1
0.00.050.120 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.121 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.121 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.121 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.121 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.121 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.121 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.131 I llm_load_print_meta: model type       = 1.4B
0.00.050.131 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.132 I llm_load_print_meta: model params     = 1.41 B
0.00.050.132 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.132 I llm_load_print_meta: general.name     = 1.4B
0.00.050.133 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.133 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.133 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.133 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.133 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.134 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.134 I llm_load_print_meta: max token length = 1024
0.00.052.066 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.066 I llm_load_tensors: offloading output layer to GPU
0.00.052.067 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.077 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.078 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.973 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.974 I llama_new_context_with_model: n_ctx         = 128
0.00.052.974 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.974 I llama_new_context_with_model: n_batch       = 128
0.00.052.974 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.974 I llama_new_context_with_model: flash_attn    = 0
0.00.052.975 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.975 I llama_new_context_with_model: freq_scale    = 1
0.00.052.975 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.976 I ggml_metal_init: allocating
0.00.052.979 I ggml_metal_init: found device: Apple M4
0.00.052.981 I ggml_metal_init: picking default device: Apple M4
0.00.053.546 I ggml_metal_init: using embedded metal library
0.00.055.870 I ggml_metal_init: GPU name:   Apple M4
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.872 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.873 I ggml_metal_init: simdgroup reduction   = true
0.00.055.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.873 I ggml_metal_init: has bfloat            = true
0.00.055.873 I ggml_metal_init: use bfloat            = true
0.00.055.873 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.874 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.864 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.869 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.885 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.755 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.756 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.757 I llama_new_context_with_model: graph nodes  = 967
0.00.067.757 I llama_new_context_with_model: graph splits = 2
0.00.067.769 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.684 I 
0.00.568.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.730 I perplexity: tokenizing the input ..
0.00.576.211 I perplexity: tokenization took 7.479 ms
0.00.576.222 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.710.385 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.711.667 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.711.687 I llama_perf_context_print:        load time =     559.92 ms
0.00.711.688 I llama_perf_context_print: prompt eval time =     133.94 ms /   128 tokens (    1.05 ms per token,   955.65 tokens per second)
0.00.711.689 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.711.689 I llama_perf_context_print:       total time =     143.01 ms /   129 tokens
0.00.712.162 I ggml_metal_free: deallocating

real	0m0.725s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.078 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.077 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.785 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.795 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.799 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.800 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.800 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.843 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.949 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.950 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.950 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.951 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.951 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.952 I llama_model_loader: - type  f32:  194 tensors
0.00.025.952 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.952 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.952 I llm_load_vocab: special tokens cache size = 25
0.00.051.796 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.798 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.799 I llm_load_print_meta: arch             = gptneox
0.00.051.799 I llm_load_print_meta: vocab type       = BPE
0.00.051.799 I llm_load_print_meta: n_vocab          = 50304
0.00.051.799 I llm_load_print_meta: n_merges         = 50009
0.00.051.800 I llm_load_print_meta: vocab_only       = 0
0.00.051.800 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.800 I llm_load_print_meta: n_embd           = 2048
0.00.051.800 I llm_load_print_meta: n_layer          = 24
0.00.051.814 I llm_load_print_meta: n_head           = 16
0.00.051.816 I llm_load_print_meta: n_head_kv        = 16
0.00.051.816 I llm_load_print_meta: n_rot            = 32
0.00.051.816 I llm_load_print_meta: n_swa            = 0
0.00.051.817 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.817 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.818 I llm_load_print_meta: n_gqa            = 1
0.00.051.818 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.819 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.820 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.820 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.820 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.820 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.820 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.821 I llm_load_print_meta: n_ff             = 8192
0.00.051.821 I llm_load_print_meta: n_expert         = 0
0.00.051.821 I llm_load_print_meta: n_expert_used    = 0
0.00.051.821 I llm_load_print_meta: causal attn      = 1
0.00.051.821 I llm_load_print_meta: pooling type     = 0
0.00.051.822 I llm_load_print_meta: rope type        = 2
0.00.051.822 I llm_load_print_meta: rope scaling     = linear
0.00.051.822 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.822 I llm_load_print_meta: freq_scale_train = 1
0.00.051.823 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.823 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.823 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.823 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.823 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.823 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.823 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.833 I llm_load_print_meta: model type       = 1.4B
0.00.051.833 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.834 I llm_load_print_meta: model params     = 1.41 B
0.00.051.834 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.834 I llm_load_print_meta: general.name     = 1.4B
0.00.051.835 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.835 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.835 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.836 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.837 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.837 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.837 I llm_load_print_meta: max token length = 1024
0.00.053.818 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.818 I llm_load_tensors: offloading output layer to GPU
0.00.053.818 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.828 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.830 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.769 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.769 I llama_new_context_with_model: n_ctx         = 128
0.00.054.770 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.770 I llama_new_context_with_model: n_batch       = 128
0.00.054.770 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.770 I llama_new_context_with_model: flash_attn    = 0
0.00.054.771 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.771 I llama_new_context_with_model: freq_scale    = 1
0.00.054.771 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.772 I ggml_metal_init: allocating
0.00.054.777 I ggml_metal_init: found device: Apple M4
0.00.054.779 I ggml_metal_init: picking default device: Apple M4
0.00.055.331 I ggml_metal_init: using embedded metal library
0.00.057.635 I ggml_metal_init: GPU name:   Apple M4
0.00.057.636 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.637 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.637 I ggml_metal_init: simdgroup reduction   = true
0.00.057.638 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.638 I ggml_metal_init: has bfloat            = true
0.00.057.638 I ggml_metal_init: use bfloat            = true
0.00.057.638 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.493 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.499 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.514 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.402 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.403 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.403 I llama_new_context_with_model: graph nodes  = 967
0.00.069.404 I llama_new_context_with_model: graph splits = 2
0.00.069.416 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.599 I 
0.00.633.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.644 I perplexity: tokenizing the input ..
0.00.641.129 I perplexity: tokenization took 7.483 ms
0.00.641.139 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.894 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.783.080 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.783.105 I llama_perf_context_print:        load time =     624.51 ms
0.00.783.107 I llama_perf_context_print: prompt eval time =     140.53 ms /   128 tokens (    1.10 ms per token,   910.85 tokens per second)
0.00.783.108 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.109 I llama_perf_context_print:       total time =     149.51 ms /   129 tokens
0.00.783.658 I ggml_metal_free: deallocating

real	0m0.799s
user	0m0.079s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.079 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.305 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.181 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.182 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.182 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.182 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.182 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.183 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.184 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.184 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.184 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.186 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.187 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.187 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.190 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.190 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.190 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.081 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.055 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.056 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.056 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.057 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.057 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.058 I llama_model_loader: - type  f32:  194 tensors
0.00.026.058 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.639 I llm_load_vocab: special tokens cache size = 25
0.00.052.583 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.586 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.586 I llm_load_print_meta: arch             = gptneox
0.00.052.586 I llm_load_print_meta: vocab type       = BPE
0.00.052.586 I llm_load_print_meta: n_vocab          = 50304
0.00.052.587 I llm_load_print_meta: n_merges         = 50009
0.00.052.587 I llm_load_print_meta: vocab_only       = 0
0.00.052.587 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.587 I llm_load_print_meta: n_embd           = 2048
0.00.052.587 I llm_load_print_meta: n_layer          = 24
0.00.052.601 I llm_load_print_meta: n_head           = 16
0.00.052.602 I llm_load_print_meta: n_head_kv        = 16
0.00.052.604 I llm_load_print_meta: n_rot            = 32
0.00.052.604 I llm_load_print_meta: n_swa            = 0
0.00.052.604 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.604 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.605 I llm_load_print_meta: n_gqa            = 1
0.00.052.606 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.607 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.607 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.608 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.608 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.608 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.609 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.609 I llm_load_print_meta: n_ff             = 8192
0.00.052.609 I llm_load_print_meta: n_expert         = 0
0.00.052.609 I llm_load_print_meta: n_expert_used    = 0
0.00.052.610 I llm_load_print_meta: causal attn      = 1
0.00.052.610 I llm_load_print_meta: pooling type     = 0
0.00.052.610 I llm_load_print_meta: rope type        = 2
0.00.052.610 I llm_load_print_meta: rope scaling     = linear
0.00.052.610 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.611 I llm_load_print_meta: freq_scale_train = 1
0.00.052.611 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.611 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.611 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.611 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.611 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.612 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.612 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.621 I llm_load_print_meta: model type       = 1.4B
0.00.052.621 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.621 I llm_load_print_meta: model params     = 1.41 B
0.00.052.622 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.622 I llm_load_print_meta: general.name     = 1.4B
0.00.052.622 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.622 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.622 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.623 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.624 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.624 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.624 I llm_load_print_meta: max token length = 1024
0.00.054.217 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.217 I llm_load_tensors: offloading output layer to GPU
0.00.054.217 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.227 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.228 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.589 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.590 I llama_new_context_with_model: n_ctx         = 128
0.00.055.590 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.590 I llama_new_context_with_model: n_batch       = 128
0.00.055.591 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.591 I llama_new_context_with_model: flash_attn    = 0
0.00.055.591 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.591 I llama_new_context_with_model: freq_scale    = 1
0.00.055.592 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.593 I ggml_metal_init: allocating
0.00.055.597 I ggml_metal_init: found device: Apple M4
0.00.055.599 I ggml_metal_init: picking default device: Apple M4
0.00.056.188 I ggml_metal_init: using embedded metal library
0.00.058.719 I ggml_metal_init: GPU name:   Apple M4
0.00.058.721 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.721 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.721 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.722 I ggml_metal_init: simdgroup reduction   = true
0.00.058.722 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.722 I ggml_metal_init: has bfloat            = true
0.00.058.722 I ggml_metal_init: use bfloat            = true
0.00.058.723 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.723 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.387 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.391 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.406 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.272 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.273 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.273 I llama_new_context_with_model: graph nodes  = 967
0.00.070.273 I llama_new_context_with_model: graph splits = 2
0.00.070.285 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.580.088 I 
0.00.580.122 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.580.130 I perplexity: tokenizing the input ..
0.00.587.585 I perplexity: tokenization took 7.453 ms
0.00.587.600 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.694 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.728.968 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.728.986 I llama_perf_context_print:        load time =     568.78 ms
0.00.728.987 I llama_perf_context_print: prompt eval time =     139.87 ms /   128 tokens (    1.09 ms per token,   915.15 tokens per second)
0.00.728.988 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.989 I llama_perf_context_print:       total time =     148.90 ms /   129 tokens
0.00.729.463 I ggml_metal_free: deallocating

real	0m0.743s
user	0m0.078s
sys	0m0.119s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.233 I build: 4320 (64ae0655) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.443 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.109 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.115 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.116 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.122 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.122 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.122 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.123 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.124 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.126 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.126 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.127 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.127 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.374 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.482 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.099 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.101 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.102 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.104 I llama_model_loader: - type  f32:  194 tensors
0.00.053.105 I llama_model_loader: - type  f16:   98 tensors
0.00.082.559 I llm_load_vocab: special tokens cache size = 25
0.00.089.457 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.460 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.460 I llm_load_print_meta: arch             = gptneox
0.00.089.461 I llm_load_print_meta: vocab type       = BPE
0.00.089.461 I llm_load_print_meta: n_vocab          = 50304
0.00.089.461 I llm_load_print_meta: n_merges         = 50009
0.00.089.461 I llm_load_print_meta: vocab_only       = 0
0.00.089.461 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.461 I llm_load_print_meta: n_embd           = 2048
0.00.089.462 I llm_load_print_meta: n_layer          = 24
0.00.089.476 I llm_load_print_meta: n_head           = 16
0.00.089.477 I llm_load_print_meta: n_head_kv        = 16
0.00.089.477 I llm_load_print_meta: n_rot            = 32
0.00.089.477 I llm_load_print_meta: n_swa            = 0
0.00.089.477 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.477 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.478 I llm_load_print_meta: n_gqa            = 1
0.00.089.479 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.479 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.480 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.480 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.480 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.480 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.482 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.482 I llm_load_print_meta: n_ff             = 8192
0.00.089.483 I llm_load_print_meta: n_expert         = 0
0.00.089.483 I llm_load_print_meta: n_expert_used    = 0
0.00.089.483 I llm_load_print_meta: causal attn      = 1
0.00.089.483 I llm_load_print_meta: pooling type     = 0
0.00.089.483 I llm_load_print_meta: rope type        = 2
0.00.089.483 I llm_load_print_meta: rope scaling     = linear
0.00.089.485 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.485 I llm_load_print_meta: freq_scale_train = 1
0.00.089.487 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.488 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.488 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.488 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.488 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.489 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.489 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.499 I llm_load_print_meta: model type       = 1.4B
0.00.089.499 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.500 I llm_load_print_meta: model params     = 1.41 B
0.00.089.500 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.500 I llm_load_print_meta: general.name     = 1.4B
0.00.089.500 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.501 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.501 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.501 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.501 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.501 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.502 I llm_load_print_meta: max token length = 1024
0.00.092.092 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.092.092 I llm_load_tensors: offloading output layer to GPU
0.00.092.092 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.092.103 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.092.104 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.103 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.104 I llama_new_context_with_model: n_ctx         = 128
0.00.093.104 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.104 I llama_new_context_with_model: n_batch       = 128
0.00.093.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.104 I llama_new_context_with_model: flash_attn    = 0
0.00.093.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.105 I llama_new_context_with_model: freq_scale    = 1
0.00.093.106 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.106 I ggml_metal_init: allocating
0.00.093.115 I ggml_metal_init: found device: Apple M4
0.00.093.117 I ggml_metal_init: picking default device: Apple M4
0.00.093.778 I ggml_metal_init: using embedded metal library
0.00.096.353 I ggml_metal_init: GPU name:   Apple M4
0.00.096.355 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.355 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.356 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.356 I ggml_metal_init: simdgroup reduction   = true
0.00.096.356 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.356 I ggml_metal_init: has bfloat            = true
0.00.096.357 I ggml_metal_init: use bfloat            = true
0.00.096.357 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.359 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.158 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.165 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.190 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.019 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.020 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.020 I llama_new_context_with_model: graph nodes  = 967
0.00.108.020 I llama_new_context_with_model: graph splits = 2
0.00.108.032 I 
0.00.108.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.108.066 I compute_imatrix: tokenizing the input ..
0.00.114.811 I compute_imatrix: tokenization took 6.744 ms
0.00.114.812 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.657.364 I compute_imatrix: 1.54 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.659.971 I llama_perf_context_print:        load time =    1634.91 ms
0.01.659.972 I llama_perf_context_print: prompt eval time =    1542.08 ms /   128 tokens (   12.05 ms per token,    83.00 tokens per second)
0.01.659.973 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.659.974 I llama_perf_context_print:       total time =    1637.51 ms /   129 tokens
0.01.660.621 I ggml_metal_free: deallocating

real	0m1.847s
user	0m0.168s
sys	0m0.225s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4320 (64ae0655)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12360a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12360a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12360af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12360b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12360baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12360c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12360c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12360cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12360d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12360d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12360db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12360e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12360eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12360f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12360fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123610260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123610980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1236110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1236117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123611f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1236126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123612dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1236134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123613d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1236144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123614770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123614d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1236159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123615f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1236161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123616690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123616950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1236171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123617720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1236179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123617e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123618320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1236187c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123618c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123619100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1236195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123619a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123619ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12361a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12361a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12361ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12361b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12361bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12361c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12361c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12361cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12361d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12361d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12361dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12361e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12361ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12361f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12361f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12361f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1236201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123620490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123620930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123620dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123621270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123621710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123621bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123622050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1236224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123622990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123622e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1236232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123623770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123623c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123624160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1236246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123624c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123625150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1236256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123625bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123626140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123626690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123626be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123627130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123627680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123627bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123628120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123628670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123628bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123629110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123629660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123629bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12362a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12362a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12362aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12362b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12362b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12362bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12361b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12362c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12362c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12362cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12362d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12362d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12362dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12362e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12362e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12362ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12362f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12362f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12362fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123630220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123630cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123631160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123631600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123631aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123631f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1236323e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123632880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123632d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1236331c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123633660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123633b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123633fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123634440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1236348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123634d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123635220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1236356c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123635b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123636000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1236364a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123636940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123636de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123637280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123637720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123637bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123638060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123638500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1236389a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123638e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1236392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123639780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123639c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12363a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12363a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12363aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12363aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12363b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12363b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12363bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12363c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12363c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12363ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12363cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12363d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12363d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12363dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12363e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12363e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12363eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12363ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12363f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12363f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12363fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1236401e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123640680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123640b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123640fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123641460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123641900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123641da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123642240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1236426e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123642b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123643020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1236434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123643960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123643e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1236442a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123644740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123644be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123645520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1236459c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123646300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1236467a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123646c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1236470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123647580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123647a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123647ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123648410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123648960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123648eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123649400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1236496c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123649cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12364a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12364a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12364b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12364b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12364b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12364be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12364c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12364cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12364d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12364d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12364da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12364e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12364e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12364ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12364f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12364f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12364fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1236501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123650710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123650c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1236511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123651700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123651c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1236521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1236526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123652c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123653190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1236536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123653c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123654180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1236546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123654c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123655170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1236556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123655c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123656160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1236566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123656c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123657150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1236576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123657bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123658140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123658690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123658be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123659130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123659680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123659bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12365a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12365a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12365abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12365b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12365b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12365bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12365c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12365c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12365cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12365d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12365d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12365db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12365e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12365e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12365eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12365f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12365f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12365fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1236600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123660610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123660b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123661000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1236614a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123661940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123661de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123662280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123662720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123662bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123663060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123663500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1236639a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123663e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1236642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123664780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123664c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1236650c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123665610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123665d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123666450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123666b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123667290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123667550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123667d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123668000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123668610 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.143.120 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1236253d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123625840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123625cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123626120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123626590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123626a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123626e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1236272e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123627750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123627bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123628030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123628610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123628f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123629680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123629e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12362a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12362ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12362b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12362ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12362c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12362ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12362d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12362d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12362df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12362e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12362eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12362ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12362f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12362f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12362fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1236300f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123630560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1236309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123630c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123631100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123631570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1236319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1236322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123632730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123632ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123633010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123633480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1236338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123633d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1236341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123634640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123634ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123634f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123635390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123635800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123635c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1236360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1236369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1236372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123637710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123637b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123638460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1236388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123638d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1236391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123639620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123639a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123639f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12363a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12363a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12363ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12363b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12363b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12363b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12363be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12363c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12363c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12363cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12363cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12363d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12363d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12363dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12363e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12363e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12363ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12363eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12363f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12363f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12363fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123640510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123640980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123640df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123641260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1236416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123641b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123641fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123642420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123642d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123643170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1236435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123643a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123643ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123644330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1236447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123644c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1236454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123645960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123645dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123646240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1236466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123646b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123646f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123647400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123647870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123647ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123648150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1236485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123648a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123648ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123649310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123649780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123649bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12364a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12364a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12364a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12364adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12364b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12364b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12364bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12364bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12364c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12364c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12364ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12364d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12364d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12364da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12364de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12364e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12364e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12364ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12364f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12364f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12364f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12364fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123650200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123650670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123650ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123650f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1236513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123651ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123652110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123652580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1236529f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123652e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1236532d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123653740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123653bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123654020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123654490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123654900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123654d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1236551e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123655ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123655f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1236563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123656810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123656c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1236570f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123657560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1236579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123657e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1236582b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123658720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123658b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123659000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123659470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1236598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123659d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12365a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12365a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12365aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12365af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12365b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12365b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12365bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12365c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12365c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12365c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12365ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12365d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12365d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12365db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12365dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12365e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12365e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12365ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12365f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12365f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12365fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12365fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123660360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1236607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123660c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1236610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123661520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123661990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123662110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123662580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1236629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123662e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1236632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123663740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123663bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123664020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123664490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123664900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123664d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1236651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123665650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123665ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123665f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1236663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123666810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123666c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1236670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123667560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1236679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123667e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1236682b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123668720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12360b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12360af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12360a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123617710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1236179d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123617e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1236182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123618720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123618b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123619000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123619470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1236198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123619d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12361a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12361a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12361aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12361af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12361b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12361b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12361bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12361c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12361c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12361c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12361ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12361d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12361d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12361db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12361dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12361e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12361e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12361ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12361f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12361f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12361fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12361fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123620360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1236207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123620c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1236210b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123621520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123621990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123621e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123622270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1236226e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123622b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123622fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123623430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1236238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123623d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123624400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1236161a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123616890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123616f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12360d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12360da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12360df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12360e370 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123615f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123616390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123616800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123616c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1236170e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12360a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1236178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123617d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1236181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123618610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123618a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123619060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123619950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12361a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12361a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12361afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12361b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12361bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12361c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12361cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12361d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12361dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12361e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12361e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12361f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12361f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12361f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12361fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123620260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1236206d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123620b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123620fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123621420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1236216e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123621b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123621fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123622430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1236228a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123622d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123623180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1236235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123623a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123623ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123624340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12360ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12360b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123624f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123625230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1236256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123625b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123625f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1236263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123626860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123626cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123627140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1236275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123627a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123627e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123628300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123628770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123628be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123629050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1236294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123629930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123629da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12362a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12362a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12362aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12362af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12362b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12362b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12362bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12362c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12362c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12362ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12362ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12362d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12362d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12362dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12362e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12362e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12362e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12362ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12362f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12362f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12362fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12362ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1236303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123630820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123630c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123631100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123631570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1236319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123631e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1236322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123632730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123632ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123633010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123633480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1236338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123633d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1236341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123634640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123634ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123634f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123635390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123635800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123635c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1236360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123636550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1236369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123636e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1236372a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123637710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123637b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123637ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123638460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1236388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123638d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1236391b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123639620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123639a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123639f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12363a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12363a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12363ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12363b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12363b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12363b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12363be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12363c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12363c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12363cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12363cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12363d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12363d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12363dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12363e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12363e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12363ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12363eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12363f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12363f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12363fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1236400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123640510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123640980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123640df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123641260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1236416d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123641b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123641fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123642420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123642d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123643170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1236435e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123643a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123643ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123644330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1236447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123644c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1236454f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123645960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123645dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123646240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1236466b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123646b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123646f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123647400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123647870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123647ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123648150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1236485c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123648a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123648ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123649310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123649780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123649bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12364a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12364a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12364a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12364adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12364b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12364b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12364bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12364bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12364c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12364c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12364ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12364d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12364d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12364da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12364de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12364e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12364e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12364ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12364f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12364f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12364f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12364fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123650200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123650670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123650ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123650f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1236513c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123651830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123651ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123652110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123652890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123652d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123653170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1236535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123653a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123653ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123654330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1236547a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123654c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123655080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1236554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123655960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123655dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123656240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1236566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123656b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123656f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123657400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123657870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123657ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123658150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1236585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123658a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123658ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123659310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123659780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123659bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12365a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12365a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12365a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12365adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12365b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12365b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12365bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12365bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12365c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12365c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12365ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12365d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12365d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12365da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12365de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12365e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12365e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12365ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12365f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12365f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12365f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12365fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123660200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123660670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123660ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123660f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1236613c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123661830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123661ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123662110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123662580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1236629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123662e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1236632d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123663740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123663bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123664020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123664490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123664900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123664d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1236651e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123665650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123665ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123665f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1236663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123666810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123667070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123667760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123667e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123668540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12360d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12360da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12360df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12360e370 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.750s
user	0m0.292s
sys	0m0.291s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4320 (64ae0655)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15260b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15260bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15260c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15260c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15260ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15260d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15260d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15260dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15260e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15260e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15260ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15260f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15260fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x152610560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x152610d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x152611490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x152611bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1526122d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1526129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1526131c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1526138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x152614000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152614720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152614fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1526156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1526159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152615fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152616c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152617160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152617420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1526178c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x152617b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152618410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152618950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152618c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1526190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152619550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1526199f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152619e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15261a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15261a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15261ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15261b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15261b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15261b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15261be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15261c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15261cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15261d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15261d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15261dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15261e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15261ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15261f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15261fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15261fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x152620340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x152620600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x152620c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x152621400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1526216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x152621b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x152622000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1526224a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x152622940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x152622de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x152623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x152623720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x152623bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x152624060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152624500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1526249a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152624e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152625390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1526258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152625e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152626380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1526268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152626e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152627370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1526278c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152627e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152628360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1526288b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152628e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152629350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1526298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152629df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15262a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15262a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15262ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15262b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15262b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15262bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15262c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15262c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15262cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15261caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15262d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15262d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15262df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15262e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15262e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15262ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15262f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15262f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15262ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x152630460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1526309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x152630f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x152631450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1526319a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x152631ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x152632390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x152632830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x152632cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x152633170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x152633610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x152633ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x152633f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1526343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x152634890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152634d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1526351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x152635670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152635b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152635fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152636450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1526368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x152636d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152637230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1526376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x152637b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152638010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1526384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152638950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152638df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152639290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152639730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152639bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15263a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15263a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15263a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15263ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15263b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15263b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15263bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15263c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15263c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15263ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15263ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15263d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15263d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15263dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15263e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15263e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15263ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15263ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15263f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15263f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15263fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x152640190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x152640630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x152640ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x152640f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x152641410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1526418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x152641d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1526421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x152642690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x152642b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x152642fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x152643470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x152643910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x152643db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x152644250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1526446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x152644b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152645030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1526454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x152645970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152645e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1526462b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152646750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152646bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152647090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152647530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1526479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152647e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152648310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1526487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152648c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1526490f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152649640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x152649b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15264a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15264a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15264a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15264af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15264b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15264bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15264c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15264c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15264ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15264d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15264d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15264de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15264e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15264e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15264ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15264f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15264f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15264feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x152650400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x152650950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x152650ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1526513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x152651940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x152651e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1526523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x152652930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x152652e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1526533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x152653920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x152653e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1526543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152654910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152654e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1526553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152655900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152655e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1526563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1526568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152656e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152657390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1526578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152657e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152658380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1526588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152658e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152659370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1526598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152659e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15265a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15265a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15265ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15265b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15265b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15265bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15265c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15265c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15265cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15265d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15265d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15265ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15265e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15265e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15265edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15265f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15265f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15265fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x152660300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x152660850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x152660da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1526612f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x152661840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x152661d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x152662230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1526626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x152662b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x152663010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1526634b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x152663950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x152663df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x152664290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152664730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152664bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152665070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152665510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1526659b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152665e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1526662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152666840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152666f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x152667680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x152667da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1526684c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x152668780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152668f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152669230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152669840 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.497 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x153804ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x153805150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1538055c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x153805a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x153805ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x153806310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x153806780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x153806bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x153807060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1538074d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x153807940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x153808020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x153808b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1538092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x153809b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15380a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15380a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15380b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15380b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15380bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15380c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15380cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15380d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15380dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15380e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15380e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15380e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15380ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15380f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15380f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15380fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15380ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1538103d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x153810690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x153810b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x153810f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1538113e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x153811850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x153811cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x153812130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1538125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x153812a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x153812e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1538132f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x153813760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x153813bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x153814040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1538144b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x153814920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x153814d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x153815200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x153815670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x153815ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x153815f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1538163c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x153816830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x153816da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1538172a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x153817710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x153817b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x153817ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x153818460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1538188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x153818d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1538191b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x153819620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x153819a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x153819f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15381a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15381a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15381ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15381b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15381b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15381b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15381be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15381c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15381c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15381cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15381cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15381d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15381d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15381dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15381e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15381e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15381ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15381eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15381f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15381f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15381fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1538200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x153820510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x153820980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x153820df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x153821260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1538216d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x153821b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x153821fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x153822420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x153822890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x153822d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x153823170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1538235e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x153823a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x153823ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x153824330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1538247a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x153824c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x153825080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1538254f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x153825960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x153825dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x153826240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1538266b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x153826b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x153826f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x153827400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x153827870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x153827ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x153828150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1538285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x153828a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x153828ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x153829310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x153829780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x153829bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15382a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15382a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15382a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15382adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15382b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15382b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15382bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15382bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15382c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15382c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15382ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15382d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15382d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15382da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15382de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15382e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15382e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15382ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15382f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15382f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15382f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15382fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x153830200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x153830670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x153830ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x153830f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1538313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x153831830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x153831ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x153832110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x153832580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1538329f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x153832e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1538332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x153833740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x153833bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x153834020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x153834490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x153834900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x153834d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1538351e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x153835650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x153835ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x153835f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1538363a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x153836810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x153836c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1538370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x153837560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1538379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x153837e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1538382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x153838720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x153838b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x153839000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x153839470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1538398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x153839d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15383a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15383a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15383aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15383af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15383b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15383b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15383bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15383c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15383c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15383c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15383ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15383d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15383d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15383db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15383dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15383e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15383e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15383ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15383f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15383f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15383fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15383fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x153840360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1538407d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x153840d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1538411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x153841640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x153842190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x153842450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x153842710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x153842b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x153842ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x153843460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1538438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x153843d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1538441b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x153844620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x153844a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x153844f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x153845370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1538457e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x153845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1538460c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x153846530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1538469a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x153846e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x153847280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1538476f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x153847b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x153847fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x153848440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1538488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x153848d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x153849190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x153849600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x153849a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x153849ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15384a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15384a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15384ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15384b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15384b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15384b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15384bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15384c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15384c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15384cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15384cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15384d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15384d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15384dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15384e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15384e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15384ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15384eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15384f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15384f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15384fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x153850080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1538504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x153850960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x153850dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x153851240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1538516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x153851b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x153851f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x153852400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x153852870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x153852ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x153853150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1538535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x153853a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x153853ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x153854310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x153854780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x153854bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x153855060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1538554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x153855940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x153855db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x153856820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x153856f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x153857660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x153857d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x153858040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1538584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x153858ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1538590c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x156e044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x156e04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x156e04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x156e05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x156e056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x156e05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x156e05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x156e063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x156e06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x156e06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x156e07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x156e078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x156e083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x156e08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x156e09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x156e09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x156e0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x156e0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x156e0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x156e0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x156e0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x156e0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x156e0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x156e0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x156e0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x156e0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x156e0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x156e0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x156e0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x156e0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x156e0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x156e0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x156e0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x156e0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x156e10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x156e107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x156e10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x156e110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x156e11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x156e119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x156e11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x156e12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x156e12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x156e12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x156e12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x156e13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x156e138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x156e13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x156e141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x156e14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x156e14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x156e14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x156e15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x156e157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x156e15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x156e160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x156e16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x156e16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x156e16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x156e17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x156e17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x156e17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x156e18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x156e185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x156e18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x156e18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x156e19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x156e19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x156e19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x156e1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x156e1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x156e1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x156e1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x156e1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x156e1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x156e1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x156e1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x156e1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x156e1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x156e1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x156e1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x156e1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x156e1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x156e1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x156e1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x156e1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x156e1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x156e1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x156e1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x156e1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x156e1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x156e20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x156e20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x156e20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x156e20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x156e213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x156e21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x156e21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x156e22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x156e22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x156e229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x156e22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x156e232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x156e23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x156e23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x156e24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x156e24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x156e24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x156e24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x156e251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x156e25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x156e25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x156e25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x156e263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x156e26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x156e26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x156e270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x156e27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x156e279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x156e27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x156e282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x156e28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x156e28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x156e29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x156e29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x156e298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x156e29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x156e2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x156e2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x156e2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x156e2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x156e2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x156e2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x156e2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x156e2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x156e2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x156e2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x156e2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x156e2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x156e2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x156e2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x156e2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x156e2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x156e2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x156e2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x156e2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x156e2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x156e2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x156e2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x156e30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x156e307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x156e30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x156e310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x156e31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x156e31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x156e31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x156e32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x156e326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x156e32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x156e32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x156e33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x156e338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x156e33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x156e34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x156e345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x156e34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x156e34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x156e35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x156e357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x156e35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x156e36090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x156e36500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x156e36970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x156e36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x156e37250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x156e376c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x156e37b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x156e37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x156e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x156e38880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x156e38cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x156e39160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x156e395d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x156e39a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x156e39eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x156e3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x156e3a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x156e3ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x156e3b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x156e3b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x156e3b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x156e3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x156e3c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x156e3c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x156e3cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x156e3cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x156e3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x156e3d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x156e3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x156e3e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x156e3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x156e3ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x156e3ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x156e3f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x156e3f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x156e3fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x156e40050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x156e405e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x156e40a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x156e40ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x156e41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x156e41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x156e41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x156e42400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x156e42870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x156e42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x156e43150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x156e435c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x156e43a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x156e43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x156e44310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x156e44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x156e44bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x156e45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x156e454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x156e45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x156e45db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x156e46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x156e46690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x156e46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x156e46f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x156e473e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x156e47850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x156e47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x156e48130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x156e485a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x156e48a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x156e48e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x156e492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x156e49760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x156e49bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x156e4a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x156e4a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x156e4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x156e4b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x156e4b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x156e4b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x156e4be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x156e4c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x156e4c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x156e4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x156e4cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x156e4d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x156e4d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x156e4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x156e4e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x156e4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x156e4ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x156e4ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x156e4f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x156e4f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x156e4fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x156e500c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x156e50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x156e509a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x156e50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x156e51280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x156e516f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x156e51b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x156e51fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x156e52440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x156e528b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x156e52d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x156e53190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x156e53600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x156e53a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x156e53ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x156e54350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x156e547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x156e54c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x156e550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x156e55510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x156e55980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x156e563f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x156e56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x156e57230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x156e57950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x156e57c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x156e58080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x156e58680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x156e58c90 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.926s
user	0m0.244s
sys	0m0.140s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
