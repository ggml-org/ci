### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.78 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.69 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.44 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.99 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.33 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    2.18 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.22 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.05 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.21 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.25 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed  179.89 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.92 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   26.23 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.33 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 222.68 sec*proc (27 tests)

Total Test time (real) = 222.69 sec

real	3m42.791s
user	7m34.132s
sys	0m6.134s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.14 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/27 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/27 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/27 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/27 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/27 Test #18: test-json-schema-to-grammar .......   Passed    2.15 sec
      Start 19: test-tokenizer-1-llama-spm
19/27 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/27 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/27 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/27 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-backend-ops
23/27 Test #23: test-backend-ops ..................   Passed   29.16 sec
      Start 26: test-barrier
24/27 Test #26: test-barrier ......................   Passed    0.40 sec
      Start 27: test-quantize-fns
25/27 Test #27: test-quantize-fns .................   Passed   14.07 sec
      Start 28: test-quantize-perf
26/27 Test #28: test-quantize-perf ................   Passed    0.24 sec
      Start 29: test-rope
27/27 Test #29: test-rope .........................   Passed    0.19 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.86 sec*proc (27 tests)

Total Test time (real) =  50.87 sec

real	0m50.882s
user	1m11.191s
sys	0m5.482s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.103 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.897 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.907 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.025.916 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.920 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.025.921 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.922 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.025.922 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.025.923 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.025.925 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.025.926 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.025.927 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.025.927 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.025.928 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.025.934 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.025.935 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.025.935 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.025.936 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.025.936 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.025.937 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.025.938 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.030.415 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.032.168 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.172 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.032.173 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.032.173 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.032.174 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.032.175 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.032.175 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.032.176 I llama_model_loader: - type  f32:  124 tensors
0.00.032.177 I llama_model_loader: - type  f16:   73 tensors
0.00.037.449 I llm_load_vocab: special tokens cache size = 5
0.00.040.307 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.040.315 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.040.315 I llm_load_print_meta: arch             = bert
0.00.040.316 I llm_load_print_meta: vocab type       = WPM
0.00.040.317 I llm_load_print_meta: n_vocab          = 30522
0.00.040.317 I llm_load_print_meta: n_merges         = 0
0.00.040.317 I llm_load_print_meta: vocab_only       = 0
0.00.040.318 I llm_load_print_meta: n_ctx_train      = 512
0.00.040.318 I llm_load_print_meta: n_embd           = 384
0.00.040.318 I llm_load_print_meta: n_layer          = 12
0.00.040.323 I llm_load_print_meta: n_head           = 12
0.00.040.352 I llm_load_print_meta: n_head_kv        = 12
0.00.040.354 I llm_load_print_meta: n_rot            = 32
0.00.040.354 I llm_load_print_meta: n_swa            = 0
0.00.040.354 I llm_load_print_meta: n_embd_head_k    = 32
0.00.040.355 I llm_load_print_meta: n_embd_head_v    = 32
0.00.040.356 I llm_load_print_meta: n_gqa            = 1
0.00.040.357 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.040.358 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.040.359 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.040.360 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.040.363 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.040.363 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.040.363 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.040.365 I llm_load_print_meta: n_ff             = 1536
0.00.040.365 I llm_load_print_meta: n_expert         = 0
0.00.040.365 I llm_load_print_meta: n_expert_used    = 0
0.00.040.365 I llm_load_print_meta: causal attn      = 0
0.00.040.366 I llm_load_print_meta: pooling type     = 2
0.00.040.366 I llm_load_print_meta: rope type        = 2
0.00.040.369 I llm_load_print_meta: rope scaling     = linear
0.00.040.370 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.040.371 I llm_load_print_meta: freq_scale_train = 1
0.00.040.371 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.040.371 I llm_load_print_meta: rope_finetuned   = unknown
0.00.040.372 I llm_load_print_meta: ssm_d_conv       = 0
0.00.040.372 I llm_load_print_meta: ssm_d_inner      = 0
0.00.040.372 I llm_load_print_meta: ssm_d_state      = 0
0.00.040.372 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.040.372 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.040.385 I llm_load_print_meta: model type       = 33M
0.00.040.386 I llm_load_print_meta: model ftype      = F16
0.00.040.387 I llm_load_print_meta: model params     = 33.21 M
0.00.040.388 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.040.389 I llm_load_print_meta: general.name     = Bge Small
0.00.040.390 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.040.390 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.040.391 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.040.391 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.040.391 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.040.392 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.040.392 I llm_load_print_meta: max token length = 21
0.00.042.910 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.042.911 I llm_load_tensors: offloading output layer to GPU
0.00.042.912 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.042.944 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.042.946 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.043.732 I llama_new_context_with_model: n_seq_max     = 1
0.00.043.734 I llama_new_context_with_model: n_ctx         = 512
0.00.043.734 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.043.735 I llama_new_context_with_model: n_batch       = 2048
0.00.043.735 I llama_new_context_with_model: n_ubatch      = 2048
0.00.043.735 I llama_new_context_with_model: flash_attn    = 0
0.00.043.736 I llama_new_context_with_model: freq_base     = 10000.0
0.00.043.737 I llama_new_context_with_model: freq_scale    = 1
0.00.043.737 I ggml_metal_init: allocating
0.00.043.753 I ggml_metal_init: found device: Apple M4
0.00.043.759 I ggml_metal_init: picking default device: Apple M4
0.00.044.870 I ggml_metal_init: using embedded metal library
0.00.049.727 I ggml_metal_init: GPU name:   Apple M4
0.00.049.730 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.049.730 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.049.731 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.049.731 I ggml_metal_init: simdgroup reduction   = true
0.00.049.731 I ggml_metal_init: simdgroup matrix mul. = true
0.00.049.732 I ggml_metal_init: has bfloat            = true
0.00.049.732 I ggml_metal_init: use bfloat            = true
0.00.049.733 I ggml_metal_init: hasUnifiedMemory      = true
0.00.049.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.063 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.065.066 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.065.067 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.066.192 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.066.194 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.066.194 I llama_new_context_with_model: graph nodes  = 429
0.00.066.195 I llama_new_context_with_model: graph splits = 2
0.00.066.220 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.066.221 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.075.523 I 
0.00.075.561 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.076.436 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.081.676 I llama_perf_context_print:        load time =      53.61 ms
0.00.081.677 I llama_perf_context_print: prompt eval time =       5.08 ms /     9 tokens (    0.56 ms per token,  1771.65 tokens per second)
0.00.081.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.081.679 I llama_perf_context_print:       total time =       6.15 ms /    10 tokens
0.00.081.850 I ggml_metal_free: deallocating

real	0m0.285s
user	0m0.055s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.033 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.108 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.295 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.299 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.300 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.301 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.301 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.301 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.302 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.303 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.303 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.303 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.304 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.304 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.306 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.306 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.307 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.307 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.307 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.308 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.308 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.848 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.556 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.557 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.557 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.557 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.558 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.558 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.558 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.559 I llama_model_loader: - type  f32:  124 tensors
0.00.014.559 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.104 I llm_load_vocab: special tokens cache size = 5
0.00.018.410 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.413 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.413 I llm_load_print_meta: arch             = bert
0.00.018.414 I llm_load_print_meta: vocab type       = WPM
0.00.018.414 I llm_load_print_meta: n_vocab          = 30522
0.00.018.414 I llm_load_print_meta: n_merges         = 0
0.00.018.414 I llm_load_print_meta: vocab_only       = 0
0.00.018.414 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.414 I llm_load_print_meta: n_embd           = 384
0.00.018.415 I llm_load_print_meta: n_layer          = 12
0.00.018.417 I llm_load_print_meta: n_head           = 12
0.00.018.424 I llm_load_print_meta: n_head_kv        = 12
0.00.018.425 I llm_load_print_meta: n_rot            = 32
0.00.018.425 I llm_load_print_meta: n_swa            = 0
0.00.018.425 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.425 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.426 I llm_load_print_meta: n_gqa            = 1
0.00.018.426 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.427 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.427 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.428 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.428 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.428 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.428 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.429 I llm_load_print_meta: n_ff             = 1536
0.00.018.429 I llm_load_print_meta: n_expert         = 0
0.00.018.429 I llm_load_print_meta: n_expert_used    = 0
0.00.018.429 I llm_load_print_meta: causal attn      = 0
0.00.018.430 I llm_load_print_meta: pooling type     = 2
0.00.018.430 I llm_load_print_meta: rope type        = 2
0.00.018.431 I llm_load_print_meta: rope scaling     = linear
0.00.018.431 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.432 I llm_load_print_meta: freq_scale_train = 1
0.00.018.432 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.434 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.439 I llm_load_print_meta: model type       = 33M
0.00.018.439 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.440 I llm_load_print_meta: model params     = 33.21 M
0.00.018.440 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.440 I llm_load_print_meta: general.name     = Bge Small
0.00.018.441 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.441 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.441 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.442 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.443 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.443 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.443 I llm_load_print_meta: max token length = 21
0.00.019.736 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.736 I llm_load_tensors: offloading output layer to GPU
0.00.019.739 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.746 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.747 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.148 I llama_new_context_with_model: n_ctx         = 512
0.00.020.148 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.149 I llama_new_context_with_model: n_batch       = 2048
0.00.020.149 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.149 I llama_new_context_with_model: flash_attn    = 0
0.00.020.150 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.150 I llama_new_context_with_model: freq_scale    = 1
0.00.020.150 I ggml_metal_init: allocating
0.00.020.156 I ggml_metal_init: found device: Apple M4
0.00.020.159 I ggml_metal_init: picking default device: Apple M4
0.00.020.788 I ggml_metal_init: using embedded metal library
0.00.023.310 I ggml_metal_init: GPU name:   Apple M4
0.00.023.312 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.312 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.313 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.313 I ggml_metal_init: simdgroup reduction   = true
0.00.023.313 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.314 I ggml_metal_init: has bfloat            = true
0.00.023.314 I ggml_metal_init: use bfloat            = true
0.00.023.314 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.086 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.088 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.089 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.671 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.672 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.672 I llama_new_context_with_model: graph nodes  = 429
0.00.034.673 I llama_new_context_with_model: graph splits = 2
0.00.034.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.965 I 
0.00.038.987 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.498 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.647 I llama_perf_context_print:        load time =      29.85 ms
0.00.042.648 I llama_perf_context_print: prompt eval time =       3.04 ms /     9 tokens (    0.34 ms per token,  2965.40 tokens per second)
0.00.042.649 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.650 I llama_perf_context_print:       total time =       3.68 ms /    10 tokens
0.00.042.843 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.145 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.373 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.378 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.381 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.382 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.383 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.384 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.385 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.386 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.387 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.388 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.388 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.389 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.392 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.393 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.394 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.394 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.395 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.240 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.501 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.285 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.051.286 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.287 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.051.287 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.051.287 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.051.288 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.051.288 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.051.288 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.051.289 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.051.289 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.051.289 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.051.290 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.051.290 I llama_model_loader: - type  f32:   41 tensors
0.00.051.291 I llama_model_loader: - type  f16:   29 tensors
0.00.069.924 W llm_load_vocab: empty token at index 5
0.00.074.430 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.708 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.734 I llm_load_vocab: special tokens cache size = 5
0.00.335.749 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.335.759 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.335.759 I llm_load_print_meta: arch             = jina-bert-v2
0.00.335.760 I llm_load_print_meta: vocab type       = BPE
0.00.335.762 I llm_load_print_meta: n_vocab          = 61056
0.00.335.762 I llm_load_print_meta: n_merges         = 39382
0.00.335.762 I llm_load_print_meta: vocab_only       = 0
0.00.335.768 I llm_load_print_meta: n_ctx_train      = 8192
0.00.335.768 I llm_load_print_meta: n_embd           = 384
0.00.335.769 I llm_load_print_meta: n_layer          = 4
0.00.335.775 I llm_load_print_meta: n_head           = 12
0.00.335.802 I llm_load_print_meta: n_head_kv        = 12
0.00.335.803 I llm_load_print_meta: n_rot            = 32
0.00.335.803 I llm_load_print_meta: n_swa            = 0
0.00.335.803 I llm_load_print_meta: n_embd_head_k    = 32
0.00.335.804 I llm_load_print_meta: n_embd_head_v    = 32
0.00.335.804 I llm_load_print_meta: n_gqa            = 1
0.00.335.805 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.335.805 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.335.806 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.335.806 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.335.806 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.335.806 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.335.807 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.335.807 I llm_load_print_meta: n_ff             = 1536
0.00.335.807 I llm_load_print_meta: n_expert         = 0
0.00.335.807 I llm_load_print_meta: n_expert_used    = 0
0.00.335.807 I llm_load_print_meta: causal attn      = 0
0.00.335.808 I llm_load_print_meta: pooling type     = -1
0.00.335.808 I llm_load_print_meta: rope type        = -1
0.00.335.808 I llm_load_print_meta: rope scaling     = linear
0.00.335.808 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.335.808 I llm_load_print_meta: freq_scale_train = 1
0.00.335.809 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.335.809 I llm_load_print_meta: rope_finetuned   = unknown
0.00.335.809 I llm_load_print_meta: ssm_d_conv       = 0
0.00.335.809 I llm_load_print_meta: ssm_d_inner      = 0
0.00.335.809 I llm_load_print_meta: ssm_d_state      = 0
0.00.335.811 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.335.811 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.335.831 I llm_load_print_meta: model type       = 33M
0.00.335.832 I llm_load_print_meta: model ftype      = F16
0.00.335.832 I llm_load_print_meta: model params     = 32.90 M
0.00.335.833 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.335.833 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.335.833 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.335.833 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.335.835 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.335.835 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.335.836 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.335.836 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.335.836 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.335.836 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.335.836 I llm_load_print_meta: max token length = 45
0.00.337.145 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.337.145 I llm_load_tensors: offloading output layer to GPU
0.00.337.146 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.337.170 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.337.171 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.338.054 I llama_new_context_with_model: n_seq_max     = 1
0.00.338.055 I llama_new_context_with_model: n_ctx         = 8192
0.00.338.056 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.338.056 I llama_new_context_with_model: n_batch       = 2048
0.00.338.056 I llama_new_context_with_model: n_ubatch      = 2048
0.00.338.057 I llama_new_context_with_model: flash_attn    = 0
0.00.338.057 I llama_new_context_with_model: freq_base     = 10000.0
0.00.338.057 I llama_new_context_with_model: freq_scale    = 1
0.00.338.058 I ggml_metal_init: allocating
0.00.338.068 I ggml_metal_init: found device: Apple M4
0.00.338.071 I ggml_metal_init: picking default device: Apple M4
0.00.339.206 I ggml_metal_init: using embedded metal library
0.00.341.724 I ggml_metal_init: GPU name:   Apple M4
0.00.341.726 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.341.726 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.341.727 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.341.727 I ggml_metal_init: simdgroup reduction   = true
0.00.341.727 I ggml_metal_init: simdgroup matrix mul. = true
0.00.341.727 I ggml_metal_init: has bfloat            = true
0.00.341.727 I ggml_metal_init: use bfloat            = true
0.00.341.728 I ggml_metal_init: hasUnifiedMemory      = true
0.00.341.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.353.531 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.353.533 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.353.534 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.354.095 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.354.096 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.354.096 I llama_new_context_with_model: graph nodes  = 154
0.00.354.096 I llama_new_context_with_model: graph splits = 2
0.00.354.114 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.354.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.365.519 I 
0.00.365.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.365.809 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.365.810 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.365.814 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.365.814 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.365.820 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.365.820 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.366.340 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.370.081 I llama_perf_context_print:        load time =     340.84 ms
0.00.370.082 I llama_perf_context_print: prompt eval time =       3.73 ms /    62 tokens (    0.06 ms per token, 16608.63 tokens per second)
0.00.370.086 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.370.088 I llama_perf_context_print:       total time =       4.56 ms /    63 tokens
0.00.370.306 I ggml_metal_free: deallocating

real	0m1.059s
user	0m0.342s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.105 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.215 I main: llama backend init
0.00.000.221 I main: load the model and apply lora adapter, if any
0.00.031.835 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.043.655 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.043.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.043.674 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.043.685 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.043.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.043.687 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.043.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.043.690 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.043.691 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.043.691 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.043.692 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.043.693 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.043.693 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.043.694 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.043.698 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.043.699 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.043.699 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.052.489 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.054.491 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.061.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.061.819 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.061.820 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.061.820 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.061.821 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.061.822 I llama_model_loader: - type  f32:  194 tensors
0.00.061.822 I llama_model_loader: - type  f16:   98 tensors
0.00.091.155 I llm_load_vocab: special tokens cache size = 25
0.00.097.859 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.097.862 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.097.862 I llm_load_print_meta: arch             = gptneox
0.00.097.862 I llm_load_print_meta: vocab type       = BPE
0.00.097.862 I llm_load_print_meta: n_vocab          = 50304
0.00.097.863 I llm_load_print_meta: n_merges         = 50009
0.00.097.863 I llm_load_print_meta: vocab_only       = 0
0.00.097.863 I llm_load_print_meta: n_ctx_train      = 2048
0.00.097.863 I llm_load_print_meta: n_embd           = 2048
0.00.097.863 I llm_load_print_meta: n_layer          = 24
0.00.097.866 I llm_load_print_meta: n_head           = 16
0.00.097.886 I llm_load_print_meta: n_head_kv        = 16
0.00.097.887 I llm_load_print_meta: n_rot            = 32
0.00.097.887 I llm_load_print_meta: n_swa            = 0
0.00.097.887 I llm_load_print_meta: n_embd_head_k    = 128
0.00.097.887 I llm_load_print_meta: n_embd_head_v    = 128
0.00.097.888 I llm_load_print_meta: n_gqa            = 1
0.00.097.888 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.097.889 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.097.890 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.097.890 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.097.890 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.097.890 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.097.892 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.097.893 I llm_load_print_meta: n_ff             = 8192
0.00.097.893 I llm_load_print_meta: n_expert         = 0
0.00.097.893 I llm_load_print_meta: n_expert_used    = 0
0.00.097.893 I llm_load_print_meta: causal attn      = 1
0.00.097.894 I llm_load_print_meta: pooling type     = 0
0.00.097.894 I llm_load_print_meta: rope type        = 2
0.00.097.894 I llm_load_print_meta: rope scaling     = linear
0.00.097.895 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.097.895 I llm_load_print_meta: freq_scale_train = 1
0.00.097.895 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.097.895 I llm_load_print_meta: rope_finetuned   = unknown
0.00.097.895 I llm_load_print_meta: ssm_d_conv       = 0
0.00.097.895 I llm_load_print_meta: ssm_d_inner      = 0
0.00.097.896 I llm_load_print_meta: ssm_d_state      = 0
0.00.097.896 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.097.896 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.097.906 I llm_load_print_meta: model type       = 1.4B
0.00.097.907 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.097.908 I llm_load_print_meta: model params     = 1.41 B
0.00.097.909 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.097.909 I llm_load_print_meta: general.name     = 1.4B
0.00.097.909 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.097.909 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.097.909 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.097.909 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.097.910 I llm_load_print_meta: LF token         = 128 ''
0.00.097.910 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.097.910 I llm_load_print_meta: max token length = 1024
0.00.100.371 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.100.371 I llm_load_tensors: offloading output layer to GPU
0.00.100.371 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.100.389 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.100.390 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.101.297 I llama_new_context_with_model: n_seq_max     = 1
0.00.101.298 I llama_new_context_with_model: n_ctx         = 2048
0.00.101.298 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.101.299 I llama_new_context_with_model: n_batch       = 2048
0.00.101.299 I llama_new_context_with_model: n_ubatch      = 512
0.00.101.299 I llama_new_context_with_model: flash_attn    = 0
0.00.101.299 I llama_new_context_with_model: freq_base     = 10000.0
0.00.101.300 I llama_new_context_with_model: freq_scale    = 1
0.00.101.300 I ggml_metal_init: allocating
0.00.101.303 I ggml_metal_init: found device: Apple M4
0.00.101.305 I ggml_metal_init: picking default device: Apple M4
0.00.101.957 I ggml_metal_init: using embedded metal library
0.00.111.803 I ggml_metal_init: GPU name:   Apple M4
0.00.111.805 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.111.805 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.111.806 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.111.806 I ggml_metal_init: simdgroup reduction   = true
0.00.111.806 I ggml_metal_init: simdgroup matrix mul. = true
0.00.111.806 I ggml_metal_init: has bfloat            = true
0.00.111.807 I ggml_metal_init: use bfloat            = true
0.00.111.807 I ggml_metal_init: hasUnifiedMemory      = true
0.00.111.808 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.154.102 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.154.108 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.154.130 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.155.086 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.155.089 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.155.089 I llama_new_context_with_model: graph nodes  = 967
0.00.155.089 I llama_new_context_with_model: graph splits = 2
0.00.155.114 I common_init_from_params: added EOS logit bias = -inf
0.00.155.115 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.155.115 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.231.985 I main: llama threadpool init, n_threads = 4
0.00.232.018 I 
0.00.232.062 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.232.064 I 
0.00.232.148 I sampler seed: 1234
0.00.232.152 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.232.175 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.232.177 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.232.177 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.084.954 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58053.97 tokens per second)
0.02.084.955 I llama_perf_context_print:        load time =     200.14 ms
0.02.084.956 I llama_perf_context_print: prompt eval time =      43.85 ms /     7 tokens (    6.26 ms per token,   159.64 tokens per second)
0.02.084.956 I llama_perf_context_print:        eval time =    1806.07 ms /    63 runs   (   28.67 ms per token,    34.88 tokens per second)
0.02.084.958 I llama_perf_context_print:       total time =    1852.97 ms /    70 tokens
0.02.085.116 I ggml_metal_free: deallocating

real	0m2.405s
user	0m0.142s
sys	0m0.098s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.697 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.032.509 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.045.241 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.045.250 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.254 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.254 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.255 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.255 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.257 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.257 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.258 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.259 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.259 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.260 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.261 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.263 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.264 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.264 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.064 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.047 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.963 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.062.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.967 I llama_model_loader: - type  f32:  194 tensors
0.00.062.967 I llama_model_loader: - type  f16:   98 tensors
0.00.092.010 I llm_load_vocab: special tokens cache size = 25
0.00.098.739 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.742 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.743 I llm_load_print_meta: arch             = gptneox
0.00.098.743 I llm_load_print_meta: vocab type       = BPE
0.00.098.743 I llm_load_print_meta: n_vocab          = 50304
0.00.098.744 I llm_load_print_meta: n_merges         = 50009
0.00.098.744 I llm_load_print_meta: vocab_only       = 0
0.00.098.744 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.744 I llm_load_print_meta: n_embd           = 2048
0.00.098.744 I llm_load_print_meta: n_layer          = 24
0.00.098.748 I llm_load_print_meta: n_head           = 16
0.00.098.760 I llm_load_print_meta: n_head_kv        = 16
0.00.098.761 I llm_load_print_meta: n_rot            = 32
0.00.098.761 I llm_load_print_meta: n_swa            = 0
0.00.098.761 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.761 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.762 I llm_load_print_meta: n_gqa            = 1
0.00.098.762 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.763 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.763 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.764 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.764 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.764 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.764 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.765 I llm_load_print_meta: n_ff             = 8192
0.00.098.765 I llm_load_print_meta: n_expert         = 0
0.00.098.765 I llm_load_print_meta: n_expert_used    = 0
0.00.098.765 I llm_load_print_meta: causal attn      = 1
0.00.098.765 I llm_load_print_meta: pooling type     = 0
0.00.098.765 I llm_load_print_meta: rope type        = 2
0.00.098.765 I llm_load_print_meta: rope scaling     = linear
0.00.098.766 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.766 I llm_load_print_meta: freq_scale_train = 1
0.00.098.766 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.766 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.767 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.767 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.767 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.767 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.767 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.776 I llm_load_print_meta: model type       = 1.4B
0.00.098.777 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.777 I llm_load_print_meta: model params     = 1.41 B
0.00.098.778 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.778 I llm_load_print_meta: general.name     = 1.4B
0.00.098.778 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.778 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.779 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.779 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.779 I llm_load_print_meta: LF token         = 128 ''
0.00.098.779 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.779 I llm_load_print_meta: max token length = 1024
0.00.101.331 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.331 I llm_load_tensors: offloading output layer to GPU
0.00.101.331 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.342 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.343 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.310 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.311 I llama_new_context_with_model: n_ctx         = 128
0.00.102.311 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.102.311 I llama_new_context_with_model: n_batch       = 128
0.00.102.312 I llama_new_context_with_model: n_ubatch      = 128
0.00.102.312 I llama_new_context_with_model: flash_attn    = 0
0.00.102.312 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.312 I llama_new_context_with_model: freq_scale    = 1
0.00.102.313 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.102.313 I ggml_metal_init: allocating
0.00.102.316 I ggml_metal_init: found device: Apple M4
0.00.102.318 I ggml_metal_init: picking default device: Apple M4
0.00.102.937 I ggml_metal_init: using embedded metal library
0.00.105.474 I ggml_metal_init: GPU name:   Apple M4
0.00.105.476 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.105.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.105.477 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.105.477 I ggml_metal_init: simdgroup reduction   = true
0.00.105.477 I ggml_metal_init: simdgroup matrix mul. = true
0.00.105.477 I ggml_metal_init: has bfloat            = true
0.00.105.477 I ggml_metal_init: use bfloat            = true
0.00.105.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.105.478 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.116.049 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.116.052 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.116.065 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.116.907 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.116.908 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.116.909 I llama_new_context_with_model: graph nodes  = 967
0.00.116.909 I llama_new_context_with_model: graph splits = 2
0.00.116.922 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.116.922 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.969.098 I 
0.00.969.175 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.969.229 I perplexity: tokenizing the input ..
0.00.982.668 I perplexity: tokenization took 13.435 ms
0.00.982.699 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.104.018 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.105.712 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.105.736 I llama_perf_context_print:        load time =     936.58 ms
0.01.105.737 I llama_perf_context_print: prompt eval time =     120.40 ms /   128 tokens (    0.94 ms per token,  1063.12 tokens per second)
0.01.105.739 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.105.739 I llama_perf_context_print:       total time =     136.64 ms /   129 tokens
0.01.106.441 I ggml_metal_free: deallocating

real	0m1.299s
user	0m0.125s
sys	0m0.197s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.009.777 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.890 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.894 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.898 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.898 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.898 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.899 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.899 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.900 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.901 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.901 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.903 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.904 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.906 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.907 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.735 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.818 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.730 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.732 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.732 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.732 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.733 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.734 I llama_model_loader: - type  f32:  194 tensors
0.00.027.734 I llama_model_loader: - type q8_0:   98 tensors
0.00.050.055 I llm_load_vocab: special tokens cache size = 25
0.00.056.051 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.055 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.056 I llm_load_print_meta: arch             = gptneox
0.00.056.056 I llm_load_print_meta: vocab type       = BPE
0.00.056.056 I llm_load_print_meta: n_vocab          = 50304
0.00.056.056 I llm_load_print_meta: n_merges         = 50009
0.00.056.057 I llm_load_print_meta: vocab_only       = 0
0.00.056.058 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.058 I llm_load_print_meta: n_embd           = 2048
0.00.056.059 I llm_load_print_meta: n_layer          = 24
0.00.056.062 I llm_load_print_meta: n_head           = 16
0.00.056.076 I llm_load_print_meta: n_head_kv        = 16
0.00.056.077 I llm_load_print_meta: n_rot            = 32
0.00.056.077 I llm_load_print_meta: n_swa            = 0
0.00.056.077 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.077 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.078 I llm_load_print_meta: n_gqa            = 1
0.00.056.079 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.079 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.080 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.080 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.080 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.080 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.081 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.081 I llm_load_print_meta: n_ff             = 8192
0.00.056.081 I llm_load_print_meta: n_expert         = 0
0.00.056.082 I llm_load_print_meta: n_expert_used    = 0
0.00.056.082 I llm_load_print_meta: causal attn      = 1
0.00.056.085 I llm_load_print_meta: pooling type     = 0
0.00.056.086 I llm_load_print_meta: rope type        = 2
0.00.056.086 I llm_load_print_meta: rope scaling     = linear
0.00.056.086 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.087 I llm_load_print_meta: freq_scale_train = 1
0.00.056.087 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.087 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.087 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.087 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.087 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.089 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.089 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.099 I llm_load_print_meta: model type       = 1.4B
0.00.056.100 I llm_load_print_meta: model ftype      = Q8_0
0.00.056.101 I llm_load_print_meta: model params     = 1.41 B
0.00.056.101 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.056.101 I llm_load_print_meta: general.name     = 1.4B
0.00.056.102 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.102 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.102 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.102 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.102 I llm_load_print_meta: LF token         = 128 ''
0.00.056.103 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.103 I llm_load_print_meta: max token length = 1024
0.00.058.564 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.564 I llm_load_tensors: offloading output layer to GPU
0.00.058.564 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.576 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.058.577 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.059.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.543 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.543 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.544 I llama_new_context_with_model: n_batch       = 2048
0.00.059.544 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.544 I llama_new_context_with_model: flash_attn    = 0
0.00.059.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.545 I llama_new_context_with_model: freq_scale    = 1
0.00.059.545 I ggml_metal_init: allocating
0.00.059.549 I ggml_metal_init: found device: Apple M4
0.00.059.550 I ggml_metal_init: picking default device: Apple M4
0.00.060.283 I ggml_metal_init: using embedded metal library
0.00.062.873 I ggml_metal_init: GPU name:   Apple M4
0.00.062.874 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.875 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.875 I ggml_metal_init: simdgroup reduction   = true
0.00.062.875 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.876 I ggml_metal_init: has bfloat            = true
0.00.062.876 I ggml_metal_init: use bfloat            = true
0.00.062.876 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.877 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.470 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.478 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.515 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.723 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.725 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.725 I llama_new_context_with_model: graph nodes  = 967
0.00.098.725 I llama_new_context_with_model: graph splits = 2
0.00.098.737 I common_init_from_params: added EOS logit bias = -inf
0.00.098.738 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.739 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.164.472 I main: llama threadpool init, n_threads = 4
0.01.164.505 I 
0.01.164.536 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.164.538 I 
0.01.164.800 I sampler seed: 1234
0.01.164.805 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.164.845 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.164.846 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.164.846 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.262.103 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60528.56 tokens per second)
0.02.262.104 I llama_perf_context_print:        load time =    1154.69 ms
0.02.262.105 I llama_perf_context_print: prompt eval time =      43.71 ms /     7 tokens (    6.24 ms per token,   160.15 tokens per second)
0.02.262.105 I llama_perf_context_print:        eval time =    1050.63 ms /    63 runs   (   16.68 ms per token,    59.96 tokens per second)
0.02.262.106 I llama_perf_context_print:       total time =    1097.63 ms /    70 tokens
0.02.262.313 I ggml_metal_free: deallocating

real	0m2.280s
user	0m0.113s
sys	0m0.204s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.131 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.982 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.552 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.557 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.558 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.559 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.559 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.559 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.560 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.561 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.562 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.562 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.563 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.565 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.565 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.565 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.660 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.030.075 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.075 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.076 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.076 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.076 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.030.077 I llama_model_loader: - type  f32:  194 tensors
0.00.030.077 I llama_model_loader: - type q8_0:   98 tensors
0.00.053.257 I llm_load_vocab: special tokens cache size = 25
0.00.059.567 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.570 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.570 I llm_load_print_meta: arch             = gptneox
0.00.059.570 I llm_load_print_meta: vocab type       = BPE
0.00.059.570 I llm_load_print_meta: n_vocab          = 50304
0.00.059.571 I llm_load_print_meta: n_merges         = 50009
0.00.059.571 I llm_load_print_meta: vocab_only       = 0
0.00.059.571 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.571 I llm_load_print_meta: n_embd           = 2048
0.00.059.571 I llm_load_print_meta: n_layer          = 24
0.00.059.575 I llm_load_print_meta: n_head           = 16
0.00.059.583 I llm_load_print_meta: n_head_kv        = 16
0.00.059.584 I llm_load_print_meta: n_rot            = 32
0.00.059.584 I llm_load_print_meta: n_swa            = 0
0.00.059.584 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.585 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.585 I llm_load_print_meta: n_gqa            = 1
0.00.059.586 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.587 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.587 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.588 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.588 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.588 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.588 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.589 I llm_load_print_meta: n_ff             = 8192
0.00.059.589 I llm_load_print_meta: n_expert         = 0
0.00.059.589 I llm_load_print_meta: n_expert_used    = 0
0.00.059.590 I llm_load_print_meta: causal attn      = 1
0.00.059.590 I llm_load_print_meta: pooling type     = 0
0.00.059.590 I llm_load_print_meta: rope type        = 2
0.00.059.590 I llm_load_print_meta: rope scaling     = linear
0.00.059.591 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.591 I llm_load_print_meta: freq_scale_train = 1
0.00.059.591 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.591 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.591 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.591 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.592 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.592 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.592 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.596 I llm_load_print_meta: model type       = 1.4B
0.00.059.597 I llm_load_print_meta: model ftype      = Q8_0
0.00.059.597 I llm_load_print_meta: model params     = 1.41 B
0.00.059.599 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.059.599 I llm_load_print_meta: general.name     = 1.4B
0.00.059.599 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.600 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.600 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.600 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.600 I llm_load_print_meta: LF token         = 128 ''
0.00.059.601 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.601 I llm_load_print_meta: max token length = 1024
0.00.061.420 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.420 I llm_load_tensors: offloading output layer to GPU
0.00.061.420 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.425 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.061.426 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.062.364 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.365 I llama_new_context_with_model: n_ctx         = 128
0.00.062.365 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.062.365 I llama_new_context_with_model: n_batch       = 128
0.00.062.365 I llama_new_context_with_model: n_ubatch      = 128
0.00.062.365 I llama_new_context_with_model: flash_attn    = 0
0.00.062.366 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.366 I llama_new_context_with_model: freq_scale    = 1
0.00.062.366 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.062.367 I ggml_metal_init: allocating
0.00.062.377 I ggml_metal_init: found device: Apple M4
0.00.062.380 I ggml_metal_init: picking default device: Apple M4
0.00.062.990 I ggml_metal_init: using embedded metal library
0.00.065.336 I ggml_metal_init: GPU name:   Apple M4
0.00.065.338 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.338 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.339 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.339 I ggml_metal_init: simdgroup reduction   = true
0.00.065.339 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.339 I ggml_metal_init: has bfloat            = true
0.00.065.339 I ggml_metal_init: use bfloat            = true
0.00.065.340 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.341 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.932 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.076.935 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.076.950 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.077.882 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.077.883 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.077.883 I llama_new_context_with_model: graph nodes  = 967
0.00.077.883 I llama_new_context_with_model: graph splits = 2
0.00.077.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.077.893 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.845.888 I 
0.00.845.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.845.927 I perplexity: tokenizing the input ..
0.00.854.134 I perplexity: tokenization took 8.205 ms
0.00.854.144 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.977.740 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.978.920 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.978.937 I llama_perf_context_print:        load time =     834.90 ms
0.00.978.938 I llama_perf_context_print: prompt eval time =     123.37 ms /   128 tokens (    0.96 ms per token,  1037.52 tokens per second)
0.00.978.939 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.978.940 I llama_perf_context_print:       total time =     133.05 ms /   129 tokens
0.00.979.281 I ggml_metal_free: deallocating

real	0m0.996s
user	0m0.089s
sys	0m0.149s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.014.953 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.796 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.797 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.799 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.799 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.800 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.802 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.802 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.924 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.024 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.040.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.026 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.026 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.027 I llama_model_loader: - type  f32:  194 tensors
0.00.040.027 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.027 I llama_model_loader: - type q6_K:    1 tensors
0.00.065.107 I llm_load_vocab: special tokens cache size = 25
0.00.072.340 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.343 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.343 I llm_load_print_meta: arch             = gptneox
0.00.072.344 I llm_load_print_meta: vocab type       = BPE
0.00.072.344 I llm_load_print_meta: n_vocab          = 50304
0.00.072.344 I llm_load_print_meta: n_merges         = 50009
0.00.072.344 I llm_load_print_meta: vocab_only       = 0
0.00.072.345 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.345 I llm_load_print_meta: n_embd           = 2048
0.00.072.345 I llm_load_print_meta: n_layer          = 24
0.00.072.349 I llm_load_print_meta: n_head           = 16
0.00.072.362 I llm_load_print_meta: n_head_kv        = 16
0.00.072.362 I llm_load_print_meta: n_rot            = 32
0.00.072.362 I llm_load_print_meta: n_swa            = 0
0.00.072.363 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.368 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.369 I llm_load_print_meta: n_gqa            = 1
0.00.072.369 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.370 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.371 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.371 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.371 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.372 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.372 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.373 I llm_load_print_meta: n_ff             = 8192
0.00.072.373 I llm_load_print_meta: n_expert         = 0
0.00.072.373 I llm_load_print_meta: n_expert_used    = 0
0.00.072.373 I llm_load_print_meta: causal attn      = 1
0.00.072.373 I llm_load_print_meta: pooling type     = 0
0.00.072.373 I llm_load_print_meta: rope type        = 2
0.00.072.373 I llm_load_print_meta: rope scaling     = linear
0.00.072.374 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.374 I llm_load_print_meta: freq_scale_train = 1
0.00.072.374 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.375 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.375 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.375 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.375 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.375 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.375 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.386 I llm_load_print_meta: model type       = 1.4B
0.00.072.386 I llm_load_print_meta: model ftype      = Q4_0
0.00.072.386 I llm_load_print_meta: model params     = 1.41 B
0.00.072.387 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.072.387 I llm_load_print_meta: general.name     = 1.4B
0.00.072.387 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.387 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.388 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.388 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.388 I llm_load_print_meta: LF token         = 128 ''
0.00.072.389 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.389 I llm_load_print_meta: max token length = 1024
0.00.074.738 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.738 I llm_load_tensors: offloading output layer to GPU
0.00.074.738 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.750 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.074.751 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.075.898 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.898 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.899 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.899 I llama_new_context_with_model: n_batch       = 2048
0.00.075.899 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.899 I llama_new_context_with_model: flash_attn    = 0
0.00.075.900 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.900 I llama_new_context_with_model: freq_scale    = 1
0.00.075.901 I ggml_metal_init: allocating
0.00.075.907 I ggml_metal_init: found device: Apple M4
0.00.075.909 I ggml_metal_init: picking default device: Apple M4
0.00.076.693 I ggml_metal_init: using embedded metal library
0.00.079.737 I ggml_metal_init: GPU name:   Apple M4
0.00.079.738 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.079.739 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.079.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.079.740 I ggml_metal_init: simdgroup reduction   = true
0.00.079.740 I ggml_metal_init: simdgroup matrix mul. = true
0.00.079.740 I ggml_metal_init: has bfloat            = true
0.00.079.740 I ggml_metal_init: use bfloat            = true
0.00.079.740 I ggml_metal_init: hasUnifiedMemory      = true
0.00.079.741 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.117.775 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.117.784 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.117.812 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.118.936 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.118.939 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.118.939 I llama_new_context_with_model: graph nodes  = 967
0.00.118.939 I llama_new_context_with_model: graph splits = 2
0.00.118.956 I common_init_from_params: added EOS logit bias = -inf
0.00.118.957 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.958 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.748.863 I main: llama threadpool init, n_threads = 4
0.00.748.898 I 
0.00.748.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.748.929 I 
0.00.749.158 I sampler seed: 1234
0.00.749.162 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.173 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.174 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.174 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.430.344 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.430.344 I llama_perf_context_print:        load time =     733.91 ms
0.01.430.345 I llama_perf_context_print: prompt eval time =      43.97 ms /     7 tokens (    6.28 ms per token,   159.21 tokens per second)
0.01.430.346 I llama_perf_context_print:        eval time =     634.05 ms /    63 runs   (   10.06 ms per token,    99.36 tokens per second)
0.01.430.350 I llama_perf_context_print:       total time =     681.48 ms /    70 tokens
0.01.430.560 I ggml_metal_free: deallocating

real	0m1.448s
user	0m0.119s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.028 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.886 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.014.890 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.892 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.893 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.894 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.895 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.897 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.631 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.643 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.372 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.373 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.374 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.375 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.375 I llama_model_loader: - type  f32:  194 tensors
0.00.023.375 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.376 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.707 I llm_load_vocab: special tokens cache size = 25
0.00.049.698 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.701 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.701 I llm_load_print_meta: arch             = gptneox
0.00.049.702 I llm_load_print_meta: vocab type       = BPE
0.00.049.702 I llm_load_print_meta: n_vocab          = 50304
0.00.049.702 I llm_load_print_meta: n_merges         = 50009
0.00.049.702 I llm_load_print_meta: vocab_only       = 0
0.00.049.702 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.703 I llm_load_print_meta: n_embd           = 2048
0.00.049.703 I llm_load_print_meta: n_layer          = 24
0.00.049.705 I llm_load_print_meta: n_head           = 16
0.00.049.717 I llm_load_print_meta: n_head_kv        = 16
0.00.049.717 I llm_load_print_meta: n_rot            = 32
0.00.049.717 I llm_load_print_meta: n_swa            = 0
0.00.049.717 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.718 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.718 I llm_load_print_meta: n_gqa            = 1
0.00.049.719 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.720 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.720 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.721 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.721 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.721 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.722 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.722 I llm_load_print_meta: n_ff             = 8192
0.00.049.722 I llm_load_print_meta: n_expert         = 0
0.00.049.722 I llm_load_print_meta: n_expert_used    = 0
0.00.049.723 I llm_load_print_meta: causal attn      = 1
0.00.049.723 I llm_load_print_meta: pooling type     = 0
0.00.049.723 I llm_load_print_meta: rope type        = 2
0.00.049.723 I llm_load_print_meta: rope scaling     = linear
0.00.049.723 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.724 I llm_load_print_meta: freq_scale_train = 1
0.00.049.724 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.724 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.724 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.724 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.724 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.725 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.725 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.733 I llm_load_print_meta: model type       = 1.4B
0.00.049.734 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.734 I llm_load_print_meta: model params     = 1.41 B
0.00.049.736 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.737 I llm_load_print_meta: general.name     = 1.4B
0.00.049.737 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.737 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.738 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.738 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.738 I llm_load_print_meta: LF token         = 128 ''
0.00.049.739 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: max token length = 1024
0.00.051.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.268 I llm_load_tensors: offloading output layer to GPU
0.00.051.268 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.278 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.279 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.123 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.124 I llama_new_context_with_model: n_ctx         = 128
0.00.052.124 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.124 I llama_new_context_with_model: n_batch       = 128
0.00.052.124 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.124 I llama_new_context_with_model: flash_attn    = 0
0.00.052.125 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.125 I llama_new_context_with_model: freq_scale    = 1
0.00.052.125 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.126 I ggml_metal_init: allocating
0.00.052.130 I ggml_metal_init: found device: Apple M4
0.00.052.133 I ggml_metal_init: picking default device: Apple M4
0.00.052.675 I ggml_metal_init: using embedded metal library
0.00.055.009 I ggml_metal_init: GPU name:   Apple M4
0.00.055.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.010 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.011 I ggml_metal_init: simdgroup reduction   = true
0.00.055.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.011 I ggml_metal_init: has bfloat            = true
0.00.055.011 I ggml_metal_init: use bfloat            = true
0.00.055.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.012 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.724 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.726 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.740 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.586 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.587 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.587 I llama_new_context_with_model: graph nodes  = 967
0.00.066.587 I llama_new_context_with_model: graph splits = 2
0.00.066.599 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.600 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.835 I 
0.00.600.880 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.892 I perplexity: tokenizing the input ..
0.00.608.941 I perplexity: tokenization took 8.047 ms
0.00.608.951 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.695 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.732.846 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.732.860 I llama_perf_context_print:        load time =     591.80 ms
0.00.732.861 I llama_perf_context_print: prompt eval time =     122.52 ms /   128 tokens (    0.96 ms per token,  1044.74 tokens per second)
0.00.732.862 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.732.862 I llama_perf_context_print:       total time =     132.03 ms /   129 tokens
0.00.733.289 I ggml_metal_free: deallocating

real	0m0.749s
user	0m0.078s
sys	0m0.107s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.010.611 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.256 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.260 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.266 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.267 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.267 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.268 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.268 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.269 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.270 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.270 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.271 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.271 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.271 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.272 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.274 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.274 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.276 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.195 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.139 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.036.141 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.141 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.142 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.142 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.143 I llama_model_loader: - type  f32:  194 tensors
0.00.036.143 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.143 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.124 I llm_load_vocab: special tokens cache size = 25
0.00.067.678 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.680 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.681 I llm_load_print_meta: arch             = gptneox
0.00.067.681 I llm_load_print_meta: vocab type       = BPE
0.00.067.681 I llm_load_print_meta: n_vocab          = 50304
0.00.067.681 I llm_load_print_meta: n_merges         = 50009
0.00.067.681 I llm_load_print_meta: vocab_only       = 0
0.00.067.682 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.682 I llm_load_print_meta: n_embd           = 2048
0.00.067.682 I llm_load_print_meta: n_layer          = 24
0.00.067.684 I llm_load_print_meta: n_head           = 16
0.00.067.697 I llm_load_print_meta: n_head_kv        = 16
0.00.067.697 I llm_load_print_meta: n_rot            = 32
0.00.067.697 I llm_load_print_meta: n_swa            = 0
0.00.067.697 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.697 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.698 I llm_load_print_meta: n_gqa            = 1
0.00.067.699 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.699 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.700 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.700 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.700 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.700 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.700 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.701 I llm_load_print_meta: n_ff             = 8192
0.00.067.701 I llm_load_print_meta: n_expert         = 0
0.00.067.701 I llm_load_print_meta: n_expert_used    = 0
0.00.067.702 I llm_load_print_meta: causal attn      = 1
0.00.067.702 I llm_load_print_meta: pooling type     = 0
0.00.067.702 I llm_load_print_meta: rope type        = 2
0.00.067.702 I llm_load_print_meta: rope scaling     = linear
0.00.067.702 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.702 I llm_load_print_meta: freq_scale_train = 1
0.00.067.703 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.703 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.704 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.704 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.704 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.704 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.704 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.713 I llm_load_print_meta: model type       = 1.4B
0.00.067.714 I llm_load_print_meta: model ftype      = Q4_1
0.00.067.714 I llm_load_print_meta: model params     = 1.41 B
0.00.067.715 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.067.715 I llm_load_print_meta: general.name     = 1.4B
0.00.067.715 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.715 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.715 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.716 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.716 I llm_load_print_meta: LF token         = 128 ''
0.00.067.716 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.716 I llm_load_print_meta: max token length = 1024
0.00.069.843 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.843 I llm_load_tensors: offloading output layer to GPU
0.00.069.843 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.853 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.069.854 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.070.798 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.799 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.799 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.800 I llama_new_context_with_model: n_batch       = 2048
0.00.070.800 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.800 I llama_new_context_with_model: flash_attn    = 0
0.00.070.801 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.801 I llama_new_context_with_model: freq_scale    = 1
0.00.070.801 I ggml_metal_init: allocating
0.00.070.807 I ggml_metal_init: found device: Apple M4
0.00.070.809 I ggml_metal_init: picking default device: Apple M4
0.00.071.424 I ggml_metal_init: using embedded metal library
0.00.074.131 I ggml_metal_init: GPU name:   Apple M4
0.00.074.133 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.133 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.134 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.134 I ggml_metal_init: simdgroup reduction   = true
0.00.074.134 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.134 I ggml_metal_init: has bfloat            = true
0.00.074.134 I ggml_metal_init: use bfloat            = true
0.00.074.135 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.136 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.254 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.104.265 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.104.281 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.105.329 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.105.331 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.105.331 I llama_new_context_with_model: graph nodes  = 967
0.00.105.331 I llama_new_context_with_model: graph splits = 2
0.00.105.345 I common_init_from_params: added EOS logit bias = -inf
0.00.105.346 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.105.346 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.756 I main: llama threadpool init, n_threads = 4
0.00.735.801 I 
0.00.735.836 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.836 I 
0.00.736.075 I sampler seed: 1234
0.00.736.079 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.115 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.126 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.129 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.464.787 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65137.61 tokens per second)
0.01.464.787 I llama_perf_context_print:        load time =     725.14 ms
0.01.464.788 I llama_perf_context_print: prompt eval time =      39.50 ms /     7 tokens (    5.64 ms per token,   177.23 tokens per second)
0.01.464.789 I llama_perf_context_print:        eval time =     686.28 ms /    63 runs   (   10.89 ms per token,    91.80 tokens per second)
0.01.464.789 I llama_perf_context_print:       total time =     729.04 ms /    70 tokens
0.01.464.980 I ggml_metal_free: deallocating

real	0m1.480s
user	0m0.114s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.753 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.709 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.713 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.715 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.715 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.716 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.716 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.716 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.717 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.718 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.718 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.718 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.719 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.719 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.719 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.723 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.723 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.724 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.566 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.637 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.585 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.586 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.586 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.587 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.587 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.588 I llama_model_loader: - type  f32:  194 tensors
0.00.023.588 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.588 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.947 I llm_load_vocab: special tokens cache size = 25
0.00.049.914 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.916 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.917 I llm_load_print_meta: arch             = gptneox
0.00.049.917 I llm_load_print_meta: vocab type       = BPE
0.00.049.917 I llm_load_print_meta: n_vocab          = 50304
0.00.049.918 I llm_load_print_meta: n_merges         = 50009
0.00.049.918 I llm_load_print_meta: vocab_only       = 0
0.00.049.918 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.918 I llm_load_print_meta: n_embd           = 2048
0.00.049.918 I llm_load_print_meta: n_layer          = 24
0.00.049.921 I llm_load_print_meta: n_head           = 16
0.00.049.933 I llm_load_print_meta: n_head_kv        = 16
0.00.049.933 I llm_load_print_meta: n_rot            = 32
0.00.049.933 I llm_load_print_meta: n_swa            = 0
0.00.049.934 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.934 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.935 I llm_load_print_meta: n_gqa            = 1
0.00.049.935 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.936 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.936 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.937 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.937 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.937 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.937 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.938 I llm_load_print_meta: n_ff             = 8192
0.00.049.939 I llm_load_print_meta: n_expert         = 0
0.00.049.939 I llm_load_print_meta: n_expert_used    = 0
0.00.049.939 I llm_load_print_meta: causal attn      = 1
0.00.049.940 I llm_load_print_meta: pooling type     = 0
0.00.049.940 I llm_load_print_meta: rope type        = 2
0.00.049.940 I llm_load_print_meta: rope scaling     = linear
0.00.049.942 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.943 I llm_load_print_meta: freq_scale_train = 1
0.00.049.943 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.943 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.944 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.944 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.944 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.944 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.944 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.954 I llm_load_print_meta: model type       = 1.4B
0.00.049.954 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.955 I llm_load_print_meta: model params     = 1.41 B
0.00.049.955 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.955 I llm_load_print_meta: general.name     = 1.4B
0.00.049.956 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.956 I llm_load_print_meta: LF token         = 128 ''
0.00.049.957 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.957 I llm_load_print_meta: max token length = 1024
0.00.051.849 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.849 I llm_load_tensors: offloading output layer to GPU
0.00.051.849 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.860 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.861 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.765 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.766 I llama_new_context_with_model: n_ctx         = 128
0.00.052.767 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.767 I llama_new_context_with_model: n_batch       = 128
0.00.052.767 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.767 I llama_new_context_with_model: flash_attn    = 0
0.00.052.767 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.768 I llama_new_context_with_model: freq_scale    = 1
0.00.052.768 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.768 I ggml_metal_init: allocating
0.00.052.772 I ggml_metal_init: found device: Apple M4
0.00.052.774 I ggml_metal_init: picking default device: Apple M4
0.00.053.348 I ggml_metal_init: using embedded metal library
0.00.055.644 I ggml_metal_init: GPU name:   Apple M4
0.00.055.645 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.646 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.646 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.646 I ggml_metal_init: simdgroup reduction   = true
0.00.055.647 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.647 I ggml_metal_init: has bfloat            = true
0.00.055.647 I ggml_metal_init: use bfloat            = true
0.00.055.647 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.648 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.377 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.379 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.392 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.314 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.316 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.316 I llama_new_context_with_model: graph nodes  = 967
0.00.067.316 I llama_new_context_with_model: graph splits = 2
0.00.067.329 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.329 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.621.466 I 
0.00.621.507 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.621.521 I perplexity: tokenizing the input ..
0.00.629.344 I perplexity: tokenization took 7.822 ms
0.00.629.354 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.752.296 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.753.528 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.753.544 I llama_perf_context_print:        load time =     612.71 ms
0.00.753.545 I llama_perf_context_print: prompt eval time =     122.70 ms /   128 tokens (    0.96 ms per token,  1043.16 tokens per second)
0.00.753.546 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.753.546 I llama_perf_context_print:       total time =     132.08 ms /   129 tokens
0.00.753.971 I ggml_metal_free: deallocating

real	0m0.767s
user	0m0.078s
sys	0m0.103s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.014.193 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.034.179 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.181 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.186 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.186 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.188 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.189 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.190 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.190 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.191 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.192 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.192 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.039.249 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.040.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.046.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.046.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.046.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.046.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.046.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.046.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.046.232 I llama_model_loader: - type  f32:  194 tensors
0.00.046.233 I llama_model_loader: - type q5_0:   97 tensors
0.00.046.233 I llama_model_loader: - type q6_K:    1 tensors
0.00.085.650 I llm_load_vocab: special tokens cache size = 25
0.00.095.670 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.674 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.674 I llm_load_print_meta: arch             = gptneox
0.00.095.675 I llm_load_print_meta: vocab type       = BPE
0.00.095.675 I llm_load_print_meta: n_vocab          = 50304
0.00.095.675 I llm_load_print_meta: n_merges         = 50009
0.00.095.675 I llm_load_print_meta: vocab_only       = 0
0.00.095.675 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.676 I llm_load_print_meta: n_embd           = 2048
0.00.095.676 I llm_load_print_meta: n_layer          = 24
0.00.095.681 I llm_load_print_meta: n_head           = 16
0.00.095.700 I llm_load_print_meta: n_head_kv        = 16
0.00.095.701 I llm_load_print_meta: n_rot            = 32
0.00.095.701 I llm_load_print_meta: n_swa            = 0
0.00.095.702 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.702 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.703 I llm_load_print_meta: n_gqa            = 1
0.00.095.704 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.705 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.705 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.710 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.710 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.710 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.710 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.713 I llm_load_print_meta: n_ff             = 8192
0.00.095.713 I llm_load_print_meta: n_expert         = 0
0.00.095.714 I llm_load_print_meta: n_expert_used    = 0
0.00.095.716 I llm_load_print_meta: causal attn      = 1
0.00.095.717 I llm_load_print_meta: pooling type     = 0
0.00.095.717 I llm_load_print_meta: rope type        = 2
0.00.095.718 I llm_load_print_meta: rope scaling     = linear
0.00.095.718 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.719 I llm_load_print_meta: freq_scale_train = 1
0.00.095.719 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.723 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.723 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.723 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.723 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.723 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.724 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.735 I llm_load_print_meta: model type       = 1.4B
0.00.095.735 I llm_load_print_meta: model ftype      = Q5_0
0.00.095.736 I llm_load_print_meta: model params     = 1.41 B
0.00.095.737 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.095.737 I llm_load_print_meta: general.name     = 1.4B
0.00.095.737 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.737 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.738 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.738 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.740 I llm_load_print_meta: LF token         = 128 ''
0.00.095.740 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.740 I llm_load_print_meta: max token length = 1024
0.00.098.395 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.396 I llm_load_tensors: offloading output layer to GPU
0.00.098.396 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.407 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.098.409 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.099.643 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.645 I llama_new_context_with_model: n_ctx         = 2048
0.00.099.645 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.099.645 I llama_new_context_with_model: n_batch       = 2048
0.00.099.645 I llama_new_context_with_model: n_ubatch      = 512
0.00.099.646 I llama_new_context_with_model: flash_attn    = 0
0.00.099.646 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.646 I llama_new_context_with_model: freq_scale    = 1
0.00.099.647 I ggml_metal_init: allocating
0.00.099.652 I ggml_metal_init: found device: Apple M4
0.00.099.654 I ggml_metal_init: picking default device: Apple M4
0.00.100.507 I ggml_metal_init: using embedded metal library
0.00.103.790 I ggml_metal_init: GPU name:   Apple M4
0.00.103.792 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.103.792 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.103.793 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.103.793 I ggml_metal_init: simdgroup reduction   = true
0.00.103.793 I ggml_metal_init: simdgroup matrix mul. = true
0.00.103.793 I ggml_metal_init: has bfloat            = true
0.00.103.793 I ggml_metal_init: use bfloat            = true
0.00.103.794 I ggml_metal_init: hasUnifiedMemory      = true
0.00.103.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.136.615 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.136.622 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.136.643 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.137.712 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.137.713 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.137.713 I llama_new_context_with_model: graph nodes  = 967
0.00.137.714 I llama_new_context_with_model: graph splits = 2
0.00.137.729 I common_init_from_params: added EOS logit bias = -inf
0.00.137.730 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.137.730 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.720 I main: llama threadpool init, n_threads = 4
0.00.817.812 I 
0.00.817.892 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.894 I 
0.00.818.440 I sampler seed: 1234
0.00.818.449 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.517 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.523 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.523 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.620.580 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.620.581 I llama_perf_context_print:        load time =     803.51 ms
0.01.620.581 I llama_perf_context_print: prompt eval time =      52.06 ms /     7 tokens (    7.44 ms per token,   134.46 tokens per second)
0.01.620.582 I llama_perf_context_print:        eval time =     746.94 ms /    63 runs   (   11.86 ms per token,    84.34 tokens per second)
0.01.620.582 I llama_perf_context_print:       total time =     802.87 ms /    70 tokens
0.01.620.791 I ggml_metal_free: deallocating

real	0m1.658s
user	0m0.151s
sys	0m0.202s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.580 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.502 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.507 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.513 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.513 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.514 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.514 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.515 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.515 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.516 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.520 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.520 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.521 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.363 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.188 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.189 I llama_model_loader: - type  f32:  194 tensors
0.00.024.189 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.189 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.475 I llm_load_vocab: special tokens cache size = 25
0.00.050.420 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.422 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.423 I llm_load_print_meta: arch             = gptneox
0.00.050.423 I llm_load_print_meta: vocab type       = BPE
0.00.050.423 I llm_load_print_meta: n_vocab          = 50304
0.00.050.424 I llm_load_print_meta: n_merges         = 50009
0.00.050.424 I llm_load_print_meta: vocab_only       = 0
0.00.050.424 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.424 I llm_load_print_meta: n_embd           = 2048
0.00.050.424 I llm_load_print_meta: n_layer          = 24
0.00.050.427 I llm_load_print_meta: n_head           = 16
0.00.050.439 I llm_load_print_meta: n_head_kv        = 16
0.00.050.440 I llm_load_print_meta: n_rot            = 32
0.00.050.440 I llm_load_print_meta: n_swa            = 0
0.00.050.440 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.440 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.442 I llm_load_print_meta: n_gqa            = 1
0.00.050.443 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.443 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.444 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.444 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.445 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.445 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.445 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.446 I llm_load_print_meta: n_ff             = 8192
0.00.050.446 I llm_load_print_meta: n_expert         = 0
0.00.050.446 I llm_load_print_meta: n_expert_used    = 0
0.00.050.446 I llm_load_print_meta: causal attn      = 1
0.00.050.446 I llm_load_print_meta: pooling type     = 0
0.00.050.446 I llm_load_print_meta: rope type        = 2
0.00.050.446 I llm_load_print_meta: rope scaling     = linear
0.00.050.447 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.447 I llm_load_print_meta: freq_scale_train = 1
0.00.050.447 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.447 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.447 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.448 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.448 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.448 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.448 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.458 I llm_load_print_meta: model type       = 1.4B
0.00.050.458 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.458 I llm_load_print_meta: model params     = 1.41 B
0.00.050.459 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.460 I llm_load_print_meta: general.name     = 1.4B
0.00.050.461 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.461 I llm_load_print_meta: LF token         = 128 ''
0.00.050.461 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.462 I llm_load_print_meta: max token length = 1024
0.00.052.369 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.369 I llm_load_tensors: offloading output layer to GPU
0.00.052.370 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.380 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.381 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.270 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.270 I llama_new_context_with_model: n_ctx         = 128
0.00.053.271 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.271 I llama_new_context_with_model: n_batch       = 128
0.00.053.271 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.271 I llama_new_context_with_model: flash_attn    = 0
0.00.053.272 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.272 I llama_new_context_with_model: freq_scale    = 1
0.00.053.272 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.273 I ggml_metal_init: allocating
0.00.053.276 I ggml_metal_init: found device: Apple M4
0.00.053.278 I ggml_metal_init: picking default device: Apple M4
0.00.053.828 I ggml_metal_init: using embedded metal library
0.00.056.105 I ggml_metal_init: GPU name:   Apple M4
0.00.056.106 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.107 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.107 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.107 I ggml_metal_init: simdgroup reduction   = true
0.00.056.108 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.108 I ggml_metal_init: has bfloat            = true
0.00.056.108 I ggml_metal_init: use bfloat            = true
0.00.056.108 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.109 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.874 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.876 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.892 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.853 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.854 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.854 I llama_new_context_with_model: graph nodes  = 967
0.00.067.855 I llama_new_context_with_model: graph splits = 2
0.00.067.867 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.868 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.508 I 
0.00.692.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.560 I perplexity: tokenizing the input ..
0.00.700.651 I perplexity: tokenization took 8.09 ms
0.00.700.666 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.591 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.836.839 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.836.856 I llama_perf_context_print:        load time =     682.92 ms
0.00.836.857 I llama_perf_context_print: prompt eval time =     134.70 ms /   128 tokens (    1.05 ms per token,   950.27 tokens per second)
0.00.836.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.858 I llama_perf_context_print:       total time =     144.35 ms /   129 tokens
0.00.837.299 I ggml_metal_free: deallocating

real	0m0.852s
user	0m0.078s
sys	0m0.113s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.698 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.881 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.886 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.888 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.891 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.892 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.892 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.896 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.897 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.897 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.823 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.866 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.773 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.775 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.775 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.775 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.775 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.776 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.776 I llama_model_loader: - type  f32:  194 tensors
0.00.025.777 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.777 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.071 I llm_load_vocab: special tokens cache size = 25
0.00.053.161 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.163 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.164 I llm_load_print_meta: arch             = gptneox
0.00.053.164 I llm_load_print_meta: vocab type       = BPE
0.00.053.164 I llm_load_print_meta: n_vocab          = 50304
0.00.053.165 I llm_load_print_meta: n_merges         = 50009
0.00.053.165 I llm_load_print_meta: vocab_only       = 0
0.00.053.165 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.165 I llm_load_print_meta: n_embd           = 2048
0.00.053.165 I llm_load_print_meta: n_layer          = 24
0.00.053.168 I llm_load_print_meta: n_head           = 16
0.00.053.180 I llm_load_print_meta: n_head_kv        = 16
0.00.053.180 I llm_load_print_meta: n_rot            = 32
0.00.053.180 I llm_load_print_meta: n_swa            = 0
0.00.053.181 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.181 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.182 I llm_load_print_meta: n_gqa            = 1
0.00.053.182 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.183 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.184 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.184 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.184 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.184 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.185 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.185 I llm_load_print_meta: n_ff             = 8192
0.00.053.185 I llm_load_print_meta: n_expert         = 0
0.00.053.186 I llm_load_print_meta: n_expert_used    = 0
0.00.053.189 I llm_load_print_meta: causal attn      = 1
0.00.053.191 I llm_load_print_meta: pooling type     = 0
0.00.053.191 I llm_load_print_meta: rope type        = 2
0.00.053.191 I llm_load_print_meta: rope scaling     = linear
0.00.053.191 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.191 I llm_load_print_meta: freq_scale_train = 1
0.00.053.192 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.192 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.192 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.192 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.193 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.196 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.196 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.206 I llm_load_print_meta: model type       = 1.4B
0.00.053.206 I llm_load_print_meta: model ftype      = Q5_1
0.00.053.206 I llm_load_print_meta: model params     = 1.41 B
0.00.053.207 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.053.207 I llm_load_print_meta: general.name     = 1.4B
0.00.053.207 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.208 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.208 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.208 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.208 I llm_load_print_meta: LF token         = 128 ''
0.00.053.208 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.208 I llm_load_print_meta: max token length = 1024
0.00.055.218 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.218 I llm_load_tensors: offloading output layer to GPU
0.00.055.219 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.229 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.055.230 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.056.169 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.170 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.170 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.170 I llama_new_context_with_model: n_batch       = 2048
0.00.056.170 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.171 I llama_new_context_with_model: flash_attn    = 0
0.00.056.171 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.171 I llama_new_context_with_model: freq_scale    = 1
0.00.056.172 I ggml_metal_init: allocating
0.00.056.175 I ggml_metal_init: found device: Apple M4
0.00.056.177 I ggml_metal_init: picking default device: Apple M4
0.00.056.790 I ggml_metal_init: using embedded metal library
0.00.059.151 I ggml_metal_init: GPU name:   Apple M4
0.00.059.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.152 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.153 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.153 I ggml_metal_init: simdgroup reduction   = true
0.00.059.154 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.155 I ggml_metal_init: has bfloat            = true
0.00.059.155 I ggml_metal_init: use bfloat            = true
0.00.059.155 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.156 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.088.756 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.763 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.782 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.848 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.849 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.849 I llama_new_context_with_model: graph nodes  = 967
0.00.089.850 I llama_new_context_with_model: graph splits = 2
0.00.089.863 I common_init_from_params: added EOS logit bias = -inf
0.00.089.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.864 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.802.233 I main: llama threadpool init, n_threads = 4
0.00.802.272 I 
0.00.802.300 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.802.301 I 
0.00.802.543 I sampler seed: 1234
0.00.802.549 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.802.560 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.802.561 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.802.562 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.645.519 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54699.54 tokens per second)
0.01.645.519 I llama_perf_context_print:        load time =     793.53 ms
0.01.645.520 I llama_perf_context_print: prompt eval time =      42.39 ms /     7 tokens (    6.06 ms per token,   165.13 tokens per second)
0.01.645.521 I llama_perf_context_print:        eval time =     797.45 ms /    63 runs   (   12.66 ms per token,    79.00 tokens per second)
0.01.645.521 I llama_perf_context_print:       total time =     843.29 ms /    70 tokens
0.01.645.719 I ggml_metal_free: deallocating

real	0m1.662s
user	0m0.111s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.752 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.504 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.508 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.510 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.511 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.511 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.511 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.511 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.512 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.513 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.514 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.516 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.517 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.517 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.517 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.522 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.522 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.522 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.382 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.298 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.299 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.299 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.300 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.300 I llama_model_loader: - type  f32:  194 tensors
0.00.023.301 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.301 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.621 I llm_load_vocab: special tokens cache size = 25
0.00.049.480 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.482 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.482 I llm_load_print_meta: arch             = gptneox
0.00.049.483 I llm_load_print_meta: vocab type       = BPE
0.00.049.483 I llm_load_print_meta: n_vocab          = 50304
0.00.049.483 I llm_load_print_meta: n_merges         = 50009
0.00.049.483 I llm_load_print_meta: vocab_only       = 0
0.00.049.484 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.484 I llm_load_print_meta: n_embd           = 2048
0.00.049.484 I llm_load_print_meta: n_layer          = 24
0.00.049.487 I llm_load_print_meta: n_head           = 16
0.00.049.499 I llm_load_print_meta: n_head_kv        = 16
0.00.049.499 I llm_load_print_meta: n_rot            = 32
0.00.049.499 I llm_load_print_meta: n_swa            = 0
0.00.049.500 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.500 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.500 I llm_load_print_meta: n_gqa            = 1
0.00.049.501 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.502 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.503 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.503 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.503 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.504 I llm_load_print_meta: n_ff             = 8192
0.00.049.504 I llm_load_print_meta: n_expert         = 0
0.00.049.504 I llm_load_print_meta: n_expert_used    = 0
0.00.049.504 I llm_load_print_meta: causal attn      = 1
0.00.049.504 I llm_load_print_meta: pooling type     = 0
0.00.049.505 I llm_load_print_meta: rope type        = 2
0.00.049.505 I llm_load_print_meta: rope scaling     = linear
0.00.049.507 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.509 I llm_load_print_meta: freq_scale_train = 1
0.00.049.509 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.509 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.509 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.509 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.509 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.509 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.510 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.519 I llm_load_print_meta: model type       = 1.4B
0.00.049.519 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.520 I llm_load_print_meta: model params     = 1.41 B
0.00.049.521 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.521 I llm_load_print_meta: general.name     = 1.4B
0.00.049.521 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.522 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.522 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.522 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.522 I llm_load_print_meta: LF token         = 128 ''
0.00.049.522 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.523 I llm_load_print_meta: max token length = 1024
0.00.051.533 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.533 I llm_load_tensors: offloading output layer to GPU
0.00.051.533 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.544 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.545 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.507 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.508 I llama_new_context_with_model: n_ctx         = 128
0.00.052.508 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.508 I llama_new_context_with_model: n_batch       = 128
0.00.052.508 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.508 I llama_new_context_with_model: flash_attn    = 0
0.00.052.509 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.509 I llama_new_context_with_model: freq_scale    = 1
0.00.052.510 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.510 I ggml_metal_init: allocating
0.00.052.517 I ggml_metal_init: found device: Apple M4
0.00.052.519 I ggml_metal_init: picking default device: Apple M4
0.00.053.119 I ggml_metal_init: using embedded metal library
0.00.055.464 I ggml_metal_init: GPU name:   Apple M4
0.00.055.466 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.466 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.466 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.467 I ggml_metal_init: simdgroup reduction   = true
0.00.055.467 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.467 I ggml_metal_init: has bfloat            = true
0.00.055.467 I ggml_metal_init: use bfloat            = true
0.00.055.469 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.470 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.230 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.232 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.246 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.156 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.157 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.157 I llama_new_context_with_model: graph nodes  = 967
0.00.067.158 I llama_new_context_with_model: graph splits = 2
0.00.067.170 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.171 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.212 I 
0.00.732.256 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.732.276 I perplexity: tokenizing the input ..
0.00.740.317 I perplexity: tokenization took 8.039 ms
0.00.740.332 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.875.322 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.876.486 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.876.502 I llama_perf_context_print:        load time =     723.46 ms
0.00.876.503 I llama_perf_context_print: prompt eval time =     134.77 ms /   128 tokens (    1.05 ms per token,   949.80 tokens per second)
0.00.876.504 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.876.505 I llama_perf_context_print:       total time =     144.29 ms /   129 tokens
0.00.876.926 I ggml_metal_free: deallocating

real	0m0.890s
user	0m0.078s
sys	0m0.115s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.060 I main: llama backend init
0.00.000.062 I main: load the model and apply lora adapter, if any
0.00.013.743 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.020.681 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.683 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.683 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.684 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.684 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.688 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.689 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.689 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.257 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.451 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.349 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.350 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.351 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.031.351 I llama_model_loader: - type  f32:  194 tensors
0.00.031.352 I llama_model_loader: - type q2_K:   49 tensors
0.00.031.352 I llama_model_loader: - type q3_K:   48 tensors
0.00.031.352 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.048 I llm_load_vocab: special tokens cache size = 25
0.00.074.024 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.027 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.028 I llm_load_print_meta: arch             = gptneox
0.00.074.028 I llm_load_print_meta: vocab type       = BPE
0.00.074.028 I llm_load_print_meta: n_vocab          = 50304
0.00.074.028 I llm_load_print_meta: n_merges         = 50009
0.00.074.029 I llm_load_print_meta: vocab_only       = 0
0.00.074.029 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.029 I llm_load_print_meta: n_embd           = 2048
0.00.074.029 I llm_load_print_meta: n_layer          = 24
0.00.074.033 I llm_load_print_meta: n_head           = 16
0.00.074.041 I llm_load_print_meta: n_head_kv        = 16
0.00.074.041 I llm_load_print_meta: n_rot            = 32
0.00.074.041 I llm_load_print_meta: n_swa            = 0
0.00.074.041 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.044 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.045 I llm_load_print_meta: n_gqa            = 1
0.00.074.046 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.047 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.048 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.048 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.053 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.053 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.054 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.055 I llm_load_print_meta: n_ff             = 8192
0.00.074.055 I llm_load_print_meta: n_expert         = 0
0.00.074.055 I llm_load_print_meta: n_expert_used    = 0
0.00.074.056 I llm_load_print_meta: causal attn      = 1
0.00.074.056 I llm_load_print_meta: pooling type     = 0
0.00.074.056 I llm_load_print_meta: rope type        = 2
0.00.074.058 I llm_load_print_meta: rope scaling     = linear
0.00.074.059 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.059 I llm_load_print_meta: freq_scale_train = 1
0.00.074.059 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.059 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.059 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.060 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.060 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.060 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.060 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.065 I llm_load_print_meta: model type       = 1.4B
0.00.074.066 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.074.067 I llm_load_print_meta: model params     = 1.41 B
0.00.074.067 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.074.067 I llm_load_print_meta: general.name     = 1.4B
0.00.074.068 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.068 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.068 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.068 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.069 I llm_load_print_meta: LF token         = 128 ''
0.00.074.069 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.069 I llm_load_print_meta: max token length = 1024
0.00.076.465 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.466 I llm_load_tensors: offloading output layer to GPU
0.00.076.466 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.472 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.076.475 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.077.890 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.891 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.891 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.892 I llama_new_context_with_model: n_batch       = 2048
0.00.077.892 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.892 I llama_new_context_with_model: flash_attn    = 0
0.00.077.893 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.893 I llama_new_context_with_model: freq_scale    = 1
0.00.077.894 I ggml_metal_init: allocating
0.00.077.902 I ggml_metal_init: found device: Apple M4
0.00.077.905 I ggml_metal_init: picking default device: Apple M4
0.00.078.725 I ggml_metal_init: using embedded metal library
0.00.082.275 I ggml_metal_init: GPU name:   Apple M4
0.00.082.277 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.278 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.278 I ggml_metal_init: simdgroup reduction   = true
0.00.082.278 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.278 I ggml_metal_init: has bfloat            = true
0.00.082.279 I ggml_metal_init: use bfloat            = true
0.00.082.279 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.280 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.250 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.271 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.246 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.248 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.248 I llama_new_context_with_model: graph nodes  = 967
0.00.115.248 I llama_new_context_with_model: graph splits = 2
0.00.115.263 I common_init_from_params: added EOS logit bias = -inf
0.00.115.264 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.115.264 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.568.906 I main: llama threadpool init, n_threads = 4
0.00.568.951 I 
0.00.568.992 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.568.993 I 
0.00.569.229 I sampler seed: 1234
0.00.569.236 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.569.265 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.569.266 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.569.266 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.249.704 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57536.47 tokens per second)
0.01.249.705 I llama_perf_context_print:        load time =     555.16 ms
0.01.249.705 I llama_perf_context_print: prompt eval time =      35.85 ms /     7 tokens (    5.12 ms per token,   195.24 tokens per second)
0.01.249.709 I llama_perf_context_print:        eval time =     641.50 ms /    63 runs   (   10.18 ms per token,    98.21 tokens per second)
0.01.249.710 I llama_perf_context_print:       total time =     680.80 ms /    70 tokens
0.01.249.911 I ggml_metal_free: deallocating

real	0m1.286s
user	0m0.134s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.537 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.118 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.119 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.120 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.120 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.121 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.121 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.122 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.123 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.124 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.124 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.124 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.125 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.125 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.127 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.127 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.128 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.015 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.041 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.913 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.914 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.915 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.915 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.916 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.916 I llama_model_loader: - type  f32:  194 tensors
0.00.023.916 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.917 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.917 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.221 I llm_load_vocab: special tokens cache size = 25
0.00.050.222 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.225 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.225 I llm_load_print_meta: arch             = gptneox
0.00.050.226 I llm_load_print_meta: vocab type       = BPE
0.00.050.226 I llm_load_print_meta: n_vocab          = 50304
0.00.050.226 I llm_load_print_meta: n_merges         = 50009
0.00.050.226 I llm_load_print_meta: vocab_only       = 0
0.00.050.227 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.227 I llm_load_print_meta: n_embd           = 2048
0.00.050.227 I llm_load_print_meta: n_layer          = 24
0.00.050.230 I llm_load_print_meta: n_head           = 16
0.00.050.242 I llm_load_print_meta: n_head_kv        = 16
0.00.050.242 I llm_load_print_meta: n_rot            = 32
0.00.050.242 I llm_load_print_meta: n_swa            = 0
0.00.050.242 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.242 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.243 I llm_load_print_meta: n_gqa            = 1
0.00.050.244 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.245 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.245 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.245 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.247 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.247 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.247 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.248 I llm_load_print_meta: n_ff             = 8192
0.00.050.248 I llm_load_print_meta: n_expert         = 0
0.00.050.248 I llm_load_print_meta: n_expert_used    = 0
0.00.050.248 I llm_load_print_meta: causal attn      = 1
0.00.050.248 I llm_load_print_meta: pooling type     = 0
0.00.050.248 I llm_load_print_meta: rope type        = 2
0.00.050.248 I llm_load_print_meta: rope scaling     = linear
0.00.050.249 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.249 I llm_load_print_meta: freq_scale_train = 1
0.00.050.249 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.249 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.250 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.250 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.250 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.250 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.250 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.259 I llm_load_print_meta: model type       = 1.4B
0.00.050.259 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.260 I llm_load_print_meta: model params     = 1.41 B
0.00.050.260 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.262 I llm_load_print_meta: general.name     = 1.4B
0.00.050.262 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.262 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.262 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.262 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.262 I llm_load_print_meta: LF token         = 128 ''
0.00.050.263 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.263 I llm_load_print_meta: max token length = 1024
0.00.051.809 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.809 I llm_load_tensors: offloading output layer to GPU
0.00.051.809 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.819 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.820 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.693 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.694 I llama_new_context_with_model: n_ctx         = 128
0.00.052.694 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.694 I llama_new_context_with_model: n_batch       = 128
0.00.052.694 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.694 I llama_new_context_with_model: flash_attn    = 0
0.00.052.695 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.695 I llama_new_context_with_model: freq_scale    = 1
0.00.052.695 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.696 I ggml_metal_init: allocating
0.00.052.699 I ggml_metal_init: found device: Apple M4
0.00.052.701 I ggml_metal_init: picking default device: Apple M4
0.00.053.228 I ggml_metal_init: using embedded metal library
0.00.055.518 I ggml_metal_init: GPU name:   Apple M4
0.00.055.519 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.520 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.520 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.520 I ggml_metal_init: simdgroup reduction   = true
0.00.055.520 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.520 I ggml_metal_init: has bfloat            = true
0.00.055.520 I ggml_metal_init: use bfloat            = true
0.00.055.521 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.521 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.366 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.368 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.390 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.311 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.312 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.312 I llama_new_context_with_model: graph nodes  = 967
0.00.067.312 I llama_new_context_with_model: graph splits = 2
0.00.067.325 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.325 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.430.676 I 
0.00.430.737 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.430.759 I perplexity: tokenizing the input ..
0.00.439.087 I perplexity: tokenization took 8.327 ms
0.00.439.097 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.570.690 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.571.856 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.571.873 I llama_perf_context_print:        load time =     421.13 ms
0.00.571.874 I llama_perf_context_print: prompt eval time =     131.37 ms /   128 tokens (    1.03 ms per token,   974.35 tokens per second)
0.00.571.875 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.571.876 I llama_perf_context_print:       total time =     141.20 ms /   129 tokens
0.00.572.391 I ggml_metal_free: deallocating

real	0m0.589s
user	0m0.079s
sys	0m0.078s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.922 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.091 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.096 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.098 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.098 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.098 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.102 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.103 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.104 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.105 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.944 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.981 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.810 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.812 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.812 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.813 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.814 I llama_model_loader: - type  f32:  194 tensors
0.00.023.814 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.814 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.814 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.814 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.285 I llm_load_vocab: special tokens cache size = 25
0.00.050.344 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.346 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.347 I llm_load_print_meta: arch             = gptneox
0.00.050.347 I llm_load_print_meta: vocab type       = BPE
0.00.050.347 I llm_load_print_meta: n_vocab          = 50304
0.00.050.347 I llm_load_print_meta: n_merges         = 50009
0.00.050.348 I llm_load_print_meta: vocab_only       = 0
0.00.050.348 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.348 I llm_load_print_meta: n_embd           = 2048
0.00.050.348 I llm_load_print_meta: n_layer          = 24
0.00.050.351 I llm_load_print_meta: n_head           = 16
0.00.050.363 I llm_load_print_meta: n_head_kv        = 16
0.00.050.363 I llm_load_print_meta: n_rot            = 32
0.00.050.363 I llm_load_print_meta: n_swa            = 0
0.00.050.363 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.364 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.365 I llm_load_print_meta: n_gqa            = 1
0.00.050.365 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.366 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.367 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.367 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.367 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.367 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.367 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.370 I llm_load_print_meta: n_ff             = 8192
0.00.050.370 I llm_load_print_meta: n_expert         = 0
0.00.050.370 I llm_load_print_meta: n_expert_used    = 0
0.00.050.370 I llm_load_print_meta: causal attn      = 1
0.00.050.370 I llm_load_print_meta: pooling type     = 0
0.00.050.371 I llm_load_print_meta: rope type        = 2
0.00.050.371 I llm_load_print_meta: rope scaling     = linear
0.00.050.371 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.371 I llm_load_print_meta: freq_scale_train = 1
0.00.050.372 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.372 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.372 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.372 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.372 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.372 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.372 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.381 I llm_load_print_meta: model type       = 1.4B
0.00.050.381 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.382 I llm_load_print_meta: model params     = 1.41 B
0.00.050.382 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.382 I llm_load_print_meta: general.name     = 1.4B
0.00.050.383 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.383 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.384 I llm_load_print_meta: LF token         = 128 ''
0.00.050.385 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.385 I llm_load_print_meta: max token length = 1024
0.00.051.888 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.888 I llm_load_tensors: offloading output layer to GPU
0.00.051.888 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.899 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.900 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.755 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.756 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.757 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.757 I llama_new_context_with_model: n_batch       = 2048
0.00.052.757 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.757 I llama_new_context_with_model: flash_attn    = 0
0.00.052.758 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.758 I llama_new_context_with_model: freq_scale    = 1
0.00.052.758 I ggml_metal_init: allocating
0.00.052.762 I ggml_metal_init: found device: Apple M4
0.00.052.764 I ggml_metal_init: picking default device: Apple M4
0.00.053.329 I ggml_metal_init: using embedded metal library
0.00.055.619 I ggml_metal_init: GPU name:   Apple M4
0.00.055.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.621 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.621 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.621 I ggml_metal_init: simdgroup reduction   = true
0.00.055.621 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.621 I ggml_metal_init: has bfloat            = true
0.00.055.622 I ggml_metal_init: use bfloat            = true
0.00.055.622 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.623 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.119 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.125 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.142 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.092 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.094 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.094 I llama_new_context_with_model: graph nodes  = 967
0.00.086.094 I llama_new_context_with_model: graph splits = 2
0.00.086.108 I common_init_from_params: added EOS logit bias = -inf
0.00.086.109 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.109 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.566.655 I main: llama threadpool init, n_threads = 4
0.00.566.699 I 
0.00.566.740 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.566.741 I 
0.00.566.985 I sampler seed: 1234
0.00.566.991 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.567.002 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.567.002 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.567.002 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.317.380 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58388.16 tokens per second)
0.01.317.381 I llama_perf_context_print:        load time =     557.73 ms
0.01.317.381 I llama_perf_context_print: prompt eval time =      44.40 ms /     7 tokens (    6.34 ms per token,   157.64 tokens per second)
0.01.317.386 I llama_perf_context_print:        eval time =     702.93 ms /    63 runs   (   11.16 ms per token,    89.62 tokens per second)
0.01.317.386 I llama_perf_context_print:       total time =     750.73 ms /    70 tokens
0.01.317.588 I ggml_metal_free: deallocating

real	0m1.334s
user	0m0.109s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.591 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.593 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.594 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.595 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.596 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.596 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.597 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.600 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.600 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.602 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.389 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.433 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.339 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.340 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.340 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.341 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.341 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.341 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.342 I llama_model_loader: - type  f32:  194 tensors
0.00.023.342 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.342 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.342 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.343 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.347 I llm_load_vocab: special tokens cache size = 25
0.00.050.294 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.297 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.297 I llm_load_print_meta: arch             = gptneox
0.00.050.298 I llm_load_print_meta: vocab type       = BPE
0.00.050.298 I llm_load_print_meta: n_vocab          = 50304
0.00.050.298 I llm_load_print_meta: n_merges         = 50009
0.00.050.298 I llm_load_print_meta: vocab_only       = 0
0.00.050.298 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.298 I llm_load_print_meta: n_embd           = 2048
0.00.050.299 I llm_load_print_meta: n_layer          = 24
0.00.050.301 I llm_load_print_meta: n_head           = 16
0.00.050.313 I llm_load_print_meta: n_head_kv        = 16
0.00.050.314 I llm_load_print_meta: n_rot            = 32
0.00.050.314 I llm_load_print_meta: n_swa            = 0
0.00.050.316 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.316 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.317 I llm_load_print_meta: n_gqa            = 1
0.00.050.318 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.318 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.319 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.320 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.320 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.321 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.321 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.321 I llm_load_print_meta: n_ff             = 8192
0.00.050.323 I llm_load_print_meta: n_expert         = 0
0.00.050.323 I llm_load_print_meta: n_expert_used    = 0
0.00.050.323 I llm_load_print_meta: causal attn      = 1
0.00.050.323 I llm_load_print_meta: pooling type     = 0
0.00.050.323 I llm_load_print_meta: rope type        = 2
0.00.050.323 I llm_load_print_meta: rope scaling     = linear
0.00.050.324 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.324 I llm_load_print_meta: freq_scale_train = 1
0.00.050.324 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.324 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.324 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.324 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.325 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.325 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.325 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.334 I llm_load_print_meta: model type       = 1.4B
0.00.050.335 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.335 I llm_load_print_meta: model params     = 1.41 B
0.00.050.336 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.336 I llm_load_print_meta: general.name     = 1.4B
0.00.050.336 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.337 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: LF token         = 128 ''
0.00.050.338 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.338 I llm_load_print_meta: max token length = 1024
0.00.052.238 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.238 I llm_load_tensors: offloading output layer to GPU
0.00.052.238 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.248 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.249 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.150 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.151 I llama_new_context_with_model: n_ctx         = 128
0.00.053.151 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.151 I llama_new_context_with_model: n_batch       = 128
0.00.053.151 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.152 I llama_new_context_with_model: flash_attn    = 0
0.00.053.152 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.152 I llama_new_context_with_model: freq_scale    = 1
0.00.053.153 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.153 I ggml_metal_init: allocating
0.00.053.156 I ggml_metal_init: found device: Apple M4
0.00.053.158 I ggml_metal_init: picking default device: Apple M4
0.00.053.743 I ggml_metal_init: using embedded metal library
0.00.056.083 I ggml_metal_init: GPU name:   Apple M4
0.00.056.084 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.085 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.085 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.085 I ggml_metal_init: simdgroup reduction   = true
0.00.056.085 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.086 I ggml_metal_init: has bfloat            = true
0.00.056.086 I ggml_metal_init: use bfloat            = true
0.00.056.086 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.087 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.008 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.011 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.025 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.946 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.948 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.948 I llama_new_context_with_model: graph nodes  = 967
0.00.067.948 I llama_new_context_with_model: graph splits = 2
0.00.067.960 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.486.049 I 
0.00.486.085 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.486.098 I perplexity: tokenizing the input ..
0.00.494.116 I perplexity: tokenization took 8.017 ms
0.00.494.127 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.626.396 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.627.646 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.627.669 I llama_perf_context_print:        load time =     477.25 ms
0.00.627.670 I llama_perf_context_print: prompt eval time =     132.03 ms /   128 tokens (    1.03 ms per token,   969.47 tokens per second)
0.00.627.672 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.627.673 I llama_perf_context_print:       total time =     141.62 ms /   129 tokens
0.00.628.244 I ggml_metal_free: deallocating

real	0m0.642s
user	0m0.079s
sys	0m0.086s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.059 I main: llama backend init
0.00.000.061 I main: load the model and apply lora adapter, if any
0.00.010.706 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.570 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.574 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.576 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.576 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.577 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.577 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.578 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.578 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.579 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.580 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.582 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.577 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.785 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.326 I llama_model_loader: - type  f32:  194 tensors
0.00.027.327 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.327 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.327 I llama_model_loader: - type q6_K:   13 tensors
0.00.054.478 I llm_load_vocab: special tokens cache size = 25
0.00.061.529 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.532 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.532 I llm_load_print_meta: arch             = gptneox
0.00.061.533 I llm_load_print_meta: vocab type       = BPE
0.00.061.533 I llm_load_print_meta: n_vocab          = 50304
0.00.061.533 I llm_load_print_meta: n_merges         = 50009
0.00.061.533 I llm_load_print_meta: vocab_only       = 0
0.00.061.533 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.533 I llm_load_print_meta: n_embd           = 2048
0.00.061.534 I llm_load_print_meta: n_layer          = 24
0.00.061.537 I llm_load_print_meta: n_head           = 16
0.00.061.549 I llm_load_print_meta: n_head_kv        = 16
0.00.061.550 I llm_load_print_meta: n_rot            = 32
0.00.061.550 I llm_load_print_meta: n_swa            = 0
0.00.061.550 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.550 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.551 I llm_load_print_meta: n_gqa            = 1
0.00.061.552 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.552 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.553 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.553 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.553 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.554 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.554 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.554 I llm_load_print_meta: n_ff             = 8192
0.00.061.554 I llm_load_print_meta: n_expert         = 0
0.00.061.555 I llm_load_print_meta: n_expert_used    = 0
0.00.061.556 I llm_load_print_meta: causal attn      = 1
0.00.061.556 I llm_load_print_meta: pooling type     = 0
0.00.061.558 I llm_load_print_meta: rope type        = 2
0.00.061.558 I llm_load_print_meta: rope scaling     = linear
0.00.061.559 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.559 I llm_load_print_meta: freq_scale_train = 1
0.00.061.559 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.563 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.563 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.563 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.563 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.564 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.564 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.573 I llm_load_print_meta: model type       = 1.4B
0.00.061.574 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.061.574 I llm_load_print_meta: model params     = 1.41 B
0.00.061.575 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.061.575 I llm_load_print_meta: general.name     = 1.4B
0.00.061.575 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.575 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.575 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.576 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.576 I llm_load_print_meta: LF token         = 128 ''
0.00.061.576 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.576 I llm_load_print_meta: max token length = 1024
0.00.063.785 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.785 I llm_load_tensors: offloading output layer to GPU
0.00.063.785 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.796 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.063.797 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.064.812 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.813 I llama_new_context_with_model: n_ctx         = 2048
0.00.064.814 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.064.814 I llama_new_context_with_model: n_batch       = 2048
0.00.064.814 I llama_new_context_with_model: n_ubatch      = 512
0.00.064.814 I llama_new_context_with_model: flash_attn    = 0
0.00.064.815 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.815 I llama_new_context_with_model: freq_scale    = 1
0.00.064.815 I ggml_metal_init: allocating
0.00.064.818 I ggml_metal_init: found device: Apple M4
0.00.064.821 I ggml_metal_init: picking default device: Apple M4
0.00.065.465 I ggml_metal_init: using embedded metal library
0.00.068.117 I ggml_metal_init: GPU name:   Apple M4
0.00.068.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.120 I ggml_metal_init: simdgroup reduction   = true
0.00.068.120 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.120 I ggml_metal_init: has bfloat            = true
0.00.068.121 I ggml_metal_init: use bfloat            = true
0.00.068.121 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.122 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.102.699 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.704 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.726 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.776 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.778 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.778 I llama_new_context_with_model: graph nodes  = 967
0.00.103.778 I llama_new_context_with_model: graph splits = 2
0.00.103.793 I common_init_from_params: added EOS logit bias = -inf
0.00.103.794 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.671.514 I main: llama threadpool init, n_threads = 4
0.00.671.554 I 
0.00.671.583 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.671.584 I 
0.00.671.819 I sampler seed: 1234
0.00.671.823 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.671.834 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.671.835 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.671.836 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.434.213 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.434.214 I llama_perf_context_print:        load time =     660.80 ms
0.01.434.215 I llama_perf_context_print: prompt eval time =      47.76 ms /     7 tokens (    6.82 ms per token,   146.57 tokens per second)
0.01.434.215 I llama_perf_context_print:        eval time =     711.56 ms /    63 runs   (   11.29 ms per token,    88.54 tokens per second)
0.01.434.216 I llama_perf_context_print:       total time =     762.70 ms /    70 tokens
0.01.434.415 I ggml_metal_free: deallocating

real	0m1.465s
user	0m0.122s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.294 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.868 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.872 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.874 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.875 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.875 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.876 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.877 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.878 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.880 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.880 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.880 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.766 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.798 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.671 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.672 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.673 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.673 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.674 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.674 I llama_model_loader: - type  f32:  194 tensors
0.00.023.674 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.675 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.675 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.126 I llm_load_vocab: special tokens cache size = 25
0.00.050.070 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.073 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.073 I llm_load_print_meta: arch             = gptneox
0.00.050.073 I llm_load_print_meta: vocab type       = BPE
0.00.050.074 I llm_load_print_meta: n_vocab          = 50304
0.00.050.074 I llm_load_print_meta: n_merges         = 50009
0.00.050.074 I llm_load_print_meta: vocab_only       = 0
0.00.050.074 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.074 I llm_load_print_meta: n_embd           = 2048
0.00.050.075 I llm_load_print_meta: n_layer          = 24
0.00.050.078 I llm_load_print_meta: n_head           = 16
0.00.050.090 I llm_load_print_meta: n_head_kv        = 16
0.00.050.090 I llm_load_print_meta: n_rot            = 32
0.00.050.090 I llm_load_print_meta: n_swa            = 0
0.00.050.092 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.093 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.093 I llm_load_print_meta: n_gqa            = 1
0.00.050.094 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.095 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.099 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.100 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.100 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.100 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.100 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.102 I llm_load_print_meta: n_ff             = 8192
0.00.050.102 I llm_load_print_meta: n_expert         = 0
0.00.050.102 I llm_load_print_meta: n_expert_used    = 0
0.00.050.102 I llm_load_print_meta: causal attn      = 1
0.00.050.103 I llm_load_print_meta: pooling type     = 0
0.00.050.103 I llm_load_print_meta: rope type        = 2
0.00.050.103 I llm_load_print_meta: rope scaling     = linear
0.00.050.103 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.103 I llm_load_print_meta: freq_scale_train = 1
0.00.050.104 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.108 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.108 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.108 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.109 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.109 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.109 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.119 I llm_load_print_meta: model type       = 1.4B
0.00.050.120 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.120 I llm_load_print_meta: model params     = 1.41 B
0.00.050.121 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.121 I llm_load_print_meta: general.name     = 1.4B
0.00.050.121 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.121 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.121 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.122 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.122 I llm_load_print_meta: LF token         = 128 ''
0.00.050.122 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.122 I llm_load_print_meta: max token length = 1024
0.00.052.037 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.037 I llm_load_tensors: offloading output layer to GPU
0.00.052.037 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.047 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.048 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.983 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.984 I llama_new_context_with_model: n_ctx         = 128
0.00.052.984 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.984 I llama_new_context_with_model: n_batch       = 128
0.00.052.985 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.985 I llama_new_context_with_model: flash_attn    = 0
0.00.052.985 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.985 I llama_new_context_with_model: freq_scale    = 1
0.00.052.986 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.986 I ggml_metal_init: allocating
0.00.052.989 I ggml_metal_init: found device: Apple M4
0.00.052.991 I ggml_metal_init: picking default device: Apple M4
0.00.053.571 I ggml_metal_init: using embedded metal library
0.00.055.888 I ggml_metal_init: GPU name:   Apple M4
0.00.055.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.890 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.891 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.891 I ggml_metal_init: simdgroup reduction   = true
0.00.055.891 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.891 I ggml_metal_init: has bfloat            = true
0.00.055.891 I ggml_metal_init: use bfloat            = true
0.00.055.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.893 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.559 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.565 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.579 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.472 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.474 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.474 I llama_new_context_with_model: graph nodes  = 967
0.00.067.474 I llama_new_context_with_model: graph splits = 2
0.00.067.487 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.488 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.564.100 I 
0.00.564.134 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.564.145 I perplexity: tokenizing the input ..
0.00.572.115 I perplexity: tokenization took 7.968 ms
0.00.572.129 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.705.862 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.707.069 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.707.087 I llama_perf_context_print:        load time =     554.80 ms
0.00.707.088 I llama_perf_context_print: prompt eval time =     133.51 ms /   128 tokens (    1.04 ms per token,   958.74 tokens per second)
0.00.707.089 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.707.090 I llama_perf_context_print:       total time =     142.99 ms /   129 tokens
0.00.707.564 I ggml_metal_free: deallocating

real	0m0.721s
user	0m0.078s
sys	0m0.099s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.008.687 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.560 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.567 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.567 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.568 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.569 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.571 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.571 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.572 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.577 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.439 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.296 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.298 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.299 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.299 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.299 I llama_model_loader: - type  f32:  194 tensors
0.00.025.300 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.300 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.906 I llm_load_vocab: special tokens cache size = 25
0.00.051.914 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.917 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.918 I llm_load_print_meta: arch             = gptneox
0.00.051.918 I llm_load_print_meta: vocab type       = BPE
0.00.051.918 I llm_load_print_meta: n_vocab          = 50304
0.00.051.918 I llm_load_print_meta: n_merges         = 50009
0.00.051.919 I llm_load_print_meta: vocab_only       = 0
0.00.051.919 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.919 I llm_load_print_meta: n_embd           = 2048
0.00.051.919 I llm_load_print_meta: n_layer          = 24
0.00.051.922 I llm_load_print_meta: n_head           = 16
0.00.051.934 I llm_load_print_meta: n_head_kv        = 16
0.00.051.935 I llm_load_print_meta: n_rot            = 32
0.00.051.935 I llm_load_print_meta: n_swa            = 0
0.00.051.935 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.936 I llm_load_print_meta: n_gqa            = 1
0.00.051.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.938 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.939 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.939 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.939 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.939 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.940 I llm_load_print_meta: n_ff             = 8192
0.00.051.940 I llm_load_print_meta: n_expert         = 0
0.00.051.940 I llm_load_print_meta: n_expert_used    = 0
0.00.051.941 I llm_load_print_meta: causal attn      = 1
0.00.051.943 I llm_load_print_meta: pooling type     = 0
0.00.051.943 I llm_load_print_meta: rope type        = 2
0.00.051.943 I llm_load_print_meta: rope scaling     = linear
0.00.051.944 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.944 I llm_load_print_meta: freq_scale_train = 1
0.00.051.944 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.944 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.945 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.946 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.946 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.946 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.946 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.955 I llm_load_print_meta: model type       = 1.4B
0.00.051.956 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.956 I llm_load_print_meta: model params     = 1.41 B
0.00.051.957 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.957 I llm_load_print_meta: general.name     = 1.4B
0.00.051.957 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.957 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.957 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.957 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.958 I llm_load_print_meta: LF token         = 128 ''
0.00.051.958 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.958 I llm_load_print_meta: max token length = 1024
0.00.053.986 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.986 I llm_load_tensors: offloading output layer to GPU
0.00.053.986 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.997 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.998 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.931 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.932 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.932 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.932 I llama_new_context_with_model: n_batch       = 2048
0.00.054.933 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.933 I llama_new_context_with_model: flash_attn    = 0
0.00.054.933 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.933 I llama_new_context_with_model: freq_scale    = 1
0.00.054.934 I ggml_metal_init: allocating
0.00.054.940 I ggml_metal_init: found device: Apple M4
0.00.054.942 I ggml_metal_init: picking default device: Apple M4
0.00.055.557 I ggml_metal_init: using embedded metal library
0.00.057.879 I ggml_metal_init: GPU name:   Apple M4
0.00.057.881 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.883 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.883 I ggml_metal_init: simdgroup reduction   = true
0.00.057.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.883 I ggml_metal_init: has bfloat            = true
0.00.057.884 I ggml_metal_init: use bfloat            = true
0.00.057.884 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.888 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.577 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.584 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.601 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.596 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.597 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.597 I llama_new_context_with_model: graph nodes  = 967
0.00.087.598 I llama_new_context_with_model: graph splits = 2
0.00.087.612 I common_init_from_params: added EOS logit bias = -inf
0.00.087.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.613 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.634 I main: llama threadpool init, n_threads = 4
0.00.749.679 I 
0.00.749.707 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.707 I 
0.00.749.942 I sampler seed: 1234
0.00.749.946 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.965 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.965 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.965 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.606.109 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61631.94 tokens per second)
0.01.606.110 I llama_perf_context_print:        load time =     740.94 ms
0.01.606.111 I llama_perf_context_print: prompt eval time =      55.47 ms /     7 tokens (    7.92 ms per token,   126.19 tokens per second)
0.01.606.111 I llama_perf_context_print:        eval time =     797.76 ms /    63 runs   (   12.66 ms per token,    78.97 tokens per second)
0.01.606.112 I llama_perf_context_print:       total time =     856.48 ms /    70 tokens
0.01.606.299 I ggml_metal_free: deallocating

real	0m1.621s
user	0m0.110s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.496 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.160 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.171 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.082 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.093 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.928 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.022.930 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.930 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.930 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.931 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.931 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.022.932 I llama_model_loader: - type  f32:  194 tensors
0.00.022.932 I llama_model_loader: - type q5_K:   61 tensors
0.00.022.932 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.920 I llm_load_vocab: special tokens cache size = 25
0.00.049.868 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.871 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.872 I llm_load_print_meta: arch             = gptneox
0.00.049.872 I llm_load_print_meta: vocab type       = BPE
0.00.049.872 I llm_load_print_meta: n_vocab          = 50304
0.00.049.872 I llm_load_print_meta: n_merges         = 50009
0.00.049.873 I llm_load_print_meta: vocab_only       = 0
0.00.049.873 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.873 I llm_load_print_meta: n_embd           = 2048
0.00.049.873 I llm_load_print_meta: n_layer          = 24
0.00.049.875 I llm_load_print_meta: n_head           = 16
0.00.049.887 I llm_load_print_meta: n_head_kv        = 16
0.00.049.887 I llm_load_print_meta: n_rot            = 32
0.00.049.887 I llm_load_print_meta: n_swa            = 0
0.00.049.888 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.888 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.888 I llm_load_print_meta: n_gqa            = 1
0.00.049.889 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.890 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.890 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.891 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.891 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.891 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.891 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.892 I llm_load_print_meta: n_ff             = 8192
0.00.049.892 I llm_load_print_meta: n_expert         = 0
0.00.049.892 I llm_load_print_meta: n_expert_used    = 0
0.00.049.892 I llm_load_print_meta: causal attn      = 1
0.00.049.892 I llm_load_print_meta: pooling type     = 0
0.00.049.893 I llm_load_print_meta: rope type        = 2
0.00.049.893 I llm_load_print_meta: rope scaling     = linear
0.00.049.893 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.893 I llm_load_print_meta: freq_scale_train = 1
0.00.049.894 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.894 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.894 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.894 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.894 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.894 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.895 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.903 I llm_load_print_meta: model type       = 1.4B
0.00.049.903 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.904 I llm_load_print_meta: model params     = 1.41 B
0.00.049.904 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.904 I llm_load_print_meta: general.name     = 1.4B
0.00.049.907 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.907 I llm_load_print_meta: LF token         = 128 ''
0.00.049.908 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.908 I llm_load_print_meta: max token length = 1024
0.00.051.460 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.460 I llm_load_tensors: offloading output layer to GPU
0.00.051.461 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.471 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.472 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.328 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.329 I llama_new_context_with_model: n_ctx         = 128
0.00.052.329 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.329 I llama_new_context_with_model: n_batch       = 128
0.00.052.329 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.329 I llama_new_context_with_model: flash_attn    = 0
0.00.052.330 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.330 I llama_new_context_with_model: freq_scale    = 1
0.00.052.330 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.331 I ggml_metal_init: allocating
0.00.052.334 I ggml_metal_init: found device: Apple M4
0.00.052.336 I ggml_metal_init: picking default device: Apple M4
0.00.052.894 I ggml_metal_init: using embedded metal library
0.00.055.233 I ggml_metal_init: GPU name:   Apple M4
0.00.055.235 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.235 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.235 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.236 I ggml_metal_init: simdgroup reduction   = true
0.00.055.236 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.236 I ggml_metal_init: has bfloat            = true
0.00.055.236 I ggml_metal_init: use bfloat            = true
0.00.055.236 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.241 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.244 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.269 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.227 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.228 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.229 I llama_new_context_with_model: graph nodes  = 967
0.00.067.229 I llama_new_context_with_model: graph splits = 2
0.00.067.242 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.242 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.660.567 I 
0.00.660.602 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.660.614 I perplexity: tokenizing the input ..
0.00.668.471 I perplexity: tokenization took 7.856 ms
0.00.668.481 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.971 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.810.190 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.810.213 I llama_perf_context_print:        load time =     652.07 ms
0.00.810.214 I llama_perf_context_print: prompt eval time =     140.26 ms /   128 tokens (    1.10 ms per token,   912.60 tokens per second)
0.00.810.215 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.810.215 I llama_perf_context_print:       total time =     149.65 ms /   129 tokens
0.00.810.725 I ggml_metal_free: deallocating

real	0m0.824s
user	0m0.079s
sys	0m0.131s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.030 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.058 I main: llama backend init
0.00.000.060 I main: load the model and apply lora adapter, if any
0.00.011.073 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.197 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.027.201 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.202 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.203 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.205 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.205 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.206 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.206 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.206 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.207 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.207 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.210 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.563 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.554 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.555 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.556 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.556 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.557 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.037.557 I llama_model_loader: - type  f32:  194 tensors
0.00.037.558 I llama_model_loader: - type q6_K:   98 tensors
0.00.067.515 I llm_load_vocab: special tokens cache size = 25
0.00.077.219 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.223 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.223 I llm_load_print_meta: arch             = gptneox
0.00.077.224 I llm_load_print_meta: vocab type       = BPE
0.00.077.224 I llm_load_print_meta: n_vocab          = 50304
0.00.077.224 I llm_load_print_meta: n_merges         = 50009
0.00.077.225 I llm_load_print_meta: vocab_only       = 0
0.00.077.225 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.225 I llm_load_print_meta: n_embd           = 2048
0.00.077.232 I llm_load_print_meta: n_layer          = 24
0.00.077.236 I llm_load_print_meta: n_head           = 16
0.00.077.244 I llm_load_print_meta: n_head_kv        = 16
0.00.077.245 I llm_load_print_meta: n_rot            = 32
0.00.077.245 I llm_load_print_meta: n_swa            = 0
0.00.077.245 I llm_load_print_meta: n_embd_head_k    = 128
0.00.077.245 I llm_load_print_meta: n_embd_head_v    = 128
0.00.077.249 I llm_load_print_meta: n_gqa            = 1
0.00.077.250 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.077.250 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.077.251 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.077.252 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.077.252 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.077.252 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.077.253 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.077.253 I llm_load_print_meta: n_ff             = 8192
0.00.077.254 I llm_load_print_meta: n_expert         = 0
0.00.077.254 I llm_load_print_meta: n_expert_used    = 0
0.00.077.254 I llm_load_print_meta: causal attn      = 1
0.00.077.256 I llm_load_print_meta: pooling type     = 0
0.00.077.258 I llm_load_print_meta: rope type        = 2
0.00.077.259 I llm_load_print_meta: rope scaling     = linear
0.00.077.259 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.077.259 I llm_load_print_meta: freq_scale_train = 1
0.00.077.260 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.077.260 I llm_load_print_meta: rope_finetuned   = unknown
0.00.077.260 I llm_load_print_meta: ssm_d_conv       = 0
0.00.077.260 I llm_load_print_meta: ssm_d_inner      = 0
0.00.077.261 I llm_load_print_meta: ssm_d_state      = 0
0.00.077.261 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.077.261 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.077.266 I llm_load_print_meta: model type       = 1.4B
0.00.077.267 I llm_load_print_meta: model ftype      = Q6_K
0.00.077.267 I llm_load_print_meta: model params     = 1.41 B
0.00.077.268 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.077.268 I llm_load_print_meta: general.name     = 1.4B
0.00.077.269 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.077.269 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.077.269 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.077.269 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.077.270 I llm_load_print_meta: LF token         = 128 ''
0.00.077.270 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.077.271 I llm_load_print_meta: max token length = 1024
0.00.079.871 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.079.872 I llm_load_tensors: offloading output layer to GPU
0.00.079.872 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.079.878 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.079.879 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.081.228 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.229 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.229 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.229 I llama_new_context_with_model: n_batch       = 2048
0.00.081.230 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.230 I llama_new_context_with_model: flash_attn    = 0
0.00.081.231 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.231 I llama_new_context_with_model: freq_scale    = 1
0.00.081.231 I ggml_metal_init: allocating
0.00.081.236 I ggml_metal_init: found device: Apple M4
0.00.081.238 I ggml_metal_init: picking default device: Apple M4
0.00.082.041 I ggml_metal_init: using embedded metal library
0.00.085.660 I ggml_metal_init: GPU name:   Apple M4
0.00.085.662 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.085.663 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.085.663 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.085.663 I ggml_metal_init: simdgroup reduction   = true
0.00.085.665 I ggml_metal_init: simdgroup matrix mul. = true
0.00.085.665 I ggml_metal_init: has bfloat            = true
0.00.085.666 I ggml_metal_init: use bfloat            = true
0.00.085.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.085.670 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.914 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.926 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.942 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.950 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.951 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.952 I llama_new_context_with_model: graph nodes  = 967
0.00.120.952 I llama_new_context_with_model: graph splits = 2
0.00.120.966 I common_init_from_params: added EOS logit bias = -inf
0.00.120.966 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.120.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.971.101 I main: llama threadpool init, n_threads = 4
0.00.971.145 I 
0.00.971.172 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.971.172 I 
0.00.971.413 I sampler seed: 1234
0.00.971.417 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.971.474 I sampler chain: logits -> logit-bias -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.971.478 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.971.478 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.853.800 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.853.801 I llama_perf_context_print:        load time =     960.02 ms
0.01.853.801 I llama_perf_context_print: prompt eval time =      54.48 ms /     7 tokens (    7.78 ms per token,   128.49 tokens per second)
0.01.853.802 I llama_perf_context_print:        eval time =     824.85 ms /    63 runs   (   13.09 ms per token,    76.38 tokens per second)
0.01.853.802 I llama_perf_context_print:       total time =     882.70 ms /    70 tokens
0.01.853.995 I ggml_metal_free: deallocating

real	0m1.883s
user	0m0.132s
sys	0m0.196s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4319 (3b47c3f2) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.352 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.006 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.010 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.016 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.017 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.017 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.018 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.018 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.019 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.019 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.020 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.020 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.020 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.021 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.021 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.023 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.023 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.023 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.691 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.692 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.692 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.693 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.693 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.693 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.694 I llama_model_loader: - type  f32:  194 tensors
0.00.023.694 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.916 I llm_load_vocab: special tokens cache size = 25
0.00.049.994 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.996 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.997 I llm_load_print_meta: arch             = gptneox
0.00.049.997 I llm_load_print_meta: vocab type       = BPE
0.00.049.997 I llm_load_print_meta: n_vocab          = 50304
0.00.049.997 I llm_load_print_meta: n_merges         = 50009
0.00.049.997 I llm_load_print_meta: vocab_only       = 0
0.00.049.998 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.998 I llm_load_print_meta: n_embd           = 2048
0.00.049.998 I llm_load_print_meta: n_layer          = 24
0.00.050.001 I llm_load_print_meta: n_head           = 16
0.00.050.013 I llm_load_print_meta: n_head_kv        = 16
0.00.050.013 I llm_load_print_meta: n_rot            = 32
0.00.050.014 I llm_load_print_meta: n_swa            = 0
0.00.050.015 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.017 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.017 I llm_load_print_meta: n_gqa            = 1
0.00.050.018 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.019 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.020 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.020 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.020 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.020 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.021 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.021 I llm_load_print_meta: n_ff             = 8192
0.00.050.022 I llm_load_print_meta: n_expert         = 0
0.00.050.022 I llm_load_print_meta: n_expert_used    = 0
0.00.050.022 I llm_load_print_meta: causal attn      = 1
0.00.050.022 I llm_load_print_meta: pooling type     = 0
0.00.050.022 I llm_load_print_meta: rope type        = 2
0.00.050.023 I llm_load_print_meta: rope scaling     = linear
0.00.050.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.025 I llm_load_print_meta: freq_scale_train = 1
0.00.050.025 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.025 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.025 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.025 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.026 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.026 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.037 I llm_load_print_meta: model type       = 1.4B
0.00.050.037 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.038 I llm_load_print_meta: model params     = 1.41 B
0.00.050.040 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.040 I llm_load_print_meta: general.name     = 1.4B
0.00.050.040 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.040 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.040 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.041 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.041 I llm_load_print_meta: LF token         = 128 ''
0.00.050.041 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.041 I llm_load_print_meta: max token length = 1024
0.00.051.983 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.984 I llm_load_tensors: offloading output layer to GPU
0.00.051.984 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.994 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.995 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.275 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.275 I llama_new_context_with_model: n_ctx         = 128
0.00.053.276 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.276 I llama_new_context_with_model: n_batch       = 128
0.00.053.276 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.276 I llama_new_context_with_model: flash_attn    = 0
0.00.053.276 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.277 I llama_new_context_with_model: freq_scale    = 1
0.00.053.277 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.278 I ggml_metal_init: allocating
0.00.053.284 I ggml_metal_init: found device: Apple M4
0.00.053.287 I ggml_metal_init: picking default device: Apple M4
0.00.053.863 I ggml_metal_init: using embedded metal library
0.00.056.188 I ggml_metal_init: GPU name:   Apple M4
0.00.056.189 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.189 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.190 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.190 I ggml_metal_init: simdgroup reduction   = true
0.00.056.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.190 I ggml_metal_init: has bfloat            = true
0.00.056.190 I ggml_metal_init: use bfloat            = true
0.00.056.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.191 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.038 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.041 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.054 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.962 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.963 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.963 I llama_new_context_with_model: graph nodes  = 967
0.00.067.964 I llama_new_context_with_model: graph splits = 2
0.00.067.976 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.977 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.237.632 I 
0.00.237.658 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.237.671 I perplexity: tokenizing the input ..
0.00.246.243 I perplexity: tokenization took 8.571 ms
0.00.246.259 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.387.235 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.388.488 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.388.503 I llama_perf_context_print:        load time =     228.28 ms
0.00.388.504 I llama_perf_context_print: prompt eval time =     140.60 ms /   128 tokens (    1.10 ms per token,   910.40 tokens per second)
0.00.388.505 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.388.506 I llama_perf_context_print:       total time =     150.87 ms /   129 tokens
0.00.388.957 I ggml_metal_free: deallocating

real	0m0.405s
user	0m0.078s
sys	0m0.052s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4319 (3b47c3f2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10850a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10850a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10850aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10850b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10850ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10850bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10850c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10850cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10850d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10850d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10850daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10850dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10850eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10850f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10850fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1085101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x108510910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x108511030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x108511750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108511f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108512640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x108512d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x108513480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108513d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108514440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x108514700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108514d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108515980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108515ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108516180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108516620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1085168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108517170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1085176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108517970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108517e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1085182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108518750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108518bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108519090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108519530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1085199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108519e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10851a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10851a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10851abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10851b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10851bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10851c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10851c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10851cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10851d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10851d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10851df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10851e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10851ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10851f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10851f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10851f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108520160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108520420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1085208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108520d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108521200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1085216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108521b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108521fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108522480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108522920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108522dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x108523260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108523700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108523ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1085240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x108524640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x108524b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1085250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x108525630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x108525b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1085260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x108526620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x108526b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1085270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x108527610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x108527b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1085280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x108528600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108528b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1085290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1085295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108529b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10852a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10852a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10852ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10852b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10852b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10852bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10851b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10852bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10852c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10852cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10852d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10852d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10852dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10852e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10852e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10852ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10852f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10852f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10852fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1085301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108530700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108530c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1085310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108531590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108531a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108531ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108532370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108532810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108532cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108533150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1085335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108533a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108533f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1085343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108534870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108534d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1085351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108535650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108535af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x108535f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108536430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1085368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108536d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108537210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1085376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108537b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108537ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108538490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108538930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108538dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108539270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108539710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108539bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10853a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10853a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10853a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10853ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10853b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10853b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10853bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10853c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10853c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10853c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10853ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10853d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10853d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10853dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10853e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10853e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10853ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10853eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10853f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10853f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10853fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108540170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108540610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108540ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108540f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1085413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108541890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108541d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1085421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108542670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108542b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108542fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108543450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1085438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108543d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108544230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1085446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108544b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108545010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1085454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108545950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108545df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x108546290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108546730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x108546bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108547070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108547510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1085479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108547e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1085483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1085488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108548e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108549390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108549650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108549c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10854a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10854a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10854b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10854b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10854b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10854bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10854c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10854cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10854d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10854d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10854d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10854e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10854e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10854ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10854f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10854f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10854fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108550150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1085506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108550bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108551140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108551690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108551be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108552130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108552680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108552bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108553120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108553670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108553bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108554110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108554660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108554bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108555100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108555650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108555ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1085560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108556640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108556b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1085570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108557630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108557b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1085580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x108558620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108558b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1085590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108559610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108559b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10855a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10855a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10855ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10855b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10855b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10855bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10855c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10855c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10855cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10855d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10855d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10855db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10855e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10855e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10855eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10855f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10855f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10855fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108560050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1085605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108560af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108560f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108561430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1085618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108561d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108562210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1085626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108562b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108562ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108563490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108563930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108563dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108564270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108564710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108564bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108565050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1085655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108565cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1085663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108566b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108567220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1085674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x108567cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108567f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1085685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.649 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108525360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1085257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108525c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1085260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108526520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108526990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108526e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108527270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1085276e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108527b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108527fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1085285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108528e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108529610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108529df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10852a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10852abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10852b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10852b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10852c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10852ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10852d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10852d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10852def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10852e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10852ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10852eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10852f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10852f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10852fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108530080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1085304f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108530960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108530c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108531090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108531500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108531970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108531de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108532250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1085326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108532b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108532fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108533410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108533880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108533cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108534160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1085345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108534a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108534eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108535320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108535790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108535c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108536070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1085364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108536950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108536dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108537230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1085376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108537b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108537f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1085383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108538860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108538cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108539140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1085395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108539a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108539e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10853a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10853a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10853abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10853b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10853b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10853b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10853bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10853c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10853c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10853caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10853cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10853d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10853d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10853dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10853e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10853e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10853ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10853ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10853f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10853f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10853fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x108540030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1085404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108540910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108540d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1085411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108541660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108541ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x108541f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1085423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x108542820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108542c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108543100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x108543570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1085439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108543e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1085442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x108544730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108544ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108545010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x108545480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1085458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x108545d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1085461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108546640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108546ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108546f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108547390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108547800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108547c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1085480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108548550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1085489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108548e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1085492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108549710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108549b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108549ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10854a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10854a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10854ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10854b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10854b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10854ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10854bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10854c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10854c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10854cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10854d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10854d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10854d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10854de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10854e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10854e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10854eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10854efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10854f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10854f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10854fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108550190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108550600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108550a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108550ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108551350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1085517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108551c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1085520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108552510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108552980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108552df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108553260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1085536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108553b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108553fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108554420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108554890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108554d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108555170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1085555e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108555a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108555ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108556330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1085567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108556c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108557080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1085574f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108557960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108557dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108558240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1085586b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108558b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108558f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108559400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108559870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108559ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10855a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10855a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10855aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10855aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10855b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10855b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10855bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10855c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10855c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10855c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10855cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10855d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10855d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10855db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10855df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10855e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10855e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10855ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10855f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10855f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10855fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10855fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1085602f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108560760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108560bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108561040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1085614b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108561920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1085620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108562510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108562980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108562df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108563260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1085636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108563b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108563fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108564420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108564890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108564d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108565170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1085655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108565a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108565ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108566330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1085667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108566c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108567080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1085674f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108567960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108567dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108568240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1085686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10850b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10850aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10850a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1085176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108517960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108517dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108518240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1085186b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108518b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108518f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108519400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108519870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108519ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10851a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10851a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10851aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10851aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10851b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10851b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10851bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10851c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10851c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10851c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10851cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10851d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10851d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10851db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10851df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10851e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10851e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10851ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10851f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10851f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10851fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10851fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1085202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108520760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108520bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108521040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1085214b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108521920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108521d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108522200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108522670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108522ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108522f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1085233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108523830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108523ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108524390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108516130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108516820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108516f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10850d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10850da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10850de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10850e300 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108515eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108516320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108516790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108516c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108517070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10850a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108517850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108517cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108518130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1085185a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108518a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108518ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1085198e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10851a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10851a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10851af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10851b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10851bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10851c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10851cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10851d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10851db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10851e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10851e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10851f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10851f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10851f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10851fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1085201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108520660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108520ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108520f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1085213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108521670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108521ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108521f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1085223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108522830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108522ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108523110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108523580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1085239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108523e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1085242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10850ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10850b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108524f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1085251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108525630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108525aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108525f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108526380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1085267f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108526c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1085270d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108527540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1085279b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108527e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108528290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108528700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108528b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108528fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108529450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1085298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108529d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10852a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10852a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10852aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10852aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10852b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10852b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10852bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10852c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10852c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10852c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10852ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10852d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10852d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10852db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10852dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10852e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10852e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10852ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10852f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10852f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10852fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10852fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108530340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1085307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x108530c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108531090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108531500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108531970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108531de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108532250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1085326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108532b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x108532fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108533410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108533880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x108533cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x108534160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1085345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108534a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x108534eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108535320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108535790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x108535c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x108536070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1085364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x108536950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108536dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108537230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1085376a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108537b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108537f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1085383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108538860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108538cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108539140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1085395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108539a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108539e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10853a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10853a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10853abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10853b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10853b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10853b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10853bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10853c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10853c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10853caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10853cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10853d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10853d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10853dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10853e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10853e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10853ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10853ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10853f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10853f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10853fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108540030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1085404a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108540910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108540d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1085411f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108541660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108541ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108541f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1085423b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108542820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108542c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108543100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108543570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1085439e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108543e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1085442c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108544730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108544ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108545010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108545480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1085458f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108545d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1085461d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108546640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108546ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108546f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108547390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108547800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108547c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1085480e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108548550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1085489c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108548e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1085492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108549710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108549b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108549ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10854a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10854a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10854ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10854b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10854b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10854ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10854bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10854c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10854c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10854cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10854d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10854d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10854d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10854de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10854e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10854e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10854eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10854efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10854f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10854f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10854fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x108550190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108550600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108550a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108550ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108551350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1085517c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108551c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1085520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108552820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108552c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108553100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108553570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1085539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108553e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1085542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108554730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108554ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108555010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108555480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1085558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108555d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1085561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108556640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108556ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108556f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108557390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108557800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108557c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1085580e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108558550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1085589c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108558e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1085592a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108559710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108559b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108559ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10855a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10855a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10855ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10855b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10855b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10855ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10855bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10855c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10855c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10855cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10855d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10855d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10855d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10855de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10855e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10855e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10855eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10855efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10855f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10855f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10855fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108560190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108560600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108560a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108560ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108561350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1085617c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108561c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1085620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108562510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108562980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108562df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108563260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1085636d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108563b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108563fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108564420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108564890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108564d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108565170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1085655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108565a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108565ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108566330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1085667a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108567000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1085676f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108567de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1085684d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10850d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10850da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10850de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10850e300 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.786s
user	0m0.292s
sys	0m0.307s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4319 (3b47c3f2)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12b70ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12b70f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12b70f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12b70ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12b710500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12b710ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12b711060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12b711610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12b711bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12b7120c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12b7125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12b712ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12b7135e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12b713d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12b7145a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12b714cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12b7153e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12b715b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12b716220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12b7169f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12b717110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12b717830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12b717f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12b7187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12b718f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12b7191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12b7197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12b71a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12b71a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12b71ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12b71b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12b71b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12b71bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12b71c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12b71c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12b71c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12b71cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12b71d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12b71d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12b71db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12b71e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12b71e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12b71e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12b71ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12b71f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12b71f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12b71fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12b7205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12b720bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12b721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12b721810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12b721e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12b722430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12b722a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12b723230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12b7236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12b723b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12b723e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12b724440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12b724c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12b724ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12b725390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12b725830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12b725cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12b726170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12b726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12b726ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12b726f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12b7273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12b727890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12b727d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12b7281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12b728670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12b728bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12b729110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12b729660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12b729bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12b72a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12b72a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12b72aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12b72b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12b72b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12b72bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12b72c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12b72c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12b72cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12b72d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12b72d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12b72db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12b72e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12b72e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12b72eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12b72f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12b72f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12b72fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12b7300a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12b7305f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12b7202d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12b730a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12b731210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12b731760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12b731cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12b732200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12b732750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12b732ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12b7331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12b733740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12b733c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12b7341e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12b734730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12b734c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12b7351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12b735720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12b735bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12b736060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12b736500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12b7369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12b736e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12b7372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12b737780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12b737c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12b7380c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12b738560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12b738a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12b738ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12b739340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12b7397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12b739c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12b73a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12b73a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12b73aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12b73af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12b73b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12b73b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12b73bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12b73c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12b73c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12b73cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12b73cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12b73d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12b73d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12b73dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12b73e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12b73e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12b73eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12b73efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12b73f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12b73f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12b73fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12b740240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12b7406e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12b740b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12b741020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12b7414c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12b741960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12b741e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12b7422a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12b742740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12b742be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12b743080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12b743520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12b7439c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12b743e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12b744300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12b7447a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12b744c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12b7450e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12b745580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12b745a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12b745ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12b746360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12b746800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12b746ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12b747140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12b7475e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12b747a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12b747f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12b7483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12b748860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12b748d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12b7491a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12b749640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12b749ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12b749f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12b74a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12b74a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12b74ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12b74b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12b74b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12b74bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12b74bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12b74c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12b74c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12b74ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12b74d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12b74d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12b74de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12b74e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12b74e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12b74ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12b74f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12b74fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12b74ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12b7502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12b7508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12b750ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12b7516b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12b751b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12b751ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12b752490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12b752c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12b753190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12b7536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12b753c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12b754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12b7546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12b754c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12b755170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12b7556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12b755c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12b756160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12b7566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12b756c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12b757150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12b7576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12b757bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12b758140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12b758690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12b758be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12b759130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12b759680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12b759bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12b75a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12b75a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12b75abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12b75b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12b75b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12b75bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12b75c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12b75c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12b75cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12b75d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12b75d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12b75db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12b75e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12b75e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12b75eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12b75f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12b75f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12b75fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12b7600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12b760610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12b760b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12b7610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12b761600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12b761b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12b7620a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12b7625f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12b762b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12b763090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12b7635e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12b763b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12b764080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12b7645d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12b764b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12b765070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12b7655c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12b765a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12b765f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12b7663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12b766840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12b766ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12b767180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12b767620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12b767ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12b767f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12b768400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12b7688a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12b768d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12b7691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12b769680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12b769b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12b76a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12b76a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12b76aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12b76b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12b76bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12b76bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12b76c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12b76ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12b76d070 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.612 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.616 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c005400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c005870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c005ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c006150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c0065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c006a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c006ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c007310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c007780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c007bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c008060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c008750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c009270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c009a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c00a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c00a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c00b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c00b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c00beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c00c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c00cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c00d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c00db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c00e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c00e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c00ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c00ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c00f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c00f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c00fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c0100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c0105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c010a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c010d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c011190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c011600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c011a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c011ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c012350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c0127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c012c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c0130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c013510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c013980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c013df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c014260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c0146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c014b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c014fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c015420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c015890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c015d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c016170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c0165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c016a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c016ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c017430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c017930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c017da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c018210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c018680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c018af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c018f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c0193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c019840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c019cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c01a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c01a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c01aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c01ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c01b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c01b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c01bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c01c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c01c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c01c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c01cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c01d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c01d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c01dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c01df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c01e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c01e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c01ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c01f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c01f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c01f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c01fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c0202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c020730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c020ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c021010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c021480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c0218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c021d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c0221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c022640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c022ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c022f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c023390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c023800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c023c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c0240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c024550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c0249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c024e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c0252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c025710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c025b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c025ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c026460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c0268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c026d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c0271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c027620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c027a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c027f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c028370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c0287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c028c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c0290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c029530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c0299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c029e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c02a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c02a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c02ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c02afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c02b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c02b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c02bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c02c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c02c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c02ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c02cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c02d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c02d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c02dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c02e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c02e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c02e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c02edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c02f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c02f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c02fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c02ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c030420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c030890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c030d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c031170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c0315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c031a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c031ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c032330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c0327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c032c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c033080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c0334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c033960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c033dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c034240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c0346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c034b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c034f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c035400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c035870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c035ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c036150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c0365c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c036a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c036ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c037310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c037780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c037bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c038060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c0384d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c038940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c038db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c039220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c039690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c039b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c039f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c03a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c03a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c03acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c03b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c03b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c1059a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c105e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c106280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c1066f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c106b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c106fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c107440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c1078b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c107d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c108190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c108600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c108a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c108ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c109350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c1097c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c109c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c10a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c10a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c10a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c10adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c10b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c10b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c10bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c10c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c10c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c10cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c10d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c10d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c10d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c10de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c10e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c10e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c10eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c10efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c10f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c10f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c10fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c110180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c1105f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c110a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c110ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c111340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c1117b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c111c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c112090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c112500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c112970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c112de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c113250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c1136c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c113b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c113fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c114410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c114880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c114cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c115160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c1155d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c115a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c115eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c116320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c116790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c116c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c117070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c1174e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c117950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c117dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c118230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c1186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c118b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c118f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c1193f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c119860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c119cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c11a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c11a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c11aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c11ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c11b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c11b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c11bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c11c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c11c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c11c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c11cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c11d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c11d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c11daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c11df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c11e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c11e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c11ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c11f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c11f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c11fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c11fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c1202e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c120d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c121470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c121b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c1222b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c122570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c1229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c122fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c1235f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12c105c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12c1060c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12c106530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12c1069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12c106e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12c107280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12c1076f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12c107b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12c107fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12c108440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12c1088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12c108e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12c109780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12c109f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12c10a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12c10add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12c10b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12c10bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12c10c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12c10cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12c10d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12c10da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12c10e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12c10e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12c10eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12c10f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12c10f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12c10fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12c110090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12c110500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12c110970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12c110de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12c111250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12c111510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12c111980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12c111df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12c112260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12c1126d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12c112b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12c112fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12c113420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12c113890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12c113d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12c114170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12c1145e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12c114a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12c114ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12c115330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12c1157a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12c115c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12c116080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12c1164f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12c116960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12c116dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12c117240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12c1176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12c117b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12c117f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12c118400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12c118870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12c118ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12c119150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12c1195c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12c119a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12c119ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12c11a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12c11a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12c11abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12c11b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12c11b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12c11b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12c11bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12c11c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12c11c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12c11cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12c11cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12c11d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12c11d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12c11dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12c11e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12c11e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12c11ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12c11ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12c11f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12c11f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12c11fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12c120040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12c1204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12c120920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12c120d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12c121200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12c121670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12c121ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12c121f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12c1223c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12c122830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12c122ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12c123110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12c123580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12c1239f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12c123e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12c1242d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12c124740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12c124fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12c125530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12c125a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12c125fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12c126520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12c126a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12c126fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12c127510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12c127a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12c127fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12c128450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12c1288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12c128d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12c129230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12c1296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12c129b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12c12a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12c12a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12c12a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12c12adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12c12b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12c12b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12c12bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12c12c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12c12c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12c12c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12c12ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12c12d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12c12d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12c12dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12c12e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12c12e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12c12ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12c12eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12c12f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12c12f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12c12fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12c130130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12c1305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12c130a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12c130f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12c1313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12c131850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12c131cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12c132190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12c132630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12c132ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12c132f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12c133410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12c1338b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12c133d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12c1341f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12c134690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12c134b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12c134fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12c135470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12c135910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12c135db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12c136250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12c1366f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12c136b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12c137030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12c1374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12c137970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12c137e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12c1382b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12c138750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12c138bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12c139090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12c139530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12c1399d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12c139e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12c13a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12c13a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12c13ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12c13b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12c13b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12c13ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12c13bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12c13c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12c13c810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12c13ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12c13d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12c13d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12c13da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12c13df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12c13e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12c13e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12c13ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12c13f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12c13f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12c13fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12c1401a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12c1406f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12c1409b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12c140fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12c1415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12c141be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12c1423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12c142870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12c142b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12c143140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12c143750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12c143f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12c1443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12c144880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12c144d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12c1454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12c145a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12c145f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12c1464c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12c146a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12c146f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12c1474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12c147a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12c147f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12c1484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12c1489f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12c148f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12c149490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12c1499e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12c149f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12c14a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12c14a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12c14af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12c14b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12c14b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12c14bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12c14c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12c14c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12c14cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12c14d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12c14d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12c14def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12c14e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12c14e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12c14eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12c14f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12c14f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12c14fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12c150420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12c150970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12c150ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12c151410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12c151960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12c151eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12c152400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12c152950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12c152ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12c1533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12c153940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12c153e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12c1543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12c154930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12c154e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12c1553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12c155920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12c155e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12c1563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12c156910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12c156e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12c1573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12c157900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12c157e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12c1582f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12c158790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12c158c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12c1590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12c159570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12c159a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12c159eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12c15a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12c15a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12c15ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12c15b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12c15b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12c15ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12c15bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12c15c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12c15c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12c15d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12c15d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12c15de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12c15e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12c15e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12c15f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12c15f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12c15f900 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.933s
user	0m0.243s
sys	0m0.145s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.53 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.12 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.73 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 24: test-model-load-cancel
1/2 Test #24: test-model-load-cancel ...........   Passed    0.25 sec
    Start 25: test-autorelease
2/2 Test #25: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
