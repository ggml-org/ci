### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.41 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    1.79 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.42 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.34 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.33 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    1.01 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.22 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.25 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    2.12 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.18 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.19 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.26 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.20 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed  177.34 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.91 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   25.58 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.32 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.21 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.21 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    = 219.43 sec*proc (27 tests)

Total Test time (real) = 219.44 sec

real	3m39.466s
user	7m40.312s
sys	0m5.713s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/27 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/27 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/27 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/27 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/27 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/27 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/27 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/27 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/27 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/27 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/27 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/27 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/27 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-tokenizer-1-llama-spm
14/27 Test #14: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 15: test-log
15/27 Test #15: test-log ..........................   Passed    0.18 sec
      Start 16: test-arg-parser
16/27 Test #16: test-arg-parser ...................   Passed    0.20 sec
      Start 17: test-sampling
17/27 Test #17: test-sampling .....................   Passed    0.91 sec
      Start 18: test-chat-template
18/27 Test #18: test-chat-template ................   Passed    0.17 sec
      Start 19: test-grammar-parser
19/27 Test #19: test-grammar-parser ...............   Passed    0.17 sec
      Start 20: test-grammar-integration
20/27 Test #20: test-grammar-integration ..........   Passed    0.17 sec
      Start 21: test-llama-grammar
21/27 Test #21: test-llama-grammar ................   Passed    0.18 sec
      Start 22: test-backend-ops
22/27 Test #22: test-backend-ops ..................   Passed   29.14 sec
      Start 25: test-barrier
23/27 Test #25: test-barrier ......................   Passed    0.27 sec
      Start 26: test-quantize-fns
24/27 Test #26: test-quantize-fns .................   Passed   14.06 sec
      Start 27: test-quantize-perf
25/27 Test #27: test-quantize-perf ................   Passed    0.21 sec
      Start 28: test-rope
26/27 Test #28: test-rope .........................   Passed    0.19 sec
      Start 29: test-json-schema-to-grammar
27/27 Test #29: test-json-schema-to-grammar .......   Passed    2.11 sec

100% tests passed, 0 tests failed out of 27

Label Time Summary:
main    =  50.76 sec*proc (27 tests)

Total Test time (real) =  50.78 sec

real	0m50.784s
user	1m11.675s
sys	0m5.277s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.153 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.095 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.077 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.086 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.089 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.030.090 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.091 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.030.092 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.030.093 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.030.094 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.030.095 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.030.096 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.030.097 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.030.097 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.030.102 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.102 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.103 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.030.104 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.030.104 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.030.105 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.030.106 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.035.924 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.037.391 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.393 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.037.393 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.037.394 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.037.394 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.037.395 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.037.395 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.037.396 I llama_model_loader: - type  f32:  124 tensors
0.00.037.396 I llama_model_loader: - type  f16:   73 tensors
0.00.041.953 I llm_load_vocab: special tokens cache size = 5
0.00.044.393 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.044.397 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.044.397 I llm_load_print_meta: arch             = bert
0.00.044.398 I llm_load_print_meta: vocab type       = WPM
0.00.044.398 I llm_load_print_meta: n_vocab          = 30522
0.00.044.399 I llm_load_print_meta: n_merges         = 0
0.00.044.399 I llm_load_print_meta: vocab_only       = 0
0.00.044.399 I llm_load_print_meta: n_ctx_train      = 512
0.00.044.399 I llm_load_print_meta: n_embd           = 384
0.00.044.400 I llm_load_print_meta: n_layer          = 12
0.00.044.404 I llm_load_print_meta: n_head           = 12
0.00.044.405 I llm_load_print_meta: n_head_kv        = 12
0.00.044.405 I llm_load_print_meta: n_rot            = 32
0.00.044.405 I llm_load_print_meta: n_swa            = 0
0.00.044.407 I llm_load_print_meta: n_embd_head_k    = 32
0.00.044.408 I llm_load_print_meta: n_embd_head_v    = 32
0.00.044.408 I llm_load_print_meta: n_gqa            = 1
0.00.044.410 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.044.411 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.044.412 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.044.414 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.044.414 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.044.414 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.044.414 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.044.415 I llm_load_print_meta: n_ff             = 1536
0.00.044.416 I llm_load_print_meta: n_expert         = 0
0.00.044.416 I llm_load_print_meta: n_expert_used    = 0
0.00.044.416 I llm_load_print_meta: causal attn      = 0
0.00.044.418 I llm_load_print_meta: pooling type     = 2
0.00.044.418 I llm_load_print_meta: rope type        = 2
0.00.044.418 I llm_load_print_meta: rope scaling     = linear
0.00.044.419 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.044.419 I llm_load_print_meta: freq_scale_train = 1
0.00.044.420 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.044.421 I llm_load_print_meta: rope_finetuned   = unknown
0.00.044.421 I llm_load_print_meta: ssm_d_conv       = 0
0.00.044.421 I llm_load_print_meta: ssm_d_inner      = 0
0.00.044.421 I llm_load_print_meta: ssm_d_state      = 0
0.00.044.422 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.044.422 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.044.435 I llm_load_print_meta: model type       = 33M
0.00.044.435 I llm_load_print_meta: model ftype      = F16
0.00.044.436 I llm_load_print_meta: model params     = 33.21 M
0.00.044.437 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.044.437 I llm_load_print_meta: general.name     = Bge Small
0.00.044.438 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.044.438 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.044.439 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.044.439 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.044.439 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.044.440 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.044.440 I llm_load_print_meta: max token length = 21
0.00.046.557 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.046.559 I llm_load_tensors: offloading output layer to GPU
0.00.046.560 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.046.589 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.046.591 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.047.239 I llama_new_context_with_model: n_seq_max     = 1
0.00.047.240 I llama_new_context_with_model: n_ctx         = 512
0.00.047.240 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.047.241 I llama_new_context_with_model: n_batch       = 2048
0.00.047.241 I llama_new_context_with_model: n_ubatch      = 2048
0.00.047.241 I llama_new_context_with_model: flash_attn    = 0
0.00.047.242 I llama_new_context_with_model: freq_base     = 10000.0
0.00.047.242 I llama_new_context_with_model: freq_scale    = 1
0.00.047.243 I ggml_metal_init: allocating
0.00.047.258 I ggml_metal_init: found device: Apple M4
0.00.047.266 I ggml_metal_init: picking default device: Apple M4
0.00.048.248 I ggml_metal_init: using embedded metal library
0.00.052.060 I ggml_metal_init: GPU name:   Apple M4
0.00.052.063 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.052.064 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.052.064 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.052.065 I ggml_metal_init: simdgroup reduction   = true
0.00.052.065 I ggml_metal_init: simdgroup matrix mul. = true
0.00.052.065 I ggml_metal_init: has bfloat            = true
0.00.052.065 I ggml_metal_init: use bfloat            = true
0.00.052.066 I ggml_metal_init: hasUnifiedMemory      = true
0.00.052.067 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.398 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.064.400 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.064.402 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.065.583 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.065.585 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.065.586 I llama_new_context_with_model: graph nodes  = 429
0.00.065.586 I llama_new_context_with_model: graph splits = 2
0.00.065.612 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.075.335 I 
0.00.075.371 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.076.264 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.081.656 I llama_perf_context_print:        load time =      50.23 ms
0.00.081.658 I llama_perf_context_print: prompt eval time =       5.24 ms /     9 tokens (    0.58 ms per token,  1719.20 tokens per second)
0.00.081.659 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.081.659 I llama_perf_context_print:       total time =       6.32 ms /    10 tokens
0.00.081.813 I ggml_metal_free: deallocating

real	0m0.292s
user	0m0.054s
sys	0m0.037s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.040 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.317 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.387 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.391 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.392 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.393 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.394 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.395 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.395 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.396 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.398 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.398 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.399 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.399 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.401 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.401 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.401 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.402 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.402 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.402 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.406 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.657 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.658 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.658 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.658 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.659 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.659 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.659 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.660 I llama_model_loader: - type  f32:  124 tensors
0.00.014.660 I llama_model_loader: - type q8_0:   73 tensors
0.00.017.132 I llm_load_vocab: special tokens cache size = 5
0.00.018.486 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.489 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.489 I llm_load_print_meta: arch             = bert
0.00.018.489 I llm_load_print_meta: vocab type       = WPM
0.00.018.489 I llm_load_print_meta: n_vocab          = 30522
0.00.018.490 I llm_load_print_meta: n_merges         = 0
0.00.018.490 I llm_load_print_meta: vocab_only       = 0
0.00.018.490 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.490 I llm_load_print_meta: n_embd           = 384
0.00.018.490 I llm_load_print_meta: n_layer          = 12
0.00.018.493 I llm_load_print_meta: n_head           = 12
0.00.018.494 I llm_load_print_meta: n_head_kv        = 12
0.00.018.494 I llm_load_print_meta: n_rot            = 32
0.00.018.494 I llm_load_print_meta: n_swa            = 0
0.00.018.494 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.495 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.495 I llm_load_print_meta: n_gqa            = 1
0.00.018.496 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.496 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.497 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.497 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.498 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.498 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.498 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.501 I llm_load_print_meta: n_ff             = 1536
0.00.018.501 I llm_load_print_meta: n_expert         = 0
0.00.018.501 I llm_load_print_meta: n_expert_used    = 0
0.00.018.501 I llm_load_print_meta: causal attn      = 0
0.00.018.501 I llm_load_print_meta: pooling type     = 2
0.00.018.501 I llm_load_print_meta: rope type        = 2
0.00.018.502 I llm_load_print_meta: rope scaling     = linear
0.00.018.502 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.503 I llm_load_print_meta: freq_scale_train = 1
0.00.018.503 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.503 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.503 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.505 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.505 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.505 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.505 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.511 I llm_load_print_meta: model type       = 33M
0.00.018.511 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.512 I llm_load_print_meta: model params     = 33.21 M
0.00.018.512 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.513 I llm_load_print_meta: general.name     = Bge Small
0.00.018.513 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.513 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.513 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.514 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.514 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.514 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.514 I llm_load_print_meta: max token length = 21
0.00.019.665 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.665 I llm_load_tensors: offloading output layer to GPU
0.00.019.665 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.671 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.672 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.031 I llama_new_context_with_model: n_seq_max     = 1
0.00.020.032 I llama_new_context_with_model: n_ctx         = 512
0.00.020.032 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.020.032 I llama_new_context_with_model: n_batch       = 2048
0.00.020.032 I llama_new_context_with_model: n_ubatch      = 2048
0.00.020.033 I llama_new_context_with_model: flash_attn    = 0
0.00.020.033 I llama_new_context_with_model: freq_base     = 10000.0
0.00.020.033 I llama_new_context_with_model: freq_scale    = 1
0.00.020.034 I ggml_metal_init: allocating
0.00.020.040 I ggml_metal_init: found device: Apple M4
0.00.020.045 I ggml_metal_init: picking default device: Apple M4
0.00.020.639 I ggml_metal_init: using embedded metal library
0.00.022.760 I ggml_metal_init: GPU name:   Apple M4
0.00.022.762 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.762 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.763 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.763 I ggml_metal_init: simdgroup reduction   = true
0.00.022.763 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.763 I ggml_metal_init: has bfloat            = true
0.00.022.763 I ggml_metal_init: use bfloat            = true
0.00.022.764 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.764 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.031.722 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.031.724 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.031.726 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.032.373 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.032.374 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.032.374 I llama_new_context_with_model: graph nodes  = 429
0.00.032.375 I llama_new_context_with_model: graph splits = 2
0.00.032.388 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.037.284 I 
0.00.037.306 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.037.845 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.042.290 I llama_perf_context_print:        load time =      27.96 ms
0.00.042.291 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2076.60 tokens per second)
0.00.042.292 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.042.292 I llama_perf_context_print:       total time =       5.01 ms /    10 tokens
0.00.042.482 I ggml_metal_free: deallocating

real	0m0.058s
user	0m0.029s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.164 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.342 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.618 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.623 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.625 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.626 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.627 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.628 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.629 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.630 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.631 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.632 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.632 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.633 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.636 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.641 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.641 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.642 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.643 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.044.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.046.978 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.047 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.052.049 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.050 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.052.050 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.052.051 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.052.051 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.052.051 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.052.052 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.052.052 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.052.052 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.052.053 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.052.053 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.052.054 I llama_model_loader: - type  f32:   41 tensors
0.00.052.060 I llama_model_loader: - type  f16:   29 tensors
0.00.070.092 W llm_load_vocab: empty token at index 5
0.00.074.533 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.075.777 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.075.803 I llm_load_vocab: special tokens cache size = 5
0.00.333.853 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.333.858 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.333.858 I llm_load_print_meta: arch             = jina-bert-v2
0.00.333.859 I llm_load_print_meta: vocab type       = BPE
0.00.333.859 I llm_load_print_meta: n_vocab          = 61056
0.00.333.859 I llm_load_print_meta: n_merges         = 39382
0.00.333.860 I llm_load_print_meta: vocab_only       = 0
0.00.333.860 I llm_load_print_meta: n_ctx_train      = 8192
0.00.333.860 I llm_load_print_meta: n_embd           = 384
0.00.333.860 I llm_load_print_meta: n_layer          = 4
0.00.333.866 I llm_load_print_meta: n_head           = 12
0.00.333.866 I llm_load_print_meta: n_head_kv        = 12
0.00.333.867 I llm_load_print_meta: n_rot            = 32
0.00.333.867 I llm_load_print_meta: n_swa            = 0
0.00.333.867 I llm_load_print_meta: n_embd_head_k    = 32
0.00.333.867 I llm_load_print_meta: n_embd_head_v    = 32
0.00.333.868 I llm_load_print_meta: n_gqa            = 1
0.00.333.869 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.333.869 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.333.870 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.333.870 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.333.870 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.333.871 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.333.871 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.333.871 I llm_load_print_meta: n_ff             = 1536
0.00.333.871 I llm_load_print_meta: n_expert         = 0
0.00.333.872 I llm_load_print_meta: n_expert_used    = 0
0.00.333.872 I llm_load_print_meta: causal attn      = 0
0.00.333.872 I llm_load_print_meta: pooling type     = -1
0.00.333.872 I llm_load_print_meta: rope type        = -1
0.00.333.872 I llm_load_print_meta: rope scaling     = linear
0.00.333.873 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.333.873 I llm_load_print_meta: freq_scale_train = 1
0.00.333.873 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.333.874 I llm_load_print_meta: rope_finetuned   = unknown
0.00.333.874 I llm_load_print_meta: ssm_d_conv       = 0
0.00.333.874 I llm_load_print_meta: ssm_d_inner      = 0
0.00.333.877 I llm_load_print_meta: ssm_d_state      = 0
0.00.333.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.333.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.333.903 I llm_load_print_meta: model type       = 33M
0.00.333.903 I llm_load_print_meta: model ftype      = F16
0.00.333.904 I llm_load_print_meta: model params     = 32.90 M
0.00.333.904 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.333.904 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.333.904 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.333.905 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.333.905 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.333.905 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.333.907 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.333.907 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.333.907 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.333.908 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.333.908 I llm_load_print_meta: max token length = 45
0.00.335.020 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.335.020 I llm_load_tensors: offloading output layer to GPU
0.00.335.020 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.335.042 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.335.043 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.335.920 I llama_new_context_with_model: n_seq_max     = 1
0.00.335.921 I llama_new_context_with_model: n_ctx         = 8192
0.00.335.922 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.335.922 I llama_new_context_with_model: n_batch       = 2048
0.00.335.922 I llama_new_context_with_model: n_ubatch      = 2048
0.00.335.922 I llama_new_context_with_model: flash_attn    = 0
0.00.335.923 I llama_new_context_with_model: freq_base     = 10000.0
0.00.335.923 I llama_new_context_with_model: freq_scale    = 1
0.00.335.924 I ggml_metal_init: allocating
0.00.335.930 I ggml_metal_init: found device: Apple M4
0.00.335.933 I ggml_metal_init: picking default device: Apple M4
0.00.336.858 I ggml_metal_init: using embedded metal library
0.00.339.332 I ggml_metal_init: GPU name:   Apple M4
0.00.339.334 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.334 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.335 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.335 I ggml_metal_init: simdgroup reduction   = true
0.00.339.335 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.335 I ggml_metal_init: has bfloat            = true
0.00.339.335 I ggml_metal_init: use bfloat            = true
0.00.339.336 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.336 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.349.447 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.349.449 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.349.450 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.350.026 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.350.027 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.350.027 I llama_new_context_with_model: graph nodes  = 154
0.00.350.028 I llama_new_context_with_model: graph splits = 2
0.00.350.045 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.315 I 
0.00.361.351 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.361.507 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.508 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.510 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.511 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.512 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.513 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.362.045 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.646 I llama_perf_context_print:        load time =     335.97 ms
0.00.365.647 I llama_perf_context_print: prompt eval time =       3.59 ms /    62 tokens (    0.06 ms per token, 17255.78 tokens per second)
0.00.365.648 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.648 I llama_perf_context_print:       total time =       4.33 ms /    63 tokens
0.00.365.892 I ggml_metal_free: deallocating

real	0m1.051s
user	0m0.337s
sys	0m0.045s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.135 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.253 I main: llama backend init
0.00.000.259 I main: load the model and apply lora adapter, if any
0.00.035.998 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.046.865 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.882 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.886 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.889 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.889 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.906 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.907 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.907 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.908 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.908 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.909 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.910 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.915 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.916 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.055.835 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.067.080 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.081 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.083 I llama_model_loader: - type  f32:  194 tensors
0.00.067.084 I llama_model_loader: - type  f16:   98 tensors
0.00.098.468 I llm_load_vocab: special tokens cache size = 25
0.00.105.181 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.105.184 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.105.184 I llm_load_print_meta: arch             = gptneox
0.00.105.184 I llm_load_print_meta: vocab type       = BPE
0.00.105.185 I llm_load_print_meta: n_vocab          = 50304
0.00.105.185 I llm_load_print_meta: n_merges         = 50009
0.00.105.185 I llm_load_print_meta: vocab_only       = 0
0.00.105.185 I llm_load_print_meta: n_ctx_train      = 2048
0.00.105.185 I llm_load_print_meta: n_embd           = 2048
0.00.105.185 I llm_load_print_meta: n_layer          = 24
0.00.105.189 I llm_load_print_meta: n_head           = 16
0.00.105.189 I llm_load_print_meta: n_head_kv        = 16
0.00.105.189 I llm_load_print_meta: n_rot            = 32
0.00.105.190 I llm_load_print_meta: n_swa            = 0
0.00.105.190 I llm_load_print_meta: n_embd_head_k    = 128
0.00.105.190 I llm_load_print_meta: n_embd_head_v    = 128
0.00.105.191 I llm_load_print_meta: n_gqa            = 1
0.00.105.191 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.105.192 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.105.193 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.105.193 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.105.193 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.105.193 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.105.194 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.105.194 I llm_load_print_meta: n_ff             = 8192
0.00.105.194 I llm_load_print_meta: n_expert         = 0
0.00.105.194 I llm_load_print_meta: n_expert_used    = 0
0.00.105.195 I llm_load_print_meta: causal attn      = 1
0.00.105.195 I llm_load_print_meta: pooling type     = 0
0.00.105.195 I llm_load_print_meta: rope type        = 2
0.00.105.195 I llm_load_print_meta: rope scaling     = linear
0.00.105.198 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.105.198 I llm_load_print_meta: freq_scale_train = 1
0.00.105.198 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.105.198 I llm_load_print_meta: rope_finetuned   = unknown
0.00.105.199 I llm_load_print_meta: ssm_d_conv       = 0
0.00.105.199 I llm_load_print_meta: ssm_d_inner      = 0
0.00.105.199 I llm_load_print_meta: ssm_d_state      = 0
0.00.105.199 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.105.199 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.105.211 I llm_load_print_meta: model type       = 1.4B
0.00.105.212 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.105.212 I llm_load_print_meta: model params     = 1.41 B
0.00.105.213 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.105.214 I llm_load_print_meta: general.name     = 1.4B
0.00.105.215 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.105.215 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.105.215 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.105.215 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.105.216 I llm_load_print_meta: LF token         = 128 ''
0.00.105.216 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.105.216 I llm_load_print_meta: max token length = 1024
0.00.107.786 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.107.786 I llm_load_tensors: offloading output layer to GPU
0.00.107.787 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.107.804 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.805 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.108.742 I llama_new_context_with_model: n_seq_max     = 1
0.00.108.743 I llama_new_context_with_model: n_ctx         = 2048
0.00.108.743 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.108.743 I llama_new_context_with_model: n_batch       = 2048
0.00.108.743 I llama_new_context_with_model: n_ubatch      = 512
0.00.108.743 I llama_new_context_with_model: flash_attn    = 0
0.00.108.744 I llama_new_context_with_model: freq_base     = 10000.0
0.00.108.744 I llama_new_context_with_model: freq_scale    = 1
0.00.108.744 I ggml_metal_init: allocating
0.00.108.747 I ggml_metal_init: found device: Apple M4
0.00.108.749 I ggml_metal_init: picking default device: Apple M4
0.00.109.392 I ggml_metal_init: using embedded metal library
0.00.117.880 I ggml_metal_init: GPU name:   Apple M4
0.00.117.882 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.117.883 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.117.883 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.117.883 I ggml_metal_init: simdgroup reduction   = true
0.00.117.884 I ggml_metal_init: simdgroup matrix mul. = true
0.00.117.884 I ggml_metal_init: has bfloat            = true
0.00.117.884 I ggml_metal_init: use bfloat            = true
0.00.117.884 I ggml_metal_init: hasUnifiedMemory      = true
0.00.117.885 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.156.105 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.156.111 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.156.139 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.157.097 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.157.099 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.157.099 I llama_new_context_with_model: graph nodes  = 967
0.00.157.099 I llama_new_context_with_model: graph splits = 2
0.00.157.122 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.233.115 I main: llama threadpool init, n_threads = 4
0.00.233.145 I 
0.00.233.175 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.233.176 I 
0.00.233.252 I sampler seed: 1234
0.00.233.257 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.233.289 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.233.289 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.233.289 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.087.342 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54033.49 tokens per second)
0.02.087.344 I llama_perf_context_print:        load time =     197.11 ms
0.02.087.344 I llama_perf_context_print: prompt eval time =      39.69 ms /     7 tokens (    5.67 ms per token,   176.38 tokens per second)
0.02.087.345 I llama_perf_context_print:        eval time =    1811.32 ms /    63 runs   (   28.75 ms per token,    34.78 tokens per second)
0.02.087.346 I llama_perf_context_print:       total time =    1854.23 ms /    70 tokens
0.02.087.534 I ggml_metal_free: deallocating

real	0m2.374s
user	0m0.144s
sys	0m0.097s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.516 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.620 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.449 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.465 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.470 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.470 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.471 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.471 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.474 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.475 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.475 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.476 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.477 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.481 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.482 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.486 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.331 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.898 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.977 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.980 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.981 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.982 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.983 I llama_model_loader: - type  f32:  194 tensors
0.00.056.983 I llama_model_loader: - type  f16:   98 tensors
0.00.088.767 I llm_load_vocab: special tokens cache size = 25
0.00.095.592 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.095.595 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.095.595 I llm_load_print_meta: arch             = gptneox
0.00.095.595 I llm_load_print_meta: vocab type       = BPE
0.00.095.595 I llm_load_print_meta: n_vocab          = 50304
0.00.095.596 I llm_load_print_meta: n_merges         = 50009
0.00.095.596 I llm_load_print_meta: vocab_only       = 0
0.00.095.596 I llm_load_print_meta: n_ctx_train      = 2048
0.00.095.596 I llm_load_print_meta: n_embd           = 2048
0.00.095.596 I llm_load_print_meta: n_layer          = 24
0.00.095.599 I llm_load_print_meta: n_head           = 16
0.00.095.600 I llm_load_print_meta: n_head_kv        = 16
0.00.095.600 I llm_load_print_meta: n_rot            = 32
0.00.095.600 I llm_load_print_meta: n_swa            = 0
0.00.095.601 I llm_load_print_meta: n_embd_head_k    = 128
0.00.095.601 I llm_load_print_meta: n_embd_head_v    = 128
0.00.095.601 I llm_load_print_meta: n_gqa            = 1
0.00.095.602 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.095.603 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.095.603 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.095.604 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.095.604 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.095.604 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.095.604 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.095.605 I llm_load_print_meta: n_ff             = 8192
0.00.095.605 I llm_load_print_meta: n_expert         = 0
0.00.095.605 I llm_load_print_meta: n_expert_used    = 0
0.00.095.605 I llm_load_print_meta: causal attn      = 1
0.00.095.605 I llm_load_print_meta: pooling type     = 0
0.00.095.605 I llm_load_print_meta: rope type        = 2
0.00.095.606 I llm_load_print_meta: rope scaling     = linear
0.00.095.606 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.095.606 I llm_load_print_meta: freq_scale_train = 1
0.00.095.606 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.095.607 I llm_load_print_meta: rope_finetuned   = unknown
0.00.095.607 I llm_load_print_meta: ssm_d_conv       = 0
0.00.095.607 I llm_load_print_meta: ssm_d_inner      = 0
0.00.095.607 I llm_load_print_meta: ssm_d_state      = 0
0.00.095.607 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.095.607 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.095.619 I llm_load_print_meta: model type       = 1.4B
0.00.095.620 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.095.620 I llm_load_print_meta: model params     = 1.41 B
0.00.095.621 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.095.621 I llm_load_print_meta: general.name     = 1.4B
0.00.095.621 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.095.621 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.095.621 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.095.622 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.095.622 I llm_load_print_meta: LF token         = 128 ''
0.00.095.622 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.095.622 I llm_load_print_meta: max token length = 1024
0.00.098.238 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.098.238 I llm_load_tensors: offloading output layer to GPU
0.00.098.238 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.098.248 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.098.249 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.099.256 I llama_new_context_with_model: n_seq_max     = 1
0.00.099.257 I llama_new_context_with_model: n_ctx         = 128
0.00.099.257 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.099.257 I llama_new_context_with_model: n_batch       = 128
0.00.099.258 I llama_new_context_with_model: n_ubatch      = 128
0.00.099.258 I llama_new_context_with_model: flash_attn    = 0
0.00.099.258 I llama_new_context_with_model: freq_base     = 10000.0
0.00.099.258 I llama_new_context_with_model: freq_scale    = 1
0.00.099.259 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.099.259 I ggml_metal_init: allocating
0.00.099.262 I ggml_metal_init: found device: Apple M4
0.00.099.264 I ggml_metal_init: picking default device: Apple M4
0.00.099.862 I ggml_metal_init: using embedded metal library
0.00.102.021 I ggml_metal_init: GPU name:   Apple M4
0.00.102.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.023 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.023 I ggml_metal_init: simdgroup reduction   = true
0.00.102.024 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.024 I ggml_metal_init: has bfloat            = true
0.00.102.024 I ggml_metal_init: use bfloat            = true
0.00.102.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.025 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.234 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.111.236 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.111.250 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.112.121 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.112.122 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.112.122 I llama_new_context_with_model: graph nodes  = 967
0.00.112.123 I llama_new_context_with_model: graph splits = 2
0.00.112.135 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.306.952 I 
0.01.307.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.307.053 I perplexity: tokenizing the input ..
0.01.321.047 I perplexity: tokenization took 13.994 ms
0.01.321.063 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.442.380 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.444.087 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.444.132 I llama_perf_context_print:        load time =    1282.31 ms
0.01.444.134 I llama_perf_context_print: prompt eval time =     120.26 ms /   128 tokens (    0.94 ms per token,  1064.36 tokens per second)
0.01.444.135 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.444.136 I llama_perf_context_print:       total time =     137.19 ms /   129 tokens
0.01.444.773 I ggml_metal_free: deallocating

real	0m1.636s
user	0m0.128s
sys	0m0.240s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.830 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.336 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.345 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.347 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.349 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.350 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.352 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.555 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.861 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.037.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.865 I llama_model_loader: - type  f32:  194 tensors
0.00.037.865 I llama_model_loader: - type q8_0:   98 tensors
0.00.064.001 I llm_load_vocab: special tokens cache size = 25
0.00.071.120 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.071.124 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.071.125 I llm_load_print_meta: arch             = gptneox
0.00.071.125 I llm_load_print_meta: vocab type       = BPE
0.00.071.125 I llm_load_print_meta: n_vocab          = 50304
0.00.071.126 I llm_load_print_meta: n_merges         = 50009
0.00.071.128 I llm_load_print_meta: vocab_only       = 0
0.00.071.128 I llm_load_print_meta: n_ctx_train      = 2048
0.00.071.128 I llm_load_print_meta: n_embd           = 2048
0.00.071.129 I llm_load_print_meta: n_layer          = 24
0.00.071.134 I llm_load_print_meta: n_head           = 16
0.00.071.135 I llm_load_print_meta: n_head_kv        = 16
0.00.071.135 I llm_load_print_meta: n_rot            = 32
0.00.071.135 I llm_load_print_meta: n_swa            = 0
0.00.071.139 I llm_load_print_meta: n_embd_head_k    = 128
0.00.071.139 I llm_load_print_meta: n_embd_head_v    = 128
0.00.071.140 I llm_load_print_meta: n_gqa            = 1
0.00.071.141 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.071.141 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.071.142 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.071.143 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.071.143 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.071.143 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.071.143 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.071.144 I llm_load_print_meta: n_ff             = 8192
0.00.071.144 I llm_load_print_meta: n_expert         = 0
0.00.071.144 I llm_load_print_meta: n_expert_used    = 0
0.00.071.145 I llm_load_print_meta: causal attn      = 1
0.00.071.145 I llm_load_print_meta: pooling type     = 0
0.00.071.145 I llm_load_print_meta: rope type        = 2
0.00.071.145 I llm_load_print_meta: rope scaling     = linear
0.00.071.146 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.071.146 I llm_load_print_meta: freq_scale_train = 1
0.00.071.146 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.071.146 I llm_load_print_meta: rope_finetuned   = unknown
0.00.071.147 I llm_load_print_meta: ssm_d_conv       = 0
0.00.071.147 I llm_load_print_meta: ssm_d_inner      = 0
0.00.071.148 I llm_load_print_meta: ssm_d_state      = 0
0.00.071.148 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.071.148 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.071.162 I llm_load_print_meta: model type       = 1.4B
0.00.071.163 I llm_load_print_meta: model ftype      = Q8_0
0.00.071.163 I llm_load_print_meta: model params     = 1.41 B
0.00.071.164 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.071.164 I llm_load_print_meta: general.name     = 1.4B
0.00.071.165 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.071.165 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.071.165 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.071.165 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.071.165 I llm_load_print_meta: LF token         = 128 ''
0.00.071.166 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.071.166 I llm_load_print_meta: max token length = 1024
0.00.073.735 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.073.735 I llm_load_tensors: offloading output layer to GPU
0.00.073.736 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.073.746 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.747 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.074.800 I llama_new_context_with_model: n_seq_max     = 1
0.00.074.801 I llama_new_context_with_model: n_ctx         = 2048
0.00.074.801 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.074.801 I llama_new_context_with_model: n_batch       = 2048
0.00.074.802 I llama_new_context_with_model: n_ubatch      = 512
0.00.074.802 I llama_new_context_with_model: flash_attn    = 0
0.00.074.802 I llama_new_context_with_model: freq_base     = 10000.0
0.00.074.802 I llama_new_context_with_model: freq_scale    = 1
0.00.074.803 I ggml_metal_init: allocating
0.00.074.806 I ggml_metal_init: found device: Apple M4
0.00.074.808 I ggml_metal_init: picking default device: Apple M4
0.00.075.523 I ggml_metal_init: using embedded metal library
0.00.077.957 I ggml_metal_init: GPU name:   Apple M4
0.00.077.958 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.077.959 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.077.959 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.077.960 I ggml_metal_init: simdgroup reduction   = true
0.00.077.960 I ggml_metal_init: simdgroup matrix mul. = true
0.00.077.960 I ggml_metal_init: has bfloat            = true
0.00.077.960 I ggml_metal_init: use bfloat            = true
0.00.077.960 I ggml_metal_init: hasUnifiedMemory      = true
0.00.077.961 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.113.868 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.113.886 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.113.913 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.031 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.033 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.033 I llama_new_context_with_model: graph nodes  = 967
0.00.115.034 I llama_new_context_with_model: graph splits = 2
0.00.115.049 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.200.682 I main: llama threadpool init, n_threads = 4
0.01.200.713 I 
0.01.200.738 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.01.200.738 I 
0.01.200.948 I sampler seed: 1234
0.01.200.953 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.200.968 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.200.969 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.200.969 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.288.486 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56709.27 tokens per second)
0.02.288.486 I llama_perf_context_print:        load time =    1190.85 ms
0.02.288.487 I llama_perf_context_print: prompt eval time =      39.68 ms /     7 tokens (    5.67 ms per token,   176.40 tokens per second)
0.02.288.487 I llama_perf_context_print:        eval time =    1044.74 ms /    63 runs   (   16.58 ms per token,    60.30 tokens per second)
0.02.288.488 I llama_perf_context_print:       total time =    1087.81 ms /    70 tokens
0.02.288.669 I ggml_metal_free: deallocating

real	0m2.307s
user	0m0.119s
sys	0m0.229s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.113 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.885 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.217 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.229 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.230 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.230 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.231 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.231 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.232 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.232 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.233 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.233 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.233 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.234 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.234 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.236 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.236 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.237 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.977 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.489 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.490 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.491 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.491 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.491 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.492 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.492 I llama_model_loader: - type  f32:  194 tensors
0.00.031.493 I llama_model_loader: - type q8_0:   98 tensors
0.00.055.274 I llm_load_vocab: special tokens cache size = 25
0.00.061.225 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.228 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.228 I llm_load_print_meta: arch             = gptneox
0.00.061.229 I llm_load_print_meta: vocab type       = BPE
0.00.061.229 I llm_load_print_meta: n_vocab          = 50304
0.00.061.229 I llm_load_print_meta: n_merges         = 50009
0.00.061.229 I llm_load_print_meta: vocab_only       = 0
0.00.061.230 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.230 I llm_load_print_meta: n_embd           = 2048
0.00.061.230 I llm_load_print_meta: n_layer          = 24
0.00.061.233 I llm_load_print_meta: n_head           = 16
0.00.061.234 I llm_load_print_meta: n_head_kv        = 16
0.00.061.234 I llm_load_print_meta: n_rot            = 32
0.00.061.234 I llm_load_print_meta: n_swa            = 0
0.00.061.235 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.235 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.235 I llm_load_print_meta: n_gqa            = 1
0.00.061.236 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.237 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.240 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.240 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.241 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.241 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.241 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.242 I llm_load_print_meta: n_ff             = 8192
0.00.061.242 I llm_load_print_meta: n_expert         = 0
0.00.061.242 I llm_load_print_meta: n_expert_used    = 0
0.00.061.242 I llm_load_print_meta: causal attn      = 1
0.00.061.242 I llm_load_print_meta: pooling type     = 0
0.00.061.242 I llm_load_print_meta: rope type        = 2
0.00.061.244 I llm_load_print_meta: rope scaling     = linear
0.00.061.244 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.245 I llm_load_print_meta: freq_scale_train = 1
0.00.061.245 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.245 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.245 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.245 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.245 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.246 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.246 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.258 I llm_load_print_meta: model type       = 1.4B
0.00.061.258 I llm_load_print_meta: model ftype      = Q8_0
0.00.061.258 I llm_load_print_meta: model params     = 1.41 B
0.00.061.259 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.061.259 I llm_load_print_meta: general.name     = 1.4B
0.00.061.259 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.259 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.259 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.260 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.260 I llm_load_print_meta: LF token         = 128 ''
0.00.061.260 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.260 I llm_load_print_meta: max token length = 1024
0.00.063.397 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.063.398 I llm_load_tensors: offloading output layer to GPU
0.00.063.398 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.063.408 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.063.409 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.064.326 I llama_new_context_with_model: n_seq_max     = 1
0.00.064.327 I llama_new_context_with_model: n_ctx         = 128
0.00.064.327 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.064.327 I llama_new_context_with_model: n_batch       = 128
0.00.064.327 I llama_new_context_with_model: n_ubatch      = 128
0.00.064.328 I llama_new_context_with_model: flash_attn    = 0
0.00.064.328 I llama_new_context_with_model: freq_base     = 10000.0
0.00.064.328 I llama_new_context_with_model: freq_scale    = 1
0.00.064.329 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.064.329 I ggml_metal_init: allocating
0.00.064.336 I ggml_metal_init: found device: Apple M4
0.00.064.338 I ggml_metal_init: picking default device: Apple M4
0.00.064.911 I ggml_metal_init: using embedded metal library
0.00.066.975 I ggml_metal_init: GPU name:   Apple M4
0.00.066.976 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.977 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.977 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.977 I ggml_metal_init: simdgroup reduction   = true
0.00.066.977 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.978 I ggml_metal_init: has bfloat            = true
0.00.066.978 I ggml_metal_init: use bfloat            = true
0.00.066.978 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.979 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.943 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.075.945 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.075.959 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.076.779 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.076.780 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.076.780 I llama_new_context_with_model: graph nodes  = 967
0.00.076.781 I llama_new_context_with_model: graph splits = 2
0.00.076.793 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.926.226 I 
0.00.926.254 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.926.259 I perplexity: tokenizing the input ..
0.00.934.316 I perplexity: tokenization took 8.056 ms
0.00.934.320 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.056.023 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.057.198 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.057.216 I llama_perf_context_print:        load time =     915.34 ms
0.01.057.216 I llama_perf_context_print: prompt eval time =     121.48 ms /   128 tokens (    0.95 ms per token,  1053.69 tokens per second)
0.01.057.217 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.057.218 I llama_perf_context_print:       total time =     130.99 ms /   129 tokens
0.01.057.490 I ggml_metal_free: deallocating

real	0m1.074s
user	0m0.087s
sys	0m0.156s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.011.796 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.691 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.696 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.703 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.704 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.704 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.706 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.706 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.706 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.801 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.879 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.979 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.980 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.980 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.981 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.981 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.981 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.982 I llama_model_loader: - type  f32:  194 tensors
0.00.027.983 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.983 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.111 I llm_load_vocab: special tokens cache size = 25
0.00.055.016 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.019 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.020 I llm_load_print_meta: arch             = gptneox
0.00.055.020 I llm_load_print_meta: vocab type       = BPE
0.00.055.021 I llm_load_print_meta: n_vocab          = 50304
0.00.055.021 I llm_load_print_meta: n_merges         = 50009
0.00.055.021 I llm_load_print_meta: vocab_only       = 0
0.00.055.021 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.021 I llm_load_print_meta: n_embd           = 2048
0.00.055.021 I llm_load_print_meta: n_layer          = 24
0.00.055.025 I llm_load_print_meta: n_head           = 16
0.00.055.026 I llm_load_print_meta: n_head_kv        = 16
0.00.055.026 I llm_load_print_meta: n_rot            = 32
0.00.055.026 I llm_load_print_meta: n_swa            = 0
0.00.055.026 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.026 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.027 I llm_load_print_meta: n_gqa            = 1
0.00.055.028 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.029 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.029 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.030 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.030 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.030 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.030 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.031 I llm_load_print_meta: n_ff             = 8192
0.00.055.031 I llm_load_print_meta: n_expert         = 0
0.00.055.031 I llm_load_print_meta: n_expert_used    = 0
0.00.055.031 I llm_load_print_meta: causal attn      = 1
0.00.055.032 I llm_load_print_meta: pooling type     = 0
0.00.055.032 I llm_load_print_meta: rope type        = 2
0.00.055.032 I llm_load_print_meta: rope scaling     = linear
0.00.055.032 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.033 I llm_load_print_meta: freq_scale_train = 1
0.00.055.033 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.033 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.033 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.033 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.033 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.034 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.034 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.039 I llm_load_print_meta: model type       = 1.4B
0.00.055.040 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.040 I llm_load_print_meta: model params     = 1.41 B
0.00.055.043 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.043 I llm_load_print_meta: general.name     = 1.4B
0.00.055.043 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.043 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.044 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.044 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.044 I llm_load_print_meta: LF token         = 128 ''
0.00.055.044 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.045 I llm_load_print_meta: max token length = 1024
0.00.057.062 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.062 I llm_load_tensors: offloading output layer to GPU
0.00.057.062 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.068 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.057.069 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.058.037 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.038 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.038 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.038 I llama_new_context_with_model: n_batch       = 2048
0.00.058.038 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.039 I llama_new_context_with_model: flash_attn    = 0
0.00.058.039 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.039 I llama_new_context_with_model: freq_scale    = 1
0.00.058.040 I ggml_metal_init: allocating
0.00.058.043 I ggml_metal_init: found device: Apple M4
0.00.058.045 I ggml_metal_init: picking default device: Apple M4
0.00.058.728 I ggml_metal_init: using embedded metal library
0.00.060.820 I ggml_metal_init: GPU name:   Apple M4
0.00.060.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.822 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.823 I ggml_metal_init: simdgroup reduction   = true
0.00.060.823 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.823 I ggml_metal_init: has bfloat            = true
0.00.060.823 I ggml_metal_init: use bfloat            = true
0.00.060.824 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.824 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.022 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.038 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.068 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.211 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.213 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.213 I llama_new_context_with_model: graph nodes  = 967
0.00.095.213 I llama_new_context_with_model: graph splits = 2
0.00.095.229 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.470 I main: llama threadpool init, n_threads = 4
0.00.681.511 I 
0.00.681.543 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.681.545 I 
0.00.681.788 I sampler seed: 1234
0.00.681.793 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.681.830 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.681.831 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.681.831 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.357.069 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.01.357.069 I llama_perf_context_print:        load time =     669.67 ms
0.01.357.070 I llama_perf_context_print: prompt eval time =      36.60 ms /     7 tokens (    5.23 ms per token,   191.26 tokens per second)
0.01.357.071 I llama_perf_context_print:        eval time =     635.61 ms /    63 runs   (   10.09 ms per token,    99.12 tokens per second)
0.01.357.071 I llama_perf_context_print:       total time =     675.60 ms /    70 tokens
0.01.357.270 I ggml_metal_free: deallocating

real	0m1.377s
user	0m0.110s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.941 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.780 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.785 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.786 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.787 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.787 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.787 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.788 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.788 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.789 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.789 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.790 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.790 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.790 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.751 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.852 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.838 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.839 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.839 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.840 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.840 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.840 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.841 I llama_model_loader: - type  f32:  194 tensors
0.00.025.841 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.841 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.469 I llm_load_vocab: special tokens cache size = 25
0.00.052.402 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.405 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.405 I llm_load_print_meta: arch             = gptneox
0.00.052.406 I llm_load_print_meta: vocab type       = BPE
0.00.052.406 I llm_load_print_meta: n_vocab          = 50304
0.00.052.406 I llm_load_print_meta: n_merges         = 50009
0.00.052.407 I llm_load_print_meta: vocab_only       = 0
0.00.052.407 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.407 I llm_load_print_meta: n_embd           = 2048
0.00.052.407 I llm_load_print_meta: n_layer          = 24
0.00.052.411 I llm_load_print_meta: n_head           = 16
0.00.052.411 I llm_load_print_meta: n_head_kv        = 16
0.00.052.414 I llm_load_print_meta: n_rot            = 32
0.00.052.414 I llm_load_print_meta: n_swa            = 0
0.00.052.414 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.414 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.415 I llm_load_print_meta: n_gqa            = 1
0.00.052.418 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.418 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.419 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.419 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.420 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.420 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.421 I llm_load_print_meta: n_ff             = 8192
0.00.052.421 I llm_load_print_meta: n_expert         = 0
0.00.052.421 I llm_load_print_meta: n_expert_used    = 0
0.00.052.421 I llm_load_print_meta: causal attn      = 1
0.00.052.421 I llm_load_print_meta: pooling type     = 0
0.00.052.422 I llm_load_print_meta: rope type        = 2
0.00.052.422 I llm_load_print_meta: rope scaling     = linear
0.00.052.422 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.424 I llm_load_print_meta: freq_scale_train = 1
0.00.052.424 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.424 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.424 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.425 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.425 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.425 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.425 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.436 I llm_load_print_meta: model type       = 1.4B
0.00.052.437 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.437 I llm_load_print_meta: model params     = 1.41 B
0.00.052.438 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.438 I llm_load_print_meta: general.name     = 1.4B
0.00.052.438 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.438 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.439 I llm_load_print_meta: LF token         = 128 ''
0.00.052.439 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.439 I llm_load_print_meta: max token length = 1024
0.00.054.026 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.027 I llm_load_tensors: offloading output layer to GPU
0.00.054.027 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.036 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.037 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.876 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.877 I llama_new_context_with_model: n_ctx         = 128
0.00.054.877 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.877 I llama_new_context_with_model: n_batch       = 128
0.00.054.877 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.878 I llama_new_context_with_model: flash_attn    = 0
0.00.054.878 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.878 I llama_new_context_with_model: freq_scale    = 1
0.00.054.879 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.879 I ggml_metal_init: allocating
0.00.054.885 I ggml_metal_init: found device: Apple M4
0.00.054.887 I ggml_metal_init: picking default device: Apple M4
0.00.055.418 I ggml_metal_init: using embedded metal library
0.00.057.338 I ggml_metal_init: GPU name:   Apple M4
0.00.057.340 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.340 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.341 I ggml_metal_init: simdgroup reduction   = true
0.00.057.341 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.341 I ggml_metal_init: has bfloat            = true
0.00.057.341 I ggml_metal_init: use bfloat            = true
0.00.057.342 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.342 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.549 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.551 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.564 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.384 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.385 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.386 I llama_new_context_with_model: graph nodes  = 967
0.00.067.386 I llama_new_context_with_model: graph splits = 2
0.00.067.398 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.628.757 I 
0.00.628.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.628.788 I perplexity: tokenizing the input ..
0.00.636.454 I perplexity: tokenization took 7.664 ms
0.00.636.461 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.758.916 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.760.088 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.760.114 I llama_perf_context_print:        load time =     617.81 ms
0.00.760.115 I llama_perf_context_print: prompt eval time =     122.23 ms /   128 tokens (    0.95 ms per token,  1047.21 tokens per second)
0.00.760.116 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.760.116 I llama_perf_context_print:       total time =     131.36 ms /   129 tokens
0.00.760.527 I ggml_metal_free: deallocating

real	0m0.777s
user	0m0.076s
sys	0m0.108s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.937 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.300 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.304 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.306 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.307 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.307 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.307 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.315 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.462 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.476 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.477 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.478 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.478 I llama_model_loader: - type  f32:  194 tensors
0.00.024.479 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.479 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.510 I llm_load_vocab: special tokens cache size = 25
0.00.050.345 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.348 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.348 I llm_load_print_meta: arch             = gptneox
0.00.050.349 I llm_load_print_meta: vocab type       = BPE
0.00.050.349 I llm_load_print_meta: n_vocab          = 50304
0.00.050.349 I llm_load_print_meta: n_merges         = 50009
0.00.050.349 I llm_load_print_meta: vocab_only       = 0
0.00.050.349 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.349 I llm_load_print_meta: n_embd           = 2048
0.00.050.350 I llm_load_print_meta: n_layer          = 24
0.00.050.352 I llm_load_print_meta: n_head           = 16
0.00.050.353 I llm_load_print_meta: n_head_kv        = 16
0.00.050.353 I llm_load_print_meta: n_rot            = 32
0.00.050.353 I llm_load_print_meta: n_swa            = 0
0.00.050.354 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.354 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.355 I llm_load_print_meta: n_gqa            = 1
0.00.050.355 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.356 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.357 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.357 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.357 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.357 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.358 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.358 I llm_load_print_meta: n_ff             = 8192
0.00.050.361 I llm_load_print_meta: n_expert         = 0
0.00.050.361 I llm_load_print_meta: n_expert_used    = 0
0.00.050.362 I llm_load_print_meta: causal attn      = 1
0.00.050.363 I llm_load_print_meta: pooling type     = 0
0.00.050.363 I llm_load_print_meta: rope type        = 2
0.00.050.363 I llm_load_print_meta: rope scaling     = linear
0.00.050.363 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.364 I llm_load_print_meta: freq_scale_train = 1
0.00.050.364 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.364 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.364 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.364 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.364 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.365 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.365 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.377 I llm_load_print_meta: model type       = 1.4B
0.00.050.377 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.377 I llm_load_print_meta: model params     = 1.41 B
0.00.050.379 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.379 I llm_load_print_meta: general.name     = 1.4B
0.00.050.379 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.379 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.380 I llm_load_print_meta: LF token         = 128 ''
0.00.050.381 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.381 I llm_load_print_meta: max token length = 1024
0.00.052.361 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.362 I llm_load_tensors: offloading output layer to GPU
0.00.052.362 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.372 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.373 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.275 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.275 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.276 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.276 I llama_new_context_with_model: n_batch       = 2048
0.00.053.276 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.276 I llama_new_context_with_model: flash_attn    = 0
0.00.053.276 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.277 I llama_new_context_with_model: freq_scale    = 1
0.00.053.277 I ggml_metal_init: allocating
0.00.053.280 I ggml_metal_init: found device: Apple M4
0.00.053.282 I ggml_metal_init: picking default device: Apple M4
0.00.053.842 I ggml_metal_init: using embedded metal library
0.00.055.756 I ggml_metal_init: GPU name:   Apple M4
0.00.055.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.758 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.758 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.759 I ggml_metal_init: simdgroup reduction   = true
0.00.055.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.759 I ggml_metal_init: has bfloat            = true
0.00.055.760 I ggml_metal_init: use bfloat            = true
0.00.055.761 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.573 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.582 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.604 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.562 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.564 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.564 I llama_new_context_with_model: graph nodes  = 967
0.00.084.564 I llama_new_context_with_model: graph splits = 2
0.00.084.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.014 I main: llama threadpool init, n_threads = 4
0.00.736.050 I 
0.00.736.076 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.736.076 I 
0.00.736.302 I sampler seed: 1234
0.00.736.306 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.321 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.322 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.323 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.466.156 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.466.157 I llama_perf_context_print:        load time =     727.07 ms
0.01.466.157 I llama_perf_context_print: prompt eval time =      38.38 ms /     7 tokens (    5.48 ms per token,   182.39 tokens per second)
0.01.466.158 I llama_perf_context_print:        eval time =     688.51 ms /    63 runs   (   10.93 ms per token,    91.50 tokens per second)
0.01.466.158 I llama_perf_context_print:       total time =     730.14 ms /    70 tokens
0.01.466.345 I ggml_metal_free: deallocating

real	0m1.483s
user	0m0.107s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.174 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.589 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.593 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.595 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.596 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.596 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.597 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.597 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.598 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.599 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.602 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.451 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.480 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.420 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.421 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.422 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.422 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.423 I llama_model_loader: - type  f32:  194 tensors
0.00.024.424 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.424 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.819 I llm_load_vocab: special tokens cache size = 25
0.00.050.723 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.726 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.726 I llm_load_print_meta: arch             = gptneox
0.00.050.726 I llm_load_print_meta: vocab type       = BPE
0.00.050.726 I llm_load_print_meta: n_vocab          = 50304
0.00.050.727 I llm_load_print_meta: n_merges         = 50009
0.00.050.727 I llm_load_print_meta: vocab_only       = 0
0.00.050.727 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.727 I llm_load_print_meta: n_embd           = 2048
0.00.050.727 I llm_load_print_meta: n_layer          = 24
0.00.050.730 I llm_load_print_meta: n_head           = 16
0.00.050.731 I llm_load_print_meta: n_head_kv        = 16
0.00.050.733 I llm_load_print_meta: n_rot            = 32
0.00.050.733 I llm_load_print_meta: n_swa            = 0
0.00.050.733 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.734 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.735 I llm_load_print_meta: n_gqa            = 1
0.00.050.740 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.741 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.741 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.742 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.742 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.742 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.744 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.744 I llm_load_print_meta: n_ff             = 8192
0.00.050.744 I llm_load_print_meta: n_expert         = 0
0.00.050.745 I llm_load_print_meta: n_expert_used    = 0
0.00.050.745 I llm_load_print_meta: causal attn      = 1
0.00.050.745 I llm_load_print_meta: pooling type     = 0
0.00.050.745 I llm_load_print_meta: rope type        = 2
0.00.050.745 I llm_load_print_meta: rope scaling     = linear
0.00.050.746 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.746 I llm_load_print_meta: freq_scale_train = 1
0.00.050.746 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.746 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.747 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.747 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.747 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.750 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.750 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.761 I llm_load_print_meta: model type       = 1.4B
0.00.050.762 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.762 I llm_load_print_meta: model params     = 1.41 B
0.00.050.762 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.763 I llm_load_print_meta: general.name     = 1.4B
0.00.050.763 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.763 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.764 I llm_load_print_meta: LF token         = 128 ''
0.00.050.764 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.764 I llm_load_print_meta: max token length = 1024
0.00.052.265 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.266 I llm_load_tensors: offloading output layer to GPU
0.00.052.266 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.275 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.276 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.096 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.097 I llama_new_context_with_model: n_ctx         = 128
0.00.053.097 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.097 I llama_new_context_with_model: n_batch       = 128
0.00.053.097 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.097 I llama_new_context_with_model: flash_attn    = 0
0.00.053.098 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.098 I llama_new_context_with_model: freq_scale    = 1
0.00.053.098 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.098 I ggml_metal_init: allocating
0.00.053.103 I ggml_metal_init: found device: Apple M4
0.00.053.105 I ggml_metal_init: picking default device: Apple M4
0.00.053.660 I ggml_metal_init: using embedded metal library
0.00.055.555 I ggml_metal_init: GPU name:   Apple M4
0.00.055.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.556 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.557 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.557 I ggml_metal_init: simdgroup reduction   = true
0.00.055.557 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.557 I ggml_metal_init: has bfloat            = true
0.00.055.557 I ggml_metal_init: use bfloat            = true
0.00.055.558 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.558 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.689 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.691 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.705 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.569 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.570 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.570 I llama_new_context_with_model: graph nodes  = 967
0.00.065.570 I llama_new_context_with_model: graph splits = 2
0.00.065.582 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.042 I 
0.00.689.073 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.689.077 I perplexity: tokenizing the input ..
0.00.697.002 I perplexity: tokenization took 7.923 ms
0.00.697.005 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.835 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.820.975 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.821.010 I llama_perf_context_print:        load time =     678.87 ms
0.00.821.012 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.05 tokens per second)
0.00.821.012 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.013 I llama_perf_context_print:       total time =     131.97 ms /   129 tokens
0.00.821.470 I ggml_metal_free: deallocating

real	0m0.834s
user	0m0.076s
sys	0m0.122s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.015.397 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.805 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.031.809 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.815 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.816 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.818 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.819 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.820 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.824 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.827 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.828 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.829 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.829 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.036.505 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.037.758 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.342 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.042.343 I llama_model_loader: - type  f32:  194 tensors
0.00.042.343 I llama_model_loader: - type q5_0:   97 tensors
0.00.042.344 I llama_model_loader: - type q6_K:    1 tensors
0.00.070.294 I llm_load_vocab: special tokens cache size = 25
0.00.080.017 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.080.021 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.080.021 I llm_load_print_meta: arch             = gptneox
0.00.080.022 I llm_load_print_meta: vocab type       = BPE
0.00.080.022 I llm_load_print_meta: n_vocab          = 50304
0.00.080.022 I llm_load_print_meta: n_merges         = 50009
0.00.080.023 I llm_load_print_meta: vocab_only       = 0
0.00.080.023 I llm_load_print_meta: n_ctx_train      = 2048
0.00.080.023 I llm_load_print_meta: n_embd           = 2048
0.00.080.023 I llm_load_print_meta: n_layer          = 24
0.00.080.027 I llm_load_print_meta: n_head           = 16
0.00.080.028 I llm_load_print_meta: n_head_kv        = 16
0.00.080.029 I llm_load_print_meta: n_rot            = 32
0.00.080.029 I llm_load_print_meta: n_swa            = 0
0.00.080.029 I llm_load_print_meta: n_embd_head_k    = 128
0.00.080.029 I llm_load_print_meta: n_embd_head_v    = 128
0.00.080.031 I llm_load_print_meta: n_gqa            = 1
0.00.080.032 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.080.033 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.080.033 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.080.034 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.080.034 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.080.037 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.080.037 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.080.038 I llm_load_print_meta: n_ff             = 8192
0.00.080.038 I llm_load_print_meta: n_expert         = 0
0.00.080.038 I llm_load_print_meta: n_expert_used    = 0
0.00.080.038 I llm_load_print_meta: causal attn      = 1
0.00.080.038 I llm_load_print_meta: pooling type     = 0
0.00.080.039 I llm_load_print_meta: rope type        = 2
0.00.080.039 I llm_load_print_meta: rope scaling     = linear
0.00.080.039 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.080.040 I llm_load_print_meta: freq_scale_train = 1
0.00.080.040 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.080.040 I llm_load_print_meta: rope_finetuned   = unknown
0.00.080.041 I llm_load_print_meta: ssm_d_conv       = 0
0.00.080.041 I llm_load_print_meta: ssm_d_inner      = 0
0.00.080.041 I llm_load_print_meta: ssm_d_state      = 0
0.00.080.041 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.080.041 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.080.054 I llm_load_print_meta: model type       = 1.4B
0.00.080.054 I llm_load_print_meta: model ftype      = Q5_0
0.00.080.057 I llm_load_print_meta: model params     = 1.41 B
0.00.080.058 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.080.058 I llm_load_print_meta: general.name     = 1.4B
0.00.080.059 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.080.059 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.080.059 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.080.059 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.080.060 I llm_load_print_meta: LF token         = 128 ''
0.00.080.060 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.080.062 I llm_load_print_meta: max token length = 1024
0.00.082.831 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.831 I llm_load_tensors: offloading output layer to GPU
0.00.082.832 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.842 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.082.844 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.084.271 I llama_new_context_with_model: n_seq_max     = 1
0.00.084.272 I llama_new_context_with_model: n_ctx         = 2048
0.00.084.272 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.084.273 I llama_new_context_with_model: n_batch       = 2048
0.00.084.273 I llama_new_context_with_model: n_ubatch      = 512
0.00.084.273 I llama_new_context_with_model: flash_attn    = 0
0.00.084.274 I llama_new_context_with_model: freq_base     = 10000.0
0.00.084.274 I llama_new_context_with_model: freq_scale    = 1
0.00.084.275 I ggml_metal_init: allocating
0.00.084.280 I ggml_metal_init: found device: Apple M4
0.00.084.282 I ggml_metal_init: picking default device: Apple M4
0.00.085.107 I ggml_metal_init: using embedded metal library
0.00.088.119 I ggml_metal_init: GPU name:   Apple M4
0.00.088.121 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.088.122 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.088.122 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.088.123 I ggml_metal_init: simdgroup reduction   = true
0.00.088.123 I ggml_metal_init: simdgroup matrix mul. = true
0.00.088.123 I ggml_metal_init: has bfloat            = true
0.00.088.123 I ggml_metal_init: use bfloat            = true
0.00.088.124 I ggml_metal_init: hasUnifiedMemory      = true
0.00.088.124 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.121.668 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.674 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.700 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.122.681 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.122.683 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.122.683 I llama_new_context_with_model: graph nodes  = 967
0.00.122.684 I llama_new_context_with_model: graph splits = 2
0.00.122.698 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.889.133 I main: llama threadpool init, n_threads = 4
0.00.889.172 I 
0.00.889.201 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.889.202 I 
0.00.889.426 I sampler seed: 1234
0.00.889.431 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.889.446 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.889.448 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.889.448 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.674.104 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.674.104 I llama_perf_context_print:        load time =     873.73 ms
0.01.674.105 I llama_perf_context_print: prompt eval time =      36.61 ms /     7 tokens (    5.23 ms per token,   191.20 tokens per second)
0.01.674.106 I llama_perf_context_print:        eval time =     744.99 ms /    63 runs   (   11.83 ms per token,    84.56 tokens per second)
0.01.674.106 I llama_perf_context_print:       total time =     784.98 ms /    70 tokens
0.01.674.292 I ggml_metal_free: deallocating

real	0m1.695s
user	0m0.126s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.890 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.288 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.294 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.295 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.295 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.296 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.296 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.297 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.297 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.297 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.299 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.299 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.299 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.166 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.217 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.217 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.218 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.219 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.219 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.219 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.219 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.220 I llama_model_loader: - type  f32:  194 tensors
0.00.024.220 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.220 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.802 I llm_load_vocab: special tokens cache size = 25
0.00.049.675 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.678 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.678 I llm_load_print_meta: arch             = gptneox
0.00.049.678 I llm_load_print_meta: vocab type       = BPE
0.00.049.679 I llm_load_print_meta: n_vocab          = 50304
0.00.049.679 I llm_load_print_meta: n_merges         = 50009
0.00.049.679 I llm_load_print_meta: vocab_only       = 0
0.00.049.679 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.679 I llm_load_print_meta: n_embd           = 2048
0.00.049.680 I llm_load_print_meta: n_layer          = 24
0.00.049.682 I llm_load_print_meta: n_head           = 16
0.00.049.683 I llm_load_print_meta: n_head_kv        = 16
0.00.049.683 I llm_load_print_meta: n_rot            = 32
0.00.049.684 I llm_load_print_meta: n_swa            = 0
0.00.049.684 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.684 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.685 I llm_load_print_meta: n_gqa            = 1
0.00.049.685 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.686 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.687 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.687 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.687 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.687 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.687 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.688 I llm_load_print_meta: n_ff             = 8192
0.00.049.688 I llm_load_print_meta: n_expert         = 0
0.00.049.688 I llm_load_print_meta: n_expert_used    = 0
0.00.049.688 I llm_load_print_meta: causal attn      = 1
0.00.049.689 I llm_load_print_meta: pooling type     = 0
0.00.049.689 I llm_load_print_meta: rope type        = 2
0.00.049.689 I llm_load_print_meta: rope scaling     = linear
0.00.049.689 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.690 I llm_load_print_meta: freq_scale_train = 1
0.00.049.690 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.690 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.690 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.690 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.690 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.691 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.691 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.702 I llm_load_print_meta: model type       = 1.4B
0.00.049.702 I llm_load_print_meta: model ftype      = Q5_0
0.00.049.702 I llm_load_print_meta: model params     = 1.41 B
0.00.049.703 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.049.703 I llm_load_print_meta: general.name     = 1.4B
0.00.049.703 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.703 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.704 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.704 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.704 I llm_load_print_meta: LF token         = 128 ''
0.00.049.704 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.704 I llm_load_print_meta: max token length = 1024
0.00.051.273 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.274 I llm_load_tensors: offloading output layer to GPU
0.00.051.274 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.283 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.051.284 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.052.136 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.136 I llama_new_context_with_model: n_ctx         = 128
0.00.052.137 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.137 I llama_new_context_with_model: n_batch       = 128
0.00.052.137 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.137 I llama_new_context_with_model: flash_attn    = 0
0.00.052.138 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.138 I llama_new_context_with_model: freq_scale    = 1
0.00.052.138 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.138 I ggml_metal_init: allocating
0.00.052.141 I ggml_metal_init: found device: Apple M4
0.00.052.143 I ggml_metal_init: picking default device: Apple M4
0.00.052.684 I ggml_metal_init: using embedded metal library
0.00.054.603 I ggml_metal_init: GPU name:   Apple M4
0.00.054.605 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.605 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.606 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.606 I ggml_metal_init: simdgroup reduction   = true
0.00.054.606 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.606 I ggml_metal_init: has bfloat            = true
0.00.054.606 I ggml_metal_init: use bfloat            = true
0.00.054.607 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.608 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.754 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.756 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.771 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.593 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.594 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.594 I llama_new_context_with_model: graph nodes  = 967
0.00.064.594 I llama_new_context_with_model: graph splits = 2
0.00.064.606 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.765 I 
0.00.734.794 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.734.799 I perplexity: tokenizing the input ..
0.00.742.436 I perplexity: tokenization took 7.636 ms
0.00.742.440 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.706 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.878.871 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.878.900 I llama_perf_context_print:        load time =     724.87 ms
0.00.878.905 I llama_perf_context_print: prompt eval time =     135.04 ms /   128 tokens (    1.05 ms per token,   947.87 tokens per second)
0.00.878.906 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.878.906 I llama_perf_context_print:       total time =     144.14 ms /   129 tokens
0.00.879.352 I ggml_metal_free: deallocating

real	0m0.895s
user	0m0.075s
sys	0m0.129s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.008.667 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.019 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.030 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.032 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.032 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.033 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.033 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.034 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.034 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.035 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.035 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.035 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.036 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.036 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.036 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.038 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.039 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.005 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.184 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.109 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.111 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.111 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.112 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.112 I llama_model_loader: - type  f32:  194 tensors
0.00.024.113 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.113 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.996 I llm_load_vocab: special tokens cache size = 25
0.00.049.837 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.840 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.840 I llm_load_print_meta: arch             = gptneox
0.00.049.841 I llm_load_print_meta: vocab type       = BPE
0.00.049.841 I llm_load_print_meta: n_vocab          = 50304
0.00.049.841 I llm_load_print_meta: n_merges         = 50009
0.00.049.841 I llm_load_print_meta: vocab_only       = 0
0.00.049.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.842 I llm_load_print_meta: n_embd           = 2048
0.00.049.842 I llm_load_print_meta: n_layer          = 24
0.00.049.845 I llm_load_print_meta: n_head           = 16
0.00.049.846 I llm_load_print_meta: n_head_kv        = 16
0.00.049.846 I llm_load_print_meta: n_rot            = 32
0.00.049.846 I llm_load_print_meta: n_swa            = 0
0.00.049.846 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.846 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.847 I llm_load_print_meta: n_gqa            = 1
0.00.049.848 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.849 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.849 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.850 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.850 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.850 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.850 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.851 I llm_load_print_meta: n_ff             = 8192
0.00.049.851 I llm_load_print_meta: n_expert         = 0
0.00.049.851 I llm_load_print_meta: n_expert_used    = 0
0.00.049.851 I llm_load_print_meta: causal attn      = 1
0.00.049.852 I llm_load_print_meta: pooling type     = 0
0.00.049.852 I llm_load_print_meta: rope type        = 2
0.00.049.852 I llm_load_print_meta: rope scaling     = linear
0.00.049.853 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.853 I llm_load_print_meta: freq_scale_train = 1
0.00.049.853 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.853 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.853 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.854 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.854 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.854 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.854 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.866 I llm_load_print_meta: model type       = 1.4B
0.00.049.866 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.867 I llm_load_print_meta: model params     = 1.41 B
0.00.049.867 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.867 I llm_load_print_meta: general.name     = 1.4B
0.00.049.868 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.868 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.870 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.870 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.870 I llm_load_print_meta: LF token         = 128 ''
0.00.049.871 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.871 I llm_load_print_meta: max token length = 1024
0.00.051.827 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.827 I llm_load_tensors: offloading output layer to GPU
0.00.051.827 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.837 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.838 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.780 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.781 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.781 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.781 I llama_new_context_with_model: n_batch       = 2048
0.00.052.782 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.782 I llama_new_context_with_model: flash_attn    = 0
0.00.052.782 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.782 I llama_new_context_with_model: freq_scale    = 1
0.00.052.783 I ggml_metal_init: allocating
0.00.052.789 I ggml_metal_init: found device: Apple M4
0.00.052.791 I ggml_metal_init: picking default device: Apple M4
0.00.053.361 I ggml_metal_init: using embedded metal library
0.00.055.288 I ggml_metal_init: GPU name:   Apple M4
0.00.055.289 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.290 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.290 I ggml_metal_init: simdgroup reduction   = true
0.00.055.290 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.291 I ggml_metal_init: has bfloat            = true
0.00.055.291 I ggml_metal_init: use bfloat            = true
0.00.055.291 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.099 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.106 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.123 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.059 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.060 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.061 I llama_new_context_with_model: graph nodes  = 967
0.00.083.061 I llama_new_context_with_model: graph splits = 2
0.00.083.074 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.721.772 I main: llama threadpool init, n_threads = 4
0.00.721.807 I 
0.00.721.849 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.721.850 I 
0.00.721.983 I sampler seed: 1234
0.00.721.988 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.722.001 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.722.002 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.722.002 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.560.722 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60477.00 tokens per second)
0.01.560.723 I llama_perf_context_print:        load time =     713.10 ms
0.01.560.724 I llama_perf_context_print: prompt eval time =      36.50 ms /     7 tokens (    5.21 ms per token,   191.77 tokens per second)
0.01.560.725 I llama_perf_context_print:        eval time =     799.30 ms /    63 runs   (   12.69 ms per token,    78.82 tokens per second)
0.01.560.725 I llama_perf_context_print:       total time =     838.95 ms /    70 tokens
0.01.560.900 I ggml_metal_free: deallocating

real	0m1.576s
user	0m0.107s
sys	0m0.162s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.680 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.164 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.168 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.170 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.170 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.171 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.171 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.171 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.172 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.173 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.114 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.778 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.753 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.754 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.754 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.754 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.755 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.755 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.756 I llama_model_loader: - type  f32:  194 tensors
0.00.023.756 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.756 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.113 I llm_load_vocab: special tokens cache size = 25
0.00.050.016 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.019 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.019 I llm_load_print_meta: arch             = gptneox
0.00.050.019 I llm_load_print_meta: vocab type       = BPE
0.00.050.019 I llm_load_print_meta: n_vocab          = 50304
0.00.050.019 I llm_load_print_meta: n_merges         = 50009
0.00.050.020 I llm_load_print_meta: vocab_only       = 0
0.00.050.020 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.020 I llm_load_print_meta: n_embd           = 2048
0.00.050.020 I llm_load_print_meta: n_layer          = 24
0.00.050.023 I llm_load_print_meta: n_head           = 16
0.00.050.024 I llm_load_print_meta: n_head_kv        = 16
0.00.050.024 I llm_load_print_meta: n_rot            = 32
0.00.050.024 I llm_load_print_meta: n_swa            = 0
0.00.050.024 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.024 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.025 I llm_load_print_meta: n_gqa            = 1
0.00.050.026 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.027 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.027 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.028 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.028 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.028 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.028 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.029 I llm_load_print_meta: n_ff             = 8192
0.00.050.029 I llm_load_print_meta: n_expert         = 0
0.00.050.029 I llm_load_print_meta: n_expert_used    = 0
0.00.050.029 I llm_load_print_meta: causal attn      = 1
0.00.050.029 I llm_load_print_meta: pooling type     = 0
0.00.050.029 I llm_load_print_meta: rope type        = 2
0.00.050.030 I llm_load_print_meta: rope scaling     = linear
0.00.050.030 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.030 I llm_load_print_meta: freq_scale_train = 1
0.00.050.031 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.031 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.031 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.031 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.031 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.033 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.034 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.044 I llm_load_print_meta: model type       = 1.4B
0.00.050.045 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.045 I llm_load_print_meta: model params     = 1.41 B
0.00.050.046 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.047 I llm_load_print_meta: general.name     = 1.4B
0.00.050.047 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.047 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.047 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.047 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.048 I llm_load_print_meta: LF token         = 128 ''
0.00.050.048 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.048 I llm_load_print_meta: max token length = 1024
0.00.051.607 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.607 I llm_load_tensors: offloading output layer to GPU
0.00.051.608 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.617 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.618 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.444 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.445 I llama_new_context_with_model: n_ctx         = 128
0.00.052.445 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.445 I llama_new_context_with_model: n_batch       = 128
0.00.052.445 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.446 I llama_new_context_with_model: flash_attn    = 0
0.00.052.446 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.446 I llama_new_context_with_model: freq_scale    = 1
0.00.052.447 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.447 I ggml_metal_init: allocating
0.00.052.450 I ggml_metal_init: found device: Apple M4
0.00.052.452 I ggml_metal_init: picking default device: Apple M4
0.00.052.981 I ggml_metal_init: using embedded metal library
0.00.054.919 I ggml_metal_init: GPU name:   Apple M4
0.00.054.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.922 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.922 I ggml_metal_init: simdgroup reduction   = true
0.00.054.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.922 I ggml_metal_init: has bfloat            = true
0.00.054.922 I ggml_metal_init: use bfloat            = true
0.00.054.923 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.923 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.994 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.998 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.012 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.839 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.841 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.841 I llama_new_context_with_model: graph nodes  = 967
0.00.064.841 I llama_new_context_with_model: graph splits = 2
0.00.064.854 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.677.983 I 
0.00.678.027 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.678.037 I perplexity: tokenizing the input ..
0.00.686.019 I perplexity: tokenization took 7.98 ms
0.00.686.023 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.258 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.821.406 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.821.435 I llama_perf_context_print:        load time =     669.29 ms
0.00.821.436 I llama_perf_context_print: prompt eval time =     134.01 ms /   128 tokens (    1.05 ms per token,   955.15 tokens per second)
0.00.821.437 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.821.438 I llama_perf_context_print:       total time =     143.46 ms /   129 tokens
0.00.821.845 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.077s
sys	0m0.132s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.927 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.448 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.452 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.454 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.454 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.455 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.455 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.455 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.458 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.458 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.459 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.459 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.460 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.461 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.461 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.462 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.554 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.661 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.707 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.709 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.710 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.710 I llama_model_loader: - type  f32:  194 tensors
0.00.024.711 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.711 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.711 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.668 I llm_load_vocab: special tokens cache size = 25
0.00.051.648 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.651 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.652 I llm_load_print_meta: arch             = gptneox
0.00.051.652 I llm_load_print_meta: vocab type       = BPE
0.00.051.652 I llm_load_print_meta: n_vocab          = 50304
0.00.051.653 I llm_load_print_meta: n_merges         = 50009
0.00.051.653 I llm_load_print_meta: vocab_only       = 0
0.00.051.653 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.653 I llm_load_print_meta: n_embd           = 2048
0.00.051.653 I llm_load_print_meta: n_layer          = 24
0.00.051.656 I llm_load_print_meta: n_head           = 16
0.00.051.657 I llm_load_print_meta: n_head_kv        = 16
0.00.051.657 I llm_load_print_meta: n_rot            = 32
0.00.051.657 I llm_load_print_meta: n_swa            = 0
0.00.051.657 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.657 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.659 I llm_load_print_meta: n_gqa            = 1
0.00.051.660 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.661 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.661 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.661 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.662 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.662 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.662 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.663 I llm_load_print_meta: n_ff             = 8192
0.00.051.663 I llm_load_print_meta: n_expert         = 0
0.00.051.663 I llm_load_print_meta: n_expert_used    = 0
0.00.051.665 I llm_load_print_meta: causal attn      = 1
0.00.051.665 I llm_load_print_meta: pooling type     = 0
0.00.051.665 I llm_load_print_meta: rope type        = 2
0.00.051.665 I llm_load_print_meta: rope scaling     = linear
0.00.051.666 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.666 I llm_load_print_meta: freq_scale_train = 1
0.00.051.666 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.666 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.667 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.667 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.667 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.667 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.667 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.679 I llm_load_print_meta: model type       = 1.4B
0.00.051.679 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.680 I llm_load_print_meta: model params     = 1.41 B
0.00.051.681 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.682 I llm_load_print_meta: general.name     = 1.4B
0.00.051.682 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.682 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.682 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.682 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.683 I llm_load_print_meta: LF token         = 128 ''
0.00.051.684 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.684 I llm_load_print_meta: max token length = 1024
0.00.053.563 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.564 I llm_load_tensors: offloading output layer to GPU
0.00.053.564 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.574 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.575 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.512 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.512 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.513 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.513 I llama_new_context_with_model: n_batch       = 2048
0.00.054.513 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.513 I llama_new_context_with_model: flash_attn    = 0
0.00.054.513 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.514 I llama_new_context_with_model: freq_scale    = 1
0.00.054.514 I ggml_metal_init: allocating
0.00.054.517 I ggml_metal_init: found device: Apple M4
0.00.054.519 I ggml_metal_init: picking default device: Apple M4
0.00.055.077 I ggml_metal_init: using embedded metal library
0.00.057.024 I ggml_metal_init: GPU name:   Apple M4
0.00.057.025 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.026 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.026 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.026 I ggml_metal_init: simdgroup reduction   = true
0.00.057.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.026 I ggml_metal_init: has bfloat            = true
0.00.057.027 I ggml_metal_init: use bfloat            = true
0.00.057.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.028 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.567 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.576 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.593 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.585 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.586 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.586 I llama_new_context_with_model: graph nodes  = 967
0.00.086.587 I llama_new_context_with_model: graph splits = 2
0.00.086.596 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.287 I main: llama threadpool init, n_threads = 4
0.00.445.325 I 
0.00.445.357 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.445.358 I 
0.00.445.598 I sampler seed: 1234
0.00.445.603 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.445.646 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.445.650 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.445.650 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.126.870 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57119.87 tokens per second)
0.01.126.870 I llama_perf_context_print:        load time =     435.35 ms
0.01.126.871 I llama_perf_context_print: prompt eval time =      35.97 ms /     7 tokens (    5.14 ms per token,   194.62 tokens per second)
0.01.126.871 I llama_perf_context_print:        eval time =     642.12 ms /    63 runs   (   10.19 ms per token,    98.11 tokens per second)
0.01.126.872 I llama_perf_context_print:       total time =     681.59 ms /    70 tokens
0.01.127.056 I ggml_metal_free: deallocating

real	0m1.146s
user	0m0.109s
sys	0m0.110s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.970 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.289 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.293 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.296 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.297 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.297 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.298 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.299 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.299 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.301 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.301 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.263 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.353 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.255 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.256 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.257 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.257 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.257 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.258 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.258 I llama_model_loader: - type  f32:  194 tensors
0.00.023.258 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.259 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.259 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.930 I llm_load_vocab: special tokens cache size = 25
0.00.048.741 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.744 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.744 I llm_load_print_meta: arch             = gptneox
0.00.048.744 I llm_load_print_meta: vocab type       = BPE
0.00.048.744 I llm_load_print_meta: n_vocab          = 50304
0.00.048.745 I llm_load_print_meta: n_merges         = 50009
0.00.048.745 I llm_load_print_meta: vocab_only       = 0
0.00.048.745 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.745 I llm_load_print_meta: n_embd           = 2048
0.00.048.746 I llm_load_print_meta: n_layer          = 24
0.00.048.748 I llm_load_print_meta: n_head           = 16
0.00.048.749 I llm_load_print_meta: n_head_kv        = 16
0.00.048.749 I llm_load_print_meta: n_rot            = 32
0.00.048.749 I llm_load_print_meta: n_swa            = 0
0.00.048.750 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.750 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.750 I llm_load_print_meta: n_gqa            = 1
0.00.048.751 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.752 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.753 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.753 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.753 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.753 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.753 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.754 I llm_load_print_meta: n_ff             = 8192
0.00.048.754 I llm_load_print_meta: n_expert         = 0
0.00.048.754 I llm_load_print_meta: n_expert_used    = 0
0.00.048.755 I llm_load_print_meta: causal attn      = 1
0.00.048.755 I llm_load_print_meta: pooling type     = 0
0.00.048.755 I llm_load_print_meta: rope type        = 2
0.00.048.755 I llm_load_print_meta: rope scaling     = linear
0.00.048.756 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.756 I llm_load_print_meta: freq_scale_train = 1
0.00.048.756 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.756 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.760 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.760 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.760 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.760 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.760 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.771 I llm_load_print_meta: model type       = 1.4B
0.00.048.772 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.048.772 I llm_load_print_meta: model params     = 1.41 B
0.00.048.773 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.048.773 I llm_load_print_meta: general.name     = 1.4B
0.00.048.773 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.773 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.773 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.774 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.774 I llm_load_print_meta: LF token         = 128 ''
0.00.048.774 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.774 I llm_load_print_meta: max token length = 1024
0.00.050.318 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.318 I llm_load_tensors: offloading output layer to GPU
0.00.050.318 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.327 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.050.328 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.183 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.184 I llama_new_context_with_model: n_ctx         = 128
0.00.051.184 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.184 I llama_new_context_with_model: n_batch       = 128
0.00.051.184 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.184 I llama_new_context_with_model: flash_attn    = 0
0.00.051.185 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.185 I llama_new_context_with_model: freq_scale    = 1
0.00.051.185 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.186 I ggml_metal_init: allocating
0.00.051.191 I ggml_metal_init: found device: Apple M4
0.00.051.195 I ggml_metal_init: picking default device: Apple M4
0.00.051.722 I ggml_metal_init: using embedded metal library
0.00.053.655 I ggml_metal_init: GPU name:   Apple M4
0.00.053.656 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.053.656 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.053.657 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.053.657 I ggml_metal_init: simdgroup reduction   = true
0.00.053.657 I ggml_metal_init: simdgroup matrix mul. = true
0.00.053.657 I ggml_metal_init: has bfloat            = true
0.00.053.657 I ggml_metal_init: use bfloat            = true
0.00.053.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.053.658 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.062.624 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.062.626 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.062.641 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.063.510 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.063.511 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.063.511 I llama_new_context_with_model: graph nodes  = 967
0.00.063.511 I llama_new_context_with_model: graph splits = 2
0.00.063.523 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.393.089 I 
0.00.393.125 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.393.129 I perplexity: tokenizing the input ..
0.00.401.277 I perplexity: tokenization took 8.148 ms
0.00.401.283 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.533.834 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.534.999 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.535.026 I llama_perf_context_print:        load time =     384.11 ms
0.00.535.027 I llama_perf_context_print: prompt eval time =     132.32 ms /   128 tokens (    1.03 ms per token,   967.36 tokens per second)
0.00.535.028 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.535.029 I llama_perf_context_print:       total time =     141.94 ms /   129 tokens
0.00.535.523 I ggml_metal_free: deallocating

real	0m0.552s
user	0m0.076s
sys	0m0.074s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.011.035 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.355 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.360 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.361 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.362 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.362 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.362 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.363 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.364 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.365 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.365 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.365 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.365 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.367 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.367 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.367 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.385 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.435 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.471 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.473 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.473 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.473 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.473 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.474 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.474 I llama_model_loader: - type  f32:  194 tensors
0.00.025.475 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.475 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.475 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.475 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.341 I llm_load_vocab: special tokens cache size = 25
0.00.051.124 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.127 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.127 I llm_load_print_meta: arch             = gptneox
0.00.051.128 I llm_load_print_meta: vocab type       = BPE
0.00.051.128 I llm_load_print_meta: n_vocab          = 50304
0.00.051.128 I llm_load_print_meta: n_merges         = 50009
0.00.051.128 I llm_load_print_meta: vocab_only       = 0
0.00.051.128 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.129 I llm_load_print_meta: n_embd           = 2048
0.00.051.129 I llm_load_print_meta: n_layer          = 24
0.00.051.131 I llm_load_print_meta: n_head           = 16
0.00.051.132 I llm_load_print_meta: n_head_kv        = 16
0.00.051.132 I llm_load_print_meta: n_rot            = 32
0.00.051.132 I llm_load_print_meta: n_swa            = 0
0.00.051.133 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.133 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.134 I llm_load_print_meta: n_gqa            = 1
0.00.051.134 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.137 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.138 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.138 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.138 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.139 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.139 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.139 I llm_load_print_meta: n_ff             = 8192
0.00.051.140 I llm_load_print_meta: n_expert         = 0
0.00.051.141 I llm_load_print_meta: n_expert_used    = 0
0.00.051.141 I llm_load_print_meta: causal attn      = 1
0.00.051.141 I llm_load_print_meta: pooling type     = 0
0.00.051.142 I llm_load_print_meta: rope type        = 2
0.00.051.142 I llm_load_print_meta: rope scaling     = linear
0.00.051.142 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.142 I llm_load_print_meta: freq_scale_train = 1
0.00.051.143 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.143 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.143 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.143 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.143 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.143 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.144 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.156 I llm_load_print_meta: model type       = 1.4B
0.00.051.156 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.157 I llm_load_print_meta: model params     = 1.41 B
0.00.051.157 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.157 I llm_load_print_meta: general.name     = 1.4B
0.00.051.158 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.158 I llm_load_print_meta: LF token         = 128 ''
0.00.051.159 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.159 I llm_load_print_meta: max token length = 1024
0.00.053.008 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.008 I llm_load_tensors: offloading output layer to GPU
0.00.053.009 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.018 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.019 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.902 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.903 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.903 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.903 I llama_new_context_with_model: n_batch       = 2048
0.00.053.903 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.904 I llama_new_context_with_model: flash_attn    = 0
0.00.053.904 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.904 I llama_new_context_with_model: freq_scale    = 1
0.00.053.905 I ggml_metal_init: allocating
0.00.053.908 I ggml_metal_init: found device: Apple M4
0.00.053.910 I ggml_metal_init: picking default device: Apple M4
0.00.054.439 I ggml_metal_init: using embedded metal library
0.00.056.328 I ggml_metal_init: GPU name:   Apple M4
0.00.056.329 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.330 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.330 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.330 I ggml_metal_init: simdgroup reduction   = true
0.00.056.330 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.330 I ggml_metal_init: has bfloat            = true
0.00.056.331 I ggml_metal_init: use bfloat            = true
0.00.056.331 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.331 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.141 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.152 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.172 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.084 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.085 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.085 I llama_new_context_with_model: graph nodes  = 967
0.00.084.085 I llama_new_context_with_model: graph splits = 2
0.00.084.094 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.539.409 I main: llama threadpool init, n_threads = 4
0.00.539.452 I 
0.00.539.479 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.539.480 I 
0.00.539.721 I sampler seed: 1234
0.00.539.727 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.539.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.539.773 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.539.773 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.286.777 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58726.22 tokens per second)
0.01.286.777 I llama_perf_context_print:        load time =     528.37 ms
0.01.286.778 I llama_perf_context_print: prompt eval time =      39.52 ms /     7 tokens (    5.65 ms per token,   177.11 tokens per second)
0.01.286.779 I llama_perf_context_print:        eval time =     704.45 ms /    63 runs   (   11.18 ms per token,    89.43 tokens per second)
0.01.286.779 I llama_perf_context_print:       total time =     747.37 ms /    70 tokens
0.01.286.975 I ggml_metal_free: deallocating

real	0m1.303s
user	0m0.106s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.813 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.420 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.425 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.427 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.427 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.428 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.428 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.429 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.429 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.430 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.430 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.418 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.456 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.437 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.438 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.438 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.439 I llama_model_loader: - type  f32:  194 tensors
0.00.023.439 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.439 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.440 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.440 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.111 I llm_load_vocab: special tokens cache size = 25
0.00.050.009 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.011 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.011 I llm_load_print_meta: arch             = gptneox
0.00.050.012 I llm_load_print_meta: vocab type       = BPE
0.00.050.012 I llm_load_print_meta: n_vocab          = 50304
0.00.050.012 I llm_load_print_meta: n_merges         = 50009
0.00.050.012 I llm_load_print_meta: vocab_only       = 0
0.00.050.013 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.013 I llm_load_print_meta: n_embd           = 2048
0.00.050.013 I llm_load_print_meta: n_layer          = 24
0.00.050.016 I llm_load_print_meta: n_head           = 16
0.00.050.017 I llm_load_print_meta: n_head_kv        = 16
0.00.050.017 I llm_load_print_meta: n_rot            = 32
0.00.050.017 I llm_load_print_meta: n_swa            = 0
0.00.050.017 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.020 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.020 I llm_load_print_meta: n_gqa            = 1
0.00.050.021 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.022 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.022 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.023 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.023 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.023 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.023 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.024 I llm_load_print_meta: n_ff             = 8192
0.00.050.025 I llm_load_print_meta: n_expert         = 0
0.00.050.025 I llm_load_print_meta: n_expert_used    = 0
0.00.050.026 I llm_load_print_meta: causal attn      = 1
0.00.050.026 I llm_load_print_meta: pooling type     = 0
0.00.050.026 I llm_load_print_meta: rope type        = 2
0.00.050.026 I llm_load_print_meta: rope scaling     = linear
0.00.050.027 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.027 I llm_load_print_meta: freq_scale_train = 1
0.00.050.027 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.027 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.027 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.028 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.028 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.028 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.028 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.040 I llm_load_print_meta: model type       = 1.4B
0.00.050.040 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.040 I llm_load_print_meta: model params     = 1.41 B
0.00.050.041 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.041 I llm_load_print_meta: general.name     = 1.4B
0.00.050.041 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.041 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.042 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.042 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.042 I llm_load_print_meta: LF token         = 128 ''
0.00.050.042 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.042 I llm_load_print_meta: max token length = 1024
0.00.051.927 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.927 I llm_load_tensors: offloading output layer to GPU
0.00.051.927 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.937 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.938 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.850 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.851 I llama_new_context_with_model: n_ctx         = 128
0.00.052.851 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.851 I llama_new_context_with_model: n_batch       = 128
0.00.052.851 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.852 I llama_new_context_with_model: flash_attn    = 0
0.00.052.852 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.852 I llama_new_context_with_model: freq_scale    = 1
0.00.052.852 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.853 I ggml_metal_init: allocating
0.00.052.856 I ggml_metal_init: found device: Apple M4
0.00.052.858 I ggml_metal_init: picking default device: Apple M4
0.00.053.405 I ggml_metal_init: using embedded metal library
0.00.055.303 I ggml_metal_init: GPU name:   Apple M4
0.00.055.305 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.305 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.305 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.306 I ggml_metal_init: simdgroup reduction   = true
0.00.055.306 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.306 I ggml_metal_init: has bfloat            = true
0.00.055.306 I ggml_metal_init: use bfloat            = true
0.00.055.307 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.307 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.456 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.459 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.472 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.414 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.415 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.415 I llama_new_context_with_model: graph nodes  = 967
0.00.065.416 I llama_new_context_with_model: graph splits = 2
0.00.065.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.491.031 I 
0.00.491.064 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.491.068 I perplexity: tokenizing the input ..
0.00.498.693 I perplexity: tokenization took 7.622 ms
0.00.498.696 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.630.780 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.631.949 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.631.975 I llama_perf_context_print:        load time =     482.21 ms
0.00.631.976 I llama_perf_context_print: prompt eval time =     131.83 ms /   128 tokens (    1.03 ms per token,   970.95 tokens per second)
0.00.631.977 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.631.977 I llama_perf_context_print:       total time =     140.94 ms /   129 tokens
0.00.632.522 I ggml_metal_free: deallocating

real	0m0.645s
user	0m0.077s
sys	0m0.093s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.688 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.151 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.162 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.163 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.163 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.164 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.165 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.165 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.166 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.166 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.166 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.167 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.167 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.169 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.169 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.171 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.364 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.423 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.466 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.467 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.468 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.469 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.469 I llama_model_loader: - type  f32:  194 tensors
0.00.024.470 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.470 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.470 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.158 I llm_load_vocab: special tokens cache size = 25
0.00.051.043 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.046 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.047 I llm_load_print_meta: arch             = gptneox
0.00.051.047 I llm_load_print_meta: vocab type       = BPE
0.00.051.047 I llm_load_print_meta: n_vocab          = 50304
0.00.051.047 I llm_load_print_meta: n_merges         = 50009
0.00.051.048 I llm_load_print_meta: vocab_only       = 0
0.00.051.048 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.048 I llm_load_print_meta: n_embd           = 2048
0.00.051.048 I llm_load_print_meta: n_layer          = 24
0.00.051.051 I llm_load_print_meta: n_head           = 16
0.00.051.052 I llm_load_print_meta: n_head_kv        = 16
0.00.051.052 I llm_load_print_meta: n_rot            = 32
0.00.051.052 I llm_load_print_meta: n_swa            = 0
0.00.051.052 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.052 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.053 I llm_load_print_meta: n_gqa            = 1
0.00.051.054 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.055 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.055 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.056 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.056 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.056 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.056 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.057 I llm_load_print_meta: n_ff             = 8192
0.00.051.057 I llm_load_print_meta: n_expert         = 0
0.00.051.057 I llm_load_print_meta: n_expert_used    = 0
0.00.051.057 I llm_load_print_meta: causal attn      = 1
0.00.051.058 I llm_load_print_meta: pooling type     = 0
0.00.051.058 I llm_load_print_meta: rope type        = 2
0.00.051.058 I llm_load_print_meta: rope scaling     = linear
0.00.051.058 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.059 I llm_load_print_meta: freq_scale_train = 1
0.00.051.059 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.059 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.059 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.059 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.059 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.060 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.060 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.072 I llm_load_print_meta: model type       = 1.4B
0.00.051.072 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.072 I llm_load_print_meta: model params     = 1.41 B
0.00.051.075 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.075 I llm_load_print_meta: general.name     = 1.4B
0.00.051.075 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.075 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.075 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.075 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.076 I llm_load_print_meta: LF token         = 128 ''
0.00.051.076 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.076 I llm_load_print_meta: max token length = 1024
0.00.053.019 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.019 I llm_load_tensors: offloading output layer to GPU
0.00.053.019 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.029 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.031 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.963 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.964 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.964 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.964 I llama_new_context_with_model: n_batch       = 2048
0.00.053.964 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.964 I llama_new_context_with_model: flash_attn    = 0
0.00.053.965 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.965 I llama_new_context_with_model: freq_scale    = 1
0.00.053.966 I ggml_metal_init: allocating
0.00.053.970 I ggml_metal_init: found device: Apple M4
0.00.053.972 I ggml_metal_init: picking default device: Apple M4
0.00.054.509 I ggml_metal_init: using embedded metal library
0.00.056.452 I ggml_metal_init: GPU name:   Apple M4
0.00.056.454 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.454 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.454 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.454 I ggml_metal_init: simdgroup reduction   = true
0.00.056.455 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.455 I ggml_metal_init: has bfloat            = true
0.00.056.455 I ggml_metal_init: use bfloat            = true
0.00.056.455 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.456 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.907 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.918 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.941 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.909 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.910 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.910 I llama_new_context_with_model: graph nodes  = 967
0.00.084.911 I llama_new_context_with_model: graph splits = 2
0.00.084.924 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.629.960 I main: llama threadpool init, n_threads = 4
0.00.630.003 I 
0.00.630.046 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.630.048 I 
0.00.630.275 I sampler seed: 1234
0.00.630.280 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.630.315 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.630.317 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.630.317 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.379.408 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.01.379.409 I llama_perf_context_print:        load time =     621.26 ms
0.01.379.411 I llama_perf_context_print: prompt eval time =      36.37 ms /     7 tokens (    5.20 ms per token,   192.48 tokens per second)
0.01.379.412 I llama_perf_context_print:        eval time =     709.64 ms /    63 runs   (   11.26 ms per token,    88.78 tokens per second)
0.01.379.414 I llama_perf_context_print:       total time =     749.45 ms /    70 tokens
0.01.379.588 I ggml_metal_free: deallocating

real	0m1.396s
user	0m0.108s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.964 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.719 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.724 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.730 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.731 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.731 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.731 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.732 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.734 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.735 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.735 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.735 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.736 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.736 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.736 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.738 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.739 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.739 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.675 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.699 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.651 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.652 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.652 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.653 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.653 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.653 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.654 I llama_model_loader: - type  f32:  194 tensors
0.00.023.654 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.654 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.655 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.052 I llm_load_vocab: special tokens cache size = 25
0.00.049.857 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.860 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.861 I llm_load_print_meta: arch             = gptneox
0.00.049.861 I llm_load_print_meta: vocab type       = BPE
0.00.049.861 I llm_load_print_meta: n_vocab          = 50304
0.00.049.861 I llm_load_print_meta: n_merges         = 50009
0.00.049.862 I llm_load_print_meta: vocab_only       = 0
0.00.049.862 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.862 I llm_load_print_meta: n_embd           = 2048
0.00.049.862 I llm_load_print_meta: n_layer          = 24
0.00.049.865 I llm_load_print_meta: n_head           = 16
0.00.049.866 I llm_load_print_meta: n_head_kv        = 16
0.00.049.866 I llm_load_print_meta: n_rot            = 32
0.00.049.867 I llm_load_print_meta: n_swa            = 0
0.00.049.867 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.867 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.868 I llm_load_print_meta: n_gqa            = 1
0.00.049.869 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.869 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.870 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.870 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.870 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.871 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.871 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.871 I llm_load_print_meta: n_ff             = 8192
0.00.049.873 I llm_load_print_meta: n_expert         = 0
0.00.049.874 I llm_load_print_meta: n_expert_used    = 0
0.00.049.874 I llm_load_print_meta: causal attn      = 1
0.00.049.874 I llm_load_print_meta: pooling type     = 0
0.00.049.874 I llm_load_print_meta: rope type        = 2
0.00.049.874 I llm_load_print_meta: rope scaling     = linear
0.00.049.874 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.875 I llm_load_print_meta: freq_scale_train = 1
0.00.049.875 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.875 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.875 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.875 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.876 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.876 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.876 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.888 I llm_load_print_meta: model type       = 1.4B
0.00.049.888 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.888 I llm_load_print_meta: model params     = 1.41 B
0.00.049.889 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.889 I llm_load_print_meta: general.name     = 1.4B
0.00.049.889 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.889 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.889 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.890 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.890 I llm_load_print_meta: LF token         = 128 ''
0.00.049.890 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.890 I llm_load_print_meta: max token length = 1024
0.00.051.576 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.577 I llm_load_tensors: offloading output layer to GPU
0.00.051.577 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.586 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.587 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.439 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.440 I llama_new_context_with_model: n_ctx         = 128
0.00.052.440 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.440 I llama_new_context_with_model: n_batch       = 128
0.00.052.440 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.440 I llama_new_context_with_model: flash_attn    = 0
0.00.052.441 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.441 I llama_new_context_with_model: freq_scale    = 1
0.00.052.441 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.442 I ggml_metal_init: allocating
0.00.052.445 I ggml_metal_init: found device: Apple M4
0.00.052.448 I ggml_metal_init: picking default device: Apple M4
0.00.052.993 I ggml_metal_init: using embedded metal library
0.00.054.903 I ggml_metal_init: GPU name:   Apple M4
0.00.054.905 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.905 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.906 I ggml_metal_init: simdgroup reduction   = true
0.00.054.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.906 I ggml_metal_init: has bfloat            = true
0.00.054.906 I ggml_metal_init: use bfloat            = true
0.00.054.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.013 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.016 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.029 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.912 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.913 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.913 I llama_new_context_with_model: graph nodes  = 967
0.00.064.913 I llama_new_context_with_model: graph splits = 2
0.00.064.925 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.575.281 I 
0.00.575.313 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.575.317 I perplexity: tokenizing the input ..
0.00.582.759 I perplexity: tokenization took 7.441 ms
0.00.582.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.716.511 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.717.654 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.717.676 I llama_perf_context_print:        load time =     566.31 ms
0.00.717.676 I llama_perf_context_print: prompt eval time =     133.49 ms /   128 tokens (    1.04 ms per token,   958.87 tokens per second)
0.00.717.677 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.717.678 I llama_perf_context_print:       total time =     142.39 ms /   129 tokens
0.00.717.963 I ggml_metal_free: deallocating

real	0m0.730s
user	0m0.076s
sys	0m0.104s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.009.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.869 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.873 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.875 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.877 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.877 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.878 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.878 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.878 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.879 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.881 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.881 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.881 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.973 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.050 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.179 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.181 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.181 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.182 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.182 I llama_model_loader: - type  f32:  194 tensors
0.00.025.183 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.183 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.047 I llm_load_vocab: special tokens cache size = 25
0.00.050.921 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.924 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.924 I llm_load_print_meta: arch             = gptneox
0.00.050.924 I llm_load_print_meta: vocab type       = BPE
0.00.050.925 I llm_load_print_meta: n_vocab          = 50304
0.00.050.925 I llm_load_print_meta: n_merges         = 50009
0.00.050.925 I llm_load_print_meta: vocab_only       = 0
0.00.050.925 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.925 I llm_load_print_meta: n_embd           = 2048
0.00.050.925 I llm_load_print_meta: n_layer          = 24
0.00.050.928 I llm_load_print_meta: n_head           = 16
0.00.050.931 I llm_load_print_meta: n_head_kv        = 16
0.00.050.931 I llm_load_print_meta: n_rot            = 32
0.00.050.931 I llm_load_print_meta: n_swa            = 0
0.00.050.931 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.931 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.932 I llm_load_print_meta: n_gqa            = 1
0.00.050.933 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.934 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.934 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.934 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.935 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.935 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.935 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.936 I llm_load_print_meta: n_ff             = 8192
0.00.050.936 I llm_load_print_meta: n_expert         = 0
0.00.050.936 I llm_load_print_meta: n_expert_used    = 0
0.00.050.937 I llm_load_print_meta: causal attn      = 1
0.00.050.938 I llm_load_print_meta: pooling type     = 0
0.00.050.938 I llm_load_print_meta: rope type        = 2
0.00.050.939 I llm_load_print_meta: rope scaling     = linear
0.00.050.939 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.939 I llm_load_print_meta: freq_scale_train = 1
0.00.050.939 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.940 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.940 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.940 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.940 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.940 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.940 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.952 I llm_load_print_meta: model type       = 1.4B
0.00.050.952 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.952 I llm_load_print_meta: model params     = 1.41 B
0.00.050.953 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.953 I llm_load_print_meta: general.name     = 1.4B
0.00.050.953 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.953 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.954 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.954 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.954 I llm_load_print_meta: LF token         = 128 ''
0.00.050.954 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.954 I llm_load_print_meta: max token length = 1024
0.00.052.493 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.493 I llm_load_tensors: offloading output layer to GPU
0.00.052.493 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.502 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.503 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.325 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.325 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.326 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.326 I llama_new_context_with_model: n_batch       = 2048
0.00.053.326 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.326 I llama_new_context_with_model: flash_attn    = 0
0.00.053.327 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.327 I llama_new_context_with_model: freq_scale    = 1
0.00.053.327 I ggml_metal_init: allocating
0.00.053.330 I ggml_metal_init: found device: Apple M4
0.00.053.332 I ggml_metal_init: picking default device: Apple M4
0.00.053.865 I ggml_metal_init: using embedded metal library
0.00.055.756 I ggml_metal_init: GPU name:   Apple M4
0.00.055.758 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.759 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.759 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.759 I ggml_metal_init: simdgroup reduction   = true
0.00.055.759 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.759 I ggml_metal_init: has bfloat            = true
0.00.055.760 I ggml_metal_init: use bfloat            = true
0.00.055.760 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.761 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.121 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.082.126 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.082.144 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.083.104 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.083.106 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.083.106 I llama_new_context_with_model: graph nodes  = 967
0.00.083.106 I llama_new_context_with_model: graph splits = 2
0.00.083.120 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.862 I main: llama threadpool init, n_threads = 4
0.00.701.897 I 
0.00.701.928 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.701.928 I 
0.00.702.146 I sampler seed: 1234
0.00.702.150 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.702.194 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.702.199 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.702.199 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.540.447 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.540.447 I llama_perf_context_print:        load time =     692.06 ms
0.01.540.448 I llama_perf_context_print: prompt eval time =      38.59 ms /     7 tokens (    5.51 ms per token,   181.40 tokens per second)
0.01.540.449 I llama_perf_context_print:        eval time =     796.58 ms /    63 runs   (   12.64 ms per token,    79.09 tokens per second)
0.01.540.449 I llama_perf_context_print:       total time =     838.59 ms /    70 tokens
0.01.540.640 I ggml_metal_free: deallocating

real	0m1.560s
user	0m0.107s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.448 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.955 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.960 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.962 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.962 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.963 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.963 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.963 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.965 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.966 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.968 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.968 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.969 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.989 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.116 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.083 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.083 I llama_model_loader: - type  f32:  194 tensors
0.00.026.084 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.084 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.582 I llm_load_vocab: special tokens cache size = 25
0.00.051.639 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.642 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.642 I llm_load_print_meta: arch             = gptneox
0.00.051.642 I llm_load_print_meta: vocab type       = BPE
0.00.051.643 I llm_load_print_meta: n_vocab          = 50304
0.00.051.643 I llm_load_print_meta: n_merges         = 50009
0.00.051.643 I llm_load_print_meta: vocab_only       = 0
0.00.051.643 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.643 I llm_load_print_meta: n_embd           = 2048
0.00.051.643 I llm_load_print_meta: n_layer          = 24
0.00.051.647 I llm_load_print_meta: n_head           = 16
0.00.051.647 I llm_load_print_meta: n_head_kv        = 16
0.00.051.648 I llm_load_print_meta: n_rot            = 32
0.00.051.650 I llm_load_print_meta: n_swa            = 0
0.00.051.650 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.650 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.651 I llm_load_print_meta: n_gqa            = 1
0.00.051.652 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.653 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.653 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.654 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.654 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.654 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.654 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.655 I llm_load_print_meta: n_ff             = 8192
0.00.051.655 I llm_load_print_meta: n_expert         = 0
0.00.051.655 I llm_load_print_meta: n_expert_used    = 0
0.00.051.655 I llm_load_print_meta: causal attn      = 1
0.00.051.656 I llm_load_print_meta: pooling type     = 0
0.00.051.656 I llm_load_print_meta: rope type        = 2
0.00.051.656 I llm_load_print_meta: rope scaling     = linear
0.00.051.657 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.657 I llm_load_print_meta: freq_scale_train = 1
0.00.051.658 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.658 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.658 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.658 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.658 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.658 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.660 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.671 I llm_load_print_meta: model type       = 1.4B
0.00.051.671 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.672 I llm_load_print_meta: model params     = 1.41 B
0.00.051.672 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.673 I llm_load_print_meta: general.name     = 1.4B
0.00.051.673 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.673 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: LF token         = 128 ''
0.00.051.674 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.674 I llm_load_print_meta: max token length = 1024
0.00.053.306 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.306 I llm_load_tensors: offloading output layer to GPU
0.00.053.306 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.316 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.317 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.139 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.140 I llama_new_context_with_model: n_ctx         = 128
0.00.054.140 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.140 I llama_new_context_with_model: n_batch       = 128
0.00.054.141 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.141 I llama_new_context_with_model: flash_attn    = 0
0.00.054.141 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.141 I llama_new_context_with_model: freq_scale    = 1
0.00.054.142 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.142 I ggml_metal_init: allocating
0.00.054.145 I ggml_metal_init: found device: Apple M4
0.00.054.147 I ggml_metal_init: picking default device: Apple M4
0.00.054.696 I ggml_metal_init: using embedded metal library
0.00.056.596 I ggml_metal_init: GPU name:   Apple M4
0.00.056.597 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.598 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.598 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.598 I ggml_metal_init: simdgroup reduction   = true
0.00.056.598 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.599 I ggml_metal_init: has bfloat            = true
0.00.056.599 I ggml_metal_init: use bfloat            = true
0.00.056.599 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.600 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.759 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.763 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.778 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.643 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.644 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.645 I llama_new_context_with_model: graph nodes  = 967
0.00.066.645 I llama_new_context_with_model: graph splits = 2
0.00.066.657 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.659.400 I 
0.00.659.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.659.504 I perplexity: tokenizing the input ..
0.00.667.606 I perplexity: tokenization took 8.1 ms
0.00.667.609 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.695 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.809.868 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.809.895 I llama_perf_context_print:        load time =     647.94 ms
0.00.809.897 I llama_perf_context_print: prompt eval time =     140.86 ms /   128 tokens (    1.10 ms per token,   908.70 tokens per second)
0.00.809.898 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.898 I llama_perf_context_print:       total time =     150.50 ms /   129 tokens
0.00.810.407 I ggml_metal_free: deallocating

real	0m0.826s
user	0m0.076s
sys	0m0.123s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.668 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.068 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.075 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.076 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.076 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.077 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.077 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.078 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.079 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.110 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.303 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.336 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.337 I llama_model_loader: - type  f32:  194 tensors
0.00.024.337 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.291 I llm_load_vocab: special tokens cache size = 25
0.00.051.227 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.230 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.231 I llm_load_print_meta: arch             = gptneox
0.00.051.231 I llm_load_print_meta: vocab type       = BPE
0.00.051.231 I llm_load_print_meta: n_vocab          = 50304
0.00.051.231 I llm_load_print_meta: n_merges         = 50009
0.00.051.232 I llm_load_print_meta: vocab_only       = 0
0.00.051.232 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.232 I llm_load_print_meta: n_embd           = 2048
0.00.051.232 I llm_load_print_meta: n_layer          = 24
0.00.051.235 I llm_load_print_meta: n_head           = 16
0.00.051.236 I llm_load_print_meta: n_head_kv        = 16
0.00.051.236 I llm_load_print_meta: n_rot            = 32
0.00.051.236 I llm_load_print_meta: n_swa            = 0
0.00.051.236 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.237 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.237 I llm_load_print_meta: n_gqa            = 1
0.00.051.238 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.239 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.239 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.240 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.240 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.242 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.242 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.243 I llm_load_print_meta: n_ff             = 8192
0.00.051.243 I llm_load_print_meta: n_expert         = 0
0.00.051.243 I llm_load_print_meta: n_expert_used    = 0
0.00.051.244 I llm_load_print_meta: causal attn      = 1
0.00.051.245 I llm_load_print_meta: pooling type     = 0
0.00.051.245 I llm_load_print_meta: rope type        = 2
0.00.051.245 I llm_load_print_meta: rope scaling     = linear
0.00.051.246 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.246 I llm_load_print_meta: freq_scale_train = 1
0.00.051.246 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.247 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.247 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.247 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.247 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.247 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.247 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.259 I llm_load_print_meta: model type       = 1.4B
0.00.051.260 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.260 I llm_load_print_meta: model params     = 1.41 B
0.00.051.260 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.260 I llm_load_print_meta: general.name     = 1.4B
0.00.051.261 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.261 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.261 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.262 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.262 I llm_load_print_meta: LF token         = 128 ''
0.00.051.262 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.262 I llm_load_print_meta: max token length = 1024
0.00.053.326 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.326 I llm_load_tensors: offloading output layer to GPU
0.00.053.326 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.336 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.337 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.309 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.310 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.310 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.310 I llama_new_context_with_model: n_batch       = 2048
0.00.054.310 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.310 I llama_new_context_with_model: flash_attn    = 0
0.00.054.311 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.311 I llama_new_context_with_model: freq_scale    = 1
0.00.054.312 I ggml_metal_init: allocating
0.00.054.317 I ggml_metal_init: found device: Apple M4
0.00.054.320 I ggml_metal_init: picking default device: Apple M4
0.00.054.853 I ggml_metal_init: using embedded metal library
0.00.056.784 I ggml_metal_init: GPU name:   Apple M4
0.00.056.785 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.786 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.786 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.786 I ggml_metal_init: simdgroup reduction   = true
0.00.056.786 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.787 I ggml_metal_init: has bfloat            = true
0.00.056.787 I ggml_metal_init: use bfloat            = true
0.00.056.787 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.788 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.333 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.083.338 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.083.355 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.259 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.084.261 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.084.261 I llama_new_context_with_model: graph nodes  = 967
0.00.084.261 I llama_new_context_with_model: graph splits = 2
0.00.084.274 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.857 I main: llama threadpool init, n_threads = 4
0.00.758.905 I 
0.00.758.935 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.758.937 I 
0.00.759.183 I sampler seed: 1234
0.00.759.188 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.233 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.238 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.238 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.628.116 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61418.69 tokens per second)
0.01.628.116 I llama_perf_context_print:        load time =     750.18 ms
0.01.628.117 I llama_perf_context_print: prompt eval time =      38.60 ms /     7 tokens (    5.51 ms per token,   181.33 tokens per second)
0.01.628.118 I llama_perf_context_print:        eval time =     827.32 ms /    63 runs   (   13.13 ms per token,    76.15 tokens per second)
0.01.628.118 I llama_perf_context_print:       total time =     869.26 ms /    70 tokens
0.01.628.307 I ggml_metal_free: deallocating

real	0m1.644s
user	0m0.107s
sys	0m0.165s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4236 (5e1ed955) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.793 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.465 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.473 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.479 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.480 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.482 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.482 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.483 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.483 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.483 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.484 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.485 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.486 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.486 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.532 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.496 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.498 I llama_model_loader: - type  f32:  194 tensors
0.00.023.498 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.034 I llm_load_vocab: special tokens cache size = 25
0.00.050.002 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.005 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.006 I llm_load_print_meta: arch             = gptneox
0.00.050.006 I llm_load_print_meta: vocab type       = BPE
0.00.050.006 I llm_load_print_meta: n_vocab          = 50304
0.00.050.006 I llm_load_print_meta: n_merges         = 50009
0.00.050.007 I llm_load_print_meta: vocab_only       = 0
0.00.050.007 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.007 I llm_load_print_meta: n_embd           = 2048
0.00.050.007 I llm_load_print_meta: n_layer          = 24
0.00.050.011 I llm_load_print_meta: n_head           = 16
0.00.050.012 I llm_load_print_meta: n_head_kv        = 16
0.00.050.012 I llm_load_print_meta: n_rot            = 32
0.00.050.012 I llm_load_print_meta: n_swa            = 0
0.00.050.012 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.012 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.013 I llm_load_print_meta: n_gqa            = 1
0.00.050.014 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.014 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.015 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.015 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.015 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.016 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.016 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.016 I llm_load_print_meta: n_ff             = 8192
0.00.050.017 I llm_load_print_meta: n_expert         = 0
0.00.050.017 I llm_load_print_meta: n_expert_used    = 0
0.00.050.017 I llm_load_print_meta: causal attn      = 1
0.00.050.018 I llm_load_print_meta: pooling type     = 0
0.00.050.019 I llm_load_print_meta: rope type        = 2
0.00.050.019 I llm_load_print_meta: rope scaling     = linear
0.00.050.019 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.020 I llm_load_print_meta: freq_scale_train = 1
0.00.050.020 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.020 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.020 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.021 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.021 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.021 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.021 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.032 I llm_load_print_meta: model type       = 1.4B
0.00.050.032 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.033 I llm_load_print_meta: model params     = 1.41 B
0.00.050.033 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.033 I llm_load_print_meta: general.name     = 1.4B
0.00.050.033 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.035 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.035 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.035 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.036 I llm_load_print_meta: LF token         = 128 ''
0.00.050.036 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.036 I llm_load_print_meta: max token length = 1024
0.00.051.572 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.572 I llm_load_tensors: offloading output layer to GPU
0.00.051.572 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.582 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.583 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.377 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.378 I llama_new_context_with_model: n_ctx         = 128
0.00.052.378 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.378 I llama_new_context_with_model: n_batch       = 128
0.00.052.378 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.378 I llama_new_context_with_model: flash_attn    = 0
0.00.052.379 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.379 I llama_new_context_with_model: freq_scale    = 1
0.00.052.379 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.380 I ggml_metal_init: allocating
0.00.052.383 I ggml_metal_init: found device: Apple M4
0.00.052.384 I ggml_metal_init: picking default device: Apple M4
0.00.052.947 I ggml_metal_init: using embedded metal library
0.00.054.871 I ggml_metal_init: GPU name:   Apple M4
0.00.054.872 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.873 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.873 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.873 I ggml_metal_init: simdgroup reduction   = true
0.00.054.873 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.874 I ggml_metal_init: has bfloat            = true
0.00.054.874 I ggml_metal_init: use bfloat            = true
0.00.054.874 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.875 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.011 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.014 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.028 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.864 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.865 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.865 I llama_new_context_with_model: graph nodes  = 967
0.00.064.865 I llama_new_context_with_model: graph splits = 2
0.00.064.877 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.528.446 I 
0.00.528.481 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | AARCH64_REPACK = 1 | 
0.00.528.485 I perplexity: tokenizing the input ..
0.00.536.316 I perplexity: tokenization took 7.828 ms
0.00.536.323 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.676.627 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.677.801 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.677.831 I llama_perf_context_print:        load time =     519.65 ms
0.00.677.832 I llama_perf_context_print: prompt eval time =     140.08 ms /   128 tokens (    1.09 ms per token,   913.78 tokens per second)
0.00.677.833 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.677.833 I llama_perf_context_print:       total time =     149.39 ms /   129 tokens
0.00.678.276 I ggml_metal_free: deallocating

real	0m0.691s
user	0m0.076s
sys	0m0.109s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4236 (5e1ed955)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12760a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12760a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12760aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12760b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12760ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12760bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12760c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12760cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12760d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12760d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12760dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12760dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12760eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12760f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12760fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1276101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1276108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x127611010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x127611730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x127611f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x127612620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x127612d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x127613460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x127613d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x127614420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1276146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x127614cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x127615960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x127615ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x127616160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x127616600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1276168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x127617150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127617950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127617df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127618290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127618730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127618bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127619070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127619510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1276199b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127619e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12761a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12761a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12761abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12761b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12761baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12761c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12761c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12761cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12761d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12761d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12761df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12761e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12761ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12761f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12761f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12761f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127620140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127620400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1276208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127620d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1276211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127621680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127621b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127621fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127622460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127622900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127622da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127623240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1276236e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127623b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127624020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1276244c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127624960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127624e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1276252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127625740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127625be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127626080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127626520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1276269c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127626e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127627300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1276277a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127627c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1276280e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127628580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127628a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x127628ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127629360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x127629800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127629ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12762a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12762a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12762aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12761b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12762b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12762b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12762ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12762beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12762c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12762c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12762cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12762d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12762d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12762da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12762df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12762e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12762e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12762ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12762f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12762f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12762fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12762ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x127630410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1276308b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127630d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1276311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127631690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127631b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127631fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127632470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127632910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127632db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127633250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1276336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127633b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127634030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1276344d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127634970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127634e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1276352b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127635750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127635bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127636090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127636530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1276369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127636e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127637310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1276377b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127637c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1276380f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127638590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127638a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127638ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x127639370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127639810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127639cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12763a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12763a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12763aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12763afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12763b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12763ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12763bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12763c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12763c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12763ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12763d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12763dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12763e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12763e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12763ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12763f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12763f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12763fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1276403b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127640900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127640e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1276413a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1276418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127641e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127642390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1276428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127642e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127643380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1276438d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127643e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127644370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1276448c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127644e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127645360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1276458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127645e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127646350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1276468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127646df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x127647340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127647890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127647de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127648330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127648880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127648dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127649320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x127649870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127649dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12764a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12764a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12764adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12764b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12764b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12764bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12764c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12764c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12764cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12764d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12764d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12764dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12764e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12764e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12764ed70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12764f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12764f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12764fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1276502b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127650800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x127650d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1276512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1276517f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127651d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127652290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1276527e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127652c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127653120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1276535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127653a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127653f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1276543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127654840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127654ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127655180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x127655620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127655ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127655f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127656400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127656950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127657070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127657790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127657eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1276585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127658890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127658ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1276594b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.153.332 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d406100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d406570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d4069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d406e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d4072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d407730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d407ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d404080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d4044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d404960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d408010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d4085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d409120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d4098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d40a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d40a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d40af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d40b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d40bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d40c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d40cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d40d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d40da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d40e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d40e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d40eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d40f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d40f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d40fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d4105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d410a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d410d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d4115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d411ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d411da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d412240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d4126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d412b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d413020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d4134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d413960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d413e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d4142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d414740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d414a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d415010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d415620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d415c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d416240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d416850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d416e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12760a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12760e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12760e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12760ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12760ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12760f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12760f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12760fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127610050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1276104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127610930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127610da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127611210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127611680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127611af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127611f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1276123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127612840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x127612cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x127613120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x127613590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x127613a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127613e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1276142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127614750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127614bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127615030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1276154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x127615910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127615d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1276161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127616660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127616ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127616f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1276173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127617820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x127617c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x127618100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x127618570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1276189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x127618e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1276192c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x127619730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x127619ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12761a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12761a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12761a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12761ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12761b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12761b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12761bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12761bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12761c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12761c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12761cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12761d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12761d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12761d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12761de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12761e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12761e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12761eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12761eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12761f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12761f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12761fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1276201b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127620620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127620a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127620f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127621370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1276217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x127621c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1276220c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127622530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1276229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x127622e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127623280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1276236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127623b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127623fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127624440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1276248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127624d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127625190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127625600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127625a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127625ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127626350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1276267c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1276270a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127627510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x127627980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x127627df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x127628260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1276286d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x127628b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x127628fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x127629420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x127629890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x127629d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12762a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12762a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12762aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12762aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12762b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12762b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12762bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12762c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12762c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12762c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12762cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12762d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12762d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12762db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12762e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12762e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12762ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12762f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12762f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12762fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12762feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127630320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127630790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127630c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127631070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1276314e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127631950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127631dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127632230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1276326a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127632b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127632f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1276333f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127633860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x127633cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127634140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1276345b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127634a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x127634e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127635300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127635770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127635be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127636050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1276364c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127636930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x127636da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x127637210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x127637680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x127637af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x127637f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1276383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x127638840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x127638cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x127639120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127639590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127639a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127639e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12763a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12763a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12763abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12763b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12763b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12763b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12763bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12763c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12763c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12763cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12763cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12763d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12763d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12763dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12763e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12763e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12763e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12763ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12763f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12763f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12763fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127640010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127640480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1276408f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127640d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1276411d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127641640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x127641ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1276421a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127642890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127642f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127643670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127643ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x127643f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1276443c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10d406560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10d4069d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10d406e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10d4072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10d407720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10d407b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10d408000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10d408470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10d4088e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10d408d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10d4091c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10d4097a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10d40a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10d40a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10d40aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10d40b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10d40bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10d40c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10d40cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10d40d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10d40dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10d40e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10d40ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10d40f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10d40f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10d40fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10d4100c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10d410530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10d4109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10d410e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10d411280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10d4116f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10d411b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10d411e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10d412290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10d412700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10d412b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10d412fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10d413450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10d4138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10d413d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10d4141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10d414610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10d414a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10d414ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10d415360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10d4157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10d415c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10d4160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10d416520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10d416990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10d416e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10d404080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10d404340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10d4047b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10d405960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10d405c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10d417620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10d417c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10d418420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10d4188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10d418d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10d419200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10d4196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10d419b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10d419fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10d41a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10d41a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10d41adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10d41b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10d41b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10d41bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10d41c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10d41c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10d41c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10d41ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10d41d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10d41d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10d41dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10d41e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10d41e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10d41e9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10d41ee80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10d41f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10d41f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10d41fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10d420100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10d4205a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10d420a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10d420ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10d421380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10d421820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10d421cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10d422160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10d422600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10d422aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10d422f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10d4233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10d423880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10d423d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10d4241c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10d424660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10d424b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10d424fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10d425440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10d4258e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10d425d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10d426220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10d4266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10d426b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10d427000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10d4274a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10d427940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10d427de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10d428280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10d428720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10d428bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10d429060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10d429500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10d4299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10d429e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10d42a2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10d42a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10d42ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10d42b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10d42b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10d42ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10d42bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10d42c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10d42c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10d42cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10d42d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10d42d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10d42da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10d42df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10d42e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10d42e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10d42ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10d42f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10d42f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10d42fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10d42ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10d430400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10d4308a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10d430d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10d4311e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10d431680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10d431b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10d431fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10d432460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10d432900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10d432da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10d433240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10d433790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10d433ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10d434230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10d434780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10d434a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10d435050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10d435660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10d435c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10d436280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10d436890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10d437080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10d437520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10d4379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10d437e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10d438610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10d438b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10d4390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10d439600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10d439b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10d43a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10d43a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10d43ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10d43b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10d43b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10d43bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10d43c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10d43c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10d43cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10d43d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10d43d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10d43db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10d43e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10d43e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10d43eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10d43f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10d43f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10d43faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10d440040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10d440590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10d440ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10d441030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10d441580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10d441ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10d442020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10d442570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10d442ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10d443010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10d443560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10d443ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10d444000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10d444550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10d444aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10d444ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10d445540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10d445a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10d445fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10d446530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10d446a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10d446fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10d447520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10d447a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10d447fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10d448510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10d448a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10d448fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10d449500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10d449a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10d449fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10d44a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10d44aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10d44af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10d44b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10d44b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10d44bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10d44c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10d44c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10d44cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10d44cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10d44d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10d44d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10d44ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10d44e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10d44e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10d44ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10d44f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10d44f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10d44ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10d450660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10d450d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10d451040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10d451650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10d451c60 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.832s
user	0m0.291s
sys	0m0.317s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4236 (5e1ed955)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15870ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15870f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15870f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15870ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x158710520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x158710ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x158711080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x158711630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x158711be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1587120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1587125e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x158712ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x158713600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x158713db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1587145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x158714ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x158715400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x158715b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x158716240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x158716a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x158717130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x158717850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x158717f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x158718810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x158718f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1587191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x158719800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15871a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15871a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15871ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15871b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15871b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15871bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15871c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15871c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15871c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15871cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15871d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15871d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15871db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15871e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15871e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15871e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15871ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15871f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15871f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15871fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158720600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x158720c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x158721220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x158721e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x158722450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x158722a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x158723250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1587236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x158723b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158723e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158724460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x158724c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x158724f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1587253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x158725850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x158725cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158726190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x158726630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x158726ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x158726f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158727410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1587278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x158727d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1587281f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x158728690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x158728b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x158728fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x158729470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x158729910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x158729db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15872a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15872a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15872ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15872b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15872b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15872b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15872be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15872c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15872c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15872cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15872d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15872d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15872d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15872de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15872e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15872e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15872ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15872f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15872f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1587202f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15872fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158730080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x158730520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1587309c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158730e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x158731300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1587317a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158731c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1587320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x158732580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x158732a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x158732ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158733360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x158733800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158733ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x158734140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1587345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158734a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x158734f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1587353c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x158735860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x158735d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1587361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x158736640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x158736ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x158736f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x158737420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1587378c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x158737d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x158738200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1587386a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x158738b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x158738fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x158739480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x158739920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x158739dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15873a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15873a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15873aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15873b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15873b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15873b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15873be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15873c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15873c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15873cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15873d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15873d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15873d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15873de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15873e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15873e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15873ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15873f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15873f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15873faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x158740040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x158740590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x158740ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x158740da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1587413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1587419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158741fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1587425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x158742bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1587433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x158743880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x158743d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1587441c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x158744970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x158744ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x158745410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x158745960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x158746400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x158746950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x158746ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1587473f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x158747940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x158747e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1587483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x158748930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x158748e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1587493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158749920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158749e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15874a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15874a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15874ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15874b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15874b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15874be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15874c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15874c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15874ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15874d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15874d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15874de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15874e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15874e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15874ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15874f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15874f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15874fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x158750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1587508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x158750e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x158751350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1587518a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x158751df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x158752340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x158752890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x158752de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x158753330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158753880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158753dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x158754320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158754870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x158754dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x158755310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x158755860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158755db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x158756300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x158756850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x158756da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1587572f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x158757790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x158757c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1587580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x158758570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x158758a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x158758eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x158759350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1587597f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x158759c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15875a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15875a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15875aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15875af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15875b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15875bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15875c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15875c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15875d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15875d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15875d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15875dfc0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.086.723 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15a006050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15a0064c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15a006930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15a006da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15a007210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15a007680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15a007af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15a007f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15a0083d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15a008840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15a008cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15a009390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15a009eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15a00a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15a00ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15a00b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15a00bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15a00c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15a00caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15a00d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15a00d9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15a00e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15a00e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15a00ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15a00f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15a00f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a00fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15a010050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a0104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15a010930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15a010da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15a0112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15a011740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15a011a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15a011e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a0122e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a012750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a012bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a013030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15a0134a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15a013910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15a013d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15a0141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a014660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15a014ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15a014f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15a0153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15a015820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a015c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a016100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15a016570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a0169e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a016e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15a0172c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15a017730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15a017ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15a018110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15a018610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15a018a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15a018ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15a019360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15a0197d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15a019c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15a01a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15a01a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a01a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15a01ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a01b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15a01b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a01bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a01bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15a01c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a01c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a01cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a01d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a01d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a01da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a01ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a01e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a01e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a01ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a01f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a01f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a01f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a01fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a020250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a0206c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a020b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a020fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a021410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a021880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a021cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a022160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a0225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a022a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a022eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a023320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a023790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a023c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a024070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a0244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a024950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a024dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a025230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a0256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a025b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a025f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a0263f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a026860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a026cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a027140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a0275b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a027a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a027e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a028300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a028770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a028be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a029050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a0294c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a029930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a029da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a02a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a02a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a02aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a02af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a02b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a02b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a02bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a02c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a02c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a02ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a02ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a02d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a02d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a02dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a02e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a02e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a02e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a02ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a02f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a02f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a02fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a02ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a0303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a030820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a030c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a031100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a031570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a0319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a031e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a0322c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a032730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a032ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a033010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a033480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a0338f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a033d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a0341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a034640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a034ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a034f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a035390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a035800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a035c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a0360e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a036550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a0369c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a037550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a037810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a037ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a037f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a0383b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a038820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a038c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a039100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a039570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a0399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a039e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a03a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a03a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a03aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a03b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a03b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a03b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a03bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a03c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a03c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a03cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a03cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a03d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a03d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a03dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a03e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a03e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a03e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a03ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a03f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a03f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a03fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a03fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a040460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a0408d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a040d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a0411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a041620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a041a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a041f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a042370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a0427e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a042c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a0430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a043530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a0439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a043e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a044280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a0446f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a044b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a044fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a045440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a0458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a045d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a046190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a046600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a046a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a046ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a047350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a0477c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a047c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a0480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a048510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a048980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a048df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a049260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a0496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a049b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a049fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a04a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a04a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a04b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a04baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a04c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a04c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a04cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a04ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a04d320 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15d6044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15d604950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15d604dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15d605230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15d6056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15d605b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15d605f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15d6063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15d606860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15d606db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15d607220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15d6078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15d6083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15d608b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15d609380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15d609aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15d60a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15d60a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15d60b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15d60b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15d60bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15d60c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15d60cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15d60d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15d60db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15d60de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15d60e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15d60e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15d60e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15d60ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15d60f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15d60f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15d60fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15d60ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15d610380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15d6107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15d610c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15d6110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15d611540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15d6119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15d611e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15d612290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15d612700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15d612b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15d612fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15d613450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15d6138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15d613d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15d6141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15d614610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15d614a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15d614ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15d615360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15d6157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15d615c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15d6160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15d616620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15d616b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15d616f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15d617400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15d617870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15d617ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15d618150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15d6185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15d618a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15d618ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15d619310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15d619780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15d619bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15d61a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15d61a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15d61a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15a005bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15a006060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a0064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15a006940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15a006db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a007220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15a007690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a007b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15a007f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15a0083e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a008850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a008cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15a009130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a0095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a009e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a00a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15a00a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a00abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15a00b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15a00b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15a00b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a00bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15a00c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15a00c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a00cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15a00cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15a00d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a00d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a00dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15a00e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a00e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15a00e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15a00ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15a00f2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15a00f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15a00fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15a010020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15a010490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15a010900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15a010d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a0111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a011650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15a011ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a011f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a0123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15a012810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a012c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a0130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15a013560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a0139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a013e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15a0142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15a014720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15a014b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15a015000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15a015470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15a0158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a015d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a0161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a016630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15a016aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a016f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15a017380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a0177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15a017c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a0180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15a018540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15a0189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15a018e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a019290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15a019700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15a019b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15a019fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15a01a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15a01a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15a01ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15a01b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15a01b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a01ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a01bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15a01c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a01c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a01cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15a01d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15a01d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15a01d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a01de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15a01e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15a01e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15a01eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15a01efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15a01f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15a01f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15a01fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a020600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a020a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a020ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a021350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15a0217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15a021c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a0220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15a022510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15a022980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15a022df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15a023260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15a0236d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15a023b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a023fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a024420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15a024890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15a024d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a025170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15a0255e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15a025a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a025ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15a026330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15a0267a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a026c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a027080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a0274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15a027960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15a027dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a028240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15a0286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a028b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a028f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a029400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a029870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15a029ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15a02a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15a02a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15a02aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15a02aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15a02b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15a02b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a02bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a02c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a02c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15a02c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15a02cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15a02d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a02d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15a02db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a02df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a02e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15a02e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15a02ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a02f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15a02f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a02fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a02fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15a0302f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15a030760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15a030bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15a031040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a0314b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15a031920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15a031d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15a032200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15a032670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15a032ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15a032f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15a0333c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a033830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15a033ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15a034390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15a034a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15a035170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15a035860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15a035cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15a036140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15a0365b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


second run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this


single seq run: The quick brown fox jumps over the lazy Dog." "Sorry, I'm a bit rubbish at this

real	0m0.913s
user	0m0.238s
sys	0m0.138s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.54 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.57 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.10 sec*proc (2 tests)

Total Test time (real) =   1.11 sec
        1.13 real         0.72 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 23: test-model-load-cancel
1/2 Test #23: test-model-load-cancel ...........   Passed    0.26 sec
    Start 24: test-autorelease
2/2 Test #24: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
