Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:321 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_nosve
-- Performing Test GGML_MACHINE_SUPPORTS_nosve - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sme
-- Performing Test GGML_MACHINE_SUPPORTS_sme - Success
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- ARM feature SME enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve+sme 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.8s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m3.051s
user	0m1.019s
sys	0m1.451s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target sha1
[  4%] Built target xxhash
[  4%] Built target build_info
[  4%] Built target sha256
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  6%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Built target ggml-blas
[ 12%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Linking CXX executable ../../bin/llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Built target llama-gguf
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 25%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 26%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Linking CXX executable ../../bin/llama-simple-chat
[ 30%] Linking C executable ../bin/test-c
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 31%] Built target llava
[ 31%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Linking CXX static library libllava_static.a
[ 35%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Linking CXX static library libcommon.a
[ 36%] Built target llama-simple-chat
[ 36%] Built target llama-simple
[ 36%] Built target llama-quantize-stats
[ 36%] Built target test-c
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 43%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-chat
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-llama-grammar
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-chat
[ 48%] Built target test-grammar-integration
[ 48%] Built target test-log
[ 48%] Built target test-sampling
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-arg-parser
[ 60%] Linking CXX executable ../bin/test-chat-template
[ 61%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-gguf
[ 62%] Linking CXX executable ../bin/test-backend-ops
[ 62%] Linking CXX executable ../bin/test-autorelease
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-model-load-cancel
[ 63%] Built target test-backend-ops
[ 63%] Built target test-chat-template
[ 63%] Built target test-arg-parser
[ 63%] Built target test-gguf
[ 63%] Built target test-barrier
[ 63%] Built target test-quantize-fns
[ 63%] Built target test-autorelease
[ 63%] Built target test-quantize-perf
[ 63%] Built target test-rope
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-batched-bench
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-eval-callback
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-infill
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-bench
[ 73%] Built target llama-batched
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-batched-bench
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-imatrix
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 73%] Built target llama-infill
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 79%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 81%] Linking CXX executable ../../bin/llama-quantize
[ 81%] Built target llama-lookahead
[ 81%] Built target llama-lookup
[ 81%] Built target llama-perplexity
[ 81%] Built target llama-lookup-merge
[ 81%] Built target llama-lookup-stats
[ 81%] Built target llama-parallel
[ 81%] Built target llama-cli
[ 81%] Built target llama-quantize
[ 81%] Built target llama-lookup-create
[ 81%] Generating loading.html.hpp
[ 82%] Generating index.html.gz.hpp
[ 82%] Built target llama-passkey
[ 82%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 83%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 83%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 85%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 86%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-tokenize
[ 88%] Linking CXX executable ../../bin/llama-run
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-speculative
[ 91%] Built target llama-save-load-state
[ 91%] Built target llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-run
[ 91%] Built target llama-tts
[ 91%] Built target llama-retrieval
[ 91%] Built target llama-speculative-simple
[ 92%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Built target llama-gen-docs
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-export-lora
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 95%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 95%] Built target llama-convert-llama2c-to-ggml
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.276s
user	0m6.454s
sys	0m10.584s

main: quantize time =  3670.79 ms
main:    total time =  3670.79 ms

main: quantize time =  2545.83 ms
main:    total time =  2545.83 ms

main: quantize time =  2158.00 ms
main:    total time =  2158.00 ms

main: quantize time =  2376.16 ms
main:    total time =  2376.16 ms

main: quantize time =  3369.52 ms
main:    total time =  3369.52 ms

main: quantize time =  5436.61 ms
main:    total time =  5436.61 ms

main: quantize time =  5782.54 ms
main:    total time =  5782.54 ms

main: quantize time =  6880.63 ms
main:    total time =  6880.63 ms

main: quantize time =  5774.96 ms
main:    total time =  5774.96 ms

main: quantize time =  4760.61 ms
main:    total time =  4760.61 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.143 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.321 I main: llama backend init
0.00.000.327 I main: load the model and apply lora adapter, if any
0.00.049.386 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.063.672 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.063.682 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.063.695 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.063.696 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.063.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.063.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.063.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.063.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.063.702 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.063.705 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.063.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.063.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.063.707 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.063.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.063.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.063.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.063.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.071.813 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.073.928 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.080.616 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.080.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.080.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.080.619 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.080.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.080.621 I llama_model_loader: - type  f32:  194 tensors
0.00.080.621 I llama_model_loader: - type  f16:   98 tensors
0.00.080.622 I print_info: file format = GGUF V3 (latest)
0.00.080.623 I print_info: file type   = all F32 (guessed)
0.00.080.626 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.584 I load: special tokens cache size = 25
0.00.101.886 I load: token to piece cache size = 0.2984 MB
0.00.101.910 I print_info: arch             = gptneox
0.00.101.911 I print_info: vocab_only       = 0
0.00.101.911 I print_info: n_ctx_train      = 2048
0.00.101.911 I print_info: n_embd           = 2048
0.00.101.912 I print_info: n_layer          = 24
0.00.101.915 I print_info: n_head           = 16
0.00.101.918 I print_info: n_head_kv        = 16
0.00.101.918 I print_info: n_rot            = 32
0.00.101.918 I print_info: n_swa            = 0
0.00.101.923 I print_info: n_embd_head_k    = 128
0.00.101.923 I print_info: n_embd_head_v    = 128
0.00.101.924 I print_info: n_gqa            = 1
0.00.101.925 I print_info: n_embd_k_gqa     = 2048
0.00.101.926 I print_info: n_embd_v_gqa     = 2048
0.00.101.927 I print_info: f_norm_eps       = 1.0e-05
0.00.101.927 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.101.928 I print_info: f_clamp_kqv      = 0.0e+00
0.00.101.928 I print_info: f_max_alibi_bias = 0.0e+00
0.00.101.928 I print_info: f_logit_scale    = 0.0e+00
0.00.101.936 I print_info: n_ff             = 8192
0.00.101.939 I print_info: n_expert         = 0
0.00.101.939 I print_info: n_expert_used    = 0
0.00.101.939 I print_info: causal attn      = 1
0.00.101.939 I print_info: pooling type     = 0
0.00.101.940 I print_info: rope type        = 2
0.00.101.941 I print_info: rope scaling     = linear
0.00.101.941 I print_info: freq_base_train  = 10000.0
0.00.101.942 I print_info: freq_scale_train = 1
0.00.101.942 I print_info: n_ctx_orig_yarn  = 2048
0.00.101.942 I print_info: rope_finetuned   = unknown
0.00.101.942 I print_info: ssm_d_conv       = 0
0.00.101.942 I print_info: ssm_d_inner      = 0
0.00.101.943 I print_info: ssm_d_state      = 0
0.00.101.943 I print_info: ssm_dt_rank      = 0
0.00.101.943 I print_info: ssm_dt_b_c_rms   = 0
0.00.101.943 I print_info: model type       = 1.4B
0.00.101.945 I print_info: model params     = 1.41 B
0.00.101.945 I print_info: general.name     = 1.4B
0.00.101.945 I print_info: vocab type       = BPE
0.00.101.946 I print_info: n_vocab          = 50304
0.00.101.946 I print_info: n_merges         = 50009
0.00.101.946 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.101.946 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.101.947 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.101.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.101.947 I print_info: LF token         = 187 'Ċ'
0.00.101.948 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.101.948 I print_info: max token length = 1024
0.00.101.948 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.146.753 I load_tensors: offloading 24 repeating layers to GPU
0.00.146.756 I load_tensors: offloading output layer to GPU
0.00.146.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.146.778 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.146.779 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.147.375 I llama_init_from_model: n_seq_max     = 1
0.00.147.376 I llama_init_from_model: n_ctx         = 2048
0.00.147.376 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.147.377 I llama_init_from_model: n_batch       = 2048
0.00.147.377 I llama_init_from_model: n_ubatch      = 512
0.00.147.377 I llama_init_from_model: flash_attn    = 0
0.00.147.377 I llama_init_from_model: freq_base     = 10000.0
0.00.147.378 I llama_init_from_model: freq_scale    = 1
0.00.147.378 I ggml_metal_init: allocating
0.00.147.399 I ggml_metal_init: found device: Apple M4
0.00.147.404 I ggml_metal_init: picking default device: Apple M4
0.00.147.911 I ggml_metal_init: using embedded metal library
0.00.163.185 I ggml_metal_init: GPU name:   Apple M4
0.00.163.186 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.163.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.163.187 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.163.187 I ggml_metal_init: simdgroup reduction   = true
0.00.163.188 I ggml_metal_init: simdgroup matrix mul. = true
0.00.163.188 I ggml_metal_init: has residency sets    = true
0.00.163.188 I ggml_metal_init: has bfloat            = true
0.00.163.188 I ggml_metal_init: use bfloat            = true
0.00.163.188 I ggml_metal_init: hasUnifiedMemory      = true
0.00.163.189 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.194.973 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.224.200 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.224.207 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.224.232 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.227.930 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.227.933 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.227.933 I llama_init_from_model: graph nodes  = 967
0.00.227.933 I llama_init_from_model: graph splits = 2
0.00.227.940 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.228.069 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.228.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.292.550 I main: llama threadpool init, n_threads = 4
0.00.292.592 I 
0.00.292.622 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.292.623 I 
0.00.292.811 I sampler seed: 1234
0.00.292.816 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.292.850 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.292.851 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.292.851 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.126.600 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56349.21 tokens per second)
0.02.126.600 I llama_perf_context_print:        load time =     242.27 ms
0.02.126.601 I llama_perf_context_print: prompt eval time =      43.57 ms /     7 tokens (    6.22 ms per token,   160.66 tokens per second)
0.02.126.602 I llama_perf_context_print:        eval time =    1787.21 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.126.602 I llama_perf_context_print:       total time =    1834.93 ms /    70 tokens
0.02.126.898 I ggml_metal_free: deallocating

real	0m2.445s
user	0m0.130s
sys	0m0.139s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.009.977 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.180 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.185 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.187 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.188 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.188 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.189 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.189 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.190 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.191 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.191 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.191 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.192 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.193 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.195 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.196 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.037 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.095 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.923 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.924 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.924 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.925 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.925 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.926 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.926 I llama_model_loader: - type  f32:  194 tensors
0.00.032.927 I llama_model_loader: - type q8_0:   98 tensors
0.00.032.927 I print_info: file format = GGUF V3 (latest)
0.00.032.928 I print_info: file type   = Q8_0
0.00.032.929 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.041.016 I load: special tokens cache size = 25
0.00.047.485 I load: token to piece cache size = 0.2984 MB
0.00.047.502 I print_info: arch             = gptneox
0.00.047.503 I print_info: vocab_only       = 0
0.00.047.504 I print_info: n_ctx_train      = 2048
0.00.047.504 I print_info: n_embd           = 2048
0.00.047.504 I print_info: n_layer          = 24
0.00.047.511 I print_info: n_head           = 16
0.00.047.512 I print_info: n_head_kv        = 16
0.00.047.512 I print_info: n_rot            = 32
0.00.047.512 I print_info: n_swa            = 0
0.00.047.515 I print_info: n_embd_head_k    = 128
0.00.047.515 I print_info: n_embd_head_v    = 128
0.00.047.516 I print_info: n_gqa            = 1
0.00.047.516 I print_info: n_embd_k_gqa     = 2048
0.00.047.517 I print_info: n_embd_v_gqa     = 2048
0.00.047.518 I print_info: f_norm_eps       = 1.0e-05
0.00.047.518 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.047.519 I print_info: f_clamp_kqv      = 0.0e+00
0.00.047.519 I print_info: f_max_alibi_bias = 0.0e+00
0.00.047.519 I print_info: f_logit_scale    = 0.0e+00
0.00.047.520 I print_info: n_ff             = 8192
0.00.047.521 I print_info: n_expert         = 0
0.00.047.521 I print_info: n_expert_used    = 0
0.00.047.521 I print_info: causal attn      = 1
0.00.047.521 I print_info: pooling type     = 0
0.00.047.521 I print_info: rope type        = 2
0.00.047.522 I print_info: rope scaling     = linear
0.00.047.526 I print_info: freq_base_train  = 10000.0
0.00.047.527 I print_info: freq_scale_train = 1
0.00.047.527 I print_info: n_ctx_orig_yarn  = 2048
0.00.047.527 I print_info: rope_finetuned   = unknown
0.00.047.527 I print_info: ssm_d_conv       = 0
0.00.047.527 I print_info: ssm_d_inner      = 0
0.00.047.528 I print_info: ssm_d_state      = 0
0.00.047.529 I print_info: ssm_dt_rank      = 0
0.00.047.529 I print_info: ssm_dt_b_c_rms   = 0
0.00.047.529 I print_info: model type       = 1.4B
0.00.047.529 I print_info: model params     = 1.41 B
0.00.047.530 I print_info: general.name     = 1.4B
0.00.047.530 I print_info: vocab type       = BPE
0.00.047.531 I print_info: n_vocab          = 50304
0.00.047.531 I print_info: n_merges         = 50009
0.00.047.531 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.047.534 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.047.536 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.047.536 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.047.536 I print_info: LF token         = 187 'Ċ'
0.00.047.536 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.047.537 I print_info: max token length = 1024
0.00.047.537 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.151.545 I load_tensors: offloading 24 repeating layers to GPU
0.01.151.550 I load_tensors: offloading output layer to GPU
0.01.151.551 I load_tensors: offloaded 25/25 layers to GPU
0.01.151.570 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.151.572 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.152.551 I llama_init_from_model: n_seq_max     = 1
0.01.152.553 I llama_init_from_model: n_ctx         = 2048
0.01.152.554 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.152.554 I llama_init_from_model: n_batch       = 2048
0.01.152.554 I llama_init_from_model: n_ubatch      = 512
0.01.152.555 I llama_init_from_model: flash_attn    = 0
0.01.152.556 I llama_init_from_model: freq_base     = 10000.0
0.01.152.556 I llama_init_from_model: freq_scale    = 1
0.01.152.557 I ggml_metal_init: allocating
0.01.152.574 I ggml_metal_init: found device: Apple M4
0.01.152.583 I ggml_metal_init: picking default device: Apple M4
0.01.153.791 I ggml_metal_init: using embedded metal library
0.01.159.584 I ggml_metal_init: GPU name:   Apple M4
0.01.159.587 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.159.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.159.589 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.159.589 I ggml_metal_init: simdgroup reduction   = true
0.01.159.589 I ggml_metal_init: simdgroup matrix mul. = true
0.01.159.590 I ggml_metal_init: has residency sets    = true
0.01.159.590 I ggml_metal_init: has bfloat            = true
0.01.159.590 I ggml_metal_init: use bfloat            = true
0.01.159.591 I ggml_metal_init: hasUnifiedMemory      = true
0.01.159.594 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.175.960 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.227.123 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.227.129 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.227.153 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.231.464 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.231.466 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.231.466 I llama_init_from_model: graph nodes  = 967
0.01.231.467 I llama_init_from_model: graph splits = 2
0.01.231.472 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.231.610 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.231.611 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.289.266 I main: llama threadpool init, n_threads = 4
0.01.289.307 I 
0.01.289.333 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.289.333 I 
0.01.289.491 I sampler seed: 1234
0.01.289.496 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.289.512 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.289.514 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.289.514 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.376.586 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.02.376.587 I llama_perf_context_print:        load time =    1278.50 ms
0.02.376.588 I llama_perf_context_print: prompt eval time =      48.84 ms /     7 tokens (    6.98 ms per token,   143.34 tokens per second)
0.02.376.589 I llama_perf_context_print:        eval time =    1035.31 ms /    63 runs   (   16.43 ms per token,    60.85 tokens per second)
0.02.376.589 I llama_perf_context_print:       total time =    1088.11 ms /    70 tokens
0.02.376.846 I ggml_metal_free: deallocating

real	0m2.396s
user	0m0.108s
sys	0m0.286s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.082 I main: llama backend init
0.00.000.084 I main: load the model and apply lora adapter, if any
0.00.019.122 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.045.278 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.045.292 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.045.294 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.045.295 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.045.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.045.296 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.045.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.045.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.045.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.045.299 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.045.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.045.301 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.045.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.045.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.045.304 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.045.304 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.045.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.051.208 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.744 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.526 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.528 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.058.529 I llama_model_loader: - type  f32:  194 tensors
0.00.058.529 I llama_model_loader: - type q4_0:   97 tensors
0.00.058.530 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.530 I print_info: file format = GGUF V3 (latest)
0.00.058.532 I print_info: file type   = Q4_0
0.00.058.533 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.071.788 I load: special tokens cache size = 25
0.00.087.031 I load: token to piece cache size = 0.2984 MB
0.00.087.054 I print_info: arch             = gptneox
0.00.087.056 I print_info: vocab_only       = 0
0.00.087.057 I print_info: n_ctx_train      = 2048
0.00.087.057 I print_info: n_embd           = 2048
0.00.087.058 I print_info: n_layer          = 24
0.00.087.070 I print_info: n_head           = 16
0.00.087.072 I print_info: n_head_kv        = 16
0.00.087.072 I print_info: n_rot            = 32
0.00.087.072 I print_info: n_swa            = 0
0.00.087.073 I print_info: n_embd_head_k    = 128
0.00.087.073 I print_info: n_embd_head_v    = 128
0.00.087.074 I print_info: n_gqa            = 1
0.00.087.075 I print_info: n_embd_k_gqa     = 2048
0.00.087.076 I print_info: n_embd_v_gqa     = 2048
0.00.087.078 I print_info: f_norm_eps       = 1.0e-05
0.00.087.078 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.087.078 I print_info: f_clamp_kqv      = 0.0e+00
0.00.087.079 I print_info: f_max_alibi_bias = 0.0e+00
0.00.087.079 I print_info: f_logit_scale    = 0.0e+00
0.00.087.080 I print_info: n_ff             = 8192
0.00.087.081 I print_info: n_expert         = 0
0.00.087.081 I print_info: n_expert_used    = 0
0.00.087.081 I print_info: causal attn      = 1
0.00.087.081 I print_info: pooling type     = 0
0.00.087.082 I print_info: rope type        = 2
0.00.087.082 I print_info: rope scaling     = linear
0.00.087.083 I print_info: freq_base_train  = 10000.0
0.00.087.083 I print_info: freq_scale_train = 1
0.00.087.084 I print_info: n_ctx_orig_yarn  = 2048
0.00.087.086 I print_info: rope_finetuned   = unknown
0.00.087.089 I print_info: ssm_d_conv       = 0
0.00.087.089 I print_info: ssm_d_inner      = 0
0.00.087.089 I print_info: ssm_d_state      = 0
0.00.087.089 I print_info: ssm_dt_rank      = 0
0.00.087.090 I print_info: ssm_dt_b_c_rms   = 0
0.00.087.090 I print_info: model type       = 1.4B
0.00.087.091 I print_info: model params     = 1.41 B
0.00.087.091 I print_info: general.name     = 1.4B
0.00.087.092 I print_info: vocab type       = BPE
0.00.087.092 I print_info: n_vocab          = 50304
0.00.087.092 I print_info: n_merges         = 50009
0.00.087.093 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.087.093 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.087.095 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.087.100 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.087.104 I print_info: LF token         = 187 'Ċ'
0.00.087.104 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.087.104 I print_info: max token length = 1024
0.00.087.105 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.278 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.293 I load_tensors: offloading output layer to GPU
0.00.647.293 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.328 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.647.330 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.648.516 I llama_init_from_model: n_seq_max     = 1
0.00.648.518 I llama_init_from_model: n_ctx         = 2048
0.00.648.519 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.648.520 I llama_init_from_model: n_batch       = 2048
0.00.648.520 I llama_init_from_model: n_ubatch      = 512
0.00.648.520 I llama_init_from_model: flash_attn    = 0
0.00.648.522 I llama_init_from_model: freq_base     = 10000.0
0.00.648.523 I llama_init_from_model: freq_scale    = 1
0.00.648.527 I ggml_metal_init: allocating
0.00.648.639 I ggml_metal_init: found device: Apple M4
0.00.648.653 I ggml_metal_init: picking default device: Apple M4
0.00.650.340 I ggml_metal_init: using embedded metal library
0.00.656.424 I ggml_metal_init: GPU name:   Apple M4
0.00.656.428 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.656.429 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.656.430 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.656.431 I ggml_metal_init: simdgroup reduction   = true
0.00.656.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.656.431 I ggml_metal_init: has residency sets    = true
0.00.656.432 I ggml_metal_init: has bfloat            = true
0.00.656.432 I ggml_metal_init: use bfloat            = true
0.00.656.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.656.442 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.094 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.738.492 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.738.498 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.738.521 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.743.169 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.743.171 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.743.171 I llama_init_from_model: graph nodes  = 967
0.00.743.171 I llama_init_from_model: graph splits = 2
0.00.743.177 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.743.305 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.743.306 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.799.344 I main: llama threadpool init, n_threads = 4
0.00.799.388 I 
0.00.799.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.799.413 I 
0.00.799.557 I sampler seed: 1234
0.00.799.562 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.799.577 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.799.578 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.799.578 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.483.618 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47843.67 tokens per second)
0.01.483.618 I llama_perf_context_print:        load time =     779.50 ms
0.01.483.621 I llama_perf_context_print: prompt eval time =      49.05 ms /     7 tokens (    7.01 ms per token,   142.71 tokens per second)
0.01.483.621 I llama_perf_context_print:        eval time =     632.01 ms /    63 runs   (   10.03 ms per token,    99.68 tokens per second)
0.01.483.623 I llama_perf_context_print:       total time =     684.99 ms /    70 tokens
0.01.483.875 I ggml_metal_free: deallocating

real	0m1.514s
user	0m0.133s
sys	0m0.220s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.806 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.024.801 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.807 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.807 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.808 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.808 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.809 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.809 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.810 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.810 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.813 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.813 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.814 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.547 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.322 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.322 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.322 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.323 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.323 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.324 I llama_model_loader: - type  f32:  194 tensors
0.00.033.324 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.324 I llama_model_loader: - type q6_K:    1 tensors
0.00.033.324 I print_info: file format = GGUF V3 (latest)
0.00.033.325 I print_info: file type   = Q4_1
0.00.033.327 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.041.545 I load: special tokens cache size = 25
0.00.048.023 I load: token to piece cache size = 0.2984 MB
0.00.048.037 I print_info: arch             = gptneox
0.00.048.038 I print_info: vocab_only       = 0
0.00.048.039 I print_info: n_ctx_train      = 2048
0.00.048.039 I print_info: n_embd           = 2048
0.00.048.039 I print_info: n_layer          = 24
0.00.048.042 I print_info: n_head           = 16
0.00.048.042 I print_info: n_head_kv        = 16
0.00.048.043 I print_info: n_rot            = 32
0.00.048.043 I print_info: n_swa            = 0
0.00.048.043 I print_info: n_embd_head_k    = 128
0.00.048.043 I print_info: n_embd_head_v    = 128
0.00.048.044 I print_info: n_gqa            = 1
0.00.048.045 I print_info: n_embd_k_gqa     = 2048
0.00.048.045 I print_info: n_embd_v_gqa     = 2048
0.00.048.046 I print_info: f_norm_eps       = 1.0e-05
0.00.048.047 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.048.047 I print_info: f_clamp_kqv      = 0.0e+00
0.00.048.047 I print_info: f_max_alibi_bias = 0.0e+00
0.00.048.047 I print_info: f_logit_scale    = 0.0e+00
0.00.048.048 I print_info: n_ff             = 8192
0.00.048.048 I print_info: n_expert         = 0
0.00.048.048 I print_info: n_expert_used    = 0
0.00.048.048 I print_info: causal attn      = 1
0.00.048.048 I print_info: pooling type     = 0
0.00.048.050 I print_info: rope type        = 2
0.00.048.051 I print_info: rope scaling     = linear
0.00.048.052 I print_info: freq_base_train  = 10000.0
0.00.048.052 I print_info: freq_scale_train = 1
0.00.048.052 I print_info: n_ctx_orig_yarn  = 2048
0.00.048.052 I print_info: rope_finetuned   = unknown
0.00.048.053 I print_info: ssm_d_conv       = 0
0.00.048.053 I print_info: ssm_d_inner      = 0
0.00.048.053 I print_info: ssm_d_state      = 0
0.00.048.056 I print_info: ssm_dt_rank      = 0
0.00.048.056 I print_info: ssm_dt_b_c_rms   = 0
0.00.048.057 I print_info: model type       = 1.4B
0.00.048.057 I print_info: model params     = 1.41 B
0.00.048.057 I print_info: general.name     = 1.4B
0.00.048.058 I print_info: vocab type       = BPE
0.00.048.058 I print_info: n_vocab          = 50304
0.00.048.058 I print_info: n_merges         = 50009
0.00.048.059 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.048.059 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.048.059 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.048.059 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.048.059 I print_info: LF token         = 187 'Ċ'
0.00.048.060 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.048.060 I print_info: max token length = 1024
0.00.048.060 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.593.353 I load_tensors: offloading 24 repeating layers to GPU
0.00.593.371 I load_tensors: offloading output layer to GPU
0.00.593.371 I load_tensors: offloaded 25/25 layers to GPU
0.00.593.404 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.593.405 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.594.746 I llama_init_from_model: n_seq_max     = 1
0.00.594.749 I llama_init_from_model: n_ctx         = 2048
0.00.594.750 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.594.750 I llama_init_from_model: n_batch       = 2048
0.00.594.751 I llama_init_from_model: n_ubatch      = 512
0.00.594.751 I llama_init_from_model: flash_attn    = 0
0.00.594.754 I llama_init_from_model: freq_base     = 10000.0
0.00.594.754 I llama_init_from_model: freq_scale    = 1
0.00.594.756 I ggml_metal_init: allocating
0.00.594.827 I ggml_metal_init: found device: Apple M4
0.00.594.841 I ggml_metal_init: picking default device: Apple M4
0.00.596.360 I ggml_metal_init: using embedded metal library
0.00.601.995 I ggml_metal_init: GPU name:   Apple M4
0.00.602.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.002 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.003 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.003 I ggml_metal_init: simdgroup reduction   = true
0.00.602.004 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.004 I ggml_metal_init: has residency sets    = true
0.00.602.004 I ggml_metal_init: has bfloat            = true
0.00.602.005 I ggml_metal_init: use bfloat            = true
0.00.602.005 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.007 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.621.917 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.677.969 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.677.976 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.677.997 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.640 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.682.643 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.682.643 I llama_init_from_model: graph nodes  = 967
0.00.682.643 I llama_init_from_model: graph splits = 2
0.00.682.648 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.682.785 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.682.786 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.189 I main: llama threadpool init, n_threads = 4
0.00.728.231 I 
0.00.728.253 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.255 I 
0.00.728.404 I sampler seed: 1234
0.00.728.409 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.728.424 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.728.425 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.728.426 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.456.298 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55425.45 tokens per second)
0.01.456.298 I llama_perf_context_print:        load time =     718.65 ms
0.01.456.300 I llama_perf_context_print: prompt eval time =      39.30 ms /     7 tokens (    5.61 ms per token,   178.14 tokens per second)
0.01.456.301 I llama_perf_context_print:        eval time =     685.85 ms /    63 runs   (   10.89 ms per token,    91.86 tokens per second)
0.01.456.301 I llama_perf_context_print:       total time =     728.84 ms /    70 tokens
0.01.456.529 I ggml_metal_free: deallocating

real	0m1.472s
user	0m0.110s
sys	0m0.187s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.921 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.927 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.927 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.928 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.928 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.928 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.929 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.930 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.930 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.930 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.931 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.931 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.931 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.933 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.636 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.272 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.273 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.273 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.274 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.275 I llama_model_loader: - type  f32:  194 tensors
0.00.025.275 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.275 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.276 I print_info: file format = GGUF V3 (latest)
0.00.025.276 I print_info: file type   = Q5_0
0.00.025.281 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.181 I load: special tokens cache size = 25
0.00.039.496 I load: token to piece cache size = 0.2984 MB
0.00.039.510 I print_info: arch             = gptneox
0.00.039.511 I print_info: vocab_only       = 0
0.00.039.511 I print_info: n_ctx_train      = 2048
0.00.039.511 I print_info: n_embd           = 2048
0.00.039.512 I print_info: n_layer          = 24
0.00.039.514 I print_info: n_head           = 16
0.00.039.516 I print_info: n_head_kv        = 16
0.00.039.516 I print_info: n_rot            = 32
0.00.039.516 I print_info: n_swa            = 0
0.00.039.517 I print_info: n_embd_head_k    = 128
0.00.039.517 I print_info: n_embd_head_v    = 128
0.00.039.518 I print_info: n_gqa            = 1
0.00.039.518 I print_info: n_embd_k_gqa     = 2048
0.00.039.519 I print_info: n_embd_v_gqa     = 2048
0.00.039.520 I print_info: f_norm_eps       = 1.0e-05
0.00.039.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.520 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.522 I print_info: f_logit_scale    = 0.0e+00
0.00.039.523 I print_info: n_ff             = 8192
0.00.039.523 I print_info: n_expert         = 0
0.00.039.523 I print_info: n_expert_used    = 0
0.00.039.523 I print_info: causal attn      = 1
0.00.039.523 I print_info: pooling type     = 0
0.00.039.523 I print_info: rope type        = 2
0.00.039.523 I print_info: rope scaling     = linear
0.00.039.524 I print_info: freq_base_train  = 10000.0
0.00.039.524 I print_info: freq_scale_train = 1
0.00.039.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.524 I print_info: rope_finetuned   = unknown
0.00.039.524 I print_info: ssm_d_conv       = 0
0.00.039.525 I print_info: ssm_d_inner      = 0
0.00.039.525 I print_info: ssm_d_state      = 0
0.00.039.525 I print_info: ssm_dt_rank      = 0
0.00.039.525 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.525 I print_info: model type       = 1.4B
0.00.039.526 I print_info: model params     = 1.41 B
0.00.039.526 I print_info: general.name     = 1.4B
0.00.039.526 I print_info: vocab type       = BPE
0.00.039.527 I print_info: n_vocab          = 50304
0.00.039.527 I print_info: n_merges         = 50009
0.00.039.527 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.528 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.528 I print_info: LF token         = 187 'Ċ'
0.00.039.528 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.528 I print_info: max token length = 1024
0.00.039.529 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.652.893 I load_tensors: offloading 24 repeating layers to GPU
0.00.652.907 I load_tensors: offloading output layer to GPU
0.00.652.908 I load_tensors: offloaded 25/25 layers to GPU
0.00.652.942 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.652.944 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.654.446 I llama_init_from_model: n_seq_max     = 1
0.00.654.449 I llama_init_from_model: n_ctx         = 2048
0.00.654.450 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.654.450 I llama_init_from_model: n_batch       = 2048
0.00.654.451 I llama_init_from_model: n_ubatch      = 512
0.00.654.451 I llama_init_from_model: flash_attn    = 0
0.00.654.452 I llama_init_from_model: freq_base     = 10000.0
0.00.654.453 I llama_init_from_model: freq_scale    = 1
0.00.654.454 I ggml_metal_init: allocating
0.00.654.463 I ggml_metal_init: found device: Apple M4
0.00.654.471 I ggml_metal_init: picking default device: Apple M4
0.00.655.725 I ggml_metal_init: using embedded metal library
0.00.662.214 I ggml_metal_init: GPU name:   Apple M4
0.00.662.217 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.662.218 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.662.219 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.662.220 I ggml_metal_init: simdgroup reduction   = true
0.00.662.220 I ggml_metal_init: simdgroup matrix mul. = true
0.00.662.220 I ggml_metal_init: has residency sets    = true
0.00.662.221 I ggml_metal_init: has bfloat            = true
0.00.662.221 I ggml_metal_init: use bfloat            = true
0.00.662.222 I ggml_metal_init: hasUnifiedMemory      = true
0.00.662.223 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.545 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.736.313 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.736.319 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.736.343 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.740.934 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.740.937 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.740.937 I llama_init_from_model: graph nodes  = 967
0.00.740.938 I llama_init_from_model: graph splits = 2
0.00.740.943 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.741.064 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.741.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.797.314 I main: llama threadpool init, n_threads = 4
0.00.797.359 I 
0.00.797.383 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.797.385 I 
0.00.797.540 I sampler seed: 1234
0.00.797.545 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.588 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.591 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.591 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.589.393 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 50105.86 tokens per second)
0.01.589.393 I llama_perf_context_print:        load time =     787.77 ms
0.01.589.394 I llama_perf_context_print: prompt eval time =      43.09 ms /     7 tokens (    6.16 ms per token,   162.44 tokens per second)
0.01.589.395 I llama_perf_context_print:        eval time =     745.72 ms /    63 runs   (   11.84 ms per token,    84.48 tokens per second)
0.01.589.395 I llama_perf_context_print:       total time =     792.80 ms /    70 tokens
0.01.589.653 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.109s
sys	0m0.205s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.011.270 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.944 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.945 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.945 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.947 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.947 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.948 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.952 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.718 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.726 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.451 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.452 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.453 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.453 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.453 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.453 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.454 I llama_model_loader: - type  f32:  194 tensors
0.00.027.454 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.454 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.455 I print_info: file format = GGUF V3 (latest)
0.00.027.455 I print_info: file type   = Q5_1
0.00.027.456 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.328 I load: special tokens cache size = 25
0.00.041.687 I load: token to piece cache size = 0.2984 MB
0.00.041.700 I print_info: arch             = gptneox
0.00.041.701 I print_info: vocab_only       = 0
0.00.041.702 I print_info: n_ctx_train      = 2048
0.00.041.702 I print_info: n_embd           = 2048
0.00.041.702 I print_info: n_layer          = 24
0.00.041.709 I print_info: n_head           = 16
0.00.041.710 I print_info: n_head_kv        = 16
0.00.041.711 I print_info: n_rot            = 32
0.00.041.716 I print_info: n_swa            = 0
0.00.041.716 I print_info: n_embd_head_k    = 128
0.00.041.716 I print_info: n_embd_head_v    = 128
0.00.041.718 I print_info: n_gqa            = 1
0.00.041.719 I print_info: n_embd_k_gqa     = 2048
0.00.041.721 I print_info: n_embd_v_gqa     = 2048
0.00.041.722 I print_info: f_norm_eps       = 1.0e-05
0.00.041.722 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.722 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.723 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.723 I print_info: f_logit_scale    = 0.0e+00
0.00.041.723 I print_info: n_ff             = 8192
0.00.041.723 I print_info: n_expert         = 0
0.00.041.724 I print_info: n_expert_used    = 0
0.00.041.724 I print_info: causal attn      = 1
0.00.041.724 I print_info: pooling type     = 0
0.00.041.726 I print_info: rope type        = 2
0.00.041.727 I print_info: rope scaling     = linear
0.00.041.728 I print_info: freq_base_train  = 10000.0
0.00.041.728 I print_info: freq_scale_train = 1
0.00.041.728 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.728 I print_info: rope_finetuned   = unknown
0.00.041.729 I print_info: ssm_d_conv       = 0
0.00.041.729 I print_info: ssm_d_inner      = 0
0.00.041.729 I print_info: ssm_d_state      = 0
0.00.041.729 I print_info: ssm_dt_rank      = 0
0.00.041.729 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.729 I print_info: model type       = 1.4B
0.00.041.730 I print_info: model params     = 1.41 B
0.00.041.730 I print_info: general.name     = 1.4B
0.00.041.730 I print_info: vocab type       = BPE
0.00.041.731 I print_info: n_vocab          = 50304
0.00.041.731 I print_info: n_merges         = 50009
0.00.041.731 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.731 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.731 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.731 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.732 I print_info: LF token         = 187 'Ċ'
0.00.041.733 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.734 I print_info: max token length = 1024
0.00.041.734 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.683.052 I load_tensors: offloading 24 repeating layers to GPU
0.00.683.067 I load_tensors: offloading output layer to GPU
0.00.683.068 I load_tensors: offloaded 25/25 layers to GPU
0.00.683.108 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.683.110 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.684.706 I llama_init_from_model: n_seq_max     = 1
0.00.684.712 I llama_init_from_model: n_ctx         = 2048
0.00.684.712 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.684.713 I llama_init_from_model: n_batch       = 2048
0.00.684.714 I llama_init_from_model: n_ubatch      = 512
0.00.684.715 I llama_init_from_model: flash_attn    = 0
0.00.684.718 I llama_init_from_model: freq_base     = 10000.0
0.00.684.718 I llama_init_from_model: freq_scale    = 1
0.00.684.720 I ggml_metal_init: allocating
0.00.684.737 I ggml_metal_init: found device: Apple M4
0.00.684.745 I ggml_metal_init: picking default device: Apple M4
0.00.685.959 I ggml_metal_init: using embedded metal library
0.00.692.362 I ggml_metal_init: GPU name:   Apple M4
0.00.692.365 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.692.366 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.692.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.692.368 I ggml_metal_init: simdgroup reduction   = true
0.00.692.368 I ggml_metal_init: simdgroup matrix mul. = true
0.00.692.369 I ggml_metal_init: has residency sets    = true
0.00.692.369 I ggml_metal_init: has bfloat            = true
0.00.692.369 I ggml_metal_init: use bfloat            = true
0.00.692.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.692.371 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.710.102 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.763.867 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.763.874 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.763.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.768.044 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.768.046 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.768.046 I llama_init_from_model: graph nodes  = 967
0.00.768.046 I llama_init_from_model: graph splits = 2
0.00.768.052 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.768.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.768.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.828.929 I main: llama threadpool init, n_threads = 4
0.00.828.971 I 
0.00.828.995 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.828.995 I 
0.00.829.139 I sampler seed: 1234
0.00.829.144 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.829.158 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.829.160 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.829.160 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.671.536 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51300.58 tokens per second)
0.01.671.536 I llama_perf_context_print:        load time =     816.93 ms
0.01.671.537 I llama_perf_context_print: prompt eval time =      50.27 ms /     7 tokens (    7.18 ms per token,   139.24 tokens per second)
0.01.671.538 I llama_perf_context_print:        eval time =     789.23 ms /    63 runs   (   12.53 ms per token,    79.83 tokens per second)
0.01.671.539 I llama_perf_context_print:       total time =     843.34 ms /    70 tokens
0.01.671.810 I ggml_metal_free: deallocating

real	0m1.691s
user	0m0.109s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.008.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.632 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.637 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.639 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.639 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.639 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.641 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.641 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.642 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.643 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.643 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.651 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.652 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.652 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.657 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.657 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.658 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.360 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.453 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.230 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.231 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.232 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.232 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.233 I llama_model_loader: - type  f32:  194 tensors
0.00.024.233 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.233 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.234 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.234 I print_info: file format = GGUF V3 (latest)
0.00.024.235 I print_info: file type   = Q2_K - Medium
0.00.024.236 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.038 I load: special tokens cache size = 25
0.00.038.168 I load: token to piece cache size = 0.2984 MB
0.00.038.181 I print_info: arch             = gptneox
0.00.038.183 I print_info: vocab_only       = 0
0.00.038.183 I print_info: n_ctx_train      = 2048
0.00.038.183 I print_info: n_embd           = 2048
0.00.038.183 I print_info: n_layer          = 24
0.00.038.191 I print_info: n_head           = 16
0.00.038.192 I print_info: n_head_kv        = 16
0.00.038.193 I print_info: n_rot            = 32
0.00.038.194 I print_info: n_swa            = 0
0.00.038.194 I print_info: n_embd_head_k    = 128
0.00.038.194 I print_info: n_embd_head_v    = 128
0.00.038.194 I print_info: n_gqa            = 1
0.00.038.195 I print_info: n_embd_k_gqa     = 2048
0.00.038.198 I print_info: n_embd_v_gqa     = 2048
0.00.038.199 I print_info: f_norm_eps       = 1.0e-05
0.00.038.199 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.199 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.199 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.199 I print_info: f_logit_scale    = 0.0e+00
0.00.038.200 I print_info: n_ff             = 8192
0.00.038.200 I print_info: n_expert         = 0
0.00.038.200 I print_info: n_expert_used    = 0
0.00.038.201 I print_info: causal attn      = 1
0.00.038.203 I print_info: pooling type     = 0
0.00.038.203 I print_info: rope type        = 2
0.00.038.204 I print_info: rope scaling     = linear
0.00.038.204 I print_info: freq_base_train  = 10000.0
0.00.038.204 I print_info: freq_scale_train = 1
0.00.038.204 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.205 I print_info: rope_finetuned   = unknown
0.00.038.205 I print_info: ssm_d_conv       = 0
0.00.038.205 I print_info: ssm_d_inner      = 0
0.00.038.205 I print_info: ssm_d_state      = 0
0.00.038.205 I print_info: ssm_dt_rank      = 0
0.00.038.206 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.206 I print_info: model type       = 1.4B
0.00.038.207 I print_info: model params     = 1.41 B
0.00.038.207 I print_info: general.name     = 1.4B
0.00.038.207 I print_info: vocab type       = BPE
0.00.038.208 I print_info: n_vocab          = 50304
0.00.038.208 I print_info: n_merges         = 50009
0.00.038.208 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.208 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.208 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.208 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.209 I print_info: LF token         = 187 'Ċ'
0.00.038.209 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.209 I print_info: max token length = 1024
0.00.038.210 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.413.080 I load_tensors: offloading 24 repeating layers to GPU
0.00.413.097 I load_tensors: offloading output layer to GPU
0.00.413.098 I load_tensors: offloaded 25/25 layers to GPU
0.00.413.134 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.413.135 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.414.279 I llama_init_from_model: n_seq_max     = 1
0.00.414.282 I llama_init_from_model: n_ctx         = 2048
0.00.414.282 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.414.283 I llama_init_from_model: n_batch       = 2048
0.00.414.283 I llama_init_from_model: n_ubatch      = 512
0.00.414.284 I llama_init_from_model: flash_attn    = 0
0.00.414.285 I llama_init_from_model: freq_base     = 10000.0
0.00.414.286 I llama_init_from_model: freq_scale    = 1
0.00.414.288 I ggml_metal_init: allocating
0.00.414.372 I ggml_metal_init: found device: Apple M4
0.00.414.385 I ggml_metal_init: picking default device: Apple M4
0.00.416.008 I ggml_metal_init: using embedded metal library
0.00.421.615 I ggml_metal_init: GPU name:   Apple M4
0.00.421.627 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.421.628 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.421.629 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.421.630 I ggml_metal_init: simdgroup reduction   = true
0.00.421.630 I ggml_metal_init: simdgroup matrix mul. = true
0.00.421.631 I ggml_metal_init: has residency sets    = true
0.00.421.631 I ggml_metal_init: has bfloat            = true
0.00.421.631 I ggml_metal_init: use bfloat            = true
0.00.421.635 I ggml_metal_init: hasUnifiedMemory      = true
0.00.421.643 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.442.646 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.501.904 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.501.916 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.501.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.506.321 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.506.323 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.506.323 I llama_init_from_model: graph nodes  = 967
0.00.506.323 I llama_init_from_model: graph splits = 2
0.00.506.333 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.506.467 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.506.468 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.563.753 I main: llama threadpool init, n_threads = 4
0.00.563.799 I 
0.00.563.822 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.563.823 I 
0.00.563.987 I sampler seed: 1234
0.00.563.992 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.564.016 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.564.016 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.564.016 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.233.751 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.233.752 I llama_perf_context_print:        load time =     554.23 ms
0.01.233.753 I llama_perf_context_print: prompt eval time =      35.43 ms /     7 tokens (    5.06 ms per token,   197.55 tokens per second)
0.01.233.753 I llama_perf_context_print:        eval time =     631.55 ms /    63 runs   (   10.02 ms per token,    99.75 tokens per second)
0.01.233.753 I llama_perf_context_print:       total time =     670.72 ms /    70 tokens
0.01.233.985 I ggml_metal_free: deallocating

real	0m1.252s
user	0m0.111s
sys	0m0.172s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.008.908 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.521 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.526 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.528 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.529 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.529 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.529 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.530 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.531 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.531 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.531 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.532 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.532 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.532 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.533 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.536 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.536 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.536 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.369 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.108 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.109 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.110 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.111 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.111 I llama_model_loader: - type  f32:  194 tensors
0.00.025.111 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.112 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.112 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.112 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.113 I print_info: file format = GGUF V3 (latest)
0.00.025.113 I print_info: file type   = Q3_K - Medium
0.00.025.114 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.362 I load: special tokens cache size = 25
0.00.039.795 I load: token to piece cache size = 0.2984 MB
0.00.039.809 I print_info: arch             = gptneox
0.00.039.811 I print_info: vocab_only       = 0
0.00.039.811 I print_info: n_ctx_train      = 2048
0.00.039.811 I print_info: n_embd           = 2048
0.00.039.811 I print_info: n_layer          = 24
0.00.039.814 I print_info: n_head           = 16
0.00.039.815 I print_info: n_head_kv        = 16
0.00.039.815 I print_info: n_rot            = 32
0.00.039.815 I print_info: n_swa            = 0
0.00.039.815 I print_info: n_embd_head_k    = 128
0.00.039.815 I print_info: n_embd_head_v    = 128
0.00.039.816 I print_info: n_gqa            = 1
0.00.039.817 I print_info: n_embd_k_gqa     = 2048
0.00.039.818 I print_info: n_embd_v_gqa     = 2048
0.00.039.818 I print_info: f_norm_eps       = 1.0e-05
0.00.039.819 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.819 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.819 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.821 I print_info: f_logit_scale    = 0.0e+00
0.00.039.821 I print_info: n_ff             = 8192
0.00.039.821 I print_info: n_expert         = 0
0.00.039.822 I print_info: n_expert_used    = 0
0.00.039.823 I print_info: causal attn      = 1
0.00.039.824 I print_info: pooling type     = 0
0.00.039.824 I print_info: rope type        = 2
0.00.039.824 I print_info: rope scaling     = linear
0.00.039.825 I print_info: freq_base_train  = 10000.0
0.00.039.825 I print_info: freq_scale_train = 1
0.00.039.825 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.826 I print_info: rope_finetuned   = unknown
0.00.039.826 I print_info: ssm_d_conv       = 0
0.00.039.826 I print_info: ssm_d_inner      = 0
0.00.039.826 I print_info: ssm_d_state      = 0
0.00.039.826 I print_info: ssm_dt_rank      = 0
0.00.039.826 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.826 I print_info: model type       = 1.4B
0.00.039.826 I print_info: model params     = 1.41 B
0.00.039.827 I print_info: general.name     = 1.4B
0.00.039.827 I print_info: vocab type       = BPE
0.00.039.827 I print_info: n_vocab          = 50304
0.00.039.827 I print_info: n_merges         = 50009
0.00.039.828 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.828 I print_info: LF token         = 187 'Ċ'
0.00.039.831 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.831 I print_info: max token length = 1024
0.00.039.835 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.471.283 I load_tensors: offloading 24 repeating layers to GPU
0.00.471.300 I load_tensors: offloading output layer to GPU
0.00.471.301 I load_tensors: offloaded 25/25 layers to GPU
0.00.471.335 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.471.337 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.472.951 I llama_init_from_model: n_seq_max     = 1
0.00.472.954 I llama_init_from_model: n_ctx         = 2048
0.00.472.955 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.472.955 I llama_init_from_model: n_batch       = 2048
0.00.472.956 I llama_init_from_model: n_ubatch      = 512
0.00.472.956 I llama_init_from_model: flash_attn    = 0
0.00.472.958 I llama_init_from_model: freq_base     = 10000.0
0.00.472.959 I llama_init_from_model: freq_scale    = 1
0.00.472.961 I ggml_metal_init: allocating
0.00.473.032 I ggml_metal_init: found device: Apple M4
0.00.473.045 I ggml_metal_init: picking default device: Apple M4
0.00.474.648 I ggml_metal_init: using embedded metal library
0.00.480.599 I ggml_metal_init: GPU name:   Apple M4
0.00.480.604 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.480.605 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.480.606 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.480.607 I ggml_metal_init: simdgroup reduction   = true
0.00.480.607 I ggml_metal_init: simdgroup matrix mul. = true
0.00.480.607 I ggml_metal_init: has residency sets    = true
0.00.480.607 I ggml_metal_init: has bfloat            = true
0.00.480.608 I ggml_metal_init: use bfloat            = true
0.00.480.609 I ggml_metal_init: hasUnifiedMemory      = true
0.00.480.610 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.500.481 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.556.560 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.556.567 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.556.589 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.561.241 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.561.243 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.561.243 I llama_init_from_model: graph nodes  = 967
0.00.561.243 I llama_init_from_model: graph splits = 2
0.00.561.248 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.561.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.561.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.937 I main: llama threadpool init, n_threads = 4
0.00.613.979 I 
0.00.614.002 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.614.002 I 
0.00.614.155 I sampler seed: 1234
0.00.614.160 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.614.174 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.614.176 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.614.176 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.371.628 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48864.42 tokens per second)
0.01.371.629 I llama_perf_context_print:        load time =     604.24 ms
0.01.371.630 I llama_perf_context_print: prompt eval time =      50.29 ms /     7 tokens (    7.18 ms per token,   139.20 tokens per second)
0.01.371.631 I llama_perf_context_print:        eval time =     704.63 ms /    63 runs   (   11.18 ms per token,    89.41 tokens per second)
0.01.371.631 I llama_perf_context_print:       total time =     758.48 ms /    70 tokens
0.01.371.859 I ggml_metal_free: deallocating

real	0m1.388s
user	0m0.111s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.091 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.104 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.750 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.758 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.762 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.762 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.762 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.763 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.764 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.764 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.764 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.765 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.765 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.765 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.766 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.771 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.771 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.672 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.554 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.555 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.555 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.556 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.556 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.557 I llama_model_loader: - type  f32:  194 tensors
0.00.026.557 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.558 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.558 I llama_model_loader: - type q6_K:   13 tensors
0.00.026.559 I print_info: file format = GGUF V3 (latest)
0.00.026.564 I print_info: file type   = Q4_K - Medium
0.00.026.565 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.862 I load: special tokens cache size = 25
0.00.041.319 I load: token to piece cache size = 0.2984 MB
0.00.041.336 I print_info: arch             = gptneox
0.00.041.337 I print_info: vocab_only       = 0
0.00.041.337 I print_info: n_ctx_train      = 2048
0.00.041.337 I print_info: n_embd           = 2048
0.00.041.338 I print_info: n_layer          = 24
0.00.041.342 I print_info: n_head           = 16
0.00.041.343 I print_info: n_head_kv        = 16
0.00.041.343 I print_info: n_rot            = 32
0.00.041.343 I print_info: n_swa            = 0
0.00.041.343 I print_info: n_embd_head_k    = 128
0.00.041.343 I print_info: n_embd_head_v    = 128
0.00.041.344 I print_info: n_gqa            = 1
0.00.041.344 I print_info: n_embd_k_gqa     = 2048
0.00.041.345 I print_info: n_embd_v_gqa     = 2048
0.00.041.346 I print_info: f_norm_eps       = 1.0e-05
0.00.041.346 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.346 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.348 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.348 I print_info: f_logit_scale    = 0.0e+00
0.00.041.348 I print_info: n_ff             = 8192
0.00.041.348 I print_info: n_expert         = 0
0.00.041.349 I print_info: n_expert_used    = 0
0.00.041.349 I print_info: causal attn      = 1
0.00.041.350 I print_info: pooling type     = 0
0.00.041.352 I print_info: rope type        = 2
0.00.041.352 I print_info: rope scaling     = linear
0.00.041.352 I print_info: freq_base_train  = 10000.0
0.00.041.353 I print_info: freq_scale_train = 1
0.00.041.353 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.354 I print_info: rope_finetuned   = unknown
0.00.041.354 I print_info: ssm_d_conv       = 0
0.00.041.354 I print_info: ssm_d_inner      = 0
0.00.041.354 I print_info: ssm_d_state      = 0
0.00.041.354 I print_info: ssm_dt_rank      = 0
0.00.041.354 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.355 I print_info: model type       = 1.4B
0.00.041.358 I print_info: model params     = 1.41 B
0.00.041.358 I print_info: general.name     = 1.4B
0.00.041.358 I print_info: vocab type       = BPE
0.00.041.359 I print_info: n_vocab          = 50304
0.00.041.359 I print_info: n_merges         = 50009
0.00.041.359 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.359 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.359 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.359 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.360 I print_info: LF token         = 187 'Ċ'
0.00.041.360 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.360 I print_info: max token length = 1024
0.00.041.360 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.523.002 I load_tensors: offloading 24 repeating layers to GPU
0.00.523.019 I load_tensors: offloading output layer to GPU
0.00.523.020 I load_tensors: offloaded 25/25 layers to GPU
0.00.523.051 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.523.052 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.524.167 I llama_init_from_model: n_seq_max     = 1
0.00.524.171 I llama_init_from_model: n_ctx         = 2048
0.00.524.172 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.524.173 I llama_init_from_model: n_batch       = 2048
0.00.524.173 I llama_init_from_model: n_ubatch      = 512
0.00.524.174 I llama_init_from_model: flash_attn    = 0
0.00.524.186 I llama_init_from_model: freq_base     = 10000.0
0.00.524.187 I llama_init_from_model: freq_scale    = 1
0.00.524.194 I ggml_metal_init: allocating
0.00.524.287 I ggml_metal_init: found device: Apple M4
0.00.524.302 I ggml_metal_init: picking default device: Apple M4
0.00.525.845 I ggml_metal_init: using embedded metal library
0.00.535.116 I ggml_metal_init: GPU name:   Apple M4
0.00.535.139 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.535.140 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.535.141 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.535.141 I ggml_metal_init: simdgroup reduction   = true
0.00.535.141 I ggml_metal_init: simdgroup matrix mul. = true
0.00.535.142 I ggml_metal_init: has residency sets    = true
0.00.535.142 I ggml_metal_init: has bfloat            = true
0.00.535.142 I ggml_metal_init: use bfloat            = true
0.00.535.144 I ggml_metal_init: hasUnifiedMemory      = true
0.00.535.149 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.023 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.613.988 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.613.995 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.614.017 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.619.071 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.619.072 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.619.073 I llama_init_from_model: graph nodes  = 967
0.00.619.073 I llama_init_from_model: graph splits = 2
0.00.619.078 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.619.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.619.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.679.765 I main: llama threadpool init, n_threads = 4
0.00.679.805 I 
0.00.679.827 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.679.827 I 
0.00.680.004 I sampler seed: 1234
0.00.680.009 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.680.024 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.680.024 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.680.024 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.454.690 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49859.55 tokens per second)
0.01.454.691 I llama_perf_context_print:        load time =     668.92 ms
0.01.454.692 I llama_perf_context_print: prompt eval time =      57.99 ms /     7 tokens (    8.28 ms per token,   120.71 tokens per second)
0.01.454.694 I llama_perf_context_print:        eval time =     713.92 ms /    63 runs   (   11.33 ms per token,    88.24 tokens per second)
0.01.454.694 I llama_perf_context_print:       total time =     775.67 ms /    70 tokens
0.01.454.928 I ggml_metal_free: deallocating

real	0m1.475s
user	0m0.114s
sys	0m0.193s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.368 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.118 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.123 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.125 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.125 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.125 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.126 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.127 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.128 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.131 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.131 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.132 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.132 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.134 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.134 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.134 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.941 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.790 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.792 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.793 I llama_model_loader: - type  f32:  194 tensors
0.00.024.793 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.793 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.794 I print_info: file format = GGUF V3 (latest)
0.00.024.794 I print_info: file type   = Q5_K - Medium
0.00.024.795 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.077 I load: special tokens cache size = 25
0.00.039.358 I load: token to piece cache size = 0.2984 MB
0.00.039.372 I print_info: arch             = gptneox
0.00.039.373 I print_info: vocab_only       = 0
0.00.039.374 I print_info: n_ctx_train      = 2048
0.00.039.374 I print_info: n_embd           = 2048
0.00.039.374 I print_info: n_layer          = 24
0.00.039.377 I print_info: n_head           = 16
0.00.039.378 I print_info: n_head_kv        = 16
0.00.039.378 I print_info: n_rot            = 32
0.00.039.378 I print_info: n_swa            = 0
0.00.039.378 I print_info: n_embd_head_k    = 128
0.00.039.379 I print_info: n_embd_head_v    = 128
0.00.039.379 I print_info: n_gqa            = 1
0.00.039.380 I print_info: n_embd_k_gqa     = 2048
0.00.039.381 I print_info: n_embd_v_gqa     = 2048
0.00.039.381 I print_info: f_norm_eps       = 1.0e-05
0.00.039.382 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.383 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.383 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.383 I print_info: f_logit_scale    = 0.0e+00
0.00.039.384 I print_info: n_ff             = 8192
0.00.039.384 I print_info: n_expert         = 0
0.00.039.384 I print_info: n_expert_used    = 0
0.00.039.386 I print_info: causal attn      = 1
0.00.039.386 I print_info: pooling type     = 0
0.00.039.386 I print_info: rope type        = 2
0.00.039.386 I print_info: rope scaling     = linear
0.00.039.386 I print_info: freq_base_train  = 10000.0
0.00.039.387 I print_info: freq_scale_train = 1
0.00.039.387 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.387 I print_info: rope_finetuned   = unknown
0.00.039.387 I print_info: ssm_d_conv       = 0
0.00.039.387 I print_info: ssm_d_inner      = 0
0.00.039.387 I print_info: ssm_d_state      = 0
0.00.039.387 I print_info: ssm_dt_rank      = 0
0.00.039.387 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.388 I print_info: model type       = 1.4B
0.00.039.391 I print_info: model params     = 1.41 B
0.00.039.391 I print_info: general.name     = 1.4B
0.00.039.393 I print_info: vocab type       = BPE
0.00.039.393 I print_info: n_vocab          = 50304
0.00.039.393 I print_info: n_merges         = 50009
0.00.039.393 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.393 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.394 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.394 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.394 I print_info: LF token         = 187 'Ċ'
0.00.039.395 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.395 I print_info: max token length = 1024
0.00.039.396 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.233 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.236 I load_tensors: offloading output layer to GPU
0.00.588.236 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.259 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.588.264 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.589.701 I llama_init_from_model: n_seq_max     = 1
0.00.589.703 I llama_init_from_model: n_ctx         = 2048
0.00.589.704 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.589.704 I llama_init_from_model: n_batch       = 2048
0.00.589.704 I llama_init_from_model: n_ubatch      = 512
0.00.589.705 I llama_init_from_model: flash_attn    = 0
0.00.589.706 I llama_init_from_model: freq_base     = 10000.0
0.00.589.706 I llama_init_from_model: freq_scale    = 1
0.00.589.708 I ggml_metal_init: allocating
0.00.589.745 I ggml_metal_init: found device: Apple M4
0.00.589.758 I ggml_metal_init: picking default device: Apple M4
0.00.591.088 I ggml_metal_init: using embedded metal library
0.00.597.149 I ggml_metal_init: GPU name:   Apple M4
0.00.597.152 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.597.153 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.597.154 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.597.154 I ggml_metal_init: simdgroup reduction   = true
0.00.597.155 I ggml_metal_init: simdgroup matrix mul. = true
0.00.597.155 I ggml_metal_init: has residency sets    = true
0.00.597.155 I ggml_metal_init: has bfloat            = true
0.00.597.155 I ggml_metal_init: use bfloat            = true
0.00.597.156 I ggml_metal_init: hasUnifiedMemory      = true
0.00.597.157 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.434 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.668.677 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.668.685 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.668.711 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.673.049 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.673.051 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.673.051 I llama_init_from_model: graph nodes  = 967
0.00.673.052 I llama_init_from_model: graph splits = 2
0.00.673.056 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.673.185 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.673.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.661 I main: llama threadpool init, n_threads = 4
0.00.734.709 I 
0.00.734.752 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.753 I 
0.00.734.941 I sampler seed: 1234
0.00.734.945 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.960 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.960 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.962 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.584.098 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.584.099 I llama_perf_context_print:        load time =     724.54 ms
0.01.584.099 I llama_perf_context_print: prompt eval time =      52.63 ms /     7 tokens (    7.52 ms per token,   133.01 tokens per second)
0.01.584.100 I llama_perf_context_print:        eval time =     793.65 ms /    63 runs   (   12.60 ms per token,    79.38 tokens per second)
0.01.584.100 I llama_perf_context_print:       total time =     850.19 ms /    70 tokens
0.01.584.325 I ggml_metal_free: deallocating

real	0m1.601s
user	0m0.109s
sys	0m0.210s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.681 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.479 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.483 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.485 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.485 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.486 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.486 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.487 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.488 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.488 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.489 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.490 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.490 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.491 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.492 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.493 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.493 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.240 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.251 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.990 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.992 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.992 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.993 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.993 I llama_model_loader: - type  f32:  194 tensors
0.00.024.994 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.994 I print_info: file format = GGUF V3 (latest)
0.00.024.995 I print_info: file type   = Q6_K
0.00.024.995 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.907 I load: special tokens cache size = 25
0.00.039.062 I load: token to piece cache size = 0.2984 MB
0.00.039.071 I print_info: arch             = gptneox
0.00.039.072 I print_info: vocab_only       = 0
0.00.039.072 I print_info: n_ctx_train      = 2048
0.00.039.072 I print_info: n_embd           = 2048
0.00.039.072 I print_info: n_layer          = 24
0.00.039.075 I print_info: n_head           = 16
0.00.039.076 I print_info: n_head_kv        = 16
0.00.039.076 I print_info: n_rot            = 32
0.00.039.076 I print_info: n_swa            = 0
0.00.039.076 I print_info: n_embd_head_k    = 128
0.00.039.077 I print_info: n_embd_head_v    = 128
0.00.039.079 I print_info: n_gqa            = 1
0.00.039.080 I print_info: n_embd_k_gqa     = 2048
0.00.039.081 I print_info: n_embd_v_gqa     = 2048
0.00.039.081 I print_info: f_norm_eps       = 1.0e-05
0.00.039.082 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.082 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.082 I print_info: f_logit_scale    = 0.0e+00
0.00.039.083 I print_info: n_ff             = 8192
0.00.039.083 I print_info: n_expert         = 0
0.00.039.083 I print_info: n_expert_used    = 0
0.00.039.083 I print_info: causal attn      = 1
0.00.039.083 I print_info: pooling type     = 0
0.00.039.085 I print_info: rope type        = 2
0.00.039.086 I print_info: rope scaling     = linear
0.00.039.086 I print_info: freq_base_train  = 10000.0
0.00.039.086 I print_info: freq_scale_train = 1
0.00.039.086 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.087 I print_info: rope_finetuned   = unknown
0.00.039.087 I print_info: ssm_d_conv       = 0
0.00.039.087 I print_info: ssm_d_inner      = 0
0.00.039.087 I print_info: ssm_d_state      = 0
0.00.039.087 I print_info: ssm_dt_rank      = 0
0.00.039.087 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.088 I print_info: model type       = 1.4B
0.00.039.088 I print_info: model params     = 1.41 B
0.00.039.088 I print_info: general.name     = 1.4B
0.00.039.088 I print_info: vocab type       = BPE
0.00.039.089 I print_info: n_vocab          = 50304
0.00.039.090 I print_info: n_merges         = 50009
0.00.039.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: LF token         = 187 'Ċ'
0.00.039.092 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.092 I print_info: max token length = 1024
0.00.039.092 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.073 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.076 I load_tensors: offloading output layer to GPU
0.00.638.077 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.101 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.638.103 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.478 I llama_init_from_model: n_seq_max     = 1
0.00.639.480 I llama_init_from_model: n_ctx         = 2048
0.00.639.481 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.639.481 I llama_init_from_model: n_batch       = 2048
0.00.639.482 I llama_init_from_model: n_ubatch      = 512
0.00.639.482 I llama_init_from_model: flash_attn    = 0
0.00.639.483 I llama_init_from_model: freq_base     = 10000.0
0.00.639.484 I llama_init_from_model: freq_scale    = 1
0.00.639.485 I ggml_metal_init: allocating
0.00.639.516 I ggml_metal_init: found device: Apple M4
0.00.639.527 I ggml_metal_init: picking default device: Apple M4
0.00.640.803 I ggml_metal_init: using embedded metal library
0.00.646.857 I ggml_metal_init: GPU name:   Apple M4
0.00.646.862 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.863 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.864 I ggml_metal_init: simdgroup reduction   = true
0.00.646.864 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.864 I ggml_metal_init: has residency sets    = true
0.00.646.864 I ggml_metal_init: has bfloat            = true
0.00.646.865 I ggml_metal_init: use bfloat            = true
0.00.646.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.869 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.690 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.722.505 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.722.512 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.722.534 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.727.774 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.727.776 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.727.777 I llama_init_from_model: graph nodes  = 967
0.00.727.777 I llama_init_from_model: graph splits = 2
0.00.727.783 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.727.911 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.727.911 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.814 I main: llama threadpool init, n_threads = 4
0.00.796.859 I 
0.00.796.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.883 I 
0.00.797.065 I sampler seed: 1234
0.00.797.070 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.797.085 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.797.085 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.797.085 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.672.188 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.01.672.189 I llama_perf_context_print:        load time =     786.40 ms
0.01.672.190 I llama_perf_context_print: prompt eval time =      57.59 ms /     7 tokens (    8.23 ms per token,   121.54 tokens per second)
0.01.672.191 I llama_perf_context_print:        eval time =     814.58 ms /    63 runs   (   12.93 ms per token,    77.34 tokens per second)
0.01.672.191 I llama_perf_context_print:       total time =     876.10 ms /    70 tokens
0.01.672.443 I ggml_metal_free: deallocating

real	0m1.688s
user	0m0.108s
sys	0m0.216s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.701 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.308 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.648 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.654 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.657 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.657 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.658 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.659 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.659 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.661 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.661 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.662 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.663 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.663 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.664 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.664 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.667 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.667 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.668 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.068 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.960 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.225 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.226 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.227 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.227 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.228 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.228 I llama_model_loader: - type  f32:  194 tensors
0.00.051.229 I llama_model_loader: - type  f16:   98 tensors
0.00.051.230 I print_info: file format = GGUF V3 (latest)
0.00.051.230 I print_info: file type   = all F32 (guessed)
0.00.051.231 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.772 I load: special tokens cache size = 25
0.00.070.885 I load: token to piece cache size = 0.2984 MB
0.00.070.893 I print_info: arch             = gptneox
0.00.070.894 I print_info: vocab_only       = 0
0.00.070.894 I print_info: n_ctx_train      = 2048
0.00.070.895 I print_info: n_embd           = 2048
0.00.070.895 I print_info: n_layer          = 24
0.00.070.898 I print_info: n_head           = 16
0.00.070.898 I print_info: n_head_kv        = 16
0.00.070.899 I print_info: n_rot            = 32
0.00.070.899 I print_info: n_swa            = 0
0.00.070.899 I print_info: n_embd_head_k    = 128
0.00.070.899 I print_info: n_embd_head_v    = 128
0.00.070.900 I print_info: n_gqa            = 1
0.00.070.901 I print_info: n_embd_k_gqa     = 2048
0.00.070.901 I print_info: n_embd_v_gqa     = 2048
0.00.070.902 I print_info: f_norm_eps       = 1.0e-05
0.00.070.902 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.903 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.903 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.903 I print_info: f_logit_scale    = 0.0e+00
0.00.070.904 I print_info: n_ff             = 8192
0.00.070.904 I print_info: n_expert         = 0
0.00.070.904 I print_info: n_expert_used    = 0
0.00.070.904 I print_info: causal attn      = 1
0.00.070.904 I print_info: pooling type     = 0
0.00.070.904 I print_info: rope type        = 2
0.00.070.904 I print_info: rope scaling     = linear
0.00.070.905 I print_info: freq_base_train  = 10000.0
0.00.070.905 I print_info: freq_scale_train = 1
0.00.070.905 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.905 I print_info: rope_finetuned   = unknown
0.00.070.906 I print_info: ssm_d_conv       = 0
0.00.070.906 I print_info: ssm_d_inner      = 0
0.00.070.906 I print_info: ssm_d_state      = 0
0.00.070.907 I print_info: ssm_dt_rank      = 0
0.00.070.907 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.908 I print_info: model type       = 1.4B
0.00.070.908 I print_info: model params     = 1.41 B
0.00.070.908 I print_info: general.name     = 1.4B
0.00.070.909 I print_info: vocab type       = BPE
0.00.070.909 I print_info: n_vocab          = 50304
0.00.070.909 I print_info: n_merges         = 50009
0.00.070.909 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.910 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.910 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.910 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.910 I print_info: LF token         = 187 'Ċ'
0.00.070.911 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.911 I print_info: max token length = 1024
0.00.070.911 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.360.075 I load_tensors: offloading 24 repeating layers to GPU
0.01.360.081 I load_tensors: offloading output layer to GPU
0.01.360.081 I load_tensors: offloaded 25/25 layers to GPU
0.01.360.102 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.360.104 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.360.800 I llama_init_from_model: n_seq_max     = 1
0.01.360.801 I llama_init_from_model: n_ctx         = 128
0.01.360.801 I llama_init_from_model: n_ctx_per_seq = 128
0.01.360.801 I llama_init_from_model: n_batch       = 128
0.01.360.802 I llama_init_from_model: n_ubatch      = 128
0.01.360.802 I llama_init_from_model: flash_attn    = 0
0.01.360.802 I llama_init_from_model: freq_base     = 10000.0
0.01.360.803 I llama_init_from_model: freq_scale    = 1
0.01.360.803 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.360.804 I ggml_metal_init: allocating
0.01.360.832 I ggml_metal_init: found device: Apple M4
0.01.360.838 I ggml_metal_init: picking default device: Apple M4
0.01.361.712 I ggml_metal_init: using embedded metal library
0.01.365.693 I ggml_metal_init: GPU name:   Apple M4
0.01.365.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.365.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.365.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.365.696 I ggml_metal_init: simdgroup reduction   = true
0.01.365.696 I ggml_metal_init: simdgroup matrix mul. = true
0.01.365.696 I ggml_metal_init: has residency sets    = true
0.01.365.697 I ggml_metal_init: has bfloat            = true
0.01.365.697 I ggml_metal_init: use bfloat            = true
0.01.365.697 I ggml_metal_init: hasUnifiedMemory      = true
0.01.365.699 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.376.382 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.378.141 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.378.143 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.378.159 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.379.755 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.379.756 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.379.756 I llama_init_from_model: graph nodes  = 967
0.01.379.756 I llama_init_from_model: graph splits = 2
0.01.379.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.379.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.415.484 I 
0.01.415.528 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.415.561 I perplexity: tokenizing the input ..
0.01.420.718 I perplexity: tokenization took 5.154 ms
0.01.420.722 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.539.747 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.541.348 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.541.367 I llama_perf_context_print:        load time =    1395.17 ms
0.01.541.368 I llama_perf_context_print: prompt eval time =     118.71 ms /   128 tokens (    0.93 ms per token,  1078.23 tokens per second)
0.01.541.368 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.541.369 I llama_perf_context_print:       total time =     125.88 ms /   129 tokens
0.01.541.798 I ggml_metal_free: deallocating

real	0m1.732s
user	0m0.096s
sys	0m0.247s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.244 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.605 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.614 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.614 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.617 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.619 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.619 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.619 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.620 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.620 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.620 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.621 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.622 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.623 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.623 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.441 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.459 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.243 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.245 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.245 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.246 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.246 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.246 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.247 I llama_model_loader: - type  f32:  194 tensors
0.00.025.247 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.248 I print_info: file format = GGUF V3 (latest)
0.00.025.249 I print_info: file type   = Q8_0
0.00.025.250 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.857 I load: special tokens cache size = 25
0.00.040.125 I load: token to piece cache size = 0.2984 MB
0.00.040.143 I print_info: arch             = gptneox
0.00.040.143 I print_info: vocab_only       = 0
0.00.040.144 I print_info: n_ctx_train      = 2048
0.00.040.144 I print_info: n_embd           = 2048
0.00.040.144 I print_info: n_layer          = 24
0.00.040.148 I print_info: n_head           = 16
0.00.040.148 I print_info: n_head_kv        = 16
0.00.040.148 I print_info: n_rot            = 32
0.00.040.148 I print_info: n_swa            = 0
0.00.040.149 I print_info: n_embd_head_k    = 128
0.00.040.149 I print_info: n_embd_head_v    = 128
0.00.040.149 I print_info: n_gqa            = 1
0.00.040.150 I print_info: n_embd_k_gqa     = 2048
0.00.040.152 I print_info: n_embd_v_gqa     = 2048
0.00.040.152 I print_info: f_norm_eps       = 1.0e-05
0.00.040.153 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.161 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.162 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.162 I print_info: f_logit_scale    = 0.0e+00
0.00.040.166 I print_info: n_ff             = 8192
0.00.040.166 I print_info: n_expert         = 0
0.00.040.166 I print_info: n_expert_used    = 0
0.00.040.167 I print_info: causal attn      = 1
0.00.040.167 I print_info: pooling type     = 0
0.00.040.167 I print_info: rope type        = 2
0.00.040.167 I print_info: rope scaling     = linear
0.00.040.167 I print_info: freq_base_train  = 10000.0
0.00.040.168 I print_info: freq_scale_train = 1
0.00.040.168 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.168 I print_info: rope_finetuned   = unknown
0.00.040.168 I print_info: ssm_d_conv       = 0
0.00.040.168 I print_info: ssm_d_inner      = 0
0.00.040.168 I print_info: ssm_d_state      = 0
0.00.040.169 I print_info: ssm_dt_rank      = 0
0.00.040.169 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.169 I print_info: model type       = 1.4B
0.00.040.169 I print_info: model params     = 1.41 B
0.00.040.169 I print_info: general.name     = 1.4B
0.00.040.170 I print_info: vocab type       = BPE
0.00.040.170 I print_info: n_vocab          = 50304
0.00.040.170 I print_info: n_merges         = 50009
0.00.040.170 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.171 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.171 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.171 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.171 I print_info: LF token         = 187 'Ċ'
0.00.040.172 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.172 I print_info: max token length = 1024
0.00.040.172 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.775.476 I load_tensors: offloading 24 repeating layers to GPU
0.00.775.484 I load_tensors: offloading output layer to GPU
0.00.775.484 I load_tensors: offloaded 25/25 layers to GPU
0.00.775.512 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.775.513 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.776.941 I llama_init_from_model: n_seq_max     = 1
0.00.776.943 I llama_init_from_model: n_ctx         = 128
0.00.776.943 I llama_init_from_model: n_ctx_per_seq = 128
0.00.776.943 I llama_init_from_model: n_batch       = 128
0.00.776.944 I llama_init_from_model: n_ubatch      = 128
0.00.776.944 I llama_init_from_model: flash_attn    = 0
0.00.776.945 I llama_init_from_model: freq_base     = 10000.0
0.00.776.945 I llama_init_from_model: freq_scale    = 1
0.00.776.946 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.776.947 I ggml_metal_init: allocating
0.00.777.040 I ggml_metal_init: found device: Apple M4
0.00.777.048 I ggml_metal_init: picking default device: Apple M4
0.00.778.261 I ggml_metal_init: using embedded metal library
0.00.783.566 I ggml_metal_init: GPU name:   Apple M4
0.00.783.570 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.783.570 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.783.571 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.783.571 I ggml_metal_init: simdgroup reduction   = true
0.00.783.572 I ggml_metal_init: simdgroup matrix mul. = true
0.00.783.572 I ggml_metal_init: has residency sets    = true
0.00.783.572 I ggml_metal_init: has bfloat            = true
0.00.783.572 I ggml_metal_init: use bfloat            = true
0.00.783.573 I ggml_metal_init: hasUnifiedMemory      = true
0.00.783.577 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.798.347 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.801.769 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.801.773 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.801.799 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.804.876 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.804.878 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.804.878 I llama_init_from_model: graph nodes  = 967
0.00.804.879 I llama_init_from_model: graph splits = 2
0.00.804.881 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.804.881 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.832.951 I 
0.00.833.018 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.833.045 I perplexity: tokenizing the input ..
0.00.840.214 I perplexity: tokenization took 7.166 ms
0.00.840.228 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.978.118 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.979.541 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.979.553 I llama_perf_context_print:        load time =     823.70 ms
0.00.979.554 I llama_perf_context_print: prompt eval time =     137.23 ms /   128 tokens (    1.07 ms per token,   932.73 tokens per second)
0.00.979.555 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.979.558 I llama_perf_context_print:       total time =     146.61 ms /   129 tokens
0.00.979.942 I ggml_metal_free: deallocating

real	0m0.994s
user	0m0.075s
sys	0m0.164s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.142 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.276 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.606 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.618 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.619 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.619 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.619 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.620 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.621 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.621 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.621 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.622 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.623 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.625 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.625 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.625 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.408 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.441 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.220 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.221 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.222 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.222 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.222 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.223 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.223 I llama_model_loader: - type  f32:  194 tensors
0.00.027.224 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.224 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.225 I print_info: file format = GGUF V3 (latest)
0.00.027.225 I print_info: file type   = Q4_0
0.00.027.226 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.465 I load: special tokens cache size = 25
0.00.041.956 I load: token to piece cache size = 0.2984 MB
0.00.041.974 I print_info: arch             = gptneox
0.00.041.975 I print_info: vocab_only       = 0
0.00.041.975 I print_info: n_ctx_train      = 2048
0.00.041.975 I print_info: n_embd           = 2048
0.00.041.976 I print_info: n_layer          = 24
0.00.041.980 I print_info: n_head           = 16
0.00.041.981 I print_info: n_head_kv        = 16
0.00.041.981 I print_info: n_rot            = 32
0.00.041.981 I print_info: n_swa            = 0
0.00.041.981 I print_info: n_embd_head_k    = 128
0.00.041.981 I print_info: n_embd_head_v    = 128
0.00.041.982 I print_info: n_gqa            = 1
0.00.041.982 I print_info: n_embd_k_gqa     = 2048
0.00.041.983 I print_info: n_embd_v_gqa     = 2048
0.00.041.984 I print_info: f_norm_eps       = 1.0e-05
0.00.041.984 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.984 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.984 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.984 I print_info: f_logit_scale    = 0.0e+00
0.00.041.985 I print_info: n_ff             = 8192
0.00.041.985 I print_info: n_expert         = 0
0.00.041.985 I print_info: n_expert_used    = 0
0.00.041.985 I print_info: causal attn      = 1
0.00.041.985 I print_info: pooling type     = 0
0.00.041.986 I print_info: rope type        = 2
0.00.041.986 I print_info: rope scaling     = linear
0.00.041.986 I print_info: freq_base_train  = 10000.0
0.00.041.986 I print_info: freq_scale_train = 1
0.00.041.986 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.987 I print_info: rope_finetuned   = unknown
0.00.041.989 I print_info: ssm_d_conv       = 0
0.00.041.990 I print_info: ssm_d_inner      = 0
0.00.041.990 I print_info: ssm_d_state      = 0
0.00.041.990 I print_info: ssm_dt_rank      = 0
0.00.041.990 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.990 I print_info: model type       = 1.4B
0.00.041.990 I print_info: model params     = 1.41 B
0.00.041.991 I print_info: general.name     = 1.4B
0.00.041.991 I print_info: vocab type       = BPE
0.00.041.991 I print_info: n_vocab          = 50304
0.00.041.991 I print_info: n_merges         = 50009
0.00.041.992 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.992 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.992 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.992 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.992 I print_info: LF token         = 187 'Ċ'
0.00.041.993 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.993 I print_info: max token length = 1024
0.00.041.993 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.588.545 I load_tensors: offloading 24 repeating layers to GPU
0.00.588.561 I load_tensors: offloading output layer to GPU
0.00.588.562 I load_tensors: offloaded 25/25 layers to GPU
0.00.588.594 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.588.596 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.590.242 I llama_init_from_model: n_seq_max     = 1
0.00.590.245 I llama_init_from_model: n_ctx         = 128
0.00.590.246 I llama_init_from_model: n_ctx_per_seq = 128
0.00.590.246 I llama_init_from_model: n_batch       = 128
0.00.590.247 I llama_init_from_model: n_ubatch      = 128
0.00.590.247 I llama_init_from_model: flash_attn    = 0
0.00.590.249 I llama_init_from_model: freq_base     = 10000.0
0.00.590.250 I llama_init_from_model: freq_scale    = 1
0.00.590.250 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.590.252 I ggml_metal_init: allocating
0.00.590.334 I ggml_metal_init: found device: Apple M4
0.00.590.348 I ggml_metal_init: picking default device: Apple M4
0.00.591.865 I ggml_metal_init: using embedded metal library
0.00.598.081 I ggml_metal_init: GPU name:   Apple M4
0.00.598.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.598.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.598.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.598.092 I ggml_metal_init: simdgroup reduction   = true
0.00.598.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.598.093 I ggml_metal_init: has residency sets    = true
0.00.598.093 I ggml_metal_init: has bfloat            = true
0.00.598.094 I ggml_metal_init: use bfloat            = true
0.00.598.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.598.099 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.630 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.621.246 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.621.253 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.621.306 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.568 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.570 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.571 I llama_init_from_model: graph nodes  = 967
0.00.624.571 I llama_init_from_model: graph splits = 2
0.00.624.574 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.574 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.654.078 I 
0.00.654.161 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.654.188 I perplexity: tokenizing the input ..
0.00.661.348 I perplexity: tokenization took 7.159 ms
0.00.661.356 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.825 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.797.168 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.797.187 I llama_perf_context_print:        load time =     642.79 ms
0.00.797.188 I llama_perf_context_print: prompt eval time =     134.24 ms /   128 tokens (    1.05 ms per token,   953.52 tokens per second)
0.00.797.188 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.797.190 I llama_perf_context_print:       total time =     143.11 ms /   129 tokens
0.00.797.581 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.080s
sys	0m0.132s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.792 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.500 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.506 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.507 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.508 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.508 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.508 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.509 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.510 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.511 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.512 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.512 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.515 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.515 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.184 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.185 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.186 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.186 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.186 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.187 I llama_model_loader: - type  f32:  194 tensors
0.00.025.187 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.188 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.188 I print_info: file format = GGUF V3 (latest)
0.00.025.189 I print_info: file type   = Q4_1
0.00.025.190 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.327 I load: special tokens cache size = 25
0.00.039.771 I load: token to piece cache size = 0.2984 MB
0.00.039.789 I print_info: arch             = gptneox
0.00.039.789 I print_info: vocab_only       = 0
0.00.039.790 I print_info: n_ctx_train      = 2048
0.00.039.790 I print_info: n_embd           = 2048
0.00.039.790 I print_info: n_layer          = 24
0.00.039.794 I print_info: n_head           = 16
0.00.039.794 I print_info: n_head_kv        = 16
0.00.039.794 I print_info: n_rot            = 32
0.00.039.795 I print_info: n_swa            = 0
0.00.039.795 I print_info: n_embd_head_k    = 128
0.00.039.795 I print_info: n_embd_head_v    = 128
0.00.039.795 I print_info: n_gqa            = 1
0.00.039.796 I print_info: n_embd_k_gqa     = 2048
0.00.039.796 I print_info: n_embd_v_gqa     = 2048
0.00.039.797 I print_info: f_norm_eps       = 1.0e-05
0.00.039.797 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.798 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.798 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.799 I print_info: f_logit_scale    = 0.0e+00
0.00.039.800 I print_info: n_ff             = 8192
0.00.039.800 I print_info: n_expert         = 0
0.00.039.800 I print_info: n_expert_used    = 0
0.00.039.800 I print_info: causal attn      = 1
0.00.039.800 I print_info: pooling type     = 0
0.00.039.800 I print_info: rope type        = 2
0.00.039.800 I print_info: rope scaling     = linear
0.00.039.801 I print_info: freq_base_train  = 10000.0
0.00.039.801 I print_info: freq_scale_train = 1
0.00.039.801 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.801 I print_info: rope_finetuned   = unknown
0.00.039.804 I print_info: ssm_d_conv       = 0
0.00.039.804 I print_info: ssm_d_inner      = 0
0.00.039.804 I print_info: ssm_d_state      = 0
0.00.039.804 I print_info: ssm_dt_rank      = 0
0.00.039.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.805 I print_info: model type       = 1.4B
0.00.039.805 I print_info: model params     = 1.41 B
0.00.039.805 I print_info: general.name     = 1.4B
0.00.039.806 I print_info: vocab type       = BPE
0.00.039.806 I print_info: n_vocab          = 50304
0.00.039.806 I print_info: n_merges         = 50009
0.00.039.806 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.806 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.807 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.807 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.807 I print_info: LF token         = 187 'Ċ'
0.00.039.807 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.809 I print_info: max token length = 1024
0.00.039.809 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.558.427 I load_tensors: offloading 24 repeating layers to GPU
0.00.558.442 I load_tensors: offloading output layer to GPU
0.00.558.443 I load_tensors: offloaded 25/25 layers to GPU
0.00.558.478 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.558.479 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.560.228 I llama_init_from_model: n_seq_max     = 1
0.00.560.230 I llama_init_from_model: n_ctx         = 128
0.00.560.231 I llama_init_from_model: n_ctx_per_seq = 128
0.00.560.232 I llama_init_from_model: n_batch       = 128
0.00.560.232 I llama_init_from_model: n_ubatch      = 128
0.00.560.232 I llama_init_from_model: flash_attn    = 0
0.00.560.234 I llama_init_from_model: freq_base     = 10000.0
0.00.560.235 I llama_init_from_model: freq_scale    = 1
0.00.560.235 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.560.237 I ggml_metal_init: allocating
0.00.560.358 I ggml_metal_init: found device: Apple M4
0.00.560.375 I ggml_metal_init: picking default device: Apple M4
0.00.561.998 I ggml_metal_init: using embedded metal library
0.00.568.911 I ggml_metal_init: GPU name:   Apple M4
0.00.568.919 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.568.920 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.568.921 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.568.922 I ggml_metal_init: simdgroup reduction   = true
0.00.568.922 I ggml_metal_init: simdgroup matrix mul. = true
0.00.568.922 I ggml_metal_init: has residency sets    = true
0.00.568.922 I ggml_metal_init: has bfloat            = true
0.00.568.923 I ggml_metal_init: use bfloat            = true
0.00.568.924 I ggml_metal_init: hasUnifiedMemory      = true
0.00.568.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.586.688 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.590.173 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.590.180 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.590.214 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.593.509 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.593.511 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.593.511 I llama_init_from_model: graph nodes  = 967
0.00.593.512 I llama_init_from_model: graph splits = 2
0.00.593.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.593.518 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.622.212 I 
0.00.622.296 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.622.325 I perplexity: tokenizing the input ..
0.00.629.468 I perplexity: tokenization took 7.139 ms
0.00.629.476 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.521 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.761.858 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.761.874 I llama_perf_context_print:        load time =     613.41 ms
0.00.761.875 I llama_perf_context_print: prompt eval time =     130.08 ms /   128 tokens (    1.02 ms per token,   984.01 tokens per second)
0.00.761.875 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.876 I llama_perf_context_print:       total time =     139.67 ms /   129 tokens
0.00.762.278 I ggml_metal_free: deallocating

real	0m0.776s
user	0m0.080s
sys	0m0.119s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.396 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.402 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.404 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.407 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.407 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.408 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.412 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.413 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.414 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.414 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.414 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.417 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.420 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.421 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.421 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.211 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.188 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.930 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.932 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.932 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.933 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.933 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.934 I llama_model_loader: - type  f32:  194 tensors
0.00.024.934 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.934 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.935 I print_info: file format = GGUF V3 (latest)
0.00.024.935 I print_info: file type   = Q5_0
0.00.024.937 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.123 I load: special tokens cache size = 25
0.00.039.587 I load: token to piece cache size = 0.2984 MB
0.00.039.604 I print_info: arch             = gptneox
0.00.039.605 I print_info: vocab_only       = 0
0.00.039.605 I print_info: n_ctx_train      = 2048
0.00.039.605 I print_info: n_embd           = 2048
0.00.039.606 I print_info: n_layer          = 24
0.00.039.609 I print_info: n_head           = 16
0.00.039.610 I print_info: n_head_kv        = 16
0.00.039.610 I print_info: n_rot            = 32
0.00.039.610 I print_info: n_swa            = 0
0.00.039.610 I print_info: n_embd_head_k    = 128
0.00.039.611 I print_info: n_embd_head_v    = 128
0.00.039.611 I print_info: n_gqa            = 1
0.00.039.612 I print_info: n_embd_k_gqa     = 2048
0.00.039.612 I print_info: n_embd_v_gqa     = 2048
0.00.039.613 I print_info: f_norm_eps       = 1.0e-05
0.00.039.613 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.616 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.616 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.617 I print_info: f_logit_scale    = 0.0e+00
0.00.039.617 I print_info: n_ff             = 8192
0.00.039.617 I print_info: n_expert         = 0
0.00.039.618 I print_info: n_expert_used    = 0
0.00.039.618 I print_info: causal attn      = 1
0.00.039.618 I print_info: pooling type     = 0
0.00.039.618 I print_info: rope type        = 2
0.00.039.618 I print_info: rope scaling     = linear
0.00.039.618 I print_info: freq_base_train  = 10000.0
0.00.039.623 I print_info: freq_scale_train = 1
0.00.039.623 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.623 I print_info: rope_finetuned   = unknown
0.00.039.623 I print_info: ssm_d_conv       = 0
0.00.039.624 I print_info: ssm_d_inner      = 0
0.00.039.624 I print_info: ssm_d_state      = 0
0.00.039.624 I print_info: ssm_dt_rank      = 0
0.00.039.624 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.628 I print_info: model type       = 1.4B
0.00.039.630 I print_info: model params     = 1.41 B
0.00.039.630 I print_info: general.name     = 1.4B
0.00.039.631 I print_info: vocab type       = BPE
0.00.039.632 I print_info: n_vocab          = 50304
0.00.039.632 I print_info: n_merges         = 50009
0.00.039.632 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.632 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.633 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.634 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.634 I print_info: LF token         = 187 'Ċ'
0.00.039.634 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.634 I print_info: max token length = 1024
0.00.039.635 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.647.622 I load_tensors: offloading 24 repeating layers to GPU
0.00.647.635 I load_tensors: offloading output layer to GPU
0.00.647.636 I load_tensors: offloaded 25/25 layers to GPU
0.00.647.671 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.647.673 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.649.233 I llama_init_from_model: n_seq_max     = 1
0.00.649.236 I llama_init_from_model: n_ctx         = 128
0.00.649.236 I llama_init_from_model: n_ctx_per_seq = 128
0.00.649.237 I llama_init_from_model: n_batch       = 128
0.00.649.237 I llama_init_from_model: n_ubatch      = 128
0.00.649.237 I llama_init_from_model: flash_attn    = 0
0.00.649.240 I llama_init_from_model: freq_base     = 10000.0
0.00.649.240 I llama_init_from_model: freq_scale    = 1
0.00.649.241 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.649.243 I ggml_metal_init: allocating
0.00.649.328 I ggml_metal_init: found device: Apple M4
0.00.649.342 I ggml_metal_init: picking default device: Apple M4
0.00.650.894 I ggml_metal_init: using embedded metal library
0.00.657.832 I ggml_metal_init: GPU name:   Apple M4
0.00.657.840 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.657.840 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.657.841 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.657.842 I ggml_metal_init: simdgroup reduction   = true
0.00.657.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.657.843 I ggml_metal_init: has residency sets    = true
0.00.657.843 I ggml_metal_init: has bfloat            = true
0.00.657.843 I ggml_metal_init: use bfloat            = true
0.00.657.844 I ggml_metal_init: hasUnifiedMemory      = true
0.00.657.847 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.013 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.679.511 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.679.519 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.679.554 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.682.994 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.682.996 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.682.997 I llama_init_from_model: graph nodes  = 967
0.00.682.997 I llama_init_from_model: graph splits = 2
0.00.683.000 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.683.000 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.268 I 
0.00.715.352 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.715.380 I perplexity: tokenizing the input ..
0.00.722.019 I perplexity: tokenization took 6.637 ms
0.00.722.025 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.861.469 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.862.874 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.862.893 I llama_perf_context_print:        load time =     706.33 ms
0.00.862.894 I llama_perf_context_print: prompt eval time =     138.89 ms /   128 tokens (    1.09 ms per token,   921.59 tokens per second)
0.00.862.895 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.862.895 I llama_perf_context_print:       total time =     147.63 ms /   129 tokens
0.00.863.303 I ggml_metal_free: deallocating

real	0m0.877s
user	0m0.079s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.957 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.438 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.444 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.446 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.448 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.449 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.449 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.449 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.450 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.451 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.451 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.452 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.452 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.453 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.454 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.455 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.456 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.456 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.401 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.281 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.281 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.282 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.282 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.282 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.283 I llama_model_loader: - type  f32:  194 tensors
0.00.026.283 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.284 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.284 I print_info: file format = GGUF V3 (latest)
0.00.026.285 I print_info: file type   = Q5_1
0.00.026.286 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.776 I load: special tokens cache size = 25
0.00.041.240 I load: token to piece cache size = 0.2984 MB
0.00.041.258 I print_info: arch             = gptneox
0.00.041.260 I print_info: vocab_only       = 0
0.00.041.260 I print_info: n_ctx_train      = 2048
0.00.041.260 I print_info: n_embd           = 2048
0.00.041.260 I print_info: n_layer          = 24
0.00.041.264 I print_info: n_head           = 16
0.00.041.265 I print_info: n_head_kv        = 16
0.00.041.265 I print_info: n_rot            = 32
0.00.041.265 I print_info: n_swa            = 0
0.00.041.265 I print_info: n_embd_head_k    = 128
0.00.041.268 I print_info: n_embd_head_v    = 128
0.00.041.268 I print_info: n_gqa            = 1
0.00.041.269 I print_info: n_embd_k_gqa     = 2048
0.00.041.269 I print_info: n_embd_v_gqa     = 2048
0.00.041.270 I print_info: f_norm_eps       = 1.0e-05
0.00.041.270 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.272 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.272 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.272 I print_info: f_logit_scale    = 0.0e+00
0.00.041.273 I print_info: n_ff             = 8192
0.00.041.273 I print_info: n_expert         = 0
0.00.041.273 I print_info: n_expert_used    = 0
0.00.041.273 I print_info: causal attn      = 1
0.00.041.273 I print_info: pooling type     = 0
0.00.041.273 I print_info: rope type        = 2
0.00.041.274 I print_info: rope scaling     = linear
0.00.041.274 I print_info: freq_base_train  = 10000.0
0.00.041.274 I print_info: freq_scale_train = 1
0.00.041.274 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.276 I print_info: rope_finetuned   = unknown
0.00.041.276 I print_info: ssm_d_conv       = 0
0.00.041.276 I print_info: ssm_d_inner      = 0
0.00.041.277 I print_info: ssm_d_state      = 0
0.00.041.277 I print_info: ssm_dt_rank      = 0
0.00.041.277 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.277 I print_info: model type       = 1.4B
0.00.041.277 I print_info: model params     = 1.41 B
0.00.041.277 I print_info: general.name     = 1.4B
0.00.041.278 I print_info: vocab type       = BPE
0.00.041.278 I print_info: n_vocab          = 50304
0.00.041.278 I print_info: n_merges         = 50009
0.00.041.279 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.279 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.279 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.279 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.279 I print_info: LF token         = 187 'Ċ'
0.00.041.280 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.280 I print_info: max token length = 1024
0.00.041.280 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.694.061 I load_tensors: offloading 24 repeating layers to GPU
0.00.694.077 I load_tensors: offloading output layer to GPU
0.00.694.078 I load_tensors: offloaded 25/25 layers to GPU
0.00.694.116 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.694.118 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.695.928 I llama_init_from_model: n_seq_max     = 1
0.00.695.931 I llama_init_from_model: n_ctx         = 128
0.00.695.931 I llama_init_from_model: n_ctx_per_seq = 128
0.00.695.932 I llama_init_from_model: n_batch       = 128
0.00.695.932 I llama_init_from_model: n_ubatch      = 128
0.00.695.933 I llama_init_from_model: flash_attn    = 0
0.00.695.935 I llama_init_from_model: freq_base     = 10000.0
0.00.695.936 I llama_init_from_model: freq_scale    = 1
0.00.695.936 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.695.941 I ggml_metal_init: allocating
0.00.696.081 I ggml_metal_init: found device: Apple M4
0.00.696.096 I ggml_metal_init: picking default device: Apple M4
0.00.697.670 I ggml_metal_init: using embedded metal library
0.00.704.363 I ggml_metal_init: GPU name:   Apple M4
0.00.704.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.704.370 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.704.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.704.371 I ggml_metal_init: simdgroup reduction   = true
0.00.704.371 I ggml_metal_init: simdgroup matrix mul. = true
0.00.704.372 I ggml_metal_init: has residency sets    = true
0.00.704.372 I ggml_metal_init: has bfloat            = true
0.00.704.372 I ggml_metal_init: use bfloat            = true
0.00.704.373 I ggml_metal_init: hasUnifiedMemory      = true
0.00.704.376 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.721.714 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.725.083 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.725.087 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.725.117 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.273 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.728.275 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.728.276 I llama_init_from_model: graph nodes  = 967
0.00.728.276 I llama_init_from_model: graph splits = 2
0.00.728.278 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.728.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.524 I 
0.00.758.607 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.633 I perplexity: tokenizing the input ..
0.00.765.739 I perplexity: tokenization took 7.102 ms
0.00.765.750 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.900.139 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.901.482 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.901.494 I llama_perf_context_print:        load time =     748.56 ms
0.00.901.495 I llama_perf_context_print: prompt eval time =     133.50 ms /   128 tokens (    1.04 ms per token,   958.80 tokens per second)
0.00.901.496 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.497 I llama_perf_context_print:       total time =     142.98 ms /   129 tokens
0.00.901.872 I ggml_metal_free: deallocating

real	0m0.918s
user	0m0.080s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.696 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.751 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.752 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.752 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.755 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.755 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.756 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.758 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.758 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.758 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.474 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.535 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.262 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.264 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.264 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.264 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.265 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.265 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.266 I llama_model_loader: - type  f32:  194 tensors
0.00.024.266 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.266 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.267 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.267 I print_info: file format = GGUF V3 (latest)
0.00.024.268 I print_info: file type   = Q2_K - Medium
0.00.024.269 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.462 I load: special tokens cache size = 25
0.00.038.713 I load: token to piece cache size = 0.2984 MB
0.00.038.731 I print_info: arch             = gptneox
0.00.038.732 I print_info: vocab_only       = 0
0.00.038.732 I print_info: n_ctx_train      = 2048
0.00.038.732 I print_info: n_embd           = 2048
0.00.038.732 I print_info: n_layer          = 24
0.00.038.736 I print_info: n_head           = 16
0.00.038.737 I print_info: n_head_kv        = 16
0.00.038.739 I print_info: n_rot            = 32
0.00.038.739 I print_info: n_swa            = 0
0.00.038.739 I print_info: n_embd_head_k    = 128
0.00.038.739 I print_info: n_embd_head_v    = 128
0.00.038.740 I print_info: n_gqa            = 1
0.00.038.740 I print_info: n_embd_k_gqa     = 2048
0.00.038.741 I print_info: n_embd_v_gqa     = 2048
0.00.038.741 I print_info: f_norm_eps       = 1.0e-05
0.00.038.742 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.742 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.742 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.742 I print_info: f_logit_scale    = 0.0e+00
0.00.038.743 I print_info: n_ff             = 8192
0.00.038.743 I print_info: n_expert         = 0
0.00.038.743 I print_info: n_expert_used    = 0
0.00.038.743 I print_info: causal attn      = 1
0.00.038.743 I print_info: pooling type     = 0
0.00.038.744 I print_info: rope type        = 2
0.00.038.744 I print_info: rope scaling     = linear
0.00.038.744 I print_info: freq_base_train  = 10000.0
0.00.038.744 I print_info: freq_scale_train = 1
0.00.038.747 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.747 I print_info: rope_finetuned   = unknown
0.00.038.747 I print_info: ssm_d_conv       = 0
0.00.038.747 I print_info: ssm_d_inner      = 0
0.00.038.747 I print_info: ssm_d_state      = 0
0.00.038.747 I print_info: ssm_dt_rank      = 0
0.00.038.747 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.748 I print_info: model type       = 1.4B
0.00.038.748 I print_info: model params     = 1.41 B
0.00.038.748 I print_info: general.name     = 1.4B
0.00.038.749 I print_info: vocab type       = BPE
0.00.038.749 I print_info: n_vocab          = 50304
0.00.038.749 I print_info: n_merges         = 50009
0.00.038.749 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.749 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: LF token         = 187 'Ċ'
0.00.038.750 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: max token length = 1024
0.00.038.751 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.399.731 I load_tensors: offloading 24 repeating layers to GPU
0.00.399.747 I load_tensors: offloading output layer to GPU
0.00.399.748 I load_tensors: offloaded 25/25 layers to GPU
0.00.399.780 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.399.781 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.401.491 I llama_init_from_model: n_seq_max     = 1
0.00.401.494 I llama_init_from_model: n_ctx         = 128
0.00.401.495 I llama_init_from_model: n_ctx_per_seq = 128
0.00.401.495 I llama_init_from_model: n_batch       = 128
0.00.401.496 I llama_init_from_model: n_ubatch      = 128
0.00.401.496 I llama_init_from_model: flash_attn    = 0
0.00.401.498 I llama_init_from_model: freq_base     = 10000.0
0.00.401.499 I llama_init_from_model: freq_scale    = 1
0.00.401.500 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.401.503 I ggml_metal_init: allocating
0.00.401.586 I ggml_metal_init: found device: Apple M4
0.00.401.601 I ggml_metal_init: picking default device: Apple M4
0.00.403.126 I ggml_metal_init: using embedded metal library
0.00.408.627 I ggml_metal_init: GPU name:   Apple M4
0.00.408.646 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.408.647 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.408.648 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.408.648 I ggml_metal_init: simdgroup reduction   = true
0.00.408.649 I ggml_metal_init: simdgroup matrix mul. = true
0.00.408.649 I ggml_metal_init: has residency sets    = true
0.00.408.649 I ggml_metal_init: has bfloat            = true
0.00.408.649 I ggml_metal_init: use bfloat            = true
0.00.408.651 I ggml_metal_init: hasUnifiedMemory      = true
0.00.408.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.430.568 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.249 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.434.266 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.434.325 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.437.767 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.437.769 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.437.770 I llama_init_from_model: graph nodes  = 967
0.00.437.770 I llama_init_from_model: graph splits = 2
0.00.437.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.437.774 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.578 I 
0.00.470.667 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.693 I perplexity: tokenizing the input ..
0.00.477.188 I perplexity: tokenization took 6.492 ms
0.00.477.193 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.621.742 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.623.082 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.623.094 I llama_perf_context_print:        load time =     461.87 ms
0.00.623.096 I llama_perf_context_print: prompt eval time =     144.16 ms /   128 tokens (    1.13 ms per token,   887.87 tokens per second)
0.00.623.097 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.623.097 I llama_perf_context_print:       total time =     152.52 ms /   129 tokens
0.00.623.491 I ggml_metal_free: deallocating

real	0m0.637s
user	0m0.081s
sys	0m0.087s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.858 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.031 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.038 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.040 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.040 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.040 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.041 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.041 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.043 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.043 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.043 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.047 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.047 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.047 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.049 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.049 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.049 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.867 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.630 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.632 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.632 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.633 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.633 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.634 I llama_model_loader: - type  f32:  194 tensors
0.00.024.634 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.634 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.635 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.635 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.636 I print_info: file format = GGUF V3 (latest)
0.00.024.636 I print_info: file type   = Q3_K - Medium
0.00.024.637 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.724 I load: special tokens cache size = 25
0.00.039.140 I load: token to piece cache size = 0.2984 MB
0.00.039.157 I print_info: arch             = gptneox
0.00.039.158 I print_info: vocab_only       = 0
0.00.039.158 I print_info: n_ctx_train      = 2048
0.00.039.159 I print_info: n_embd           = 2048
0.00.039.159 I print_info: n_layer          = 24
0.00.039.163 I print_info: n_head           = 16
0.00.039.164 I print_info: n_head_kv        = 16
0.00.039.164 I print_info: n_rot            = 32
0.00.039.164 I print_info: n_swa            = 0
0.00.039.164 I print_info: n_embd_head_k    = 128
0.00.039.166 I print_info: n_embd_head_v    = 128
0.00.039.166 I print_info: n_gqa            = 1
0.00.039.167 I print_info: n_embd_k_gqa     = 2048
0.00.039.167 I print_info: n_embd_v_gqa     = 2048
0.00.039.168 I print_info: f_norm_eps       = 1.0e-05
0.00.039.168 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.168 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.168 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.169 I print_info: f_logit_scale    = 0.0e+00
0.00.039.169 I print_info: n_ff             = 8192
0.00.039.169 I print_info: n_expert         = 0
0.00.039.169 I print_info: n_expert_used    = 0
0.00.039.169 I print_info: causal attn      = 1
0.00.039.170 I print_info: pooling type     = 0
0.00.039.170 I print_info: rope type        = 2
0.00.039.170 I print_info: rope scaling     = linear
0.00.039.170 I print_info: freq_base_train  = 10000.0
0.00.039.171 I print_info: freq_scale_train = 1
0.00.039.171 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.171 I print_info: rope_finetuned   = unknown
0.00.039.171 I print_info: ssm_d_conv       = 0
0.00.039.171 I print_info: ssm_d_inner      = 0
0.00.039.171 I print_info: ssm_d_state      = 0
0.00.039.171 I print_info: ssm_dt_rank      = 0
0.00.039.171 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.172 I print_info: model type       = 1.4B
0.00.039.172 I print_info: model params     = 1.41 B
0.00.039.172 I print_info: general.name     = 1.4B
0.00.039.172 I print_info: vocab type       = BPE
0.00.039.173 I print_info: n_vocab          = 50304
0.00.039.173 I print_info: n_merges         = 50009
0.00.039.173 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.173 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.173 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: LF token         = 187 'Ċ'
0.00.039.174 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.174 I print_info: max token length = 1024
0.00.039.175 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.466.605 I load_tensors: offloading 24 repeating layers to GPU
0.00.466.620 I load_tensors: offloading output layer to GPU
0.00.466.621 I load_tensors: offloaded 25/25 layers to GPU
0.00.466.655 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.466.657 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.468.230 I llama_init_from_model: n_seq_max     = 1
0.00.468.233 I llama_init_from_model: n_ctx         = 128
0.00.468.234 I llama_init_from_model: n_ctx_per_seq = 128
0.00.468.234 I llama_init_from_model: n_batch       = 128
0.00.468.234 I llama_init_from_model: n_ubatch      = 128
0.00.468.235 I llama_init_from_model: flash_attn    = 0
0.00.468.237 I llama_init_from_model: freq_base     = 10000.0
0.00.468.237 I llama_init_from_model: freq_scale    = 1
0.00.468.238 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.468.241 I ggml_metal_init: allocating
0.00.468.341 I ggml_metal_init: found device: Apple M4
0.00.468.356 I ggml_metal_init: picking default device: Apple M4
0.00.469.936 I ggml_metal_init: using embedded metal library
0.00.476.553 I ggml_metal_init: GPU name:   Apple M4
0.00.476.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.476.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.476.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.476.564 I ggml_metal_init: simdgroup reduction   = true
0.00.476.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.476.564 I ggml_metal_init: has residency sets    = true
0.00.476.565 I ggml_metal_init: has bfloat            = true
0.00.476.565 I ggml_metal_init: use bfloat            = true
0.00.476.566 I ggml_metal_init: hasUnifiedMemory      = true
0.00.476.571 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.495.537 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.499.040 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.499.047 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.499.086 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.502.440 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.502.442 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.502.442 I llama_init_from_model: graph nodes  = 967
0.00.502.443 I llama_init_from_model: graph splits = 2
0.00.502.446 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.502.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.286 I 
0.00.532.367 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.394 I perplexity: tokenizing the input ..
0.00.539.687 I perplexity: tokenization took 7.29 ms
0.00.539.698 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.684.785 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.686.140 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.686.153 I llama_perf_context_print:        load time =     523.42 ms
0.00.686.154 I llama_perf_context_print: prompt eval time =     144.23 ms /   128 tokens (    1.13 ms per token,   887.49 tokens per second)
0.00.686.155 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.686.155 I llama_perf_context_print:       total time =     153.87 ms /   129 tokens
0.00.686.524 I ggml_metal_free: deallocating

real	0m0.699s
user	0m0.080s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.193 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.166 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.019.173 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.175 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.176 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.176 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.176 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.177 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.178 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.178 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.178 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.179 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.179 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.179 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.180 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.182 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.182 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.977 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.798 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.800 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.800 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.801 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.801 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.801 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.027.802 I llama_model_loader: - type  f32:  194 tensors
0.00.027.802 I llama_model_loader: - type q4_K:   61 tensors
0.00.027.802 I llama_model_loader: - type q5_K:   24 tensors
0.00.027.803 I llama_model_loader: - type q6_K:   13 tensors
0.00.027.803 I print_info: file format = GGUF V3 (latest)
0.00.027.803 I print_info: file type   = Q4_K - Medium
0.00.027.805 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.036.232 I load: special tokens cache size = 25
0.00.042.838 I load: token to piece cache size = 0.2984 MB
0.00.042.856 I print_info: arch             = gptneox
0.00.042.857 I print_info: vocab_only       = 0
0.00.042.857 I print_info: n_ctx_train      = 2048
0.00.042.858 I print_info: n_embd           = 2048
0.00.042.858 I print_info: n_layer          = 24
0.00.042.861 I print_info: n_head           = 16
0.00.042.862 I print_info: n_head_kv        = 16
0.00.042.862 I print_info: n_rot            = 32
0.00.042.862 I print_info: n_swa            = 0
0.00.042.862 I print_info: n_embd_head_k    = 128
0.00.042.862 I print_info: n_embd_head_v    = 128
0.00.042.863 I print_info: n_gqa            = 1
0.00.042.863 I print_info: n_embd_k_gqa     = 2048
0.00.042.864 I print_info: n_embd_v_gqa     = 2048
0.00.042.865 I print_info: f_norm_eps       = 1.0e-05
0.00.042.865 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.865 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.865 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.865 I print_info: f_logit_scale    = 0.0e+00
0.00.042.866 I print_info: n_ff             = 8192
0.00.042.866 I print_info: n_expert         = 0
0.00.042.866 I print_info: n_expert_used    = 0
0.00.042.866 I print_info: causal attn      = 1
0.00.042.867 I print_info: pooling type     = 0
0.00.042.867 I print_info: rope type        = 2
0.00.042.867 I print_info: rope scaling     = linear
0.00.042.867 I print_info: freq_base_train  = 10000.0
0.00.042.868 I print_info: freq_scale_train = 1
0.00.042.868 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.868 I print_info: rope_finetuned   = unknown
0.00.042.868 I print_info: ssm_d_conv       = 0
0.00.042.868 I print_info: ssm_d_inner      = 0
0.00.042.868 I print_info: ssm_d_state      = 0
0.00.042.868 I print_info: ssm_dt_rank      = 0
0.00.042.869 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.869 I print_info: model type       = 1.4B
0.00.042.869 I print_info: model params     = 1.41 B
0.00.042.869 I print_info: general.name     = 1.4B
0.00.042.870 I print_info: vocab type       = BPE
0.00.042.870 I print_info: n_vocab          = 50304
0.00.042.870 I print_info: n_merges         = 50009
0.00.042.870 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.870 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.871 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.871 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.871 I print_info: LF token         = 187 'Ċ'
0.00.042.873 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.874 I print_info: max token length = 1024
0.00.042.875 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.549.294 I load_tensors: offloading 24 repeating layers to GPU
0.00.549.307 I load_tensors: offloading output layer to GPU
0.00.549.308 I load_tensors: offloaded 25/25 layers to GPU
0.00.549.340 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.549.341 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.550.855 I llama_init_from_model: n_seq_max     = 1
0.00.550.860 I llama_init_from_model: n_ctx         = 128
0.00.550.860 I llama_init_from_model: n_ctx_per_seq = 128
0.00.550.861 I llama_init_from_model: n_batch       = 128
0.00.550.861 I llama_init_from_model: n_ubatch      = 128
0.00.550.861 I llama_init_from_model: flash_attn    = 0
0.00.550.863 I llama_init_from_model: freq_base     = 10000.0
0.00.550.863 I llama_init_from_model: freq_scale    = 1
0.00.550.864 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.550.866 I ggml_metal_init: allocating
0.00.550.950 I ggml_metal_init: found device: Apple M4
0.00.550.965 I ggml_metal_init: picking default device: Apple M4
0.00.552.438 I ggml_metal_init: using embedded metal library
0.00.558.046 I ggml_metal_init: GPU name:   Apple M4
0.00.558.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.558.070 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.558.071 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.558.071 I ggml_metal_init: simdgroup reduction   = true
0.00.558.072 I ggml_metal_init: simdgroup matrix mul. = true
0.00.558.072 I ggml_metal_init: has residency sets    = true
0.00.558.072 I ggml_metal_init: has bfloat            = true
0.00.558.072 I ggml_metal_init: use bfloat            = true
0.00.558.074 I ggml_metal_init: hasUnifiedMemory      = true
0.00.558.079 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.578.395 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.581.978 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.581.986 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.582.039 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.585.227 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.585.229 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.585.230 I llama_init_from_model: graph nodes  = 967
0.00.585.230 I llama_init_from_model: graph splits = 2
0.00.585.233 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.585.233 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.733 I 
0.00.615.800 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.826 I perplexity: tokenizing the input ..
0.00.622.404 I perplexity: tokenization took 6.576 ms
0.00.622.409 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.765.484 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.766.798 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.766.815 I llama_perf_context_print:        load time =     605.53 ms
0.00.766.816 I llama_perf_context_print: prompt eval time =     142.20 ms /   128 tokens (    1.11 ms per token,   900.12 tokens per second)
0.00.766.817 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.766.817 I llama_perf_context_print:       total time =     151.08 ms /   129 tokens
0.00.767.187 I ggml_metal_free: deallocating

real	0m0.783s
user	0m0.080s
sys	0m0.145s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.695 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.455 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.462 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.463 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.464 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.464 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.465 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.465 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.466 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.467 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.468 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.469 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.470 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.472 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.260 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.138 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.140 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.140 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.140 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.141 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.141 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.142 I llama_model_loader: - type  f32:  194 tensors
0.00.024.142 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.142 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.143 I print_info: file format = GGUF V3 (latest)
0.00.024.143 I print_info: file type   = Q5_K - Medium
0.00.024.151 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.461 I load: special tokens cache size = 25
0.00.039.116 I load: token to piece cache size = 0.2984 MB
0.00.039.134 I print_info: arch             = gptneox
0.00.039.135 I print_info: vocab_only       = 0
0.00.039.135 I print_info: n_ctx_train      = 2048
0.00.039.135 I print_info: n_embd           = 2048
0.00.039.135 I print_info: n_layer          = 24
0.00.039.140 I print_info: n_head           = 16
0.00.039.141 I print_info: n_head_kv        = 16
0.00.039.144 I print_info: n_rot            = 32
0.00.039.144 I print_info: n_swa            = 0
0.00.039.144 I print_info: n_embd_head_k    = 128
0.00.039.144 I print_info: n_embd_head_v    = 128
0.00.039.145 I print_info: n_gqa            = 1
0.00.039.146 I print_info: n_embd_k_gqa     = 2048
0.00.039.146 I print_info: n_embd_v_gqa     = 2048
0.00.039.147 I print_info: f_norm_eps       = 1.0e-05
0.00.039.147 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.147 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.148 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.149 I print_info: f_logit_scale    = 0.0e+00
0.00.039.149 I print_info: n_ff             = 8192
0.00.039.155 I print_info: n_expert         = 0
0.00.039.156 I print_info: n_expert_used    = 0
0.00.039.157 I print_info: causal attn      = 1
0.00.039.157 I print_info: pooling type     = 0
0.00.039.157 I print_info: rope type        = 2
0.00.039.157 I print_info: rope scaling     = linear
0.00.039.158 I print_info: freq_base_train  = 10000.0
0.00.039.159 I print_info: freq_scale_train = 1
0.00.039.159 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.160 I print_info: rope_finetuned   = unknown
0.00.039.160 I print_info: ssm_d_conv       = 0
0.00.039.160 I print_info: ssm_d_inner      = 0
0.00.039.160 I print_info: ssm_d_state      = 0
0.00.039.160 I print_info: ssm_dt_rank      = 0
0.00.039.160 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.160 I print_info: model type       = 1.4B
0.00.039.161 I print_info: model params     = 1.41 B
0.00.039.161 I print_info: general.name     = 1.4B
0.00.039.161 I print_info: vocab type       = BPE
0.00.039.162 I print_info: n_vocab          = 50304
0.00.039.162 I print_info: n_merges         = 50009
0.00.039.162 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.162 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.162 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.163 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.163 I print_info: LF token         = 187 'Ċ'
0.00.039.165 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.165 I print_info: max token length = 1024
0.00.039.166 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.579.820 I load_tensors: offloading 24 repeating layers to GPU
0.00.579.838 I load_tensors: offloading output layer to GPU
0.00.579.839 I load_tensors: offloaded 25/25 layers to GPU
0.00.579.879 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.579.881 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.581.494 I llama_init_from_model: n_seq_max     = 1
0.00.581.498 I llama_init_from_model: n_ctx         = 128
0.00.581.498 I llama_init_from_model: n_ctx_per_seq = 128
0.00.581.498 I llama_init_from_model: n_batch       = 128
0.00.581.499 I llama_init_from_model: n_ubatch      = 128
0.00.581.499 I llama_init_from_model: flash_attn    = 0
0.00.581.501 I llama_init_from_model: freq_base     = 10000.0
0.00.581.502 I llama_init_from_model: freq_scale    = 1
0.00.581.502 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.581.505 I ggml_metal_init: allocating
0.00.581.589 I ggml_metal_init: found device: Apple M4
0.00.581.601 I ggml_metal_init: picking default device: Apple M4
0.00.583.075 I ggml_metal_init: using embedded metal library
0.00.589.508 I ggml_metal_init: GPU name:   Apple M4
0.00.589.513 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.589.513 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.589.514 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.589.515 I ggml_metal_init: simdgroup reduction   = true
0.00.589.515 I ggml_metal_init: simdgroup matrix mul. = true
0.00.589.515 I ggml_metal_init: has residency sets    = true
0.00.589.516 I ggml_metal_init: has bfloat            = true
0.00.589.516 I ggml_metal_init: use bfloat            = true
0.00.589.517 I ggml_metal_init: hasUnifiedMemory      = true
0.00.589.519 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.606.964 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.610.485 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.610.489 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.610.519 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.613.665 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.613.666 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.613.667 I llama_init_from_model: graph nodes  = 967
0.00.613.667 I llama_init_from_model: graph splits = 2
0.00.613.670 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.613.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.685 I 
0.00.642.777 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.806 I perplexity: tokenizing the input ..
0.00.650.110 I perplexity: tokenization took 7.3 ms
0.00.650.117 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.788.715 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.790.349 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.790.361 I llama_perf_context_print:        load time =     633.98 ms
0.00.790.362 I llama_perf_context_print: prompt eval time =     137.62 ms /   128 tokens (    1.08 ms per token,   930.08 tokens per second)
0.00.790.362 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.363 I llama_perf_context_print:       total time =     147.68 ms /   129 tokens
0.00.790.808 I ggml_metal_free: deallocating

real	0m0.805s
user	0m0.082s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.162 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.717 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.722 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.724 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.727 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.728 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.729 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.730 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.730 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.731 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.731 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.732 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.733 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.734 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.734 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.463 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.519 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.252 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.253 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.254 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.254 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.255 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.255 I llama_model_loader: - type  f32:  194 tensors
0.00.025.256 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.256 I print_info: file format = GGUF V3 (latest)
0.00.025.257 I print_info: file type   = Q6_K
0.00.025.258 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.576 I load: special tokens cache size = 25
0.00.040.115 I load: token to piece cache size = 0.2984 MB
0.00.040.133 I print_info: arch             = gptneox
0.00.040.134 I print_info: vocab_only       = 0
0.00.040.134 I print_info: n_ctx_train      = 2048
0.00.040.134 I print_info: n_embd           = 2048
0.00.040.134 I print_info: n_layer          = 24
0.00.040.138 I print_info: n_head           = 16
0.00.040.139 I print_info: n_head_kv        = 16
0.00.040.139 I print_info: n_rot            = 32
0.00.040.139 I print_info: n_swa            = 0
0.00.040.139 I print_info: n_embd_head_k    = 128
0.00.040.139 I print_info: n_embd_head_v    = 128
0.00.040.140 I print_info: n_gqa            = 1
0.00.040.140 I print_info: n_embd_k_gqa     = 2048
0.00.040.141 I print_info: n_embd_v_gqa     = 2048
0.00.040.141 I print_info: f_norm_eps       = 1.0e-05
0.00.040.142 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.142 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.142 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.142 I print_info: f_logit_scale    = 0.0e+00
0.00.040.143 I print_info: n_ff             = 8192
0.00.040.143 I print_info: n_expert         = 0
0.00.040.143 I print_info: n_expert_used    = 0
0.00.040.143 I print_info: causal attn      = 1
0.00.040.143 I print_info: pooling type     = 0
0.00.040.144 I print_info: rope type        = 2
0.00.040.144 I print_info: rope scaling     = linear
0.00.040.144 I print_info: freq_base_train  = 10000.0
0.00.040.144 I print_info: freq_scale_train = 1
0.00.040.145 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.145 I print_info: rope_finetuned   = unknown
0.00.040.145 I print_info: ssm_d_conv       = 0
0.00.040.145 I print_info: ssm_d_inner      = 0
0.00.040.145 I print_info: ssm_d_state      = 0
0.00.040.145 I print_info: ssm_dt_rank      = 0
0.00.040.145 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.146 I print_info: model type       = 1.4B
0.00.040.146 I print_info: model params     = 1.41 B
0.00.040.146 I print_info: general.name     = 1.4B
0.00.040.147 I print_info: vocab type       = BPE
0.00.040.147 I print_info: n_vocab          = 50304
0.00.040.147 I print_info: n_merges         = 50009
0.00.040.147 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.147 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.147 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.148 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.148 I print_info: LF token         = 187 'Ċ'
0.00.040.151 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.151 I print_info: max token length = 1024
0.00.040.151 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.637.396 I load_tensors: offloading 24 repeating layers to GPU
0.00.637.403 I load_tensors: offloading output layer to GPU
0.00.637.404 I load_tensors: offloaded 25/25 layers to GPU
0.00.637.431 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.637.433 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.639.082 I llama_init_from_model: n_seq_max     = 1
0.00.639.085 I llama_init_from_model: n_ctx         = 128
0.00.639.085 I llama_init_from_model: n_ctx_per_seq = 128
0.00.639.085 I llama_init_from_model: n_batch       = 128
0.00.639.086 I llama_init_from_model: n_ubatch      = 128
0.00.639.086 I llama_init_from_model: flash_attn    = 0
0.00.639.087 I llama_init_from_model: freq_base     = 10000.0
0.00.639.088 I llama_init_from_model: freq_scale    = 1
0.00.639.088 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.639.090 I ggml_metal_init: allocating
0.00.639.145 I ggml_metal_init: found device: Apple M4
0.00.639.157 I ggml_metal_init: picking default device: Apple M4
0.00.640.327 I ggml_metal_init: using embedded metal library
0.00.646.304 I ggml_metal_init: GPU name:   Apple M4
0.00.646.308 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.646.309 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.646.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.646.311 I ggml_metal_init: simdgroup reduction   = true
0.00.646.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.646.311 I ggml_metal_init: has residency sets    = true
0.00.646.311 I ggml_metal_init: has bfloat            = true
0.00.646.312 I ggml_metal_init: use bfloat            = true
0.00.646.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.646.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.663.089 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.666.635 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.666.641 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.666.670 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.669.840 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.669.842 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.669.842 I llama_init_from_model: graph nodes  = 967
0.00.669.842 I llama_init_from_model: graph splits = 2
0.00.669.845 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.669.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.699.984 I 
0.00.700.038 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.057 I perplexity: tokenizing the input ..
0.00.706.855 I perplexity: tokenization took 6.796 ms
0.00.706.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.896 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.841.201 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.841.219 I llama_perf_context_print:        load time =     689.82 ms
0.00.841.220 I llama_perf_context_print: prompt eval time =     132.15 ms /   128 tokens (    1.03 ms per token,   968.57 tokens per second)
0.00.841.220 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.221 I llama_perf_context_print:       total time =     141.24 ms /   129 tokens
0.00.841.585 I ggml_metal_free: deallocating

real	0m0.858s
user	0m0.079s
sys	0m0.128s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.357 I build: 4831 (5e43f104) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.716 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.039 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.048 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.051 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.052 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.053 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.054 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.057 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.058 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.059 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.063 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.064 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.064 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.849 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.781 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.051.633 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.051.635 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.051.635 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.051.636 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.051.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.051.637 I llama_model_loader: - type  f32:  194 tensors
0.00.051.638 I llama_model_loader: - type  f16:   98 tensors
0.00.051.639 I print_info: file format = GGUF V3 (latest)
0.00.051.640 I print_info: file type   = all F32 (guessed)
0.00.051.641 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.064.195 I load: special tokens cache size = 25
0.00.072.528 I load: token to piece cache size = 0.2984 MB
0.00.072.543 I print_info: arch             = gptneox
0.00.072.544 I print_info: vocab_only       = 0
0.00.072.545 I print_info: n_ctx_train      = 2048
0.00.072.545 I print_info: n_embd           = 2048
0.00.072.545 I print_info: n_layer          = 24
0.00.072.548 I print_info: n_head           = 16
0.00.072.549 I print_info: n_head_kv        = 16
0.00.072.549 I print_info: n_rot            = 32
0.00.072.550 I print_info: n_swa            = 0
0.00.072.550 I print_info: n_embd_head_k    = 128
0.00.072.552 I print_info: n_embd_head_v    = 128
0.00.072.553 I print_info: n_gqa            = 1
0.00.072.554 I print_info: n_embd_k_gqa     = 2048
0.00.072.554 I print_info: n_embd_v_gqa     = 2048
0.00.072.555 I print_info: f_norm_eps       = 1.0e-05
0.00.072.555 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.556 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.556 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.556 I print_info: f_logit_scale    = 0.0e+00
0.00.072.557 I print_info: n_ff             = 8192
0.00.072.557 I print_info: n_expert         = 0
0.00.072.557 I print_info: n_expert_used    = 0
0.00.072.557 I print_info: causal attn      = 1
0.00.072.558 I print_info: pooling type     = 0
0.00.072.558 I print_info: rope type        = 2
0.00.072.558 I print_info: rope scaling     = linear
0.00.072.560 I print_info: freq_base_train  = 10000.0
0.00.072.560 I print_info: freq_scale_train = 1
0.00.072.560 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.560 I print_info: rope_finetuned   = unknown
0.00.072.560 I print_info: ssm_d_conv       = 0
0.00.072.567 I print_info: ssm_d_inner      = 0
0.00.072.568 I print_info: ssm_d_state      = 0
0.00.072.568 I print_info: ssm_dt_rank      = 0
0.00.072.568 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.569 I print_info: model type       = 1.4B
0.00.072.569 I print_info: model params     = 1.41 B
0.00.072.570 I print_info: general.name     = 1.4B
0.00.072.570 I print_info: vocab type       = BPE
0.00.072.570 I print_info: n_vocab          = 50304
0.00.072.571 I print_info: n_merges         = 50009
0.00.072.571 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.571 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.571 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.572 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.572 I print_info: LF token         = 187 'Ċ'
0.00.072.572 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.573 I print_info: max token length = 1024
0.00.072.573 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.265.225 I load_tensors: offloading 24 repeating layers to GPU
0.01.265.229 I load_tensors: offloading output layer to GPU
0.01.265.230 I load_tensors: offloaded 25/25 layers to GPU
0.01.265.257 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.265.259 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.266.144 I llama_init_from_model: n_seq_max     = 1
0.01.266.145 I llama_init_from_model: n_ctx         = 128
0.01.266.145 I llama_init_from_model: n_ctx_per_seq = 128
0.01.266.146 I llama_init_from_model: n_batch       = 128
0.01.266.146 I llama_init_from_model: n_ubatch      = 128
0.01.266.146 I llama_init_from_model: flash_attn    = 0
0.01.266.147 I llama_init_from_model: freq_base     = 10000.0
0.01.266.147 I llama_init_from_model: freq_scale    = 1
0.01.266.147 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.266.151 I ggml_metal_init: allocating
0.01.266.261 I ggml_metal_init: found device: Apple M4
0.01.266.269 I ggml_metal_init: picking default device: Apple M4
0.01.267.287 I ggml_metal_init: using embedded metal library
0.01.271.269 I ggml_metal_init: GPU name:   Apple M4
0.01.271.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.271.272 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.271.273 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.271.273 I ggml_metal_init: simdgroup reduction   = true
0.01.271.273 I ggml_metal_init: simdgroup matrix mul. = true
0.01.271.274 I ggml_metal_init: has residency sets    = true
0.01.271.274 I ggml_metal_init: has bfloat            = true
0.01.271.274 I ggml_metal_init: use bfloat            = true
0.01.271.274 I ggml_metal_init: hasUnifiedMemory      = true
0.01.271.276 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.282.372 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.284.078 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.284.080 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.284.096 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.285.755 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.285.756 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.285.757 I llama_init_from_model: graph nodes  = 967
0.01.285.757 I llama_init_from_model: graph splits = 2
0.01.285.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.285.758 I 
0.01.285.795 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.285.796 I compute_imatrix: tokenizing the input ..
0.01.289.825 I compute_imatrix: tokenization took 4.028 ms
0.01.289.826 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.559.636 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.562.368 I llama_perf_context_print:        load time =    1538.91 ms
0.01.562.368 I llama_perf_context_print: prompt eval time =     268.07 ms /   128 tokens (    2.09 ms per token,   477.49 tokens per second)
0.01.562.369 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.562.369 I llama_perf_context_print:       total time =    1541.64 ms /   129 tokens
0.01.562.924 I ggml_metal_free: deallocating

real	0m1.769s
user	0m0.125s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4831 (5e43f104)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135205090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135205700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x135205b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135205fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135208e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135209140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1352095b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135209a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135209e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13520a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13520a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13520ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13520b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13520c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13520c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13520d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13520d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13520de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13520e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13520ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13520f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13520fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1352102c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135210b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x135211280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x135211540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x135211800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135211c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135212390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x135212800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x135212dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1352132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135213740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135213a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135213e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1352142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135214750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135214bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135215030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1352154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135215910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135215d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1352161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135216660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135216ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x135216f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1352173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135217820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135217fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135218420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135218890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135218d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135219170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1352195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135219a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13521a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13521a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13521a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13521ad40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13521b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13521b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13521bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13521bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13521c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13521c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13521ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13521d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13521d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13521ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13521e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13521e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13521ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13521f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13521f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13521fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x135220230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1352207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x135220d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x135221340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1352218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x135221ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x135222450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x135222a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x135222fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x135223560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x135223b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1352240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x135224670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x135224c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1352251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135225780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x135225d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1352262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x135226890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135226e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1352273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1352279a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135217ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135228100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135228570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1352289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135228f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135229540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135229af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13522a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13522a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13522ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13522b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13522b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13522bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13522c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13522c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13522ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13522d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13522d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13522ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13522e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13522e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13522ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13522f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13522f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13522fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1352300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1352305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135230ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135230fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1352314d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1352319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x135231ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1352323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1352328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x135232dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1352332d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1352337d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x135233cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1352341d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1352346d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x135234bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1352350d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1352355d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135235ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135235fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1352364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1352369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x135236ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1352373d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1352378d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x135237dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1352382d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1352387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135238cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1352391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1352396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x135239bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13523a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13523a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13523aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13523afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13523b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13523b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13523bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13523c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13523c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13523cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13523d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13523d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13523dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13523e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13523e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13523ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13523f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13523f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13523fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13523ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1352404d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1352409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x135240ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1352413d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1352418d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x135241dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1352422d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1352427d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x135242cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1352431d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1352436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x135243bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1352440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1352445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x135244ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x135244fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1352454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1352459d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x135245ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1352463d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x135246980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135246f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1352474e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x135247a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1352480a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1352486b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x135248cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1352494b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x135249950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x135249c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13524a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13524a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13524b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13524b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13524b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13524be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13524c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13524cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13524d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13524d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13524daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13524e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13524e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13524eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13524f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13524f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13524fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135250020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135250570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135250ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135251010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x135251560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135251ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135252000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x135252550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135252aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135252ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x135253540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x135253a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135253fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135254530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x135254a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135254fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135255520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x135255a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135255fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135256510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x135256a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x135256fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x135257500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x135104230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1351046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x135104b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x135104f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1351053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x135105860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x135105cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135106140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1351065b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x135106a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x135106e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x135107300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x135107770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x135107be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135108050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1351084c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135108930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135108da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x135109210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135109680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135109af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135109f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13510a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13510a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13510acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13510b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13510b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13510ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13510be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13510c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13510c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13510cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13510d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13510d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13510d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13510dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13510e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13510e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13510ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13510ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13510f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13510f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13510fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x135110100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x135110570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1351109e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x135110e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1351112c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135111730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1351121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135112900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135113020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135113740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135113a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135113cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135114130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1351145a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.728.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.456 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13521ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135225490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13521f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x1352276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135224ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13522c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13522bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13522ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135227100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x135221bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x135229db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135246c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135226b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135221600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135224930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135223270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x135229800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x135246690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13522b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1352265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x135221050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x135224380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x135222cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135229250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13522aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x135225ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x135220aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135223dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135228ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13522a910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x135225a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135223820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13522a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135247d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135248970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13524a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135210800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13521b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1352062a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135219d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135227c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13524aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135248f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13520aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135211f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1352577c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135257a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135257d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135258000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1352582c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135258580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135258840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x135258b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135258dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135259080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135259340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135259600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1352598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x135259b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135259e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13525a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13525a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13525a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13525a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13525ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13525aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13525b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13525b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13525b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13525b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13525bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13525bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13525c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13525c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13525c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13525ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13525cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13525cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13525d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13525d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13525d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13525dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13525dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13525e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13525e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13525e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13525e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13525eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13525ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13525f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13525f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13525f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13525f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13525fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13525fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x135260140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135260400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1352606c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135260980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135260c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x135260f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1352611c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135261480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x135261740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x135261a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135261cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135261f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x135262240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135262500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1352627c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135262a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135262d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135263000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1352632c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135263580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135263840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135263b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135263dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x135264080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135264340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135264600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1352648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135264b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135264e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135265100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1352653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x135265680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x135265940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x135265c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x135265ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x135266180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x135266440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x135266700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1352669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x135266c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x135266f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x135267200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1352674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x135267780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x135267a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135267d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135267fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x135268280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135268540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x135268800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x135268ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135268d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x135269040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x135269300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1352695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x135269880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x135269b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135269e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13526a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13526a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13526a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13526a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13526abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13526ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13526b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13526b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13526b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13526b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13526bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13526bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13526c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13526c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13526c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13526ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13526ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13526cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13526d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13526d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13526d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13526da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13526df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13526e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13526e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13526ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13526f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13526f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13526fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13526ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135270420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1352708c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x135270d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x135271200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1352716a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x135271b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x135271fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x135272480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x135272920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x135272dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x135273310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x135273860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135273db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x135274300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x113504430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1135048a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x113504d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x113505180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1135055f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x113505a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x113505ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x113506340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1135067b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x113506c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x113507090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x113507500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x113507970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x113508540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x113508800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x113508ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x113508f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1135093a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x113509810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x113509c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11350a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11350a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11350a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11350ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11350b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11350b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11350bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11350c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11350c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11350c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11350cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11350d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11350d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11350daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11350df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11350e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11350e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11350ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11350f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11350f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11350f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11350fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x113510290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x113510700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x113510b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x113510fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x113511450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1135118c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x113511d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1135121a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x113512610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x113512a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x113512ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x113513360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1135137d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x113513c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1135140b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x113514520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x113514990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x113514e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x113515270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1135156e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x113515b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x113515fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x113516430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1135168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x113516d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x113517180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1135175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x113517a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x113517ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x113518340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1135187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x113518c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x113519090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x113519500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x113519970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x113519de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11351a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11351a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11351ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11351afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11351b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11351b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11351bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11351c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11351c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11351ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11351ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11351d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11351d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11351dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11351e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11351e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11351e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11351edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11351f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11351ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1135206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x113520dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x113521090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x113521350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1135217c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x113521c30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x135116e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x135116500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1351170f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x135117660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x135117920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x135117be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x135117ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x135118160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x135118420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1351186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1351189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x135118c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x135119230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x135119800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x135119e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13511a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13511a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13511ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13511b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13511b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13511bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13511c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13511c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13511cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13511d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13511d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13511dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13511dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13511e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13511e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13511e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13511eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13511edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13511f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13511f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13511f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13511fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x135120240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1351206b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135120b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x135120f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135121400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135121870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x135121ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x135122150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1351225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x135122a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x135122ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x135123310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x135123780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x135123bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x135124060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1351244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x135124940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x135124db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x135125220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x135125730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x135125c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1351260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135126510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x135126980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135126df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135127260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1351276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135127b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135127fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135128420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135128890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x135128d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135129170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1351295e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x135129a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x135129ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13512a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13512a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13512ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13512b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13512b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13512b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13512bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13512c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13512c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13512cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13512cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13512d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13512d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13512dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13512e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13512e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13512ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13512eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13512f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13512f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13512fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x135130060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1351304d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x135130940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x135130db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x135131610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x135131b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1351320e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x135132690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x135132c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1351331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1351337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x135133d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x135134300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1351348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x135134e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x135135410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1351359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x135135f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135136520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135136ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135136fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1351374d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1351379d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135137ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1351383d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1351388d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x135138dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1351392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1351397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135139cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13513a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13513a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13513abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13513b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13513b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13513bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13513bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13513c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13513c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13513ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13513d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13513d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13513ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13513e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13513e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13513ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13513f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13513f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13513fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1351400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1351405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x135140ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x135140fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1351414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1351419d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x135141ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1351423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1351428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x135142dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1351432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1351437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x135143cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1351441d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1351446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x135144bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1351450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1351455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x135145ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x135145fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1351464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1351469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135146ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1351473d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1351478d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135147dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1351482d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1351487d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135148cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1351491d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1351496d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135149bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13514a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13514a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13514aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13514afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13514b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13514b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13514bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13514c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13514c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13514cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13514d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13514d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13514dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13514e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13514e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13514ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13514f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13514f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13514fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x135150080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135150630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x135150be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x135151190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1351517a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x135151db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1351523c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x135152bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x135153050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x135153310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x135153920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x135153f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x135154720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x135154bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x135155060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x135155500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x135155cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x135156200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x135156750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x135156ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1351571f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x135157740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x135157c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1351581e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135158730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x135158c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1351591d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135159720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x135159c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13515a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13515a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13515ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13515b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13515b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13515bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13515c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13515c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13515cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13515d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13515d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13515dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13515e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13515e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13515ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13515f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13515f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13515fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x135160160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1351606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x135160c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x135161150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1351616a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x135161bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x135162140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x135162690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x135162be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x135163130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x135163680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x135163bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x135164120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x135164670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x135164bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x135165110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x135165660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x135165bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x135166100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x135166650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x135166ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1351670f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x135167640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x135167b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1351680e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135168630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135168ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x135168f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135169410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1351698b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x135169d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13516a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13516a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13516ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13516afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13516b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13516b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13516bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13516c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13516c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13516cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13516d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13516d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13516d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13516de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13516e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13516e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13516ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13516f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13516f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13516f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13516ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135170640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135170d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135171480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x135171ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135171e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135172650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135172910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135172f20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.764s
user	0m0.280s
sys	0m0.336s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4831 (5e43f104)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14170d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14170dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14170e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14170e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14170edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14170f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14170f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14170fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x141710490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x141710990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x141710e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x141711390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x141711eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x141712660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x141712e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x141713590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x141713cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1417143d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141714af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1417152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1417159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141716100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141716820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1417170c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1417177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x141717aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1417180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141718d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141719260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141719520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1417199c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x141719c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14171a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14171aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14171ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14171b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14171b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14171baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14171bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14171c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14171c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14171cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14171d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14171d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14171d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14171df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14171e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14171eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14171f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14171fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1417200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1417206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x141720d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x141721310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x141721b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x141721fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x141722440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x141722700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x141722d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141723500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1417237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141723c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141724100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1417245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141724a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141724ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141725380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141725820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x141725cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141726160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141726600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x141726aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141726f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x141727490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1417279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141727f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x141728480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1417289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141728f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141729470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1417299c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141729f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14172a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14172a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14172af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14172b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14172b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14172bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14172c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14172c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14172cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14172d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14172d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14172ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14172e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14172e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14172eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14171eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14172f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14172fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x141730030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x141730580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x141730ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x141731020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x141731570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x141731ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x141732010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x141732560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x141732ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x141733000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141733550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x141733aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141733ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x141734490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141734930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x141734dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141735270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141735710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x141735bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141736050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1417364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x141736990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141736e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1417372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141737770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141737c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1417380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141738550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1417389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141738e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141739330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1417397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141739c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14173a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14173a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14173aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14173aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14173b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14173b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14173bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14173c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14173c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14173cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14173cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14173d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14173d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14173dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14173e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14173e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14173eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14173efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14173f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14173f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14173fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x141740230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1417406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x141740b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x141741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1417414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x141741950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x141741df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x141742290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x141742730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x141742bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x141743070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141743510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1417439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141743e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1417442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x141744790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141744c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1417450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141745570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141745a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141745eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141746350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1417467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x141746c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141747130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1417475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141747a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141747f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1417483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141748850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141748cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141749190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141749630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x141749ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141749f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14174a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14174a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14174ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14174b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14174b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14174bc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14174c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14174c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14174c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14174d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14174d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14174dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14174e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14174e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14174eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14174f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14174f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14174ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x141750420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1417508c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x141750d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x141751510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x141751a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x141751fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x141752500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x141752a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x141752fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1417534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141753a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141753f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1417544e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141754a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141754f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1417554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141755a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141755f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1417564c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141756a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141756f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1417574b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141757a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141757f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1417584a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1417589f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141758f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x141759490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1417599e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141759f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14175a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14175a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14175af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14175b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14175b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14175bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14175c460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14175c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14175cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14175d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14175d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14175def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14175e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14175e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14175eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14175f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14175f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14175fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x141760420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x141760970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x141760ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x141761410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x141761960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x141761eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x141762400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x141762950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x141762ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1417633f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141763940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141763e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141764330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1417647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141764c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141765110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1417655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141765a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141765ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141766390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141766830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x141766cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141767170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141767610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x141767ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141767f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1417683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x141768890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x141768d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1417691d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x141769670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x141769b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x141769fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14176a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14176a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14176ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14176b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14176b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14176bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14176c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14176cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14176d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14176d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14176deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14176e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14176e780 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.515 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.519 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x142804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x142804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1428053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1428069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1428072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1428090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14280a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14280a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14280ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14280b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14280bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14280c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14280cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14280d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14280d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14280e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14280e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14280e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14280eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14280ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14280f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14280f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14280fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1428101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1428111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x142811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1428123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1428130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1428139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1428142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1428158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1428161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1428170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1428186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14281a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14281a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14281aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14281aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14281b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14281b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14281bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14281c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14281c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14281c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14281cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14281d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14281d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14281db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14281df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14281e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14281e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14281ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14281f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14281f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14281fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14281fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x142821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1428214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x142821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1428233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1428245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1428252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1428264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x142826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1428283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1428299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14282a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14282a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14282abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14282b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14282b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14282b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14282bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14282c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14282c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14282cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14282cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14282d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14282d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14282dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14282e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14282e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14282e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14282ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14282f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14282f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14282fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1428308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1428311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1428327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1428330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1428339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142835c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142835ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142836190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142836a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142836ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142837350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1428377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142837c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1428380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142838510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142838980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142838df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142839260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1428396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142839b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142839fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14283a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14283a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14283ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14283b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14283b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14283ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14283bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14283c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14283c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14283cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14283d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14283d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14283d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14283ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14283e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14283e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14283eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14283ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14283f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14283f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14283fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1428402e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x142840750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x142840bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142841030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x142841550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1428425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142842890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142842e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1428439d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142843f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142844550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142844b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1428450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142845690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142845c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142846210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1428467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142846d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142847350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142847910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142848490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142848a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142849010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1428495d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142849b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14284a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14284a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14284acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14284b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14284b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14284be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14284c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14284c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14284cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14284d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14284dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14284e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14284e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14284ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14284f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14284f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14284fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1428508d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142850e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142851450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142851a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142852590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142852b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142853110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1428536d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142853c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142854250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142854dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142855390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142855950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142855f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1428564d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142856a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x142856f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142857490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142857990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x142857e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142858390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142858890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x142858d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142859290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142859790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142859c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14285a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14285a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14285ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14285b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x14285b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x14285ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14285bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14285c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14285c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14285ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14285d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14285d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14285dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14285e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14285e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14285f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14285f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14285ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x142860700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1428609c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1428611b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142861470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142861a80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14285ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14284c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14284b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x142848190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142845950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x142855090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142852850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1428505d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14284e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1428464d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142843c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x142848d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142849e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14284f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14284c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x142853f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142846a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14284eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142849890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x142842b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14284d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142848750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142852e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14284dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1428436d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142845390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x142855c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14284af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1428533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x1428492d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14284bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14284fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14284a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142847050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x142851710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142845f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142854510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142851cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14284d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142856790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142844dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1428561d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142844250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142854ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14284e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142850b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x142853990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142852290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14284a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142841d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142804680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142860c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142807a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142861d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142862160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142862420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1428626e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142862c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142862ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1428631a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142863460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142863720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1428639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142863ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142863f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142864220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1428644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1428647a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142864a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142864d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142864fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1428652a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142865560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142865ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142865d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142866030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1428662f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1428665b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142866870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142866b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142866df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1428670b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x142867370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142867630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1428678f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x142867bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x142867e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x142868130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1428683f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1428686b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x142868970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x142868c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x142868ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1428691b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x142869470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x142869730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1428699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x142869cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142869f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14286a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14286a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14286a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14286aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14286ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14286aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14286b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14286b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14286b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14286baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14286bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14286c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14286c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14286c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14286c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14286cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14286ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14286d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14286d3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14286d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14286d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14286dbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14286deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14286e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14286e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14286e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14286e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14286ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14286ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14286f1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14286f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14286f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14286fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14286fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14286ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x142870270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142870530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1428707f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142870ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x142870d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142871030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1428712f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1428715b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142871870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142871b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142871df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1428720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x142872370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142872630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1428728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142872bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x142872e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142873130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1428733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1428736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142873970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142873c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142873ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1428741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142874470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142874730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1428749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142874cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142874f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142875230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1428754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1428757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142875a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142875d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142875ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1428762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142876570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142876830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142876af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142876db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142877070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142877330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1428775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1428778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142877b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x142877e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1428780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1428783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x142878670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142878930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142878bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x142878eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x142879170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x142879430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1428796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1428799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x142879c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x142879f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14287a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14287a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14287a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14287aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14287acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14287afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14287b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14287b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14287b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14287bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14287bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14287c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14287c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14287c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14287c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14287cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14287cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14287d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14287d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14287d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14287dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14287dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14287e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14287e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14287e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14287e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14287ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14287ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14287f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14287f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14287f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14287fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14287fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14287ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142880280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142880540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142880800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142880ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142880d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1428812d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142881820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142881d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1428822c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142882810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142882d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1428832b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142883800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142883d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1428842a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1428847f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142884d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x142885290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1428857e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x142885d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x142886280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1428867d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x142886d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x142887270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1428877c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142887d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142888260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1428887b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142888d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142889250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1428897a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142889cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14288a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14288a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14288ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14288b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14288b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14288bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14288c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14288c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14288ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14288cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14288d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14288d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14288dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14288e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14288e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14288eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14288f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14288f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14288fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14288ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142890440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142890940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142890e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142891340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x142891840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x142891d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x142892240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x142892740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x142892c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x142893140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x142893640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x142893b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x142894040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x142894540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x142894a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x142895450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x142895b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x142896290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1428969b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x142896c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x142897460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x142897720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x142897d30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.961s
user	0m0.232s
sys	0m0.186s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
