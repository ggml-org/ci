Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.572s
user	0m0.893s
sys	0m1.236s
++ nproc
+ make -j10
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Built target build_info
[  6%] Built target sha1
[  6%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target xxhash
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 14%] Linking CXX shared library libggml-blas.dylib
[ 14%] Linking CXX shared library libggml-cpu.dylib
[ 15%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 15%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 15%] Built target ggml-blas
[ 15%] Built target ggml-cpu
[ 15%] Linking C shared library libggml-metal.dylib
[ 15%] Built target ggml-metal
[ 16%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 16%] Linking CXX shared library libggml.dylib
[ 16%] Built target ggml
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Linking CXX shared library libllama.dylib
[ 22%] Built target llama-gguf
[ 22%] Built target llama-gguf-hash
[ 22%] Built target llama
[ 23%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 23%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 30%] Built target llava
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Linking CXX shared library libllava_shared.dylib
[ 32%] Linking CXX static library libllava_static.a
[ 32%] Linking CXX static library libcommon.a
[ 32%] Built target llama-simple
[ 32%] Built target test-c
[ 32%] Built target llama-quantize-stats
[ 32%] Built target llama-simple-chat
[ 32%] Built target llava_static
[ 32%] Built target llava_shared
[ 32%] Built target common
[ 32%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 40%] Linking CXX executable ../bin/test-tokenizer-0
[ 41%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Linking CXX executable ../bin/test-llama-grammar
[ 43%] Linking CXX executable ../bin/test-grammar-parser
[ 44%] Linking CXX executable ../bin/test-grammar-integration
[ 45%] Linking CXX executable ../bin/test-sampling
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-arg-parser
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Built target test-tokenizer-0
[ 46%] Built target test-tokenizer-1-bpe
[ 46%] Built target test-tokenizer-1-spm
[ 46%] Built target test-llama-grammar
[ 46%] Built target test-grammar-parser
[ 46%] Built target test-grammar-integration
[ 46%] Built target test-log
[ 46%] Built target test-sampling
[ 46%] Built target test-json-schema-to-grammar
[ 47%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 47%] Built target test-arg-parser
[ 47%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 51%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 53%] Linking CXX executable ../bin/test-gguf
[ 55%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-chat-template
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../../bin/llama-batched-bench
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 60%] Linking CXX executable ../bin/test-rope
[ 60%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 61%] Built target test-chat-template
[ 61%] Built target test-gguf
[ 61%] Built target llama-batched-bench
[ 61%] Built target test-model-load-cancel
[ 61%] Built target test-rope
[ 61%] Built target test-quantize-fns
[ 61%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 61%] Built target test-backend-ops
[ 61%] Built target test-barrier
[ 61%] Built target test-quantize-perf
[ 61%] Built target test-autorelease
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 64%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-batched
[ 67%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 68%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 68%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Linking CXX executable ../../bin/llama-gritlm
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Linking CXX executable ../../bin/llama-infill
[ 70%] Linking CXX executable ../../bin/llama-bench
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-lookahead
[ 72%] Built target llama-batched
[ 72%] Built target llama-embedding
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-gritlm
[ 72%] Built target llama-infill
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-bench
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 72%] Built target llama-lookahead
[ 72%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 72%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 72%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 73%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 75%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-lookup-stats
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 82%] Linking CXX executable ../../bin/llama-passkey
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 82%] Linking CXX executable ../../bin/llama-cli
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Linking CXX executable ../../bin/llama-retrieval
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup-create
[ 82%] Built target llama-lookup
[ 82%] Built target llama-parallel
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-cli
[ 82%] Built target llama-passkey
[ 82%] Built target llama-quantize
[ 82%] Built target llama-perplexity
[ 82%] Generating loading.html.hpp
[ 82%] Built target llama-retrieval
[ 83%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 85%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 85%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 89%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-save-load-state
[ 91%] Linking CXX executable ../../bin/llama-speculative-simple
[ 91%] Linking CXX executable ../../bin/llama-run
[ 92%] Linking CXX executable ../../bin/llama-tts
[ 93%] Linking CXX executable ../../bin/llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-tokenize
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-gen-docs
[ 93%] Built target llama-cvector-generator
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.924s
user	0m5.869s
sys	0m9.566s

main: quantize time =  3041.44 ms
main:    total time =  3041.44 ms

main: quantize time =  1327.92 ms
main:    total time =  1327.92 ms

main: quantize time =  1496.31 ms
main:    total time =  1496.31 ms

main: quantize time =  1371.01 ms
main:    total time =  1371.01 ms

main: quantize time =  1940.36 ms
main:    total time =  1940.36 ms

main: quantize time =  4960.30 ms
main:    total time =  4960.30 ms

main: quantize time =  5640.49 ms
main:    total time =  5640.49 ms

main: quantize time =  7105.32 ms
main:    total time =  7105.32 ms

main: quantize time =  6005.13 ms
main:    total time =  6005.13 ms

main: quantize time =  4584.17 ms
main:    total time =  4584.17 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.117 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.237 I main: llama backend init
0.00.000.244 I main: load the model and apply lora adapter, if any
0.00.060.732 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.072.289 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.072.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.072.310 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.072.310 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.072.311 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.072.312 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.072.312 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.072.314 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.072.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.072.316 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.072.320 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.072.320 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.072.321 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.072.322 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.072.325 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.072.326 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.072.326 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.079.567 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.081.870 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.089.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.089.084 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.089.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.089.086 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.089.086 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.089.088 I llama_model_loader: - type  f32:  194 tensors
0.00.089.088 I llama_model_loader: - type  f16:   98 tensors
0.00.127.775 I llm_load_vocab: special tokens cache size = 25
0.00.135.593 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.135.597 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.135.597 I llm_load_print_meta: arch             = gptneox
0.00.135.598 I llm_load_print_meta: vocab type       = BPE
0.00.135.598 I llm_load_print_meta: n_vocab          = 50304
0.00.135.598 I llm_load_print_meta: n_merges         = 50009
0.00.135.598 I llm_load_print_meta: vocab_only       = 0
0.00.135.598 I llm_load_print_meta: n_ctx_train      = 2048
0.00.135.598 I llm_load_print_meta: n_embd           = 2048
0.00.135.599 I llm_load_print_meta: n_layer          = 24
0.00.135.603 I llm_load_print_meta: n_head           = 16
0.00.135.606 I llm_load_print_meta: n_head_kv        = 16
0.00.135.606 I llm_load_print_meta: n_rot            = 32
0.00.135.606 I llm_load_print_meta: n_swa            = 0
0.00.135.607 I llm_load_print_meta: n_embd_head_k    = 128
0.00.135.607 I llm_load_print_meta: n_embd_head_v    = 128
0.00.135.608 I llm_load_print_meta: n_gqa            = 1
0.00.135.608 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.135.609 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.135.610 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.135.610 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.135.610 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.135.610 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.135.610 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.135.611 I llm_load_print_meta: n_ff             = 8192
0.00.135.611 I llm_load_print_meta: n_expert         = 0
0.00.135.611 I llm_load_print_meta: n_expert_used    = 0
0.00.135.611 I llm_load_print_meta: causal attn      = 1
0.00.135.612 I llm_load_print_meta: pooling type     = 0
0.00.135.612 I llm_load_print_meta: rope type        = 2
0.00.135.614 I llm_load_print_meta: rope scaling     = linear
0.00.135.614 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.135.614 I llm_load_print_meta: freq_scale_train = 1
0.00.135.614 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.135.615 I llm_load_print_meta: rope_finetuned   = unknown
0.00.135.615 I llm_load_print_meta: ssm_d_conv       = 0
0.00.135.615 I llm_load_print_meta: ssm_d_inner      = 0
0.00.135.615 I llm_load_print_meta: ssm_d_state      = 0
0.00.135.615 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.135.615 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.135.616 I llm_load_print_meta: model type       = 1.4B
0.00.135.616 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.135.616 I llm_load_print_meta: model params     = 1.41 B
0.00.135.617 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.135.617 I llm_load_print_meta: general.name     = 1.4B
0.00.135.617 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.135.618 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.135.618 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.135.618 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.135.618 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.135.622 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.135.622 I llm_load_print_meta: max token length = 1024
0.00.138.377 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.138.377 I llm_load_tensors: offloading output layer to GPU
0.00.138.377 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.138.397 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.138.398 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.139.483 I llama_new_context_with_model: n_seq_max     = 1
0.00.139.484 I llama_new_context_with_model: n_ctx         = 2048
0.00.139.484 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.139.484 I llama_new_context_with_model: n_batch       = 2048
0.00.139.484 I llama_new_context_with_model: n_ubatch      = 512
0.00.139.485 I llama_new_context_with_model: flash_attn    = 0
0.00.139.485 I llama_new_context_with_model: freq_base     = 10000.0
0.00.139.486 I llama_new_context_with_model: freq_scale    = 1
0.00.139.486 I ggml_metal_init: allocating
0.00.139.496 I ggml_metal_init: found device: Apple M4
0.00.139.499 I ggml_metal_init: picking default device: Apple M4
0.00.140.229 I ggml_metal_init: using embedded metal library
0.00.150.291 I ggml_metal_init: GPU name:   Apple M4
0.00.150.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.150.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.150.294 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.150.295 I ggml_metal_init: simdgroup reduction   = true
0.00.150.295 I ggml_metal_init: simdgroup matrix mul. = true
0.00.150.295 I ggml_metal_init: has bfloat            = true
0.00.150.295 I ggml_metal_init: use bfloat            = true
0.00.150.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.150.296 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.175.285 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.196.001 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.196.007 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.196.027 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.197.034 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.197.036 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.197.036 I llama_new_context_with_model: graph nodes  = 967
0.00.197.036 I llama_new_context_with_model: graph splits = 2
0.00.197.060 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.197.204 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.197.205 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.284.397 I main: llama threadpool init, n_threads = 4
0.00.284.445 I 
0.00.284.474 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.284.476 I 
0.00.284.696 I sampler seed: 1234
0.00.284.702 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.284.727 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.284.729 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.284.729 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.129.044 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60118.54 tokens per second)
0.02.129.045 I llama_perf_context_print:        load time =     223.65 ms
0.02.129.046 I llama_perf_context_print: prompt eval time =      54.49 ms /     7 tokens (    7.78 ms per token,   128.47 tokens per second)
0.02.129.047 I llama_perf_context_print:        eval time =    1787.01 ms /    63 runs   (   28.37 ms per token,    35.25 tokens per second)
0.02.129.047 I llama_perf_context_print:       total time =    1844.65 ms /    70 tokens
0.02.129.250 I ggml_metal_free: deallocating

real	0m2.458s
user	0m0.153s
sys	0m0.114s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.009.853 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.855 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.024.860 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.862 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.862 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.863 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.863 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.863 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.865 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.865 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.865 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.866 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.866 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.866 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.870 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.871 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.871 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.888 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.990 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.140 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.142 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.142 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.142 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.143 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.143 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.144 I llama_model_loader: - type  f32:  194 tensors
0.00.034.144 I llama_model_loader: - type q8_0:   98 tensors
0.00.057.012 I llm_load_vocab: special tokens cache size = 25
0.00.062.909 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.913 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.913 I llm_load_print_meta: arch             = gptneox
0.00.062.914 I llm_load_print_meta: vocab type       = BPE
0.00.062.914 I llm_load_print_meta: n_vocab          = 50304
0.00.062.914 I llm_load_print_meta: n_merges         = 50009
0.00.062.914 I llm_load_print_meta: vocab_only       = 0
0.00.062.915 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.915 I llm_load_print_meta: n_embd           = 2048
0.00.062.917 I llm_load_print_meta: n_layer          = 24
0.00.062.923 I llm_load_print_meta: n_head           = 16
0.00.062.924 I llm_load_print_meta: n_head_kv        = 16
0.00.062.924 I llm_load_print_meta: n_rot            = 32
0.00.062.924 I llm_load_print_meta: n_swa            = 0
0.00.062.925 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.925 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.926 I llm_load_print_meta: n_gqa            = 1
0.00.062.927 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.927 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.928 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.928 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.929 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.931 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.931 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.932 I llm_load_print_meta: n_ff             = 8192
0.00.062.932 I llm_load_print_meta: n_expert         = 0
0.00.062.932 I llm_load_print_meta: n_expert_used    = 0
0.00.062.932 I llm_load_print_meta: causal attn      = 1
0.00.062.933 I llm_load_print_meta: pooling type     = 0
0.00.062.933 I llm_load_print_meta: rope type        = 2
0.00.062.933 I llm_load_print_meta: rope scaling     = linear
0.00.062.933 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.934 I llm_load_print_meta: freq_scale_train = 1
0.00.062.934 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.934 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.934 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.934 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.934 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.934 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.935 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.938 I llm_load_print_meta: model type       = 1.4B
0.00.062.939 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.939 I llm_load_print_meta: model params     = 1.41 B
0.00.062.940 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.941 I llm_load_print_meta: general.name     = 1.4B
0.00.062.941 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.941 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.941 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.941 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.942 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.062.942 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.942 I llm_load_print_meta: max token length = 1024
0.00.065.404 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.405 I llm_load_tensors: offloading output layer to GPU
0.00.065.405 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.416 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.418 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.333 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.333 I llama_new_context_with_model: n_ctx         = 2048
0.00.066.334 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.066.334 I llama_new_context_with_model: n_batch       = 2048
0.00.066.334 I llama_new_context_with_model: n_ubatch      = 512
0.00.066.334 I llama_new_context_with_model: flash_attn    = 0
0.00.066.335 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.335 I llama_new_context_with_model: freq_scale    = 1
0.00.066.335 I ggml_metal_init: allocating
0.00.066.341 I ggml_metal_init: found device: Apple M4
0.00.066.343 I ggml_metal_init: picking default device: Apple M4
0.00.067.089 I ggml_metal_init: using embedded metal library
0.00.069.664 I ggml_metal_init: GPU name:   Apple M4
0.00.069.665 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.665 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.666 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.666 I ggml_metal_init: simdgroup reduction   = true
0.00.069.666 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.666 I ggml_metal_init: has bfloat            = true
0.00.069.667 I ggml_metal_init: use bfloat            = true
0.00.069.667 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.669 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.048 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.518 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.531 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.568 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.678 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.679 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.680 I llama_new_context_with_model: graph nodes  = 967
0.00.107.680 I llama_new_context_with_model: graph splits = 2
0.00.107.697 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.837 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.404.367 I main: llama threadpool init, n_threads = 4
0.01.404.405 I 
0.01.404.433 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.404.435 I 
0.01.404.677 I sampler seed: 1234
0.01.404.682 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.404.721 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.404.723 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.404.723 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.501.830 I llama_perf_sampler_print:    sampling time =       1.42 ms /    71 runs   (    0.02 ms per token, 49929.68 tokens per second)
0.02.501.830 I llama_perf_context_print:        load time =    1394.51 ms
0.02.501.831 I llama_perf_context_print: prompt eval time =      43.53 ms /     7 tokens (    6.22 ms per token,   160.81 tokens per second)
0.02.501.832 I llama_perf_context_print:        eval time =    1051.02 ms /    63 runs   (   16.68 ms per token,    59.94 tokens per second)
0.02.501.832 I llama_perf_context_print:       total time =    1097.46 ms /    70 tokens
0.02.502.039 I ggml_metal_free: deallocating

real	0m2.519s
user	0m0.115s
sys	0m0.234s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.019.798 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.040.715 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.040.722 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.040.724 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.040.725 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.040.728 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.040.728 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.040.728 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.040.730 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.040.730 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.040.731 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.040.731 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.040.732 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.040.732 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.040.733 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.040.736 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.040.736 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.040.737 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.745 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.358 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.054.617 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.054.619 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.054.619 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.054.620 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.054.620 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.054.621 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.054.622 I llama_model_loader: - type  f32:  194 tensors
0.00.054.622 I llama_model_loader: - type q4_0:   97 tensors
0.00.054.623 I llama_model_loader: - type q6_K:    1 tensors
0.00.097.021 I llm_load_vocab: special tokens cache size = 25
0.00.105.949 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.105.953 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.105.953 I llm_load_print_meta: arch             = gptneox
0.00.105.954 I llm_load_print_meta: vocab type       = BPE
0.00.105.954 I llm_load_print_meta: n_vocab          = 50304
0.00.105.954 I llm_load_print_meta: n_merges         = 50009
0.00.105.955 I llm_load_print_meta: vocab_only       = 0
0.00.105.955 I llm_load_print_meta: n_ctx_train      = 2048
0.00.105.955 I llm_load_print_meta: n_embd           = 2048
0.00.105.955 I llm_load_print_meta: n_layer          = 24
0.00.105.961 I llm_load_print_meta: n_head           = 16
0.00.105.962 I llm_load_print_meta: n_head_kv        = 16
0.00.105.962 I llm_load_print_meta: n_rot            = 32
0.00.105.963 I llm_load_print_meta: n_swa            = 0
0.00.105.963 I llm_load_print_meta: n_embd_head_k    = 128
0.00.105.963 I llm_load_print_meta: n_embd_head_v    = 128
0.00.105.964 I llm_load_print_meta: n_gqa            = 1
0.00.105.965 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.105.967 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.105.967 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.105.968 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.105.968 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.105.968 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.105.970 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.105.973 I llm_load_print_meta: n_ff             = 8192
0.00.105.973 I llm_load_print_meta: n_expert         = 0
0.00.105.973 I llm_load_print_meta: n_expert_used    = 0
0.00.105.974 I llm_load_print_meta: causal attn      = 1
0.00.105.974 I llm_load_print_meta: pooling type     = 0
0.00.105.974 I llm_load_print_meta: rope type        = 2
0.00.105.974 I llm_load_print_meta: rope scaling     = linear
0.00.105.974 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.105.975 I llm_load_print_meta: freq_scale_train = 1
0.00.105.975 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.105.975 I llm_load_print_meta: rope_finetuned   = unknown
0.00.105.976 I llm_load_print_meta: ssm_d_conv       = 0
0.00.105.976 I llm_load_print_meta: ssm_d_inner      = 0
0.00.105.976 I llm_load_print_meta: ssm_d_state      = 0
0.00.105.976 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.105.980 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.105.981 I llm_load_print_meta: model type       = 1.4B
0.00.105.984 I llm_load_print_meta: model ftype      = Q4_0
0.00.105.984 I llm_load_print_meta: model params     = 1.41 B
0.00.105.985 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.105.987 I llm_load_print_meta: general.name     = 1.4B
0.00.105.987 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.105.987 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.105.987 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.105.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.105.988 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.105.988 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.105.988 I llm_load_print_meta: max token length = 1024
0.00.108.729 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.108.730 I llm_load_tensors: offloading output layer to GPU
0.00.108.730 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.108.742 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.108.743 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.110.000 I llama_new_context_with_model: n_seq_max     = 1
0.00.110.001 I llama_new_context_with_model: n_ctx         = 2048
0.00.110.001 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.110.002 I llama_new_context_with_model: n_batch       = 2048
0.00.110.002 I llama_new_context_with_model: n_ubatch      = 512
0.00.110.002 I llama_new_context_with_model: flash_attn    = 0
0.00.110.003 I llama_new_context_with_model: freq_base     = 10000.0
0.00.110.003 I llama_new_context_with_model: freq_scale    = 1
0.00.110.003 I ggml_metal_init: allocating
0.00.110.007 I ggml_metal_init: found device: Apple M4
0.00.110.009 I ggml_metal_init: picking default device: Apple M4
0.00.110.921 I ggml_metal_init: using embedded metal library
0.00.114.088 I ggml_metal_init: GPU name:   Apple M4
0.00.114.090 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.114.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.114.093 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.114.093 I ggml_metal_init: simdgroup reduction   = true
0.00.114.093 I ggml_metal_init: simdgroup matrix mul. = true
0.00.114.093 I ggml_metal_init: has bfloat            = true
0.00.114.094 I ggml_metal_init: use bfloat            = true
0.00.114.101 I ggml_metal_init: hasUnifiedMemory      = true
0.00.114.101 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.129.579 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.152.891 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.152.900 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.152.922 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.153.865 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.153.866 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.153.867 I llama_new_context_with_model: graph nodes  = 967
0.00.153.867 I llama_new_context_with_model: graph splits = 2
0.00.153.884 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.154.026 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.154.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.795.290 I main: llama threadpool init, n_threads = 4
0.00.795.373 I 
0.00.795.424 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.795.424 I 
0.00.795.934 I sampler seed: 1234
0.00.795.942 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.013 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.020 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.020 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.487.534 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59166.67 tokens per second)
0.01.487.535 I llama_perf_context_print:        load time =     775.48 ms
0.01.487.536 I llama_perf_context_print: prompt eval time =      50.90 ms /     7 tokens (    7.27 ms per token,   137.53 tokens per second)
0.01.487.536 I llama_perf_context_print:        eval time =     637.66 ms /    63 runs   (   10.12 ms per token,    98.80 tokens per second)
0.01.487.538 I llama_perf_context_print:       total time =     692.25 ms /    70 tokens
0.01.487.723 I ggml_metal_free: deallocating

real	0m1.522s
user	0m0.155s
sys	0m0.177s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.008.834 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.818 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.819 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.819 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.820 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.820 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.821 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.821 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.822 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.822 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.825 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.826 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.826 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.827 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.895 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.896 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.897 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.897 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.897 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.897 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.898 I llama_model_loader: - type  f32:  194 tensors
0.00.036.898 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.898 I llama_model_loader: - type q6_K:    1 tensors
0.00.061.038 I llm_load_vocab: special tokens cache size = 25
0.00.068.681 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.684 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.685 I llm_load_print_meta: arch             = gptneox
0.00.068.685 I llm_load_print_meta: vocab type       = BPE
0.00.068.685 I llm_load_print_meta: n_vocab          = 50304
0.00.068.685 I llm_load_print_meta: n_merges         = 50009
0.00.068.686 I llm_load_print_meta: vocab_only       = 0
0.00.068.686 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.686 I llm_load_print_meta: n_embd           = 2048
0.00.068.686 I llm_load_print_meta: n_layer          = 24
0.00.068.689 I llm_load_print_meta: n_head           = 16
0.00.068.690 I llm_load_print_meta: n_head_kv        = 16
0.00.068.690 I llm_load_print_meta: n_rot            = 32
0.00.068.692 I llm_load_print_meta: n_swa            = 0
0.00.068.693 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.693 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.693 I llm_load_print_meta: n_gqa            = 1
0.00.068.695 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.695 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.696 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.696 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.696 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.697 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.697 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.700 I llm_load_print_meta: n_ff             = 8192
0.00.068.700 I llm_load_print_meta: n_expert         = 0
0.00.068.700 I llm_load_print_meta: n_expert_used    = 0
0.00.068.700 I llm_load_print_meta: causal attn      = 1
0.00.068.700 I llm_load_print_meta: pooling type     = 0
0.00.068.700 I llm_load_print_meta: rope type        = 2
0.00.068.701 I llm_load_print_meta: rope scaling     = linear
0.00.068.701 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.701 I llm_load_print_meta: freq_scale_train = 1
0.00.068.701 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.702 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.702 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.702 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.702 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.702 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.702 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.703 I llm_load_print_meta: model type       = 1.4B
0.00.068.703 I llm_load_print_meta: model ftype      = Q4_1
0.00.068.703 I llm_load_print_meta: model params     = 1.41 B
0.00.068.705 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.068.705 I llm_load_print_meta: general.name     = 1.4B
0.00.068.705 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.706 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.706 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.706 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.706 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.707 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.707 I llm_load_print_meta: max token length = 1024
0.00.070.871 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.871 I llm_load_tensors: offloading output layer to GPU
0.00.070.872 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.877 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.070.878 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.072.014 I llama_new_context_with_model: n_seq_max     = 1
0.00.072.015 I llama_new_context_with_model: n_ctx         = 2048
0.00.072.016 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.072.016 I llama_new_context_with_model: n_batch       = 2048
0.00.072.016 I llama_new_context_with_model: n_ubatch      = 512
0.00.072.016 I llama_new_context_with_model: flash_attn    = 0
0.00.072.017 I llama_new_context_with_model: freq_base     = 10000.0
0.00.072.017 I llama_new_context_with_model: freq_scale    = 1
0.00.072.017 I ggml_metal_init: allocating
0.00.072.021 I ggml_metal_init: found device: Apple M4
0.00.072.023 I ggml_metal_init: picking default device: Apple M4
0.00.072.745 I ggml_metal_init: using embedded metal library
0.00.075.786 I ggml_metal_init: GPU name:   Apple M4
0.00.075.787 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.075.788 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.075.788 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.075.788 I ggml_metal_init: simdgroup reduction   = true
0.00.075.788 I ggml_metal_init: simdgroup matrix mul. = true
0.00.075.789 I ggml_metal_init: has bfloat            = true
0.00.075.789 I ggml_metal_init: use bfloat            = true
0.00.075.789 I ggml_metal_init: hasUnifiedMemory      = true
0.00.075.790 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.792 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.107.285 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.291 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.308 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.299 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.108.300 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.108.301 I llama_new_context_with_model: graph nodes  = 967
0.00.108.301 I llama_new_context_with_model: graph splits = 2
0.00.108.312 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.452 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.453 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.922.158 I main: llama threadpool init, n_threads = 4
0.00.922.197 I 
0.00.922.255 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.922.273 I 
0.00.922.521 I sampler seed: 1234
0.00.922.526 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.922.546 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.922.546 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.922.546 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.650.602 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65257.35 tokens per second)
0.01.650.602 I llama_perf_context_print:        load time =     913.32 ms
0.01.650.603 I llama_perf_context_print: prompt eval time =      42.60 ms /     7 tokens (    6.09 ms per token,   164.33 tokens per second)
0.01.650.604 I llama_perf_context_print:        eval time =     682.62 ms /    63 runs   (   10.84 ms per token,    92.29 tokens per second)
0.01.650.605 I llama_perf_context_print:       total time =     728.45 ms /    70 tokens
0.01.650.815 I ggml_metal_free: deallocating

real	0m1.668s
user	0m0.118s
sys	0m0.145s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.010.440 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.330 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.330 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.330 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.335 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.336 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.337 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.337 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.337 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.338 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.342 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.342 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.342 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.173 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.205 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.033 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.034 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.035 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.035 I llama_model_loader: - type  f32:  194 tensors
0.00.025.036 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.036 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.217 I llm_load_vocab: special tokens cache size = 25
0.00.051.174 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.177 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.177 I llm_load_print_meta: arch             = gptneox
0.00.051.178 I llm_load_print_meta: vocab type       = BPE
0.00.051.178 I llm_load_print_meta: n_vocab          = 50304
0.00.051.178 I llm_load_print_meta: n_merges         = 50009
0.00.051.178 I llm_load_print_meta: vocab_only       = 0
0.00.051.178 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.179 I llm_load_print_meta: n_embd           = 2048
0.00.051.179 I llm_load_print_meta: n_layer          = 24
0.00.051.181 I llm_load_print_meta: n_head           = 16
0.00.051.182 I llm_load_print_meta: n_head_kv        = 16
0.00.051.182 I llm_load_print_meta: n_rot            = 32
0.00.051.182 I llm_load_print_meta: n_swa            = 0
0.00.051.183 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.183 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.183 I llm_load_print_meta: n_gqa            = 1
0.00.051.184 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.186 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.186 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.187 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.187 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.189 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.189 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.189 I llm_load_print_meta: n_ff             = 8192
0.00.051.190 I llm_load_print_meta: n_expert         = 0
0.00.051.190 I llm_load_print_meta: n_expert_used    = 0
0.00.051.191 I llm_load_print_meta: causal attn      = 1
0.00.051.193 I llm_load_print_meta: pooling type     = 0
0.00.051.193 I llm_load_print_meta: rope type        = 2
0.00.051.193 I llm_load_print_meta: rope scaling     = linear
0.00.051.193 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.194 I llm_load_print_meta: freq_scale_train = 1
0.00.051.194 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.194 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.194 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.194 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.194 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.195 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.195 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.195 I llm_load_print_meta: model type       = 1.4B
0.00.051.195 I llm_load_print_meta: model ftype      = Q5_0
0.00.051.196 I llm_load_print_meta: model params     = 1.41 B
0.00.051.196 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.051.197 I llm_load_print_meta: general.name     = 1.4B
0.00.051.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.197 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.198 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.198 I llm_load_print_meta: max token length = 1024
0.00.053.194 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.194 I llm_load_tensors: offloading output layer to GPU
0.00.053.194 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.205 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.053.206 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.054.134 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.134 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.135 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.135 I llama_new_context_with_model: n_batch       = 2048
0.00.054.135 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.135 I llama_new_context_with_model: flash_attn    = 0
0.00.054.136 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.136 I llama_new_context_with_model: freq_scale    = 1
0.00.054.136 I ggml_metal_init: allocating
0.00.054.140 I ggml_metal_init: found device: Apple M4
0.00.054.142 I ggml_metal_init: picking default device: Apple M4
0.00.054.731 I ggml_metal_init: using embedded metal library
0.00.057.009 I ggml_metal_init: GPU name:   Apple M4
0.00.057.010 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.010 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.011 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.011 I ggml_metal_init: simdgroup reduction   = true
0.00.057.011 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.011 I ggml_metal_init: has bfloat            = true
0.00.057.011 I ggml_metal_init: use bfloat            = true
0.00.057.012 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.014 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.564 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.597 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.611 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.637 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.703 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.704 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.705 I llama_new_context_with_model: graph nodes  = 967
0.00.086.705 I llama_new_context_with_model: graph splits = 2
0.00.086.721 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.869 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.488 I main: llama threadpool init, n_threads = 4
0.00.771.530 I 
0.00.771.554 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.554 I 
0.00.771.721 I sampler seed: 1234
0.00.771.726 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.746 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.746 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.746 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.600.049 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 55124.22 tokens per second)
0.01.600.050 I llama_perf_context_print:        load time =     761.04 ms
0.01.600.050 I llama_perf_context_print: prompt eval time =      43.20 ms /     7 tokens (    6.17 ms per token,   162.03 tokens per second)
0.01.600.051 I llama_perf_context_print:        eval time =     781.95 ms /    63 runs   (   12.41 ms per token,    80.57 tokens per second)
0.01.600.051 I llama_perf_context_print:       total time =     828.56 ms /    70 tokens
0.01.600.226 I ggml_metal_free: deallocating

real	0m1.617s
user	0m0.109s
sys	0m0.158s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.009.223 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.449 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.453 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.455 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.455 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.456 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.456 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.456 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.457 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.458 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.458 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.458 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.459 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.459 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.459 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.462 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.463 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.464 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.328 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.381 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.123 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.124 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.125 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.125 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.125 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.126 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.126 I llama_model_loader: - type  f32:  194 tensors
0.00.024.127 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.127 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.940 I llm_load_vocab: special tokens cache size = 25
0.00.050.867 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.870 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.870 I llm_load_print_meta: arch             = gptneox
0.00.050.871 I llm_load_print_meta: vocab type       = BPE
0.00.050.871 I llm_load_print_meta: n_vocab          = 50304
0.00.050.871 I llm_load_print_meta: n_merges         = 50009
0.00.050.871 I llm_load_print_meta: vocab_only       = 0
0.00.050.872 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.872 I llm_load_print_meta: n_embd           = 2048
0.00.050.872 I llm_load_print_meta: n_layer          = 24
0.00.050.875 I llm_load_print_meta: n_head           = 16
0.00.050.876 I llm_load_print_meta: n_head_kv        = 16
0.00.050.877 I llm_load_print_meta: n_rot            = 32
0.00.050.878 I llm_load_print_meta: n_swa            = 0
0.00.050.878 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.878 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.879 I llm_load_print_meta: n_gqa            = 1
0.00.050.880 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.880 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.881 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.881 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.881 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.881 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.882 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.882 I llm_load_print_meta: n_ff             = 8192
0.00.050.882 I llm_load_print_meta: n_expert         = 0
0.00.050.883 I llm_load_print_meta: n_expert_used    = 0
0.00.050.883 I llm_load_print_meta: causal attn      = 1
0.00.050.883 I llm_load_print_meta: pooling type     = 0
0.00.050.884 I llm_load_print_meta: rope type        = 2
0.00.050.884 I llm_load_print_meta: rope scaling     = linear
0.00.050.885 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.885 I llm_load_print_meta: freq_scale_train = 1
0.00.050.885 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.885 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.885 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.885 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.886 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.886 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.887 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.887 I llm_load_print_meta: model type       = 1.4B
0.00.050.888 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.888 I llm_load_print_meta: model params     = 1.41 B
0.00.050.889 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.889 I llm_load_print_meta: general.name     = 1.4B
0.00.050.889 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.889 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.890 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.890 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.890 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.891 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.891 I llm_load_print_meta: max token length = 1024
0.00.052.562 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.562 I llm_load_tensors: offloading output layer to GPU
0.00.052.563 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.573 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.574 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.438 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.438 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.438 I llama_new_context_with_model: n_batch       = 2048
0.00.053.438 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.439 I llama_new_context_with_model: flash_attn    = 0
0.00.053.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.439 I llama_new_context_with_model: freq_scale    = 1
0.00.053.440 I ggml_metal_init: allocating
0.00.053.443 I ggml_metal_init: found device: Apple M4
0.00.053.445 I ggml_metal_init: picking default device: Apple M4
0.00.054.063 I ggml_metal_init: using embedded metal library
0.00.056.408 I ggml_metal_init: GPU name:   Apple M4
0.00.056.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.410 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.411 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.411 I ggml_metal_init: simdgroup reduction   = true
0.00.056.411 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.411 I ggml_metal_init: has bfloat            = true
0.00.056.411 I ggml_metal_init: use bfloat            = true
0.00.056.412 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.412 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.416 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.075 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.082 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.106 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.055 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.057 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.057 I llama_new_context_with_model: graph nodes  = 967
0.00.086.057 I llama_new_context_with_model: graph splits = 2
0.00.086.073 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.213 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.213 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.707.248 I main: llama threadpool init, n_threads = 4
0.00.707.302 I 
0.00.707.349 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.707.351 I 
0.00.707.528 I sampler seed: 1234
0.00.707.535 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.707.555 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.707.555 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.707.555 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.582.118 I llama_perf_sampler_print:    sampling time =       1.15 ms /    71 runs   (    0.02 ms per token, 61578.49 tokens per second)
0.01.582.118 I llama_perf_context_print:        load time =     698.02 ms
0.01.582.119 I llama_perf_context_print: prompt eval time =      42.33 ms /     7 tokens (    6.05 ms per token,   165.37 tokens per second)
0.01.582.121 I llama_perf_context_print:        eval time =     829.39 ms /    63 runs   (   13.16 ms per token,    75.96 tokens per second)
0.01.582.121 I llama_perf_context_print:       total time =     874.87 ms /    70 tokens
0.01.582.295 I ggml_metal_free: deallocating

real	0m1.598s
user	0m0.111s
sys	0m0.157s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.032 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.061 I main: llama backend init
0.00.000.063 I main: load the model and apply lora adapter, if any
0.00.012.160 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.562 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.571 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.572 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.572 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.573 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.574 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.575 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.575 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.415 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.490 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.297 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.299 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.299 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.299 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.300 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.300 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.026.300 I llama_model_loader: - type  f32:  194 tensors
0.00.026.301 I llama_model_loader: - type q2_K:   49 tensors
0.00.026.301 I llama_model_loader: - type q3_K:   48 tensors
0.00.026.301 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.496 I llm_load_vocab: special tokens cache size = 25
0.00.052.516 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.519 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.519 I llm_load_print_meta: arch             = gptneox
0.00.052.520 I llm_load_print_meta: vocab type       = BPE
0.00.052.520 I llm_load_print_meta: n_vocab          = 50304
0.00.052.520 I llm_load_print_meta: n_merges         = 50009
0.00.052.520 I llm_load_print_meta: vocab_only       = 0
0.00.052.521 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.521 I llm_load_print_meta: n_embd           = 2048
0.00.052.521 I llm_load_print_meta: n_layer          = 24
0.00.052.524 I llm_load_print_meta: n_head           = 16
0.00.052.525 I llm_load_print_meta: n_head_kv        = 16
0.00.052.525 I llm_load_print_meta: n_rot            = 32
0.00.052.526 I llm_load_print_meta: n_swa            = 0
0.00.052.526 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.526 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.527 I llm_load_print_meta: n_gqa            = 1
0.00.052.528 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.528 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.529 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.529 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.529 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.529 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.529 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.530 I llm_load_print_meta: n_ff             = 8192
0.00.052.530 I llm_load_print_meta: n_expert         = 0
0.00.052.531 I llm_load_print_meta: n_expert_used    = 0
0.00.052.531 I llm_load_print_meta: causal attn      = 1
0.00.052.531 I llm_load_print_meta: pooling type     = 0
0.00.052.531 I llm_load_print_meta: rope type        = 2
0.00.052.531 I llm_load_print_meta: rope scaling     = linear
0.00.052.532 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.532 I llm_load_print_meta: freq_scale_train = 1
0.00.052.532 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.533 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.533 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.533 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.533 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.533 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.533 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.534 I llm_load_print_meta: model type       = 1.4B
0.00.052.534 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.534 I llm_load_print_meta: model params     = 1.41 B
0.00.052.535 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.535 I llm_load_print_meta: general.name     = 1.4B
0.00.052.538 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.538 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.538 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.538 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.538 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.539 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.539 I llm_load_print_meta: max token length = 1024
0.00.054.419 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.419 I llm_load_tensors: offloading output layer to GPU
0.00.054.420 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.430 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.431 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.323 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.324 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.324 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.324 I llama_new_context_with_model: n_batch       = 2048
0.00.055.324 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.324 I llama_new_context_with_model: flash_attn    = 0
0.00.055.325 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.325 I llama_new_context_with_model: freq_scale    = 1
0.00.055.326 I ggml_metal_init: allocating
0.00.055.333 I ggml_metal_init: found device: Apple M4
0.00.055.335 I ggml_metal_init: picking default device: Apple M4
0.00.055.923 I ggml_metal_init: using embedded metal library
0.00.058.260 I ggml_metal_init: GPU name:   Apple M4
0.00.058.262 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.263 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.263 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.263 I ggml_metal_init: simdgroup reduction   = true
0.00.058.264 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.264 I ggml_metal_init: has bfloat            = true
0.00.058.264 I ggml_metal_init: use bfloat            = true
0.00.058.265 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.265 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.806 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.088.341 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.346 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.364 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.434 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.435 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.436 I llama_new_context_with_model: graph nodes  = 967
0.00.089.436 I llama_new_context_with_model: graph splits = 2
0.00.089.451 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.595 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.595 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.445.216 I main: llama threadpool init, n_threads = 4
0.00.445.253 I 
0.00.445.273 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.445.273 I 
0.00.445.499 I sampler seed: 1234
0.00.445.505 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.445.535 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.445.536 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.445.536 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.123.315 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61259.71 tokens per second)
0.01.123.316 I llama_perf_context_print:        load time =     433.05 ms
0.01.123.317 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.68 tokens per second)
0.01.123.317 I llama_perf_context_print:        eval time =     639.05 ms /    63 runs   (   10.14 ms per token,    98.58 tokens per second)
0.01.123.321 I llama_perf_context_print:       total time =     678.10 ms /    70 tokens
0.01.123.504 I ggml_metal_free: deallocating

real	0m1.142s
user	0m0.110s
sys	0m0.113s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.062 I main: llama backend init
0.00.000.064 I main: load the model and apply lora adapter, if any
0.00.009.687 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.101 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.107 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.108 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.109 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.109 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.109 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.110 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.110 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.111 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.111 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.113 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.113 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.114 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.062 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.140 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.008 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.009 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.009 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.009 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.010 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.010 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.010 I llama_model_loader: - type  f32:  194 tensors
0.00.025.011 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.011 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.011 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.012 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.140 I llm_load_vocab: special tokens cache size = 25
0.00.050.997 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.000 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.000 I llm_load_print_meta: arch             = gptneox
0.00.051.001 I llm_load_print_meta: vocab type       = BPE
0.00.051.001 I llm_load_print_meta: n_vocab          = 50304
0.00.051.001 I llm_load_print_meta: n_merges         = 50009
0.00.051.001 I llm_load_print_meta: vocab_only       = 0
0.00.051.001 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.002 I llm_load_print_meta: n_embd           = 2048
0.00.051.002 I llm_load_print_meta: n_layer          = 24
0.00.051.004 I llm_load_print_meta: n_head           = 16
0.00.051.005 I llm_load_print_meta: n_head_kv        = 16
0.00.051.005 I llm_load_print_meta: n_rot            = 32
0.00.051.006 I llm_load_print_meta: n_swa            = 0
0.00.051.006 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.006 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.007 I llm_load_print_meta: n_gqa            = 1
0.00.051.008 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.008 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.009 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.009 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.009 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.009 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.009 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.010 I llm_load_print_meta: n_ff             = 8192
0.00.051.012 I llm_load_print_meta: n_expert         = 0
0.00.051.013 I llm_load_print_meta: n_expert_used    = 0
0.00.051.013 I llm_load_print_meta: causal attn      = 1
0.00.051.014 I llm_load_print_meta: pooling type     = 0
0.00.051.014 I llm_load_print_meta: rope type        = 2
0.00.051.014 I llm_load_print_meta: rope scaling     = linear
0.00.051.014 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.015 I llm_load_print_meta: freq_scale_train = 1
0.00.051.015 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.015 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.015 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.017 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.017 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.017 I llm_load_print_meta: model type       = 1.4B
0.00.051.018 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.018 I llm_load_print_meta: model params     = 1.41 B
0.00.051.019 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.019 I llm_load_print_meta: general.name     = 1.4B
0.00.051.019 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.019 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.020 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.020 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.020 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.020 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.020 I llm_load_print_meta: max token length = 1024
0.00.052.974 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.974 I llm_load_tensors: offloading output layer to GPU
0.00.052.974 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.985 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.986 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.929 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.929 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.930 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.930 I llama_new_context_with_model: n_batch       = 2048
0.00.053.930 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.930 I llama_new_context_with_model: flash_attn    = 0
0.00.053.930 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.931 I llama_new_context_with_model: freq_scale    = 1
0.00.053.931 I ggml_metal_init: allocating
0.00.053.934 I ggml_metal_init: found device: Apple M4
0.00.053.936 I ggml_metal_init: picking default device: Apple M4
0.00.054.534 I ggml_metal_init: using embedded metal library
0.00.056.824 I ggml_metal_init: GPU name:   Apple M4
0.00.056.827 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.827 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.828 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.828 I ggml_metal_init: simdgroup reduction   = true
0.00.056.828 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.828 I ggml_metal_init: has bfloat            = true
0.00.056.828 I ggml_metal_init: use bfloat            = true
0.00.056.829 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.829 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.455 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.085.454 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.459 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.478 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.455 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.456 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.456 I llama_new_context_with_model: graph nodes  = 967
0.00.086.457 I llama_new_context_with_model: graph splits = 2
0.00.086.471 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.534.108 I main: llama threadpool init, n_threads = 4
0.00.534.144 I 
0.00.534.167 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.534.167 I 
0.00.534.401 I sampler seed: 1234
0.00.534.405 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.534.455 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.534.456 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.534.456 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.285.083 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54198.47 tokens per second)
0.01.285.084 I llama_perf_context_print:        load time =     524.41 ms
0.01.285.085 I llama_perf_context_print: prompt eval time =      44.39 ms /     7 tokens (    6.34 ms per token,   157.68 tokens per second)
0.01.285.086 I llama_perf_context_print:        eval time =     703.22 ms /    63 runs   (   11.16 ms per token,    89.59 tokens per second)
0.01.285.086 I llama_perf_context_print:       total time =     750.98 ms /    70 tokens
0.01.285.267 I ggml_metal_free: deallocating

real	0m1.301s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.913 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.142 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.143 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.144 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.144 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.144 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.146 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.148 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.149 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.149 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.152 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.152 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.153 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.932 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.804 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.804 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.805 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.805 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.806 I llama_model_loader: - type  f32:  194 tensors
0.00.023.806 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.806 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.807 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.873 I llm_load_vocab: special tokens cache size = 25
0.00.051.964 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.968 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.968 I llm_load_print_meta: arch             = gptneox
0.00.051.968 I llm_load_print_meta: vocab type       = BPE
0.00.051.969 I llm_load_print_meta: n_vocab          = 50304
0.00.051.969 I llm_load_print_meta: n_merges         = 50009
0.00.051.969 I llm_load_print_meta: vocab_only       = 0
0.00.051.969 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.969 I llm_load_print_meta: n_embd           = 2048
0.00.051.969 I llm_load_print_meta: n_layer          = 24
0.00.051.976 I llm_load_print_meta: n_head           = 16
0.00.051.977 I llm_load_print_meta: n_head_kv        = 16
0.00.051.977 I llm_load_print_meta: n_rot            = 32
0.00.051.977 I llm_load_print_meta: n_swa            = 0
0.00.051.977 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.977 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.978 I llm_load_print_meta: n_gqa            = 1
0.00.051.979 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.979 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.980 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.980 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.981 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.981 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.981 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.981 I llm_load_print_meta: n_ff             = 8192
0.00.051.981 I llm_load_print_meta: n_expert         = 0
0.00.051.982 I llm_load_print_meta: n_expert_used    = 0
0.00.051.982 I llm_load_print_meta: causal attn      = 1
0.00.051.982 I llm_load_print_meta: pooling type     = 0
0.00.051.983 I llm_load_print_meta: rope type        = 2
0.00.051.985 I llm_load_print_meta: rope scaling     = linear
0.00.051.985 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.985 I llm_load_print_meta: freq_scale_train = 1
0.00.051.985 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.986 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.986 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.986 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.986 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.986 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.986 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.986 I llm_load_print_meta: model type       = 1.4B
0.00.051.987 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.987 I llm_load_print_meta: model params     = 1.41 B
0.00.051.987 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.988 I llm_load_print_meta: general.name     = 1.4B
0.00.051.988 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.988 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.988 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.988 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.988 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.989 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.989 I llm_load_print_meta: max token length = 1024
0.00.053.975 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.975 I llm_load_tensors: offloading output layer to GPU
0.00.053.975 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.986 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.987 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.937 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.938 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.938 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.938 I llama_new_context_with_model: n_batch       = 2048
0.00.054.938 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.939 I llama_new_context_with_model: flash_attn    = 0
0.00.054.939 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.939 I llama_new_context_with_model: freq_scale    = 1
0.00.054.940 I ggml_metal_init: allocating
0.00.054.944 I ggml_metal_init: found device: Apple M4
0.00.054.946 I ggml_metal_init: picking default device: Apple M4
0.00.055.576 I ggml_metal_init: using embedded metal library
0.00.058.890 I ggml_metal_init: GPU name:   Apple M4
0.00.058.892 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.893 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.893 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.893 I ggml_metal_init: simdgroup reduction   = true
0.00.058.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.894 I ggml_metal_init: has bfloat            = true
0.00.058.895 I ggml_metal_init: use bfloat            = true
0.00.058.896 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.897 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.383 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.929 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.944 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.959 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.960 I llama_new_context_with_model: graph nodes  = 967
0.00.088.961 I llama_new_context_with_model: graph splits = 2
0.00.088.975 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.105 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.106 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.974 I main: llama threadpool init, n_threads = 4
0.00.597.010 I 
0.00.597.030 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.597.030 I 
0.00.597.215 I sampler seed: 1234
0.00.597.219 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.597.258 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.597.259 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.597.275 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.356.694 I llama_perf_sampler_print:    sampling time =       1.31 ms /    71 runs   (    0.02 ms per token, 54115.85 tokens per second)
0.01.356.695 I llama_perf_context_print:        load time =     588.06 ms
0.01.356.695 I llama_perf_context_print: prompt eval time =      47.05 ms /     7 tokens (    6.72 ms per token,   148.79 tokens per second)
0.01.356.696 I llama_perf_context_print:        eval time =     709.20 ms /    63 runs   (   11.26 ms per token,    88.83 tokens per second)
0.01.356.697 I llama_perf_context_print:       total time =     759.72 ms /    70 tokens
0.01.356.913 I ggml_metal_free: deallocating

real	0m1.375s
user	0m0.111s
sys	0m0.127s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.011.208 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.466 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.473 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.474 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.474 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.475 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.476 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.476 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.477 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.478 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.483 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.438 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.509 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.460 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.461 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.462 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.462 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.462 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.463 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.463 I llama_model_loader: - type  f32:  194 tensors
0.00.026.463 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.464 I llama_model_loader: - type q6_K:   37 tensors
0.00.047.507 I llm_load_vocab: special tokens cache size = 25
0.00.053.419 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.422 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.422 I llm_load_print_meta: arch             = gptneox
0.00.053.422 I llm_load_print_meta: vocab type       = BPE
0.00.053.422 I llm_load_print_meta: n_vocab          = 50304
0.00.053.423 I llm_load_print_meta: n_merges         = 50009
0.00.053.423 I llm_load_print_meta: vocab_only       = 0
0.00.053.423 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.423 I llm_load_print_meta: n_embd           = 2048
0.00.053.423 I llm_load_print_meta: n_layer          = 24
0.00.053.426 I llm_load_print_meta: n_head           = 16
0.00.053.428 I llm_load_print_meta: n_head_kv        = 16
0.00.053.428 I llm_load_print_meta: n_rot            = 32
0.00.053.428 I llm_load_print_meta: n_swa            = 0
0.00.053.429 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.429 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.430 I llm_load_print_meta: n_gqa            = 1
0.00.053.430 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.431 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.432 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.432 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.432 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.432 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.432 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.433 I llm_load_print_meta: n_ff             = 8192
0.00.053.433 I llm_load_print_meta: n_expert         = 0
0.00.053.433 I llm_load_print_meta: n_expert_used    = 0
0.00.053.435 I llm_load_print_meta: causal attn      = 1
0.00.053.437 I llm_load_print_meta: pooling type     = 0
0.00.053.437 I llm_load_print_meta: rope type        = 2
0.00.053.437 I llm_load_print_meta: rope scaling     = linear
0.00.053.438 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.438 I llm_load_print_meta: freq_scale_train = 1
0.00.053.438 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.439 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.439 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.439 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.439 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.439 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.439 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.439 I llm_load_print_meta: model type       = 1.4B
0.00.053.440 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.053.440 I llm_load_print_meta: model params     = 1.41 B
0.00.053.441 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.053.441 I llm_load_print_meta: general.name     = 1.4B
0.00.053.441 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.442 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.443 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.443 I llm_load_print_meta: max token length = 1024
0.00.055.462 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.462 I llm_load_tensors: offloading output layer to GPU
0.00.055.462 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.473 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.055.474 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.056.374 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.375 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.375 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.375 I llama_new_context_with_model: n_batch       = 2048
0.00.056.376 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.376 I llama_new_context_with_model: flash_attn    = 0
0.00.056.376 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.376 I llama_new_context_with_model: freq_scale    = 1
0.00.056.377 I ggml_metal_init: allocating
0.00.056.380 I ggml_metal_init: found device: Apple M4
0.00.056.382 I ggml_metal_init: picking default device: Apple M4
0.00.057.014 I ggml_metal_init: using embedded metal library
0.00.059.365 I ggml_metal_init: GPU name:   Apple M4
0.00.059.366 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.369 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.369 I ggml_metal_init: simdgroup reduction   = true
0.00.059.369 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.369 I ggml_metal_init: has bfloat            = true
0.00.059.369 I ggml_metal_init: use bfloat            = true
0.00.059.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.184 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.546 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.561 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.585 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.643 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.644 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.645 I llama_new_context_with_model: graph nodes  = 967
0.00.090.645 I llama_new_context_with_model: graph splits = 2
0.00.090.660 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.809 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.809 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.705.860 I main: llama threadpool init, n_threads = 4
0.00.705.904 I 
0.00.705.924 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.705.924 I 
0.00.706.157 I sampler seed: 1234
0.00.706.161 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.706.184 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.706.185 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.706.186 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.551.025 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58292.28 tokens per second)
0.01.551.026 I llama_perf_context_print:        load time =     694.65 ms
0.01.551.027 I llama_perf_context_print: prompt eval time =      51.57 ms /     7 tokens (    7.37 ms per token,   135.75 tokens per second)
0.01.551.027 I llama_perf_context_print:        eval time =     790.24 ms /    63 runs   (   12.54 ms per token,    79.72 tokens per second)
0.01.551.028 I llama_perf_context_print:       total time =     845.17 ms /    70 tokens
0.01.551.187 I ggml_metal_free: deallocating

real	0m1.569s
user	0m0.110s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.828 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.926 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.930 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.932 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.933 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.933 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.933 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.934 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.935 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.935 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.935 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.936 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.936 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.939 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.940 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.940 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.793 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.812 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.623 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.624 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.625 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.625 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.626 I llama_model_loader: - type  f32:  194 tensors
0.00.024.626 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.913 I llm_load_vocab: special tokens cache size = 25
0.00.050.892 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.894 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.894 I llm_load_print_meta: arch             = gptneox
0.00.050.895 I llm_load_print_meta: vocab type       = BPE
0.00.050.895 I llm_load_print_meta: n_vocab          = 50304
0.00.050.895 I llm_load_print_meta: n_merges         = 50009
0.00.050.896 I llm_load_print_meta: vocab_only       = 0
0.00.050.896 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.896 I llm_load_print_meta: n_embd           = 2048
0.00.050.896 I llm_load_print_meta: n_layer          = 24
0.00.050.899 I llm_load_print_meta: n_head           = 16
0.00.050.899 I llm_load_print_meta: n_head_kv        = 16
0.00.050.900 I llm_load_print_meta: n_rot            = 32
0.00.050.900 I llm_load_print_meta: n_swa            = 0
0.00.050.900 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.900 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.901 I llm_load_print_meta: n_gqa            = 1
0.00.050.902 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.902 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.903 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.903 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.904 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.905 I llm_load_print_meta: n_ff             = 8192
0.00.050.905 I llm_load_print_meta: n_expert         = 0
0.00.050.905 I llm_load_print_meta: n_expert_used    = 0
0.00.050.905 I llm_load_print_meta: causal attn      = 1
0.00.050.908 I llm_load_print_meta: pooling type     = 0
0.00.050.909 I llm_load_print_meta: rope type        = 2
0.00.050.909 I llm_load_print_meta: rope scaling     = linear
0.00.050.910 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.910 I llm_load_print_meta: freq_scale_train = 1
0.00.050.910 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.911 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.911 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.911 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.911 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.911 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.911 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.911 I llm_load_print_meta: model type       = 1.4B
0.00.050.912 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.912 I llm_load_print_meta: model params     = 1.41 B
0.00.050.913 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.913 I llm_load_print_meta: general.name     = 1.4B
0.00.050.913 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.913 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.914 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.914 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.918 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.918 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.919 I llm_load_print_meta: max token length = 1024
0.00.052.984 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.984 I llm_load_tensors: offloading output layer to GPU
0.00.052.984 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.994 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.995 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.976 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.977 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.977 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.977 I llama_new_context_with_model: n_batch       = 2048
0.00.053.977 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.977 I llama_new_context_with_model: flash_attn    = 0
0.00.053.978 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.978 I llama_new_context_with_model: freq_scale    = 1
0.00.053.979 I ggml_metal_init: allocating
0.00.053.982 I ggml_metal_init: found device: Apple M4
0.00.053.984 I ggml_metal_init: picking default device: Apple M4
0.00.054.588 I ggml_metal_init: using embedded metal library
0.00.056.902 I ggml_metal_init: GPU name:   Apple M4
0.00.056.903 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.903 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.904 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.904 I ggml_metal_init: simdgroup reduction   = true
0.00.056.905 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.906 I ggml_metal_init: has bfloat            = true
0.00.056.906 I ggml_metal_init: use bfloat            = true
0.00.056.906 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.907 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.564 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.896 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.904 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.926 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.857 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.858 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.859 I llama_new_context_with_model: graph nodes  = 967
0.00.087.859 I llama_new_context_with_model: graph splits = 2
0.00.087.868 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.997 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.998 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.502 I main: llama threadpool init, n_threads = 4
0.00.741.548 I 
0.00.741.588 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.590 I 
0.00.741.811 I sampler seed: 1234
0.00.741.815 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.857 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.858 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.858 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.622.616 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57911.91 tokens per second)
0.01.622.617 I llama_perf_context_print:        load time =     732.67 ms
0.01.622.617 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.52 tokens per second)
0.01.622.618 I llama_perf_context_print:        eval time =     823.28 ms /    63 runs   (   13.07 ms per token,    76.52 tokens per second)
0.01.622.618 I llama_perf_context_print:       total time =     881.12 ms /    70 tokens
0.01.622.792 I ggml_metal_free: deallocating

real	0m1.640s
user	0m0.110s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.665 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.138 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.724 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.733 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.737 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.737 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.738 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.738 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.742 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.742 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.747 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.747 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.748 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.075 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.121 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.885 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.887 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.888 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.888 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.888 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.889 I llama_model_loader: - type  f32:  194 tensors
0.00.053.890 I llama_model_loader: - type  f16:   98 tensors
0.00.082.961 I llm_load_vocab: special tokens cache size = 25
0.00.089.209 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.089.212 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.089.212 I llm_load_print_meta: arch             = gptneox
0.00.089.212 I llm_load_print_meta: vocab type       = BPE
0.00.089.212 I llm_load_print_meta: n_vocab          = 50304
0.00.089.213 I llm_load_print_meta: n_merges         = 50009
0.00.089.213 I llm_load_print_meta: vocab_only       = 0
0.00.089.213 I llm_load_print_meta: n_ctx_train      = 2048
0.00.089.213 I llm_load_print_meta: n_embd           = 2048
0.00.089.213 I llm_load_print_meta: n_layer          = 24
0.00.089.216 I llm_load_print_meta: n_head           = 16
0.00.089.217 I llm_load_print_meta: n_head_kv        = 16
0.00.089.217 I llm_load_print_meta: n_rot            = 32
0.00.089.217 I llm_load_print_meta: n_swa            = 0
0.00.089.218 I llm_load_print_meta: n_embd_head_k    = 128
0.00.089.218 I llm_load_print_meta: n_embd_head_v    = 128
0.00.089.218 I llm_load_print_meta: n_gqa            = 1
0.00.089.219 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.089.220 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.089.220 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.089.220 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.089.221 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.089.221 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.089.221 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.089.221 I llm_load_print_meta: n_ff             = 8192
0.00.089.222 I llm_load_print_meta: n_expert         = 0
0.00.089.222 I llm_load_print_meta: n_expert_used    = 0
0.00.089.222 I llm_load_print_meta: causal attn      = 1
0.00.089.222 I llm_load_print_meta: pooling type     = 0
0.00.089.222 I llm_load_print_meta: rope type        = 2
0.00.089.222 I llm_load_print_meta: rope scaling     = linear
0.00.089.223 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.089.223 I llm_load_print_meta: freq_scale_train = 1
0.00.089.223 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.089.223 I llm_load_print_meta: rope_finetuned   = unknown
0.00.089.224 I llm_load_print_meta: ssm_d_conv       = 0
0.00.089.224 I llm_load_print_meta: ssm_d_inner      = 0
0.00.089.224 I llm_load_print_meta: ssm_d_state      = 0
0.00.089.224 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.089.224 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.089.224 I llm_load_print_meta: model type       = 1.4B
0.00.089.225 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.089.225 I llm_load_print_meta: model params     = 1.41 B
0.00.089.226 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.089.226 I llm_load_print_meta: general.name     = 1.4B
0.00.089.226 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.089.226 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.089.226 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.089.227 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.089.227 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.089.229 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.089.229 I llm_load_print_meta: max token length = 1024
0.00.091.737 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.737 I llm_load_tensors: offloading output layer to GPU
0.00.091.738 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.748 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.749 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.720 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.721 I llama_new_context_with_model: n_ctx         = 128
0.00.092.721 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.721 I llama_new_context_with_model: n_batch       = 128
0.00.092.722 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.722 I llama_new_context_with_model: flash_attn    = 0
0.00.092.722 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.722 I llama_new_context_with_model: freq_scale    = 1
0.00.092.723 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.723 I ggml_metal_init: allocating
0.00.092.727 I ggml_metal_init: found device: Apple M4
0.00.092.729 I ggml_metal_init: picking default device: Apple M4
0.00.093.326 I ggml_metal_init: using embedded metal library
0.00.095.876 I ggml_metal_init: GPU name:   Apple M4
0.00.095.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.878 I ggml_metal_init: simdgroup reduction   = true
0.00.095.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.879 I ggml_metal_init: has bfloat            = true
0.00.095.879 I ggml_metal_init: use bfloat            = true
0.00.095.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.105.210 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.566 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.568 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.582 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.455 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.457 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.457 I llama_new_context_with_model: graph nodes  = 967
0.00.107.457 I llama_new_context_with_model: graph splits = 2
0.00.107.469 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.470 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.539.317 I 
0.01.539.386 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.539.447 I perplexity: tokenizing the input ..
0.01.552.568 I perplexity: tokenization took 13.118 ms
0.01.552.573 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.674.158 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.676.015 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.676.047 I llama_perf_context_print:        load time =    1515.16 ms
0.01.676.049 I llama_perf_context_print: prompt eval time =     121.04 ms /   128 tokens (    0.95 ms per token,  1057.48 tokens per second)
0.01.676.050 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.676.051 I llama_perf_context_print:       total time =     136.74 ms /   129 tokens
0.01.676.913 I ggml_metal_free: deallocating

real	0m1.868s
user	0m0.124s
sys	0m0.254s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.347 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.699 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.825 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.834 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.837 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.837 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.838 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.838 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.839 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.841 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.847 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.847 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.848 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.800 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.453 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.480 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.481 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.483 I llama_model_loader: - type  f32:  194 tensors
0.00.040.483 I llama_model_loader: - type q8_0:   98 tensors
0.00.068.601 I llm_load_vocab: special tokens cache size = 25
0.00.075.281 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.075.284 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.075.284 I llm_load_print_meta: arch             = gptneox
0.00.075.285 I llm_load_print_meta: vocab type       = BPE
0.00.075.285 I llm_load_print_meta: n_vocab          = 50304
0.00.075.285 I llm_load_print_meta: n_merges         = 50009
0.00.075.285 I llm_load_print_meta: vocab_only       = 0
0.00.075.285 I llm_load_print_meta: n_ctx_train      = 2048
0.00.075.285 I llm_load_print_meta: n_embd           = 2048
0.00.075.286 I llm_load_print_meta: n_layer          = 24
0.00.075.290 I llm_load_print_meta: n_head           = 16
0.00.075.291 I llm_load_print_meta: n_head_kv        = 16
0.00.075.291 I llm_load_print_meta: n_rot            = 32
0.00.075.291 I llm_load_print_meta: n_swa            = 0
0.00.075.291 I llm_load_print_meta: n_embd_head_k    = 128
0.00.075.291 I llm_load_print_meta: n_embd_head_v    = 128
0.00.075.292 I llm_load_print_meta: n_gqa            = 1
0.00.075.293 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.075.293 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.075.294 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.075.294 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.075.295 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.075.295 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.075.295 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.075.295 I llm_load_print_meta: n_ff             = 8192
0.00.075.297 I llm_load_print_meta: n_expert         = 0
0.00.075.297 I llm_load_print_meta: n_expert_used    = 0
0.00.075.299 I llm_load_print_meta: causal attn      = 1
0.00.075.299 I llm_load_print_meta: pooling type     = 0
0.00.075.299 I llm_load_print_meta: rope type        = 2
0.00.075.299 I llm_load_print_meta: rope scaling     = linear
0.00.075.300 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.075.300 I llm_load_print_meta: freq_scale_train = 1
0.00.075.300 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.075.301 I llm_load_print_meta: rope_finetuned   = unknown
0.00.075.301 I llm_load_print_meta: ssm_d_conv       = 0
0.00.075.301 I llm_load_print_meta: ssm_d_inner      = 0
0.00.075.301 I llm_load_print_meta: ssm_d_state      = 0
0.00.075.305 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.075.305 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.075.305 I llm_load_print_meta: model type       = 1.4B
0.00.075.306 I llm_load_print_meta: model ftype      = Q8_0
0.00.075.306 I llm_load_print_meta: model params     = 1.41 B
0.00.075.307 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.075.307 I llm_load_print_meta: general.name     = 1.4B
0.00.075.307 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.075.307 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.075.308 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.075.308 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.075.308 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.075.308 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.075.308 I llm_load_print_meta: max token length = 1024
0.00.077.886 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.886 I llm_load_tensors: offloading output layer to GPU
0.00.077.887 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.898 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.077.899 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.078.949 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.950 I llama_new_context_with_model: n_ctx         = 128
0.00.078.950 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.078.950 I llama_new_context_with_model: n_batch       = 128
0.00.078.951 I llama_new_context_with_model: n_ubatch      = 128
0.00.078.951 I llama_new_context_with_model: flash_attn    = 0
0.00.078.951 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.952 I llama_new_context_with_model: freq_scale    = 1
0.00.078.952 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.078.953 I ggml_metal_init: allocating
0.00.078.956 I ggml_metal_init: found device: Apple M4
0.00.078.959 I ggml_metal_init: picking default device: Apple M4
0.00.079.687 I ggml_metal_init: using embedded metal library
0.00.082.437 I ggml_metal_init: GPU name:   Apple M4
0.00.082.438 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.439 I ggml_metal_init: simdgroup reduction   = true
0.00.082.440 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.440 I ggml_metal_init: has bfloat            = true
0.00.082.440 I ggml_metal_init: use bfloat            = true
0.00.082.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.441 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.093.870 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.095.575 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.095.578 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.095.595 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.671 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.096.672 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.096.673 I llama_new_context_with_model: graph nodes  = 967
0.00.096.673 I llama_new_context_with_model: graph splits = 2
0.00.096.686 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.096.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.035.608 I 
0.01.035.636 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.035.651 I perplexity: tokenizing the input ..
0.01.043.839 I perplexity: tokenization took 8.185 ms
0.01.043.842 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.168.091 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.169.238 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.169.255 I llama_perf_context_print:        load time =    1020.90 ms
0.01.169.256 I llama_perf_context_print: prompt eval time =     124.03 ms /   128 tokens (    0.97 ms per token,  1032.05 tokens per second)
0.01.169.257 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.169.258 I llama_perf_context_print:       total time =     133.65 ms /   129 tokens
0.01.169.586 I ggml_metal_free: deallocating

real	0m1.192s
user	0m0.103s
sys	0m0.178s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.271 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.207 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.879 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.883 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.887 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.887 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.888 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.890 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.890 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.892 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.893 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.894 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.794 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.877 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.756 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.758 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.758 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.759 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.759 I llama_model_loader: - type  f32:  194 tensors
0.00.024.760 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.760 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.654 I llm_load_vocab: special tokens cache size = 25
0.00.051.569 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.572 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.572 I llm_load_print_meta: arch             = gptneox
0.00.051.573 I llm_load_print_meta: vocab type       = BPE
0.00.051.573 I llm_load_print_meta: n_vocab          = 50304
0.00.051.573 I llm_load_print_meta: n_merges         = 50009
0.00.051.573 I llm_load_print_meta: vocab_only       = 0
0.00.051.573 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.574 I llm_load_print_meta: n_embd           = 2048
0.00.051.574 I llm_load_print_meta: n_layer          = 24
0.00.051.577 I llm_load_print_meta: n_head           = 16
0.00.051.577 I llm_load_print_meta: n_head_kv        = 16
0.00.051.578 I llm_load_print_meta: n_rot            = 32
0.00.051.578 I llm_load_print_meta: n_swa            = 0
0.00.051.578 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.578 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.579 I llm_load_print_meta: n_gqa            = 1
0.00.051.580 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.580 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.581 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.582 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.582 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.582 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.582 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.583 I llm_load_print_meta: n_ff             = 8192
0.00.051.583 I llm_load_print_meta: n_expert         = 0
0.00.051.583 I llm_load_print_meta: n_expert_used    = 0
0.00.051.583 I llm_load_print_meta: causal attn      = 1
0.00.051.583 I llm_load_print_meta: pooling type     = 0
0.00.051.584 I llm_load_print_meta: rope type        = 2
0.00.051.584 I llm_load_print_meta: rope scaling     = linear
0.00.051.586 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.587 I llm_load_print_meta: freq_scale_train = 1
0.00.051.587 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.587 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.587 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.588 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.588 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.588 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.588 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.588 I llm_load_print_meta: model type       = 1.4B
0.00.051.588 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.589 I llm_load_print_meta: model params     = 1.41 B
0.00.051.589 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.590 I llm_load_print_meta: general.name     = 1.4B
0.00.051.594 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.594 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.595 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.595 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.595 I llm_load_print_meta: max token length = 1024
0.00.053.533 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.533 I llm_load_tensors: offloading output layer to GPU
0.00.053.533 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.544 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.545 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.054.454 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.455 I llama_new_context_with_model: n_ctx         = 128
0.00.054.455 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.455 I llama_new_context_with_model: n_batch       = 128
0.00.054.455 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.455 I llama_new_context_with_model: flash_attn    = 0
0.00.054.456 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.456 I llama_new_context_with_model: freq_scale    = 1
0.00.054.456 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.457 I ggml_metal_init: allocating
0.00.054.463 I ggml_metal_init: found device: Apple M4
0.00.054.465 I ggml_metal_init: picking default device: Apple M4
0.00.055.027 I ggml_metal_init: using embedded metal library
0.00.057.368 I ggml_metal_init: GPU name:   Apple M4
0.00.057.369 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.369 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.370 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.370 I ggml_metal_init: simdgroup reduction   = true
0.00.057.370 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.370 I ggml_metal_init: has bfloat            = true
0.00.057.370 I ggml_metal_init: use bfloat            = true
0.00.057.371 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.372 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.904 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.129 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.136 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.151 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.978 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.979 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.980 I llama_new_context_with_model: graph nodes  = 967
0.00.068.980 I llama_new_context_with_model: graph splits = 2
0.00.068.992 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.993 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.681.705 I 
0.00.681.735 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.681.748 I perplexity: tokenizing the input ..
0.00.689.521 I perplexity: tokenization took 7.771 ms
0.00.689.524 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.812.466 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.813.633 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.813.648 I llama_perf_context_print:        load time =     671.49 ms
0.00.813.649 I llama_perf_context_print: prompt eval time =     122.71 ms /   128 tokens (    0.96 ms per token,  1043.14 tokens per second)
0.00.813.650 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.813.651 I llama_perf_context_print:       total time =     131.95 ms /   129 tokens
0.00.814.079 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.816 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.338 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.342 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.345 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.345 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.345 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.346 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.347 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.347 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.348 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.348 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.349 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.351 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.351 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.207 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.226 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.161 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.162 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.163 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.163 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.163 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.163 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.164 I llama_model_loader: - type  f32:  194 tensors
0.00.023.164 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.164 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.045 I llm_load_vocab: special tokens cache size = 25
0.00.049.998 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.001 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.001 I llm_load_print_meta: arch             = gptneox
0.00.050.001 I llm_load_print_meta: vocab type       = BPE
0.00.050.002 I llm_load_print_meta: n_vocab          = 50304
0.00.050.002 I llm_load_print_meta: n_merges         = 50009
0.00.050.002 I llm_load_print_meta: vocab_only       = 0
0.00.050.002 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.002 I llm_load_print_meta: n_embd           = 2048
0.00.050.002 I llm_load_print_meta: n_layer          = 24
0.00.050.005 I llm_load_print_meta: n_head           = 16
0.00.050.006 I llm_load_print_meta: n_head_kv        = 16
0.00.050.007 I llm_load_print_meta: n_rot            = 32
0.00.050.007 I llm_load_print_meta: n_swa            = 0
0.00.050.007 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.007 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.008 I llm_load_print_meta: n_gqa            = 1
0.00.050.009 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.010 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.010 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.011 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.011 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.011 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.011 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.012 I llm_load_print_meta: n_ff             = 8192
0.00.050.012 I llm_load_print_meta: n_expert         = 0
0.00.050.012 I llm_load_print_meta: n_expert_used    = 0
0.00.050.012 I llm_load_print_meta: causal attn      = 1
0.00.050.012 I llm_load_print_meta: pooling type     = 0
0.00.050.013 I llm_load_print_meta: rope type        = 2
0.00.050.013 I llm_load_print_meta: rope scaling     = linear
0.00.050.014 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.016 I llm_load_print_meta: freq_scale_train = 1
0.00.050.016 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.017 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.017 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.017 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.018 I llm_load_print_meta: model type       = 1.4B
0.00.050.018 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.018 I llm_load_print_meta: model params     = 1.41 B
0.00.050.022 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.022 I llm_load_print_meta: general.name     = 1.4B
0.00.050.023 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.023 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.023 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.023 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.023 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.024 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.024 I llm_load_print_meta: max token length = 1024
0.00.052.010 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.010 I llm_load_tensors: offloading output layer to GPU
0.00.052.010 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.021 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.022 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.946 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.947 I llama_new_context_with_model: n_ctx         = 128
0.00.052.947 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.947 I llama_new_context_with_model: n_batch       = 128
0.00.052.947 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.948 I llama_new_context_with_model: flash_attn    = 0
0.00.052.948 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.948 I llama_new_context_with_model: freq_scale    = 1
0.00.052.949 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.949 I ggml_metal_init: allocating
0.00.052.952 I ggml_metal_init: found device: Apple M4
0.00.052.954 I ggml_metal_init: picking default device: Apple M4
0.00.053.536 I ggml_metal_init: using embedded metal library
0.00.055.876 I ggml_metal_init: GPU name:   Apple M4
0.00.055.877 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.878 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.878 I ggml_metal_init: simdgroup reduction   = true
0.00.055.878 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.879 I ggml_metal_init: has bfloat            = true
0.00.055.879 I ggml_metal_init: use bfloat            = true
0.00.055.879 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.880 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.682 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.964 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.967 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.980 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.948 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.949 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.949 I llama_new_context_with_model: graph nodes  = 967
0.00.067.949 I llama_new_context_with_model: graph splits = 2
0.00.067.962 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.963 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.656.410 I 
0.00.656.459 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.656.472 I perplexity: tokenizing the input ..
0.00.664.302 I perplexity: tokenization took 7.828 ms
0.00.664.305 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.213 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.788.370 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.788.384 I llama_perf_context_print:        load time =     647.59 ms
0.00.788.385 I llama_perf_context_print: prompt eval time =     122.68 ms /   128 tokens (    0.96 ms per token,  1043.36 tokens per second)
0.00.788.386 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.387 I llama_perf_context_print:       total time =     131.98 ms /   129 tokens
0.00.788.837 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.079s
sys	0m0.096s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.942 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.422 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.431 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.432 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.432 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.432 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.433 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.435 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.435 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.436 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.436 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.436 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.437 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.437 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.439 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.439 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.439 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.083 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.083 I llama_model_loader: - type  f32:  194 tensors
0.00.024.084 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.084 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.785 I llm_load_vocab: special tokens cache size = 25
0.00.050.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.617 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.617 I llm_load_print_meta: arch             = gptneox
0.00.050.618 I llm_load_print_meta: vocab type       = BPE
0.00.050.618 I llm_load_print_meta: n_vocab          = 50304
0.00.050.618 I llm_load_print_meta: n_merges         = 50009
0.00.050.618 I llm_load_print_meta: vocab_only       = 0
0.00.050.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.619 I llm_load_print_meta: n_embd           = 2048
0.00.050.619 I llm_load_print_meta: n_layer          = 24
0.00.050.622 I llm_load_print_meta: n_head           = 16
0.00.050.622 I llm_load_print_meta: n_head_kv        = 16
0.00.050.623 I llm_load_print_meta: n_rot            = 32
0.00.050.623 I llm_load_print_meta: n_swa            = 0
0.00.050.623 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.623 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.624 I llm_load_print_meta: n_gqa            = 1
0.00.050.625 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.630 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.630 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.631 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.631 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.631 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.631 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.632 I llm_load_print_meta: n_ff             = 8192
0.00.050.632 I llm_load_print_meta: n_expert         = 0
0.00.050.632 I llm_load_print_meta: n_expert_used    = 0
0.00.050.633 I llm_load_print_meta: causal attn      = 1
0.00.050.633 I llm_load_print_meta: pooling type     = 0
0.00.050.635 I llm_load_print_meta: rope type        = 2
0.00.050.635 I llm_load_print_meta: rope scaling     = linear
0.00.050.635 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.636 I llm_load_print_meta: freq_scale_train = 1
0.00.050.636 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.636 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.636 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.636 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.638 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.638 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.638 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.639 I llm_load_print_meta: model type       = 1.4B
0.00.050.639 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.639 I llm_load_print_meta: model params     = 1.41 B
0.00.050.640 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.640 I llm_load_print_meta: general.name     = 1.4B
0.00.050.640 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.640 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.641 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.642 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.642 I llm_load_print_meta: max token length = 1024
0.00.052.565 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.565 I llm_load_tensors: offloading output layer to GPU
0.00.052.565 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.575 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.577 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.468 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.468 I llama_new_context_with_model: n_ctx         = 128
0.00.053.469 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.469 I llama_new_context_with_model: n_batch       = 128
0.00.053.469 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.469 I llama_new_context_with_model: flash_attn    = 0
0.00.053.470 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.470 I llama_new_context_with_model: freq_scale    = 1
0.00.053.470 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.471 I ggml_metal_init: allocating
0.00.053.476 I ggml_metal_init: found device: Apple M4
0.00.053.480 I ggml_metal_init: picking default device: Apple M4
0.00.054.051 I ggml_metal_init: using embedded metal library
0.00.056.379 I ggml_metal_init: GPU name:   Apple M4
0.00.056.381 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.381 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.381 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.382 I ggml_metal_init: simdgroup reduction   = true
0.00.056.382 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.382 I ggml_metal_init: has bfloat            = true
0.00.056.382 I ggml_metal_init: use bfloat            = true
0.00.056.383 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.383 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.587 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.960 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.964 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.977 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.793 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.794 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.794 I llama_new_context_with_model: graph nodes  = 967
0.00.067.794 I llama_new_context_with_model: graph splits = 2
0.00.067.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.807 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.728.283 I 
0.00.728.331 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.728.357 I perplexity: tokenizing the input ..
0.00.736.511 I perplexity: tokenization took 8.152 ms
0.00.736.515 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.871.092 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.872.247 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.872.269 I llama_perf_context_print:        load time =     718.34 ms
0.00.872.272 I llama_perf_context_print: prompt eval time =     134.35 ms /   128 tokens (    1.05 ms per token,   952.72 tokens per second)
0.00.872.273 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.872.274 I llama_perf_context_print:       total time =     143.99 ms /   129 tokens
0.00.872.733 I ggml_metal_free: deallocating

real	0m0.888s
user	0m0.078s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.006 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.497 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.501 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.503 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.504 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.504 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.505 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.505 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.506 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.507 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.507 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.507 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.508 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.509 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.510 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.514 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.514 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.514 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.270 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.047 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.048 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.049 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.049 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.049 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.049 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.050 I llama_model_loader: - type  f32:  194 tensors
0.00.023.050 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.051 I llama_model_loader: - type q6_K:    1 tensors
0.00.042.973 I llm_load_vocab: special tokens cache size = 25
0.00.048.670 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.672 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.673 I llm_load_print_meta: arch             = gptneox
0.00.048.673 I llm_load_print_meta: vocab type       = BPE
0.00.048.673 I llm_load_print_meta: n_vocab          = 50304
0.00.048.673 I llm_load_print_meta: n_merges         = 50009
0.00.048.673 I llm_load_print_meta: vocab_only       = 0
0.00.048.674 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.674 I llm_load_print_meta: n_embd           = 2048
0.00.048.674 I llm_load_print_meta: n_layer          = 24
0.00.048.677 I llm_load_print_meta: n_head           = 16
0.00.048.677 I llm_load_print_meta: n_head_kv        = 16
0.00.048.677 I llm_load_print_meta: n_rot            = 32
0.00.048.678 I llm_load_print_meta: n_swa            = 0
0.00.048.678 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.678 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.679 I llm_load_print_meta: n_gqa            = 1
0.00.048.679 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.682 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.683 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.683 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.684 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.684 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.684 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.685 I llm_load_print_meta: n_ff             = 8192
0.00.048.685 I llm_load_print_meta: n_expert         = 0
0.00.048.685 I llm_load_print_meta: n_expert_used    = 0
0.00.048.685 I llm_load_print_meta: causal attn      = 1
0.00.048.685 I llm_load_print_meta: pooling type     = 0
0.00.048.685 I llm_load_print_meta: rope type        = 2
0.00.048.686 I llm_load_print_meta: rope scaling     = linear
0.00.048.686 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.686 I llm_load_print_meta: freq_scale_train = 1
0.00.048.687 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.687 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.687 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.687 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.689 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.689 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.689 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.689 I llm_load_print_meta: model type       = 1.4B
0.00.048.690 I llm_load_print_meta: model ftype      = Q5_1
0.00.048.690 I llm_load_print_meta: model params     = 1.41 B
0.00.048.691 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.048.691 I llm_load_print_meta: general.name     = 1.4B
0.00.048.691 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.691 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.691 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.691 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.695 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.696 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.696 I llm_load_print_meta: max token length = 1024
0.00.050.670 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.670 I llm_load_tensors: offloading output layer to GPU
0.00.050.670 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.681 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.050.682 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.051.627 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.627 I llama_new_context_with_model: n_ctx         = 128
0.00.051.628 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.628 I llama_new_context_with_model: n_batch       = 128
0.00.051.628 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.628 I llama_new_context_with_model: flash_attn    = 0
0.00.051.629 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.629 I llama_new_context_with_model: freq_scale    = 1
0.00.051.629 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.630 I ggml_metal_init: allocating
0.00.051.636 I ggml_metal_init: found device: Apple M4
0.00.051.638 I ggml_metal_init: picking default device: Apple M4
0.00.052.211 I ggml_metal_init: using embedded metal library
0.00.054.529 I ggml_metal_init: GPU name:   Apple M4
0.00.054.531 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.531 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.531 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.532 I ggml_metal_init: simdgroup reduction   = true
0.00.054.532 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.532 I ggml_metal_init: has bfloat            = true
0.00.054.532 I ggml_metal_init: use bfloat            = true
0.00.054.532 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.533 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.918 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.216 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.218 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.231 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.070 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.070 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.071 I llama_new_context_with_model: graph nodes  = 967
0.00.066.071 I llama_new_context_with_model: graph splits = 2
0.00.066.083 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.084 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.507.997 I 
0.00.508.024 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.508.049 I perplexity: tokenizing the input ..
0.00.516.256 I perplexity: tokenization took 8.206 ms
0.00.516.260 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.651.168 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.652.425 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.652.441 I llama_perf_context_print:        load time =     498.99 ms
0.00.652.442 I llama_perf_context_print: prompt eval time =     134.68 ms /   128 tokens (    1.05 ms per token,   950.38 tokens per second)
0.00.652.443 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.652.443 I llama_perf_context_print:       total time =     144.45 ms /   129 tokens
0.00.652.874 I ggml_metal_free: deallocating

real	0m0.666s
user	0m0.077s
sys	0m0.099s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.875 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.265 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.269 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.271 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.272 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.272 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.272 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.272 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.274 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.274 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.275 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.275 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.275 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.279 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.279 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.280 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.084 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.118 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.068 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.069 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.070 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.070 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.070 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.071 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.071 I llama_model_loader: - type  f32:  194 tensors
0.00.024.071 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.072 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.072 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.077 I llm_load_vocab: special tokens cache size = 25
0.00.049.927 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.930 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.930 I llm_load_print_meta: arch             = gptneox
0.00.049.931 I llm_load_print_meta: vocab type       = BPE
0.00.049.931 I llm_load_print_meta: n_vocab          = 50304
0.00.049.931 I llm_load_print_meta: n_merges         = 50009
0.00.049.931 I llm_load_print_meta: vocab_only       = 0
0.00.049.931 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.932 I llm_load_print_meta: n_embd           = 2048
0.00.049.932 I llm_load_print_meta: n_layer          = 24
0.00.049.935 I llm_load_print_meta: n_head           = 16
0.00.049.936 I llm_load_print_meta: n_head_kv        = 16
0.00.049.936 I llm_load_print_meta: n_rot            = 32
0.00.049.936 I llm_load_print_meta: n_swa            = 0
0.00.049.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.937 I llm_load_print_meta: n_gqa            = 1
0.00.049.938 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.939 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.940 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.941 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.941 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.941 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.942 I llm_load_print_meta: n_ff             = 8192
0.00.049.942 I llm_load_print_meta: n_expert         = 0
0.00.049.942 I llm_load_print_meta: n_expert_used    = 0
0.00.049.942 I llm_load_print_meta: causal attn      = 1
0.00.049.942 I llm_load_print_meta: pooling type     = 0
0.00.049.943 I llm_load_print_meta: rope type        = 2
0.00.049.943 I llm_load_print_meta: rope scaling     = linear
0.00.049.945 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.945 I llm_load_print_meta: freq_scale_train = 1
0.00.049.945 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.946 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.946 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.946 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.946 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.946 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.946 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.946 I llm_load_print_meta: model type       = 1.4B
0.00.049.947 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.947 I llm_load_print_meta: model params     = 1.41 B
0.00.049.948 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.952 I llm_load_print_meta: general.name     = 1.4B
0.00.049.952 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.953 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.953 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.954 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.954 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.954 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.955 I llm_load_print_meta: max token length = 1024
0.00.051.845 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.845 I llm_load_tensors: offloading output layer to GPU
0.00.051.845 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.855 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.856 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.750 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.751 I llama_new_context_with_model: n_ctx         = 128
0.00.052.751 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.751 I llama_new_context_with_model: n_batch       = 128
0.00.052.751 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.751 I llama_new_context_with_model: flash_attn    = 0
0.00.052.752 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.752 I llama_new_context_with_model: freq_scale    = 1
0.00.052.753 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.753 I ggml_metal_init: allocating
0.00.052.756 I ggml_metal_init: found device: Apple M4
0.00.052.758 I ggml_metal_init: picking default device: Apple M4
0.00.053.329 I ggml_metal_init: using embedded metal library
0.00.055.623 I ggml_metal_init: GPU name:   Apple M4
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.625 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.625 I ggml_metal_init: simdgroup reduction   = true
0.00.055.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.625 I ggml_metal_init: has bfloat            = true
0.00.055.625 I ggml_metal_init: use bfloat            = true
0.00.055.626 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.626 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.084 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.306 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.308 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.321 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.260 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.261 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.261 I llama_new_context_with_model: graph nodes  = 967
0.00.067.261 I llama_new_context_with_model: graph splits = 2
0.00.067.274 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.275 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.471.231 I 
0.00.471.261 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.471.275 I perplexity: tokenizing the input ..
0.00.479.184 I perplexity: tokenization took 7.908 ms
0.00.479.188 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.611.628 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.612.786 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.612.803 I llama_perf_context_print:        load time =     461.35 ms
0.00.612.804 I llama_perf_context_print: prompt eval time =     132.21 ms /   128 tokens (    1.03 ms per token,   968.13 tokens per second)
0.00.612.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.612.807 I llama_perf_context_print:       total time =     141.57 ms /   129 tokens
0.00.613.314 I ggml_metal_free: deallocating

real	0m0.628s
user	0m0.078s
sys	0m0.077s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.149 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.985 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.990 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.996 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.997 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.997 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.998 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.998 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.999 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.999 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.000 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.000 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.000 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.001 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.001 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.003 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.003 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.003 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.822 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.856 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.681 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.683 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.683 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.683 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.684 I llama_model_loader: - type  f32:  194 tensors
0.00.023.684 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.684 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.685 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.685 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.441 I llm_load_vocab: special tokens cache size = 25
0.00.050.324 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.327 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.327 I llm_load_print_meta: arch             = gptneox
0.00.050.328 I llm_load_print_meta: vocab type       = BPE
0.00.050.328 I llm_load_print_meta: n_vocab          = 50304
0.00.050.328 I llm_load_print_meta: n_merges         = 50009
0.00.050.328 I llm_load_print_meta: vocab_only       = 0
0.00.050.329 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.329 I llm_load_print_meta: n_embd           = 2048
0.00.050.329 I llm_load_print_meta: n_layer          = 24
0.00.050.332 I llm_load_print_meta: n_head           = 16
0.00.050.332 I llm_load_print_meta: n_head_kv        = 16
0.00.050.333 I llm_load_print_meta: n_rot            = 32
0.00.050.333 I llm_load_print_meta: n_swa            = 0
0.00.050.333 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.333 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.334 I llm_load_print_meta: n_gqa            = 1
0.00.050.335 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.335 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.337 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.338 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.338 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.338 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.340 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.341 I llm_load_print_meta: n_ff             = 8192
0.00.050.341 I llm_load_print_meta: n_expert         = 0
0.00.050.341 I llm_load_print_meta: n_expert_used    = 0
0.00.050.341 I llm_load_print_meta: causal attn      = 1
0.00.050.341 I llm_load_print_meta: pooling type     = 0
0.00.050.341 I llm_load_print_meta: rope type        = 2
0.00.050.342 I llm_load_print_meta: rope scaling     = linear
0.00.050.342 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.342 I llm_load_print_meta: freq_scale_train = 1
0.00.050.343 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.343 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.343 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.343 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.343 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.343 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.344 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.344 I llm_load_print_meta: model type       = 1.4B
0.00.050.349 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.349 I llm_load_print_meta: model params     = 1.41 B
0.00.050.350 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.350 I llm_load_print_meta: general.name     = 1.4B
0.00.050.351 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.351 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.351 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.351 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.351 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.352 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.352 I llm_load_print_meta: max token length = 1024
0.00.052.280 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.280 I llm_load_tensors: offloading output layer to GPU
0.00.052.280 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.290 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.292 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.203 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.204 I llama_new_context_with_model: n_ctx         = 128
0.00.053.204 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.204 I llama_new_context_with_model: n_batch       = 128
0.00.053.204 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.204 I llama_new_context_with_model: flash_attn    = 0
0.00.053.205 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.205 I llama_new_context_with_model: freq_scale    = 1
0.00.053.205 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.206 I ggml_metal_init: allocating
0.00.053.213 I ggml_metal_init: found device: Apple M4
0.00.053.215 I ggml_metal_init: picking default device: Apple M4
0.00.053.763 I ggml_metal_init: using embedded metal library
0.00.056.110 I ggml_metal_init: GPU name:   Apple M4
0.00.056.111 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.112 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.112 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.112 I ggml_metal_init: simdgroup reduction   = true
0.00.056.112 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.113 I ggml_metal_init: has bfloat            = true
0.00.056.113 I ggml_metal_init: use bfloat            = true
0.00.056.113 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.114 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.582 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.855 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.857 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.872 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.750 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.750 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.751 I llama_new_context_with_model: graph nodes  = 967
0.00.067.751 I llama_new_context_with_model: graph splits = 2
0.00.067.763 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.764 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.476.632 I 
0.00.476.661 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.476.676 I perplexity: tokenizing the input ..
0.00.484.313 I perplexity: tokenization took 7.634 ms
0.00.484.316 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.616.848 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.618.101 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.618.125 I llama_perf_context_print:        load time =     467.48 ms
0.00.618.126 I llama_perf_context_print: prompt eval time =     132.30 ms /   128 tokens (    1.03 ms per token,   967.48 tokens per second)
0.00.618.127 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.618.127 I llama_perf_context_print:       total time =     141.49 ms /   129 tokens
0.00.618.706 I ggml_metal_free: deallocating

real	0m0.632s
user	0m0.078s
sys	0m0.084s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.690 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.568 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.573 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.575 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.575 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.576 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.576 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.576 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.577 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.578 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.578 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.578 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.579 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.579 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.579 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.583 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.585 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.392 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.423 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.232 I llama_model_loader: - type  f32:  194 tensors
0.00.023.233 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.233 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.233 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.197 I llm_load_vocab: special tokens cache size = 25
0.00.048.922 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.048.925 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.048.925 I llm_load_print_meta: arch             = gptneox
0.00.048.926 I llm_load_print_meta: vocab type       = BPE
0.00.048.926 I llm_load_print_meta: n_vocab          = 50304
0.00.048.926 I llm_load_print_meta: n_merges         = 50009
0.00.048.926 I llm_load_print_meta: vocab_only       = 0
0.00.048.926 I llm_load_print_meta: n_ctx_train      = 2048
0.00.048.926 I llm_load_print_meta: n_embd           = 2048
0.00.048.927 I llm_load_print_meta: n_layer          = 24
0.00.048.929 I llm_load_print_meta: n_head           = 16
0.00.048.930 I llm_load_print_meta: n_head_kv        = 16
0.00.048.930 I llm_load_print_meta: n_rot            = 32
0.00.048.932 I llm_load_print_meta: n_swa            = 0
0.00.048.932 I llm_load_print_meta: n_embd_head_k    = 128
0.00.048.932 I llm_load_print_meta: n_embd_head_v    = 128
0.00.048.933 I llm_load_print_meta: n_gqa            = 1
0.00.048.934 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.048.934 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.048.935 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.048.935 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.048.935 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.048.935 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.048.936 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.048.936 I llm_load_print_meta: n_ff             = 8192
0.00.048.936 I llm_load_print_meta: n_expert         = 0
0.00.048.937 I llm_load_print_meta: n_expert_used    = 0
0.00.048.937 I llm_load_print_meta: causal attn      = 1
0.00.048.937 I llm_load_print_meta: pooling type     = 0
0.00.048.937 I llm_load_print_meta: rope type        = 2
0.00.048.940 I llm_load_print_meta: rope scaling     = linear
0.00.048.941 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.048.941 I llm_load_print_meta: freq_scale_train = 1
0.00.048.941 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.048.942 I llm_load_print_meta: rope_finetuned   = unknown
0.00.048.942 I llm_load_print_meta: ssm_d_conv       = 0
0.00.048.942 I llm_load_print_meta: ssm_d_inner      = 0
0.00.048.942 I llm_load_print_meta: ssm_d_state      = 0
0.00.048.942 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.048.942 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.048.943 I llm_load_print_meta: model type       = 1.4B
0.00.048.943 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.048.944 I llm_load_print_meta: model params     = 1.41 B
0.00.048.944 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.048.944 I llm_load_print_meta: general.name     = 1.4B
0.00.048.945 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.048.945 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.048.945 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.048.945 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.048.946 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.048.946 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.048.946 I llm_load_print_meta: max token length = 1024
0.00.050.881 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.881 I llm_load_tensors: offloading output layer to GPU
0.00.050.881 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.891 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.050.893 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.866 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.867 I llama_new_context_with_model: n_ctx         = 128
0.00.051.867 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.867 I llama_new_context_with_model: n_batch       = 128
0.00.051.867 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.868 I llama_new_context_with_model: flash_attn    = 0
0.00.051.868 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.868 I llama_new_context_with_model: freq_scale    = 1
0.00.051.868 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.869 I ggml_metal_init: allocating
0.00.051.872 I ggml_metal_init: found device: Apple M4
0.00.051.875 I ggml_metal_init: picking default device: Apple M4
0.00.052.467 I ggml_metal_init: using embedded metal library
0.00.054.771 I ggml_metal_init: GPU name:   Apple M4
0.00.054.772 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.772 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.773 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.773 I ggml_metal_init: simdgroup reduction   = true
0.00.054.773 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.773 I ggml_metal_init: has bfloat            = true
0.00.054.775 I ggml_metal_init: use bfloat            = true
0.00.054.776 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.364 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.065.639 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.642 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.656 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.559 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.560 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.561 I llama_new_context_with_model: graph nodes  = 967
0.00.066.561 I llama_new_context_with_model: graph splits = 2
0.00.066.575 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.575 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.567.092 I 
0.00.567.123 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.567.136 I perplexity: tokenizing the input ..
0.00.574.857 I perplexity: tokenization took 7.719 ms
0.00.574.860 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.709.081 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.710.256 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.710.278 I llama_perf_context_print:        load time =     558.40 ms
0.00.710.280 I llama_perf_context_print: prompt eval time =     134.00 ms /   128 tokens (    1.05 ms per token,   955.26 tokens per second)
0.00.710.280 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.710.281 I llama_perf_context_print:       total time =     143.19 ms /   129 tokens
0.00.710.754 I ggml_metal_free: deallocating

real	0m0.725s
user	0m0.077s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.170 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.776 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.778 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.778 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.778 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.779 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.780 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.780 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.780 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.781 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.781 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.781 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.782 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.783 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.784 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.784 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.675 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.442 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.443 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.444 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.444 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.444 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.445 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.445 I llama_model_loader: - type  f32:  194 tensors
0.00.024.445 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.445 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.325 I llm_load_vocab: special tokens cache size = 25
0.00.050.240 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.242 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.243 I llm_load_print_meta: arch             = gptneox
0.00.050.243 I llm_load_print_meta: vocab type       = BPE
0.00.050.243 I llm_load_print_meta: n_vocab          = 50304
0.00.050.243 I llm_load_print_meta: n_merges         = 50009
0.00.050.244 I llm_load_print_meta: vocab_only       = 0
0.00.050.244 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.244 I llm_load_print_meta: n_embd           = 2048
0.00.050.244 I llm_load_print_meta: n_layer          = 24
0.00.050.247 I llm_load_print_meta: n_head           = 16
0.00.050.248 I llm_load_print_meta: n_head_kv        = 16
0.00.050.248 I llm_load_print_meta: n_rot            = 32
0.00.050.248 I llm_load_print_meta: n_swa            = 0
0.00.050.248 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.248 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.249 I llm_load_print_meta: n_gqa            = 1
0.00.050.250 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.251 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.251 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.252 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.252 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.252 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.252 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.253 I llm_load_print_meta: n_ff             = 8192
0.00.050.253 I llm_load_print_meta: n_expert         = 0
0.00.050.253 I llm_load_print_meta: n_expert_used    = 0
0.00.050.254 I llm_load_print_meta: causal attn      = 1
0.00.050.254 I llm_load_print_meta: pooling type     = 0
0.00.050.254 I llm_load_print_meta: rope type        = 2
0.00.050.254 I llm_load_print_meta: rope scaling     = linear
0.00.050.255 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.255 I llm_load_print_meta: freq_scale_train = 1
0.00.050.255 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.255 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.255 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.256 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.256 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.256 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.256 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.256 I llm_load_print_meta: model type       = 1.4B
0.00.050.257 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.257 I llm_load_print_meta: model params     = 1.41 B
0.00.050.258 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.258 I llm_load_print_meta: general.name     = 1.4B
0.00.050.258 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.259 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.259 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.259 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.259 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.260 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.260 I llm_load_print_meta: max token length = 1024
0.00.052.196 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.197 I llm_load_tensors: offloading output layer to GPU
0.00.052.197 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.207 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.208 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.103 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.104 I llama_new_context_with_model: n_ctx         = 128
0.00.053.104 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.104 I llama_new_context_with_model: n_batch       = 128
0.00.053.104 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.104 I llama_new_context_with_model: flash_attn    = 0
0.00.053.105 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.105 I llama_new_context_with_model: freq_scale    = 1
0.00.053.106 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.106 I ggml_metal_init: allocating
0.00.053.109 I ggml_metal_init: found device: Apple M4
0.00.053.111 I ggml_metal_init: picking default device: Apple M4
0.00.053.692 I ggml_metal_init: using embedded metal library
0.00.056.012 I ggml_metal_init: GPU name:   Apple M4
0.00.056.014 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.014 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.014 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.015 I ggml_metal_init: simdgroup reduction   = true
0.00.056.015 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.015 I ggml_metal_init: has bfloat            = true
0.00.056.015 I ggml_metal_init: use bfloat            = true
0.00.056.015 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.016 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.454 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.066.727 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.729 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.744 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.660 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.661 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.661 I llama_new_context_with_model: graph nodes  = 967
0.00.067.661 I llama_new_context_with_model: graph splits = 2
0.00.067.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.658 I 
0.00.650.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.701 I perplexity: tokenizing the input ..
0.00.658.777 I perplexity: tokenization took 8.074 ms
0.00.658.780 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.196 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.800.470 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.800.485 I llama_perf_context_print:        load time =     640.48 ms
0.00.800.486 I llama_perf_context_print: prompt eval time =     140.18 ms /   128 tokens (    1.10 ms per token,   913.13 tokens per second)
0.00.800.487 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.487 I llama_perf_context_print:       total time =     149.83 ms /   129 tokens
0.00.800.858 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.078s
sys	0m0.122s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.862 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.586 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.593 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.593 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.594 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.594 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.595 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.598 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.598 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.601 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.452 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.463 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.305 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.306 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.307 I llama_model_loader: - type  f32:  194 tensors
0.00.023.307 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.026 I llm_load_vocab: special tokens cache size = 25
0.00.049.922 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.925 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.925 I llm_load_print_meta: arch             = gptneox
0.00.049.926 I llm_load_print_meta: vocab type       = BPE
0.00.049.926 I llm_load_print_meta: n_vocab          = 50304
0.00.049.926 I llm_load_print_meta: n_merges         = 50009
0.00.049.926 I llm_load_print_meta: vocab_only       = 0
0.00.049.927 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.927 I llm_load_print_meta: n_embd           = 2048
0.00.049.927 I llm_load_print_meta: n_layer          = 24
0.00.049.930 I llm_load_print_meta: n_head           = 16
0.00.049.930 I llm_load_print_meta: n_head_kv        = 16
0.00.049.930 I llm_load_print_meta: n_rot            = 32
0.00.049.931 I llm_load_print_meta: n_swa            = 0
0.00.049.933 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.933 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.933 I llm_load_print_meta: n_gqa            = 1
0.00.049.934 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.935 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.935 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.936 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.936 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.936 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.936 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.937 I llm_load_print_meta: n_ff             = 8192
0.00.049.937 I llm_load_print_meta: n_expert         = 0
0.00.049.937 I llm_load_print_meta: n_expert_used    = 0
0.00.049.937 I llm_load_print_meta: causal attn      = 1
0.00.049.938 I llm_load_print_meta: pooling type     = 0
0.00.049.938 I llm_load_print_meta: rope type        = 2
0.00.049.938 I llm_load_print_meta: rope scaling     = linear
0.00.049.942 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.942 I llm_load_print_meta: freq_scale_train = 1
0.00.049.943 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.944 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.944 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.945 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.945 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.945 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.945 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.945 I llm_load_print_meta: model type       = 1.4B
0.00.049.946 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.946 I llm_load_print_meta: model params     = 1.41 B
0.00.049.946 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.947 I llm_load_print_meta: general.name     = 1.4B
0.00.049.947 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.950 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.950 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.951 I llm_load_print_meta: max token length = 1024
0.00.051.968 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.968 I llm_load_tensors: offloading output layer to GPU
0.00.051.969 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.979 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.980 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.877 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.877 I llama_new_context_with_model: n_ctx         = 128
0.00.052.878 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.878 I llama_new_context_with_model: n_batch       = 128
0.00.052.878 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.878 I llama_new_context_with_model: flash_attn    = 0
0.00.052.878 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.879 I llama_new_context_with_model: freq_scale    = 1
0.00.052.879 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.879 I ggml_metal_init: allocating
0.00.052.882 I ggml_metal_init: found device: Apple M4
0.00.052.884 I ggml_metal_init: picking default device: Apple M4
0.00.053.452 I ggml_metal_init: using embedded metal library
0.00.055.802 I ggml_metal_init: GPU name:   Apple M4
0.00.055.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.805 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.805 I ggml_metal_init: simdgroup reduction   = true
0.00.055.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.805 I ggml_metal_init: has bfloat            = true
0.00.055.805 I ggml_metal_init: use bfloat            = true
0.00.055.806 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.807 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.026 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.263 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.265 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.281 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.209 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.210 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.210 I llama_new_context_with_model: graph nodes  = 967
0.00.068.210 I llama_new_context_with_model: graph splits = 2
0.00.068.223 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.224 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.641.031 I 
0.00.641.063 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.641.076 I perplexity: tokenizing the input ..
0.00.649.348 I perplexity: tokenization took 8.27 ms
0.00.649.355 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.789.562 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.790.823 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.790.839 I llama_perf_context_print:        load time =     632.17 ms
0.00.790.840 I llama_perf_context_print: prompt eval time =     139.98 ms /   128 tokens (    1.09 ms per token,   914.42 tokens per second)
0.00.790.841 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.790.841 I llama_perf_context_print:       total time =     149.81 ms /   129 tokens
0.00.791.304 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.079s
sys	0m0.112s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.176 I build: 4396 (fdd21889) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.209 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.323 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.331 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.333 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.333 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.334 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.334 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.334 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.335 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.336 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.336 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.337 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.337 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.337 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.338 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.340 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.340 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.340 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.123 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.919 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.920 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.920 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.921 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.921 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.921 I llama_model_loader: - type  f32:  194 tensors
0.00.038.922 I llama_model_loader: - type  f16:   98 tensors
0.00.060.583 I llm_load_vocab: special tokens cache size = 25
0.00.066.620 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.626 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.626 I llm_load_print_meta: arch             = gptneox
0.00.066.626 I llm_load_print_meta: vocab type       = BPE
0.00.066.628 I llm_load_print_meta: n_vocab          = 50304
0.00.066.628 I llm_load_print_meta: n_merges         = 50009
0.00.066.629 I llm_load_print_meta: vocab_only       = 0
0.00.066.629 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.629 I llm_load_print_meta: n_embd           = 2048
0.00.066.629 I llm_load_print_meta: n_layer          = 24
0.00.066.633 I llm_load_print_meta: n_head           = 16
0.00.066.639 I llm_load_print_meta: n_head_kv        = 16
0.00.066.639 I llm_load_print_meta: n_rot            = 32
0.00.066.639 I llm_load_print_meta: n_swa            = 0
0.00.066.639 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.639 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.640 I llm_load_print_meta: n_gqa            = 1
0.00.066.641 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.641 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.643 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.643 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.644 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.644 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.644 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.644 I llm_load_print_meta: n_ff             = 8192
0.00.066.645 I llm_load_print_meta: n_expert         = 0
0.00.066.645 I llm_load_print_meta: n_expert_used    = 0
0.00.066.645 I llm_load_print_meta: causal attn      = 1
0.00.066.645 I llm_load_print_meta: pooling type     = 0
0.00.066.645 I llm_load_print_meta: rope type        = 2
0.00.066.645 I llm_load_print_meta: rope scaling     = linear
0.00.066.646 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.646 I llm_load_print_meta: freq_scale_train = 1
0.00.066.646 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.646 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.646 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.647 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.647 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.647 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.647 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.647 I llm_load_print_meta: model type       = 1.4B
0.00.066.648 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.066.648 I llm_load_print_meta: model params     = 1.41 B
0.00.066.649 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.066.649 I llm_load_print_meta: general.name     = 1.4B
0.00.066.649 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.649 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.649 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.649 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.650 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.650 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.650 I llm_load_print_meta: max token length = 1024
0.00.068.612 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.616 I llm_load_tensors: offloading output layer to GPU
0.00.068.616 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.623 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.068.624 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.069.542 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.543 I llama_new_context_with_model: n_ctx         = 128
0.00.069.543 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.069.543 I llama_new_context_with_model: n_batch       = 128
0.00.069.543 I llama_new_context_with_model: n_ubatch      = 128
0.00.069.544 I llama_new_context_with_model: flash_attn    = 0
0.00.069.544 I llama_new_context_with_model: freq_base     = 10000.0
0.00.069.544 I llama_new_context_with_model: freq_scale    = 1
0.00.069.545 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.069.545 I ggml_metal_init: allocating
0.00.069.550 I ggml_metal_init: found device: Apple M4
0.00.069.552 I ggml_metal_init: picking default device: Apple M4
0.00.070.189 I ggml_metal_init: using embedded metal library
0.00.072.606 I ggml_metal_init: GPU name:   Apple M4
0.00.072.608 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.609 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.609 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.609 I ggml_metal_init: simdgroup reduction   = true
0.00.072.609 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.610 I ggml_metal_init: has bfloat            = true
0.00.072.610 I ggml_metal_init: use bfloat            = true
0.00.072.610 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.611 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.918 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.083.516 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.519 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.534 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.353 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.354 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.354 I llama_new_context_with_model: graph nodes  = 967
0.00.084.354 I llama_new_context_with_model: graph splits = 2
0.00.084.365 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.366 I 
0.00.084.391 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.084.392 I compute_imatrix: tokenizing the input ..
0.00.091.859 I compute_imatrix: tokenization took 7.465 ms
0.00.091.862 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.504.570 I compute_imatrix: 1.41 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.510.373 I llama_perf_context_print:        load time =    1484.36 ms
0.01.510.373 I llama_perf_context_print: prompt eval time =    1411.95 ms /   128 tokens (   11.03 ms per token,    90.65 tokens per second)
0.01.510.374 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.510.375 I llama_perf_context_print:       total time =    1490.16 ms /   129 tokens
0.01.510.934 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.135s
sys	0m0.225s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4396 (fdd21889)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12de0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12de0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12de0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12de0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12de0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12de0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12de0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12de0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12de0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12de0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12de0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12de0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12de0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12de0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12de0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12de101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12de10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12de11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12de11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12de11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12de12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12de12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12de13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12de13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12de14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12de14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12de14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12de15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12de15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12de16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12de16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12de168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12de17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12de176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12de17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12de17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12de182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12de18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12de18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12de19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12de19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12de199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12de19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12de1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12de1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12de1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12de1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12de1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12de1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12de1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12de1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12de1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12de1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12de1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12de1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12de1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12de1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12de1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12de1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12de20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12de20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12de208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12de20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12de21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12de216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12de21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12de21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12de22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12de22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12de22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12de23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12de23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12de23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12de240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12de24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12de24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12de250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12de25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12de25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12de260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12de26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12de26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12de270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12de27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12de27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12de280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12de28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12de28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12de290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12de295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12de29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12de2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12de2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12de2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12de2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12de2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12de2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12de1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12de2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12de2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12de2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12de2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12de2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12de2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12de2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12de2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12de2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12de2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12de2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12de2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12de301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12de30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12de30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12de310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12de31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12de31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12de31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12de32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12de32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12de32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12de33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12de335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12de33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12de33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12de343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12de34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12de34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12de351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12de35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12de35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12de35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12de36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12de368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12de36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12de37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12de376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12de37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12de37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12de38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12de38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12de38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12de39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12de39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12de39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12de3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12de3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12de3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12de3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12de3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12de3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12de3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12de3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12de3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12de3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12de3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12de3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12de3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12de3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12de3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12de3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12de3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12de3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12de3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12de3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12de3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12de40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12de40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12de40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12de40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12de413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12de41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12de41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12de421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12de42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12de42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12de42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12de43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12de438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12de43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12de44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12de446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12de44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12de45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12de454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12de45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12de45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12de46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12de46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12de46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12de47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12de47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12de479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12de47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12de483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12de488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12de48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12de49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12de49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12de49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12de4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12de4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12de4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12de4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12de4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12de4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12de4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12de4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12de4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12de4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12de4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12de4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12de4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12de4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12de4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12de4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12de4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12de50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12de506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12de50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12de51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12de51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12de51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12de52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12de52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12de52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12de53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12de53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12de53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12de54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12de54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12de54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12de55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12de55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12de55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12de560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12de56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12de56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12de570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12de57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12de57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12de580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12de58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12de58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12de590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12de59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12de59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12de5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12de5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12de5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12de5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12de5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12de5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12de5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12de5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12de5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12de5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12de5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12de5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12de5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12de5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12de5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12de5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12de5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12de5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12de60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12de605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12de60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12de60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12de61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12de618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12de61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12de62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12de626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12de62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12de62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12de63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12de63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12de63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12de64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12de64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12de64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12de65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12de655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12de65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12de663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12de66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12de67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12de674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12de67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12de67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12de685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.145.182 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.145.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10df06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10df06570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10df069e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10df06e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10df072c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10df07730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10df07ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10df04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10df044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10df04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10df08010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10df085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10df09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10df098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10df0a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10df0a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10df0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10df0b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10df0bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10df0c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10df0cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10df0d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10df0da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10df0e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10df0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10df0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10df0f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10df0f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10df0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10df105b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10df10a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10df10d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10df115a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10df11ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10df11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10df12240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10df126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10df12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10df13020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10df134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10df13960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10df13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10df142a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10df14740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10df14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10df15010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10df15620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10df15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10df16240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10df16850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10df16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10df17470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10df17a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10df18090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10df18880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10df18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10df191c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10df19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10df19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10df1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10df1a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10df1abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10df1b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10df1b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10df1b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10df1be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10df1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10df1c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10df1cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10df1d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10df1d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10df1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10df1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10df1e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10df1e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10df1ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10df1f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10df1f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10df1fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10df203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10df20920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10df20e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10df213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10df21910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10df21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10df223b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10df22900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10df22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10df233a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10df238f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10df23e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10df24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10df248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10df24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10df25380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10df258d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10df25e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10df26370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10df268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10df26e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10df27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10df278b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10df27e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10df28350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10df288a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10df28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10df29340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10df29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10df29de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10df2a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10df2a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10df2add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10df2b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10df2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10df2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10df2c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10df2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10df2ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10df2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10df2d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10df2d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10df2dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10df2e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10df2e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10df2eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10df2ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10df2f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10df2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10df2fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10df301c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10df30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10df30b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10df30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10df31440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10df318e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10df31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10df32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10df326c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10df32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10df33000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10df334a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10df33940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10df33de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10df34280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10df34720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10df34bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10df35060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10df35500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10df359a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10df35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10df362e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10df36780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10df36c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10df370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10df37560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10df37a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10df37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10df38340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10df387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10df38c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10df39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10df395c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10df39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10df39f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10df3a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10df3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10df3ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10df3b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10df3b620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10df3bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10df3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10df3c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10df3c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10df3cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10df3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10df3d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10df3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10df3dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10df3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10df3e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10df3eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10df3f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10df3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10df3fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10df40020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10df404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10df40960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10df40e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10df412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10df41740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10df41be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10df42080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10df42520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10df42a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10df42fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10df43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10df43a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10df43d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10df44330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10df44940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10df44f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10df45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10df45be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10df45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10df464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10df46ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10df472b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10df47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10df47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10df48090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10df48840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10df48d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10df492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10df49830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10df49d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10df4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10df4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10df4ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10df4b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10df4b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10df4bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10df4c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10df4c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10df4cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10df4d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10df4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10df4dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10df4e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10df4e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10df4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10df4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10df4f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10df4fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10df50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10df507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10df50d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10df51260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10df517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10df51d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10df52250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10df527a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10df52cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10df53240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10df53790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10df53ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10df54230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10df54780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10df54cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10df55220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10df55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10df55cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10df56210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10df56760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10df56cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10df57200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10df57750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10df57ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10df581f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10df58740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10df58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10df591e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10df59730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10df59c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10df5a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10df5a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10df5ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10df5b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10df5b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10df5bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10df5bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10df5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10df5c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10df5cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10df5d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10df5d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10df5db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10df5e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10df5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10df5e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10df5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10df5f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10df5f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10df5fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10df60390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10df60ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10df611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10df618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10df61bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10df623a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10df62660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10df62c70 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x10ef044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x10ef04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x10ef04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x10ef05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x10ef056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x10ef05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x10ef05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x10ef063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x10ef06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x10ef06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x10ef07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x10ef078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x10ef083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x10ef08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x10ef09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x10ef09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x10ef0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x10ef0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x10ef0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x10ef0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x10ef0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x10ef0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x10ef0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x10ef0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x10ef0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x10ef0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x10ef0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x10ef0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x10ef0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x10ef0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x10ef0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x10ef0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x10ef0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x10ef0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x10ef10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x10ef107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x10ef10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x10ef110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x10ef11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x10ef119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x10ef11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x10ef12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x10ef12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x10ef12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x10ef12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x10ef13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x10ef138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x10ef13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x10ef141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x10ef14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x10ef14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x10ef14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x10ef15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x10ef157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x10ef15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x10ef160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x10df062b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x10df06720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x10df06b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x10df07000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x10df07470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x10df078e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x10df07d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x10df081c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x10df08630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x10df08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x10df08f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x10df09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x10df097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x10df09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x10df0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x10df0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x10df0a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x10df0ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x10df0b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x10df0b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x10df0bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x10df0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x10df0c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x10df0c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x10df0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x10df0d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x10df0d610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x10df0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x10df0def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x10df0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x10df0e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x10df0ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x10df0f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x10df0f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x10df0f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x10df0fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x10df10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x10df106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x10df10b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x10df10fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x10df11430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x10df118a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x10df11d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x10df12180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x10df125f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x10df12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x10df12ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x10df13340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x10df137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x10df13c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x10df14090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x10df14500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x10df14970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x10df14de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x10df15250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x10df156c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x10df15b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x10df15fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x10df16410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x10df16880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x10df16cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x10df17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x10df175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x10df17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x10df17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x10df18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x10df18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x10df18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x10df19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x10df194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x10df19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x10df19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x10df1a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x10df1a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x10df1ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x10df1af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x10df1b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x10df1b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x10df1bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x10df1c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x10df1c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x10df1ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x10df1ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x10df1d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x10df1d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x10df1dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x10df1e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x10df1e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x10df1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x10df1eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x10df1f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x10df1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x10df1faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x10df1ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x10df203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x10df20840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x10df20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x10df21120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x10df21590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x10df21a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x10df21e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x10df222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x10df22750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x10df22bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x10df23030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x10df234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x10df23910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x10df23d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x10df241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x10df24660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x10df24ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x10df24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x10df253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x10df25820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x10df25c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x10df26100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x10df26570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x10df269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x10df26e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x10df272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x10df27730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x10df27ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x10df28010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x10df28480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x10df288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x10df28d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x10df291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x10df29640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x10df29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x10df29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x10df2a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x10df2a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x10df2ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x10df2b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x10df2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x10df2b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x10df2be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x10df2c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x10df2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x10df2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x10df2cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x10df2d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x10df2d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10df2dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10df2e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10df2e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10df2ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10df2ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10df2f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10df2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10df2fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10df300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10df30530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10df309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10df31290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10df31700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10df31b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10df31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10df32450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10df328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10df32d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10df331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10df33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10df33a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10df33ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10df34360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10df347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10df34c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10df350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10df35520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10df35990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10df35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10df36270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10df366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10df36b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10df36fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10df37430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10df378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10df37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10df38180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10df385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10df38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10df38ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10df39340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10df397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10df39c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10df3a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10df3a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10df3ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10df3b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10df3b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10df3b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10df3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10df3c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10df3c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10df3cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10df3cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10df3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10df3d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10df3dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10df3e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10df3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10df3ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10df3eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10df3f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10df3f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10df3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10df40070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10df404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10df40950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10df40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10df41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10df416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10df41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10df41f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10df423f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10df42860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10df42cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10df43140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10df435b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10df43a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10df43e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10df44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10df44770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10df44be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10df45050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10df454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10df45bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10df462a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10df46990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10df47080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10df474f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10df47960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10df47dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10df48240 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.848s
user	0m0.292s
sys	0m0.313s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4396 (fdd21889)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cf0e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cf0e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cf0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cf0f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cf0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cf0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cf104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cf10a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cf11020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cf11520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cf11a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cf11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cf12a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cf131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cf13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cf14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cf14840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cf14f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cf15680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cf15e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cf16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cf16c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cf173b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cf17c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cf18370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cf18630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cf18c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cf198b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cf19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cf1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cf1a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cf1a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cf1b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cf1b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cf1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cf1bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cf1c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cf1c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cf1cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cf1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cf1d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cf1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cf1dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cf1e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cf1e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cf1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cf1f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cf1fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cf20050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cf20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cf20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cf21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cf21890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cf21ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cf22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cf22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cf22fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cf23290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cf238a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cf24090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cf24350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cf247f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cf24c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cf25130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cf255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cf25a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cf25f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cf263b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cf26850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cf26cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cf27190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cf27630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cf27ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cf28020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cf28570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cf28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cf29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cf29560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cf29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cf2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cf2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cf2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cf2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cf2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cf2ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cf2bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cf2c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cf2ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cf2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cf2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cf2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cf2dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cf2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cf2ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cf2efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cf2f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cf2fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cf1f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cf2fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cf30670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cf30bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cf31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cf31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cf31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cf32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cf32650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cf32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cf330f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cf33640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cf33b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cf340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cf34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cf34b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cf35020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cf354c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cf35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cf35e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cf362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cf36740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cf36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cf37080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cf37520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cf379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cf37e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cf38300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cf387a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cf38c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cf390e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cf39580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cf39a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cf39ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cf3a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cf3a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cf3aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cf3b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cf3b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cf3ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cf3bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cf3c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cf3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cf3cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cf3d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cf3d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cf3dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cf3df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cf3e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cf3e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cf3ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cf3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cf3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cf3fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cf3ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cf40480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cf40920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cf40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cf41260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cf41700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cf41ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cf42040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cf424e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cf42980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cf42e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cf432c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cf43760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cf43c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cf440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cf44540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cf449e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cf44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cf45320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cf457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cf45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cf46100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cf465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cf46a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cf46ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cf47380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cf47820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cf47cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cf48160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cf48600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cf48aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cf48f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cf493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cf49880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cf49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cf4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cf4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cf4ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cf4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cf4b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cf4b8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cf4bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cf4c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cf4c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cf4cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cf4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cf4d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cf4db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cf4e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cf4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cf4efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cf4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cf4f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cf4fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cf50320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cf50b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cf50fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cf51450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cf518f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cf520a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cf525f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cf52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cf53090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cf535e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cf53b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cf54080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cf545d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cf54b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cf55070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cf555c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cf55b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cf56060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cf565b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cf56b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cf57050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cf575a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cf57af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cf58040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cf58590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cf58ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cf59030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cf59580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cf59ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cf5a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cf5a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cf5aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cf5b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cf5b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cf5bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cf5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cf5c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cf5caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cf5cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cf5d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cf5da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cf5dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cf5e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cf5ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cf5efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cf5f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cf5fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cf5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cf60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cf60a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cf60fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cf61500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cf61a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cf61fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cf624f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cf62a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cf62f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cf634e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cf63a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cf63f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cf644d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cf64a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cf64ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cf65360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cf65800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cf65ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cf66140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cf665e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cf66a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cf66f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cf673c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cf67860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cf67d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cf681a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cf68640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cf68ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cf68f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cf694d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cf69bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cf6a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cf6aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cf6b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cf6b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cf6bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cf6bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cf6c4d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.094.307 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.310 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ea04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ea05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ea056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ea05b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ea05f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ea06400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ea06870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ea06ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ea07150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ea075c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ea07a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ea08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ea08c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ea093f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ea09c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ea0a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ea0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ea0b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ea0b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ea0bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ea0c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ea0cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ea0d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ea0dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ea0e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ea0e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ea0e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ea0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ea0f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ea0f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ea0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ea0ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ea10430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ea106f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ea10b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ea10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ea11440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ea118b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ea11d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ea12190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ea12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ea12a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ea12ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ea13350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ea137c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ea13c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ea140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ea14510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ea14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ea14df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ea15260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ea156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ea15b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ea15fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ea16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ea16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ea16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ea17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ea17770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ea17be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ea18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ea184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ea18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ea18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ea19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ea19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ea19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ea19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ea1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ea1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ea1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ea1b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ea1b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ea1ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ea1be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ea1c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ea1c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ea1cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ea1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ea1d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ea1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ea1dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ea1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ea1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ea1ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ea1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ea1f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ea1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ea1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ea20100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ea20570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ea209e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ea20e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ea212c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ea21730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ea21ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ea22010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ea22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ea228f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ea22d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ea231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ea23640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ea23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ea23f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ea24390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ea24800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ea24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ea250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ea25550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ea259c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ea25e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ea262a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ea26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ea26b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ea26ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ea27460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ea278d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ea27d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ea281b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ea28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ea28a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ea28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ea29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ea297e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ea29c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ea2a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ea2a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ea2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ea2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ea2b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ea2b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ea2bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ea2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ea2c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ea2c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ea2cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ea2d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ea2d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ea2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ea2dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ea2e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ea2e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ea2ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ea2f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ea2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ea2f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ea2fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ea30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ea306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ea30b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ea30fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ea31420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ea31890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ea31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ea32170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ea325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ea32a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ea32ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ea33330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ea337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ea33c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ea34080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ea344f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ea34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ea34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ea35240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ea356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ea35b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ea35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ea36400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ea36870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ea36ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ea37150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ea375c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ea37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ea37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ea38310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ea38780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ea38bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ea39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ea394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ea39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ea39db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ea3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ea3a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ea3ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ea3af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ea3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ea3b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ea3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ea3c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ea3c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ea3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ea3ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ea3d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ea3d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ea3dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ea3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ea3e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ea3e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ea3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ea3f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ea3f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ea3fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ea3ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ea403c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ea40830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ea40dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ea41230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ea416a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ea421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ea424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ea42770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ea42be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ea43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ea434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ea43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ea43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ea44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ea44680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ea44af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ea44f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ea453d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ea45840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ea45cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ea46120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ea46590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ea46a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ea46e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ea472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ea47750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ea47bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ea48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ea484a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ea48910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ea48d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ea491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ea49660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ea49ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ea49f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ea4a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ea4a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ea4ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ea4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ea4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ea4b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ea4be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ea4c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ea4c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ea4cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ea4d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ea4d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ea4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ea4dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ea4e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ea4e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ea4eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ea4ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ea4f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ea4f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ea4fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ea500e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ea50550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ea509c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ea50e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ea512a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ea51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ea51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ea51ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ea52460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ea528d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ea52d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ea531b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ea53620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ea53a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ea53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ea54370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ea547e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ea54c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ea550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ea55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ea559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ea55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ea56880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ea56fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ea576c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ea57de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ea580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ea58510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ea58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ea59120 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12cf0f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12cf0ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12cf0f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12cf0fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12cf10240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12cf106b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12cf10b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12cf10f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12cf0e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12cf28a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12cf28cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12cf29160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12cf29a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12cf2a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12cf2a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12cf2b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12cf2b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12cf2be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12cf2c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12cf2cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12cf2d5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12cf2dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12cf2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12cf2eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12cf2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12cf2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12cf2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12cf2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12cf30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12cf307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12cf30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12cf310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12cf31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12cf317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12cf31c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12cf320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12cf32530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12cf329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12cf32e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12cf33280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12cf336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12cf33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12cf33fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12cf34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12cf348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12cf34d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12cf35190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12cf35600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12cf35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12cf35ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12cf36350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12cf367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12cf36c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12cf370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12cf37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12cf37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12cf37df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12cf38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12cf386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12cf38b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12cf38fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12cf39420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12cf39890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12cf39d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12cf3a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12cf3a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12cf3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12cf3aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12cf3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12cf3b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12cf3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12cf3c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12cf3c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12cf3c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12cf3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12cf3d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12cf3d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12cf3db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12cf3df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12cf3e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12cf3e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12cf3ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12cf3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12cf3f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12cf3fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12cf3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12cf40310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12cf40780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12cf40bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12cf41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12cf414d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12cf41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12cf41db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12cf42220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12cf42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12cf42b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12cf42f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12cf433e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12cf43850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12cf43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12cf44130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12cf445a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12cf44a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12cf44e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12cf452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12cf45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12cf45bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12cf46040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12cf464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12cf46920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12cf46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12cf47200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12cf47670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12cf47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12cf47f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12cf483c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12cf48830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12cf48ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12cf49110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12cf49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12cf499f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12cf49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12cf4a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12cf4a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12cf4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12cf4b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12cf4b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12cf4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12cf4bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12cf4c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12cf4c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12cf4cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12cf4cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12cf4d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12cf4d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12cf4dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12cf4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12cf4e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12cf4e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12cf4ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12cf4f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12cf4f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12cf4fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12cf50000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12cf50470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12cf508e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12cf50d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12cf511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12cf51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12cf51aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12cf51f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12cf52380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12cf527f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12cf52c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12cf530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12cf53540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12cf539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12cf53e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12cf54290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12cf54700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12cf54b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12cf54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12cf55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12cf558c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12cf55d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12cf561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12cf56610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12cf56a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12cf56ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12cf57360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12cf577d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12cf57c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12cf580b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12cf58520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12cf58990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12cf58e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12cf59270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12cf596e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12cf59b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12cf59fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12cf5a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12cf5a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12cf5ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12cf5b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12cf5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12cf5ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12cf5bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12cf5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12cf5c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12cf5cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12cf5d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12cf5d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12cf5d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12cf5dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12cf5e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12cf5e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12cf5eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12cf5efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12cf5f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12cf5f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12cf5fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12cf60160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12cf605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12cf60a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12cf60eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12cf61320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12cf61790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12cf61c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12cf62070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12cf624e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12cf62c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12cf630d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12cf63540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12cf639b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12cf63e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12cf64290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12cf64700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12cf64b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12cf64fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12cf65450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12cf658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12cf65d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12cf661a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12cf66610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12cf66a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12cf66ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12cf67360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12cf677d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12cf67c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12cf680b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12cf68520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12cf68990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12cf68e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12cf69270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12cf696e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12cf69b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12cf69fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12cf6a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12cf6a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12cf6ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12cf6b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12cf6b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12cf6ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12cf6bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12cf6c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12cf1b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12cf1ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12cf1beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12cf1c320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12cf1c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12cf1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12cf1d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12cf1d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12cf1d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12cf1ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12cf1e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12cf1e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12cf1eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12cf1ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12cf1f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12cf1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12cf1fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12cf20140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12cf205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12cf20a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12cf20e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12cf21300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12cf21770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12cf21be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12cf22050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12cf224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12cf22930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12cf22da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12cf23210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12cf23680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12cf23af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12cf23f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12cf243d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12cf24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12cf24cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12cf25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12cf25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12cf25a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12cf260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12cf267e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12cf26ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12cf275c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12cf27a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12cf27ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12cf28310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12cf19de0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.935s
user	0m0.243s
sys	0m0.143s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
