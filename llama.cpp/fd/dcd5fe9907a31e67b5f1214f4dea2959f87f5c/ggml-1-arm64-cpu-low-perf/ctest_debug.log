+ cmake -DCMAKE_BUILD_TYPE=Debug -DLLAMA_FATAL_WARNINGS=ON ..
-- The C compiler identification is GNU 11.4.0
-- The CXX compiler identification is GNU 11.4.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.34.1") 
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE  
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: aarch64
-- Including CPU backend
-- Found OpenMP_C: -fopenmp (found version "4.5") 
-- Found OpenMP_CXX: -fopenmp (found version "4.5") 
-- Found OpenMP: TRUE (found version "4.5")  
-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Failed
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=ares+crypto+noprofile+dotprod+noi8mm+nosve 
-- Configuring done
-- Generating done
-- Build files have been written to: /home/ggml/work/llama.cpp/build-ci-debug

real	0m2.063s
user	0m1.302s
sys	0m0.557s
++ nproc
+ make -j4
[  0%] Generating build details from Git
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
-- Found Git: /usr/bin/git (found version "2.34.1") 
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Built target sha256
[  3%] Built target xxhash
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Built target sha1
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  6%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  7%] Built target build_info
[  7%] Linking CXX shared library libggml-base.so
[  7%] Built target ggml-base
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.so
[ 12%] Built target ggml-cpu
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 13%] Linking CXX shared library libggml.so
[ 13%] Built target ggml
[ 13%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 13%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Linking CXX executable ../../bin/llama-gguf
[ 16%] Linking CXX executable ../../bin/llama-gguf-hash
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Built target llama-gguf-hash
[ 18%] Built target llama-gguf
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
In file included from /home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1:
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:117:14: error: ‘function’ in namespace ‘std’ does not name a template type
  117 |         std::function<void(const void * src, size_t size)> write;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:9:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
    8 | #include <vector>
  +++ |+#include <functional>
    9 | 
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:118:14: error: ‘function’ in namespace ‘std’ does not name a template type
  118 |         std::function<void(const struct ggml_tensor * tensor, size_t offset, size_t size)> write_tensor_data;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:118:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  118 |         std::function<void(const struct ggml_tensor * tensor, size_t offset, size_t size)> write_tensor_data;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:120:14: error: ‘function’ in namespace ‘std’ does not name a template type
  120 |         std::function<const uint8_t * (size_t size)> read;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:120:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  120 |         std::function<const uint8_t * (size_t size)> read;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:121:14: error: ‘function’ in namespace ‘std’ does not name a template type
  121 |         std::function<void(void * dst, size_t size)> read_to;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:121:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  121 |         std::function<void(void * dst, size_t size)> read_to;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp: In member function ‘void llama_kv_cache::state_write(const llama_kv_cache::io&, const llama_hparams&, llama_seq_id) const’:
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:731:8: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  731 |     io.write(&cell_count, sizeof(cell_count));
      |        ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp: In member function ‘void llama_kv_cache::state_read(const llama_kv_cache::io&, const llama_hparams&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:739:8: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  739 |     io.read_to(&cell_count, sizeof(cell_count));
      |        ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:751:20: error: ‘runtime_error’ is not a member of ‘std’
  751 |         throw std::runtime_error("failed to restore kv cache");
      |                    ^~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp: In member function ‘void llama_kv_cache::state_write_meta(const llama_kv_cache::io&, const std::vector<std::pair<unsigned int, unsigned int> >&, llama_seq_id) const’:
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:762:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  762 |             io.write(&pos,      sizeof(pos));
      |                ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:763:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  763 |             io.write(&n_seq_id, sizeof(n_seq_id));
      |                ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:767:24: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  767 |                     io.write(&seq_id, sizeof(seq_id));
      |                        ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp: In member function ‘void llama_kv_cache::state_write_data(const llama_kv_cache::io&, const std::vector<std::pair<unsigned int, unsigned int> >&, const llama_hparams&) const’:
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:778:8: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  778 |     io.write(&v_trans, sizeof(v_trans));
      |        ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:779:8: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  779 |     io.write(&n_layer, sizeof(n_layer));
      |        ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:790:12: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  790 |         io.write(&k_type_i, sizeof(k_type_i));
      |            ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:794:12: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  794 |         io.write(&k_size_row, sizeof(k_size_row));
      |            ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:800:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write_tensor_data’
  800 |             io.write_tensor_data(k_l[il], range.first * k_size_row, buf_size);
      |                ^~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:810:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  810 |             io.write(&v_type_i, sizeof(v_type_i));
      |                ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:814:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  814 |             io.write(&v_size_row, sizeof(v_size_row));
      |                ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:820:20: error: ‘const struct llama_kv_cache::io’ has no member named ‘write_tensor_data’
  820 |                 io.write_tensor_data(v_l[il], range.first * v_size_row, buf_size);
      |                    ^~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:831:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  831 |             io.write(&v_type_i, sizeof(v_type_i));
      |                ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:835:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  835 |             io.write(&v_size_el, sizeof(v_size_el));
      |                ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:838:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘write’
  838 |             io.write(&n_embd_v_gqa, sizeof(n_embd_v_gqa));
      |                ^~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:847:24: error: ‘const struct llama_kv_cache::io’ has no member named ‘write_tensor_data’
  847 |                     io.write_tensor_data(v_l[il], src_offset, buf_size);
      |                        ^~~~~~~~~~~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp: In member function ‘bool llama_kv_cache::state_read_meta(const llama_kv_cache::io&, uint32_t, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:871:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  871 |             io.read_to(&pos,      sizeof(pos));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:872:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  872 |             io.read_to(&n_seq_id, sizeof(n_seq_id));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:911:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  911 |             io.read_to(&pos,      sizeof(pos));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:912:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  912 |             io.read_to(&n_seq_id, sizeof(n_seq_id));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:918:20: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  918 |                 io.read_to(&seq_id, sizeof(seq_id));
      |                    ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp: In member function ‘bool llama_kv_cache::state_read_data(const llama_kv_cache::io&, const llama_hparams&, uint32_t)’:
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:959:8: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  959 |     io.read_to(&v_trans, sizeof(v_trans));
      |        ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:960:8: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  960 |     io.read_to(&n_layer, sizeof(n_layer));
      |        ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:981:12: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  981 |         io.read_to(&k_type_i_ref, sizeof(k_type_i_ref));
      |            ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:990:12: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
  990 |         io.read_to(&k_size_row_ref, sizeof(k_size_row_ref));
      |            ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:999:49: error: ‘const struct llama_kv_cache::io’ has no member named ‘read’
  999 |             ggml_backend_tensor_set(k_l[il], io.read(cell_count * k_size_row), head * k_size_row, cell_count * k_size_row);
      |                                                 ^~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1009:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
 1009 |             io.read_to(&v_type_i_ref, sizeof(v_type_i_ref));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1018:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
 1018 |             io.read_to(&v_size_row_ref, sizeof(v_size_row_ref));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1027:53: error: ‘const struct llama_kv_cache::io’ has no member named ‘read’
 1027 |                 ggml_backend_tensor_set(v_l[il], io.read(cell_count * v_size_row), head * v_size_row, cell_count * v_size_row);
      |                                                     ^~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1037:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
 1037 |             io.read_to(&v_type_i_ref, sizeof(v_type_i_ref));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1046:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
 1046 |             io.read_to(&v_size_el_ref, sizeof(v_size_el_ref));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1055:16: error: ‘const struct llama_kv_cache::io’ has no member named ‘read_to’
 1055 |             io.read_to(&n_embd_v_gqa_ref, sizeof(n_embd_v_gqa_ref));
      |                ^~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.cpp:1065:57: error: ‘const struct llama_kv_cache::io’ has no member named ‘read’
 1065 |                     ggml_backend_tensor_set(v_l[il], io.read(cell_count * v_size_el), dst_offset, cell_count * v_size_el);
      |                                                         ^~~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:202: src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o] Error 1
make[2]: *** Waiting for unfinished jobs....
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:7,
                 from /home/ggml/work/llama.cpp/src/llama-context.cpp:1:
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:117:14: error: ‘function’ in namespace ‘std’ does not name a template type
  117 |         std::function<void(const void * src, size_t size)> write;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:8:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
    7 | #include <set>
  +++ |+#include <functional>
    8 | #include <vector>
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:118:14: error: ‘function’ in namespace ‘std’ does not name a template type
  118 |         std::function<void(const struct ggml_tensor * tensor, size_t offset, size_t size)> write_tensor_data;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:118:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  118 |         std::function<void(const struct ggml_tensor * tensor, size_t offset, size_t size)> write_tensor_data;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:120:14: error: ‘function’ in namespace ‘std’ does not name a template type
  120 |         std::function<const uint8_t * (size_t size)> read;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:120:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  120 |         std::function<const uint8_t * (size_t size)> read;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:121:14: error: ‘function’ in namespace ‘std’ does not name a template type
  121 |         std::function<void(void * dst, size_t size)> read_to;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:121:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  121 |         std::function<void(void * dst, size_t size)> read_to;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_get_data_internal(llama_context*, llama_data_write&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1150:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1150 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1153:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1153 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1156:5: error: too many initializers for ‘llama_kv_cache::io’
 1156 |     };
      |     ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_set_data_internal(llama_context*, llama_data_read&)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1196:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1196 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1199:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1199 |         .read_to = [&](void * dst, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1202:5: error: too many initializers for ‘llama_kv_cache::io’
 1202 |     };
      |     ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_get_data_internal(llama_context*, llama_data_write&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1303:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1303 |         .write = [&](const void * src, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1306:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1306 |         .write_tensor_data = [&](const struct ggml_tensor * tensor, size_t offset, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1309:5: error: too many initializers for ‘llama_kv_cache::io’
 1309 |     };
      |     ^
/home/ggml/work/llama.cpp/src/llama-context.cpp: In function ‘size_t llama_state_seq_set_data_internal(llama_context*, llama_data_read&, llama_seq_id)’:
/home/ggml/work/llama.cpp/src/llama-context.cpp:1335:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1335 |         .read = [&](size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1338:9: error: C++ designated initializers only available with ‘-std=c++20’ or ‘-std=gnu++20’ [-Werror=pedantic]
 1338 |         .read_to = [&](void * dst, size_t size) {
      |         ^
/home/ggml/work/llama.cpp/src/llama-context.cpp:1341:5: error: too many initializers for ‘llama_kv_cache::io’
 1341 |     };
      |     ^
cc1plus: all warnings being treated as errors
make[2]: *** [src/CMakeFiles/llama.dir/build.make:146: src/CMakeFiles/llama.dir/llama-context.cpp.o] Error 1
In file included from /home/ggml/work/llama.cpp/src/llama-context.h:7,
                 from /home/ggml/work/llama.cpp/src/llama.cpp:5:
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:117:14: error: ‘function’ in namespace ‘std’ does not name a template type
  117 |         std::function<void(const void * src, size_t size)> write;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:8:1: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
    7 | #include <set>
  +++ |+#include <functional>
    8 | #include <vector>
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:118:14: error: ‘function’ in namespace ‘std’ does not name a template type
  118 |         std::function<void(const struct ggml_tensor * tensor, size_t offset, size_t size)> write_tensor_data;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:118:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  118 |         std::function<void(const struct ggml_tensor * tensor, size_t offset, size_t size)> write_tensor_data;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:120:14: error: ‘function’ in namespace ‘std’ does not name a template type
  120 |         std::function<const uint8_t * (size_t size)> read;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:120:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  120 |         std::function<const uint8_t * (size_t size)> read;
      |         ^~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:121:14: error: ‘function’ in namespace ‘std’ does not name a template type
  121 |         std::function<void(void * dst, size_t size)> read_to;
      |              ^~~~~~~~
/home/ggml/work/llama.cpp/src/llama-kv-cache.h:121:9: note: ‘std::function’ is defined in header ‘<functional>’; did you forget to ‘#include <functional>’?
  121 |         std::function<void(void * dst, size_t size)> read_to;
      |         ^~~
make[2]: *** [src/CMakeFiles/llama.dir/build.make:76: src/CMakeFiles/llama.dir/llama.cpp.o] Error 1
make[1]: *** [CMakeFiles/Makefile2:1767: src/CMakeFiles/llama.dir/all] Error 2
make: *** [Makefile:146: all] Error 2

real	0m1.957s
user	0m3.602s
sys	0m1.009s
