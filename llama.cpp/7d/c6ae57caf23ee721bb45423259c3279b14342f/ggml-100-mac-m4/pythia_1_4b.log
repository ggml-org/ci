Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:43 (message):
  OpenMP not found


-- Using llamafile
-- ARM detected
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- Using runtime weight conversion of Q4_0 to Q4_0_x_x to enable optimized GEMM/GEMV kernels
-- Including CPU backend
CMake Warning at ggml/src/ggml-amx/CMakeLists.txt:106 (message):
  AMX requires x86 and gcc version > 11.0.  Turning off GGML_AMX.


-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (1.0s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m1.291s
user	0m0.560s
sys	0m0.751s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  2%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  6%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-aarch64.c.o
[  6%] Built target sha1
[  6%] Built target build_info
[  6%] Built target xxhash
[  6%] Built target sha256
[  7%] Linking CXX shared library libggml-base.dylib
[  7%] Built target ggml-base
[  8%] Generate assembly for embedded Metal library
Embedding Metal library
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.c.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-aarch64.c.o
[ 11%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu.cpp.o
[ 11%] Building C object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/ggml-cpu-quants.c.o
[ 12%] Building CXX object ggml/src/ggml-cpu/CMakeFiles/ggml-cpu.dir/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-cpu
[ 13%] Built target ggml-blas
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 20%] Linking CXX shared library libllama.dylib
[ 20%] Built target llama-gguf
[ 20%] Built target llama-gguf-hash
[ 20%] Built target llama
[ 20%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 21%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 21%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 22%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 22%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 23%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Linking C executable ../bin/test-c
[ 27%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 29%] Linking CXX executable ../../bin/llama-simple
[ 29%] Linking CXX executable ../../bin/llama-simple-chat
[ 29%] Linking CXX executable ../../bin/llama-quantize-stats
[ 29%] Built target llava
[ 30%] Linking CXX static library libcommon.a
[ 31%] Linking CXX static library libllava_static.a
[ 31%] Built target llama-simple
[ 31%] Linking CXX shared library libllava_shared.dylib
[ 31%] Built target llama-simple-chat
[ 31%] Built target test-c
[ 31%] Built target llama-quantize-stats
[ 31%] Built target common
[ 31%] Built target llava_static
[ 31%] Built target llava_shared
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 31%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 32%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 33%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 34%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 35%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 35%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 37%] Linking CXX executable ../bin/test-tokenizer-0
[ 37%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 39%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 40%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 42%] Linking CXX executable ../bin/test-arg-parser
[ 42%] Linking CXX executable ../bin/test-log
[ 43%] Linking CXX executable ../bin/test-sampling
[ 43%] Linking CXX executable ../bin/test-chat-template
[ 43%] Linking CXX executable ../bin/test-quantize-perf
[ 44%] Linking CXX executable ../bin/test-quantize-fns
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Built target test-tokenizer-1-bpe
[ 45%] Built target test-tokenizer-1-spm
[ 45%] Built target test-tokenizer-0
[ 45%] Built target test-log
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 46%] Built target test-chat-template
[ 46%] Built target test-grammar-parser
[ 47%] Built target test-sampling
[ 47%] Built target test-quantize-perf
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 47%] Built target test-arg-parser
[ 47%] Built target test-quantize-fns
[ 47%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 48%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 51%] Linking CXX executable ../bin/test-llama-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-grammar-integration
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 54%] Linking CXX executable ../bin/test-barrier
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 57%] Linking CXX executable ../bin/test-rope
[ 58%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 60%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 60%] Built target test-grammar-integration
[ 60%] Built target test-llama-grammar
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Built target test-barrier
[ 60%] Built target test-backend-ops
[ 61%] Linking CXX executable ../../bin/llama-batched-bench
[ 61%] Built target test-rope
[ 61%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 62%] Linking CXX executable ../../bin/llama-cvector-generator
[ 62%] Built target test-autorelease
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 63%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 63%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 65%] Built target test-model-load-cancel
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 67%] Built target test-json-schema-to-grammar
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Built target llama-batched-bench
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Linking CXX executable ../../bin/llama-export-lora
[ 69%] Built target llama-cvector-generator
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Built target llama-convert-llama2c-to-ggml
[ 70%] Built target llama-embedding
[ 70%] Built target llama-batched
[ 70%] Built target llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-gbnf-validator
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Built target llama-export-lora
[ 73%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 73%] Built target llama-gguf-split
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-bench
[ 77%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookahead
[ 79%] Linking CXX executable ../../bin/llama-llava-cli
[ 79%] Built target llama-gritlm
[ 79%] Built target llama-imatrix
[ 79%] Built target llama-infill
[ 80%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup
[ 80%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 81%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 81%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 82%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 82%] Built target llama-bench
[ 82%] Linking CXX executable ../../bin/llama-lookup-merge
[ 82%] Built target llama-lookahead
[ 82%] Built target llama-llava-cli
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-lookup-stats
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-minicpmv-cli
[ 83%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-parallel
[ 84%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 84%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 85%] Generating loading.html.hpp
[ 86%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 86%] Built target llama-lookup-merge
[ 87%] Linking CXX executable ../../bin/llama-passkey
[ 88%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 88%] Generating completion.js.hpp
[ 89%] Linking CXX executable ../../bin/llama-quantize
[ 90%] Linking CXX executable ../../bin/llama-perplexity
[ 90%] Built target llama-cli
[ 90%] Built target llama-lookup-stats
[ 90%] Linking CXX executable ../../bin/llama-retrieval
[ 90%] Built target llama-parallel
[ 90%] Generating deps_daisyui.min.css.hpp
[ 90%] Linking CXX executable ../../bin/llama-save-load-state
[ 91%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 92%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 92%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 92%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-speculative
[ 92%] Built target llama-quantize
[ 92%] Built target llama-passkey
[ 92%] Built target llama-perplexity
[ 93%] Linking CXX executable ../../bin/llama-speculative-simple
[ 94%] Generating deps_markdown-it.js.hpp
[ 95%] Linking CXX executable ../../bin/llama-tokenize
[ 95%] Built target llama-retrieval
[ 95%] Built target llama-save-load-state
[ 96%] Generating deps_tailwindcss.js.hpp
[ 97%] Linking CXX executable ../../bin/llama-vdot
[ 97%] Generating deps_vue.esm-browser.js.hpp
[ 98%] Generating index.html.hpp
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-speculative
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-tokenize
[ 99%] Built target llama-speculative-simple
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.518s
user	0m5.800s
sys	0m8.925s

main: quantize time =  5942.81 ms
main:    total time =  5942.81 ms

main: quantize time =  2066.02 ms
main:    total time =  2066.02 ms

main: quantize time =  1847.44 ms
main:    total time =  1847.44 ms

main: quantize time =  2013.05 ms
main:    total time =  2013.05 ms

main: quantize time =  2732.38 ms
main:    total time =  2732.38 ms

main: quantize time =  5547.56 ms
main:    total time =  5547.56 ms

main: quantize time =  6392.78 ms
main:    total time =  6392.78 ms

main: quantize time =  7018.18 ms
main:    total time =  7018.18 ms

main: quantize time =  7138.76 ms
main:    total time =  7138.76 ms

main: quantize time =  4691.50 ms
main:    total time =  4691.50 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.128 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.295 I main: llama backend init
0.00.000.303 I main: load the model and apply lora adapter, if any
0.00.106.768 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.121.135 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.121.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.121.160 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.121.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.121.162 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.121.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.121.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.121.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.121.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.121.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.121.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.121.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.121.172 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.121.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.121.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.121.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.121.178 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.128.302 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.130.571 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.137.661 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.137.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.137.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.137.672 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.137.673 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.137.674 I llama_model_loader: - type  f32:  194 tensors
0.00.137.675 I llama_model_loader: - type  f16:   98 tensors
0.00.176.850 I llm_load_vocab: special tokens cache size = 25
0.00.184.623 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.184.627 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.184.627 I llm_load_print_meta: arch             = gptneox
0.00.184.628 I llm_load_print_meta: vocab type       = BPE
0.00.184.628 I llm_load_print_meta: n_vocab          = 50304
0.00.184.628 I llm_load_print_meta: n_merges         = 50009
0.00.184.628 I llm_load_print_meta: vocab_only       = 0
0.00.184.628 I llm_load_print_meta: n_ctx_train      = 2048
0.00.184.628 I llm_load_print_meta: n_embd           = 2048
0.00.184.629 I llm_load_print_meta: n_layer          = 24
0.00.184.632 I llm_load_print_meta: n_head           = 16
0.00.184.633 I llm_load_print_meta: n_head_kv        = 16
0.00.184.634 I llm_load_print_meta: n_rot            = 32
0.00.184.634 I llm_load_print_meta: n_swa            = 0
0.00.184.634 I llm_load_print_meta: n_embd_head_k    = 128
0.00.184.635 I llm_load_print_meta: n_embd_head_v    = 128
0.00.184.635 I llm_load_print_meta: n_gqa            = 1
0.00.184.636 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.184.637 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.184.637 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.184.637 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.184.638 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.184.638 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.184.638 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.184.639 I llm_load_print_meta: n_ff             = 8192
0.00.184.639 I llm_load_print_meta: n_expert         = 0
0.00.184.639 I llm_load_print_meta: n_expert_used    = 0
0.00.184.639 I llm_load_print_meta: causal attn      = 1
0.00.184.640 I llm_load_print_meta: pooling type     = 0
0.00.184.640 I llm_load_print_meta: rope type        = 2
0.00.184.640 I llm_load_print_meta: rope scaling     = linear
0.00.184.641 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.184.641 I llm_load_print_meta: freq_scale_train = 1
0.00.184.641 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.184.641 I llm_load_print_meta: rope_finetuned   = unknown
0.00.184.641 I llm_load_print_meta: ssm_d_conv       = 0
0.00.184.641 I llm_load_print_meta: ssm_d_inner      = 0
0.00.184.642 I llm_load_print_meta: ssm_d_state      = 0
0.00.184.642 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.184.642 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.184.653 I llm_load_print_meta: model type       = 1.4B
0.00.184.654 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.184.654 I llm_load_print_meta: model params     = 1.41 B
0.00.184.655 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.184.655 I llm_load_print_meta: general.name     = 1.4B
0.00.184.655 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.184.655 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.184.656 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.184.656 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.184.656 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.184.656 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.184.656 I llm_load_print_meta: max token length = 1024
0.00.186.696 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.186.696 I llm_load_tensors: offloading output layer to GPU
0.00.186.697 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.186.714 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.186.715 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.187.696 I llama_new_context_with_model: n_seq_max     = 1
0.00.187.697 I llama_new_context_with_model: n_ctx         = 2048
0.00.187.697 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.187.697 I llama_new_context_with_model: n_batch       = 2048
0.00.187.697 I llama_new_context_with_model: n_ubatch      = 512
0.00.187.697 I llama_new_context_with_model: flash_attn    = 0
0.00.187.698 I llama_new_context_with_model: freq_base     = 10000.0
0.00.187.698 I llama_new_context_with_model: freq_scale    = 1
0.00.187.699 I ggml_metal_init: allocating
0.00.187.706 I ggml_metal_init: found device: Apple M4
0.00.187.709 I ggml_metal_init: picking default device: Apple M4
0.00.188.369 I ggml_metal_init: using embedded metal library
0.00.240.843 I ggml_metal_init: GPU name:   Apple M4
0.00.240.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.240.846 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.240.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.240.847 I ggml_metal_init: simdgroup reduction   = true
0.00.240.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.240.847 I ggml_metal_init: has bfloat            = true
0.00.240.847 I ggml_metal_init: use bfloat            = true
0.00.240.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.240.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.283.856 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.283.861 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.283.878 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.284.920 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.284.922 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.284.922 I llama_new_context_with_model: graph nodes  = 967
0.00.284.922 I llama_new_context_with_model: graph splits = 2
0.00.284.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.483.707 I main: llama threadpool init, n_threads = 4
0.00.483.757 I 
0.00.483.778 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.483.780 I 
0.00.483.938 I sampler seed: 1234
0.00.483.943 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.483.965 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.483.967 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.483.967 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling depressed, and I've had a hard time getting out of bed. I think the only thing I

0.02.342.744 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60425.53 tokens per second)
0.02.342.745 I llama_perf_context_print:        load time =     376.92 ms
0.02.342.745 I llama_perf_context_print: prompt eval time =      38.14 ms /     7 tokens (    5.45 ms per token,   183.53 tokens per second)
0.02.342.746 I llama_perf_context_print:        eval time =    1817.82 ms /    63 runs   (   28.85 ms per token,    34.66 tokens per second)
0.02.342.747 I llama_perf_context_print:       total time =    1859.04 ms /    70 tokens
0.02.342.916 I ggml_metal_free: deallocating

real	0m2.671s
user	0m0.157s
sys	0m0.156s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.077 I main: load the model and apply lora adapter, if any
0.00.009.442 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.242 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.029.249 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.251 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.251 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.252 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.257 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.257 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.259 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.259 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.259 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.260 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.260 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.260 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.261 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.263 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.264 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.264 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.033.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.596 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.038.837 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.038.839 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.038.839 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.038.839 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.038.840 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.038.840 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.038.841 I llama_model_loader: - type  f32:  194 tensors
0.00.038.841 I llama_model_loader: - type q8_0:   98 tensors
0.00.066.065 I llm_load_vocab: special tokens cache size = 25
0.00.073.927 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.073.930 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.073.931 I llm_load_print_meta: arch             = gptneox
0.00.073.931 I llm_load_print_meta: vocab type       = BPE
0.00.073.931 I llm_load_print_meta: n_vocab          = 50304
0.00.073.931 I llm_load_print_meta: n_merges         = 50009
0.00.073.932 I llm_load_print_meta: vocab_only       = 0
0.00.073.932 I llm_load_print_meta: n_ctx_train      = 2048
0.00.073.932 I llm_load_print_meta: n_embd           = 2048
0.00.073.934 I llm_load_print_meta: n_layer          = 24
0.00.073.939 I llm_load_print_meta: n_head           = 16
0.00.073.939 I llm_load_print_meta: n_head_kv        = 16
0.00.073.940 I llm_load_print_meta: n_rot            = 32
0.00.073.940 I llm_load_print_meta: n_swa            = 0
0.00.073.940 I llm_load_print_meta: n_embd_head_k    = 128
0.00.073.940 I llm_load_print_meta: n_embd_head_v    = 128
0.00.073.941 I llm_load_print_meta: n_gqa            = 1
0.00.073.942 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.073.948 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.073.949 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.073.949 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.073.950 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.073.950 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.073.950 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.073.951 I llm_load_print_meta: n_ff             = 8192
0.00.073.951 I llm_load_print_meta: n_expert         = 0
0.00.073.951 I llm_load_print_meta: n_expert_used    = 0
0.00.073.951 I llm_load_print_meta: causal attn      = 1
0.00.073.951 I llm_load_print_meta: pooling type     = 0
0.00.073.951 I llm_load_print_meta: rope type        = 2
0.00.073.952 I llm_load_print_meta: rope scaling     = linear
0.00.073.952 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.073.953 I llm_load_print_meta: freq_scale_train = 1
0.00.073.953 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.073.953 I llm_load_print_meta: rope_finetuned   = unknown
0.00.073.953 I llm_load_print_meta: ssm_d_conv       = 0
0.00.073.953 I llm_load_print_meta: ssm_d_inner      = 0
0.00.073.953 I llm_load_print_meta: ssm_d_state      = 0
0.00.073.957 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.073.957 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.073.971 I llm_load_print_meta: model type       = 1.4B
0.00.073.973 I llm_load_print_meta: model ftype      = Q8_0
0.00.073.973 I llm_load_print_meta: model params     = 1.41 B
0.00.073.974 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.073.974 I llm_load_print_meta: general.name     = 1.4B
0.00.073.974 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.073.974 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.073.975 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.073.975 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.073.975 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.073.975 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.073.976 I llm_load_print_meta: max token length = 1024
0.00.076.676 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.076.676 I llm_load_tensors: offloading output layer to GPU
0.00.076.676 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.076.688 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.076.690 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.077.969 I llama_new_context_with_model: n_seq_max     = 1
0.00.077.969 I llama_new_context_with_model: n_ctx         = 2048
0.00.077.970 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.077.970 I llama_new_context_with_model: n_batch       = 2048
0.00.077.970 I llama_new_context_with_model: n_ubatch      = 512
0.00.077.970 I llama_new_context_with_model: flash_attn    = 0
0.00.077.971 I llama_new_context_with_model: freq_base     = 10000.0
0.00.077.971 I llama_new_context_with_model: freq_scale    = 1
0.00.077.972 I ggml_metal_init: allocating
0.00.077.975 I ggml_metal_init: found device: Apple M4
0.00.077.978 I ggml_metal_init: picking default device: Apple M4
0.00.078.785 I ggml_metal_init: using embedded metal library
0.00.082.019 I ggml_metal_init: GPU name:   Apple M4
0.00.082.022 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.082.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.082.023 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.082.023 I ggml_metal_init: simdgroup reduction   = true
0.00.082.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.082.024 I ggml_metal_init: has bfloat            = true
0.00.082.024 I ggml_metal_init: use bfloat            = true
0.00.082.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.082.026 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.119.533 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.119.540 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.119.564 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.120.655 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.120.656 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.120.656 I llama_new_context_with_model: graph nodes  = 967
0.00.120.656 I llama_new_context_with_model: graph splits = 2
0.00.120.665 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.994.792 I main: llama threadpool init, n_threads = 4
0.01.994.837 I 
0.01.994.855 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.994.856 I 
0.01.995.074 I sampler seed: 1234
0.01.995.077 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.995.099 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.995.101 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.995.101 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.03.087.653 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61206.90 tokens per second)
0.03.087.654 I llama_perf_context_print:        load time =    1985.35 ms
0.03.087.654 I llama_perf_context_print: prompt eval time =      33.75 ms /     7 tokens (    4.82 ms per token,   207.42 tokens per second)
0.03.087.655 I llama_perf_context_print:        eval time =    1056.00 ms /    63 runs   (   16.76 ms per token,    59.66 tokens per second)
0.03.087.655 I llama_perf_context_print:       total time =    1092.86 ms /    70 tokens
0.03.087.830 I ggml_metal_free: deallocating

real	0m3.109s
user	0m0.128s
sys	0m0.307s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.071 I main: llama backend init
0.00.000.073 I main: load the model and apply lora adapter, if any
0.00.012.068 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.021.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.816 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.817 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.817 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.818 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.824 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.825 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.762 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.887 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.888 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.888 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.889 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.889 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.889 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.030.890 I llama_model_loader: - type  f32:  194 tensors
0.00.030.890 I llama_model_loader: - type q4_0:   97 tensors
0.00.030.891 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.780 I llm_load_vocab: special tokens cache size = 25
0.00.058.783 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.786 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.786 I llm_load_print_meta: arch             = gptneox
0.00.058.787 I llm_load_print_meta: vocab type       = BPE
0.00.058.787 I llm_load_print_meta: n_vocab          = 50304
0.00.058.787 I llm_load_print_meta: n_merges         = 50009
0.00.058.787 I llm_load_print_meta: vocab_only       = 0
0.00.058.787 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.788 I llm_load_print_meta: n_embd           = 2048
0.00.058.788 I llm_load_print_meta: n_layer          = 24
0.00.058.791 I llm_load_print_meta: n_head           = 16
0.00.058.792 I llm_load_print_meta: n_head_kv        = 16
0.00.058.792 I llm_load_print_meta: n_rot            = 32
0.00.058.795 I llm_load_print_meta: n_swa            = 0
0.00.058.795 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.795 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.796 I llm_load_print_meta: n_gqa            = 1
0.00.058.797 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.798 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.798 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.799 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.799 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.799 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.799 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.800 I llm_load_print_meta: n_ff             = 8192
0.00.058.800 I llm_load_print_meta: n_expert         = 0
0.00.058.801 I llm_load_print_meta: n_expert_used    = 0
0.00.058.801 I llm_load_print_meta: causal attn      = 1
0.00.058.801 I llm_load_print_meta: pooling type     = 0
0.00.058.801 I llm_load_print_meta: rope type        = 2
0.00.058.802 I llm_load_print_meta: rope scaling     = linear
0.00.058.802 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.802 I llm_load_print_meta: freq_scale_train = 1
0.00.058.802 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.802 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.803 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.803 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.803 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.803 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.803 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.816 I llm_load_print_meta: model type       = 1.4B
0.00.058.817 I llm_load_print_meta: model ftype      = Q4_0
0.00.058.817 I llm_load_print_meta: model params     = 1.41 B
0.00.058.817 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.058.817 I llm_load_print_meta: general.name     = 1.4B
0.00.058.818 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.818 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.818 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.818 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.818 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.058.819 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.819 I llm_load_print_meta: max token length = 1024
0.00.061.103 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.104 I llm_load_tensors: offloading output layer to GPU
0.00.061.104 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.115 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.061.116 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.062.099 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.100 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.100 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.100 I llama_new_context_with_model: n_batch       = 2048
0.00.062.100 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.100 I llama_new_context_with_model: flash_attn    = 0
0.00.062.101 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.101 I llama_new_context_with_model: freq_scale    = 1
0.00.062.101 I ggml_metal_init: allocating
0.00.062.105 I ggml_metal_init: found device: Apple M4
0.00.062.107 I ggml_metal_init: picking default device: Apple M4
0.00.062.814 I ggml_metal_init: using embedded metal library
0.00.064.982 I ggml_metal_init: GPU name:   Apple M4
0.00.064.983 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.984 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.984 I ggml_metal_init: simdgroup reduction   = true
0.00.064.984 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.984 I ggml_metal_init: has bfloat            = true
0.00.064.985 I ggml_metal_init: use bfloat            = true
0.00.064.985 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.986 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.100.477 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.100.488 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.100.510 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.793 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.101.795 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.101.795 I llama_new_context_with_model: graph nodes  = 967
0.00.101.795 I llama_new_context_with_model: graph splits = 2
0.00.101.804 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.359.275 I main: llama threadpool init, n_threads = 4
0.01.359.315 I 
0.01.359.335 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.359.336 I 
0.01.359.551 I sampler seed: 1234
0.01.359.557 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.359.568 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.359.568 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.359.568 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.02.039.697 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60944.21 tokens per second)
0.02.039.697 I llama_perf_context_print:        load time =    1347.20 ms
0.02.039.698 I llama_perf_context_print: prompt eval time =      32.71 ms /     7 tokens (    4.67 ms per token,   213.98 tokens per second)
0.02.039.699 I llama_perf_context_print:        eval time =     644.43 ms /    63 runs   (   10.23 ms per token,    97.76 tokens per second)
0.02.039.699 I llama_perf_context_print:       total time =     680.42 ms /    70 tokens
0.02.039.881 I ggml_metal_free: deallocating

real	0m2.059s
user	0m0.111s
sys	0m0.202s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.550 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.156 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.024.160 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.167 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.169 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.172 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.100 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.148 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.107 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.108 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.109 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.109 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.109 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.109 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.033.110 I llama_model_loader: - type  f32:  194 tensors
0.00.033.110 I llama_model_loader: - type q4_1:   97 tensors
0.00.033.110 I llama_model_loader: - type q6_K:    1 tensors
0.00.055.240 I llm_load_vocab: special tokens cache size = 25
0.00.061.253 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.061.255 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.061.256 I llm_load_print_meta: arch             = gptneox
0.00.061.256 I llm_load_print_meta: vocab type       = BPE
0.00.061.256 I llm_load_print_meta: n_vocab          = 50304
0.00.061.256 I llm_load_print_meta: n_merges         = 50009
0.00.061.257 I llm_load_print_meta: vocab_only       = 0
0.00.061.257 I llm_load_print_meta: n_ctx_train      = 2048
0.00.061.257 I llm_load_print_meta: n_embd           = 2048
0.00.061.257 I llm_load_print_meta: n_layer          = 24
0.00.061.259 I llm_load_print_meta: n_head           = 16
0.00.061.260 I llm_load_print_meta: n_head_kv        = 16
0.00.061.260 I llm_load_print_meta: n_rot            = 32
0.00.061.261 I llm_load_print_meta: n_swa            = 0
0.00.061.261 I llm_load_print_meta: n_embd_head_k    = 128
0.00.061.261 I llm_load_print_meta: n_embd_head_v    = 128
0.00.061.262 I llm_load_print_meta: n_gqa            = 1
0.00.061.262 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.061.263 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.061.263 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.061.264 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.061.264 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.061.264 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.061.264 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.061.265 I llm_load_print_meta: n_ff             = 8192
0.00.061.265 I llm_load_print_meta: n_expert         = 0
0.00.061.265 I llm_load_print_meta: n_expert_used    = 0
0.00.061.266 I llm_load_print_meta: causal attn      = 1
0.00.061.266 I llm_load_print_meta: pooling type     = 0
0.00.061.266 I llm_load_print_meta: rope type        = 2
0.00.061.266 I llm_load_print_meta: rope scaling     = linear
0.00.061.267 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.061.267 I llm_load_print_meta: freq_scale_train = 1
0.00.061.267 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.061.267 I llm_load_print_meta: rope_finetuned   = unknown
0.00.061.268 I llm_load_print_meta: ssm_d_conv       = 0
0.00.061.268 I llm_load_print_meta: ssm_d_inner      = 0
0.00.061.268 I llm_load_print_meta: ssm_d_state      = 0
0.00.061.268 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.061.268 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.061.279 I llm_load_print_meta: model type       = 1.4B
0.00.061.279 I llm_load_print_meta: model ftype      = Q4_1
0.00.061.280 I llm_load_print_meta: model params     = 1.41 B
0.00.061.280 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.061.280 I llm_load_print_meta: general.name     = 1.4B
0.00.061.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.061.281 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.061.281 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.061.281 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.061.281 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.061.283 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.061.283 I llm_load_print_meta: max token length = 1024
0.00.062.930 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.930 I llm_load_tensors: offloading output layer to GPU
0.00.062.930 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.940 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.062.941 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.063.801 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.802 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.803 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.803 I llama_new_context_with_model: n_batch       = 2048
0.00.063.803 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.803 I llama_new_context_with_model: flash_attn    = 0
0.00.063.804 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.804 I llama_new_context_with_model: freq_scale    = 1
0.00.063.804 I ggml_metal_init: allocating
0.00.063.810 I ggml_metal_init: found device: Apple M4
0.00.063.812 I ggml_metal_init: picking default device: Apple M4
0.00.064.375 I ggml_metal_init: using embedded metal library
0.00.066.509 I ggml_metal_init: GPU name:   Apple M4
0.00.066.511 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.511 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.512 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.512 I ggml_metal_init: simdgroup reduction   = true
0.00.066.512 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.512 I ggml_metal_init: has bfloat            = true
0.00.066.512 I ggml_metal_init: use bfloat            = true
0.00.066.513 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.513 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.094.287 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.292 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.313 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.327 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.328 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.329 I llama_new_context_with_model: graph nodes  = 967
0.00.095.329 I llama_new_context_with_model: graph splits = 2
0.00.095.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.395.738 I main: llama threadpool init, n_threads = 4
0.01.395.778 I 
0.01.395.798 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.395.799 I 
0.01.395.936 I sampler seed: 1234
0.01.395.941 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.395.950 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.395.951 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.395.951 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.02.123.885 I llama_perf_sampler_print:    sampling time =       1.09 ms /    71 runs   (    0.02 ms per token, 65077.91 tokens per second)
0.02.123.886 I llama_perf_context_print:        load time =    1387.18 ms
0.02.123.887 I llama_perf_context_print: prompt eval time =      32.74 ms /     7 tokens (    4.68 ms per token,   213.83 tokens per second)
0.02.123.887 I llama_perf_context_print:        eval time =     692.37 ms /    63 runs   (   10.99 ms per token,    90.99 tokens per second)
0.02.123.888 I llama_perf_context_print:       total time =     728.15 ms /    70 tokens
0.02.124.051 I ggml_metal_free: deallocating

real	0m2.137s
user	0m0.110s
sys	0m0.182s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.033 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.063 I main: llama backend init
0.00.000.065 I main: load the model and apply lora adapter, if any
0.00.018.818 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.159 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.037.163 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.165 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.166 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.166 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.166 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.167 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.168 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.168 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.168 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.169 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.169 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.170 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.170 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.172 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.172 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.173 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.209 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.587 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.594 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.595 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.595 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.596 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.596 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.596 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.048.597 I llama_model_loader: - type  f32:  194 tensors
0.00.048.597 I llama_model_loader: - type q5_0:   97 tensors
0.00.048.598 I llama_model_loader: - type q6_K:    1 tensors
0.00.084.405 I llm_load_vocab: special tokens cache size = 25
0.00.094.715 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.718 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.719 I llm_load_print_meta: arch             = gptneox
0.00.094.719 I llm_load_print_meta: vocab type       = BPE
0.00.094.719 I llm_load_print_meta: n_vocab          = 50304
0.00.094.719 I llm_load_print_meta: n_merges         = 50009
0.00.094.720 I llm_load_print_meta: vocab_only       = 0
0.00.094.720 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.720 I llm_load_print_meta: n_embd           = 2048
0.00.094.720 I llm_load_print_meta: n_layer          = 24
0.00.094.723 I llm_load_print_meta: n_head           = 16
0.00.094.724 I llm_load_print_meta: n_head_kv        = 16
0.00.094.724 I llm_load_print_meta: n_rot            = 32
0.00.094.724 I llm_load_print_meta: n_swa            = 0
0.00.094.725 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.725 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.726 I llm_load_print_meta: n_gqa            = 1
0.00.094.727 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.728 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.728 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.729 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.732 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.732 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.732 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.733 I llm_load_print_meta: n_ff             = 8192
0.00.094.733 I llm_load_print_meta: n_expert         = 0
0.00.094.734 I llm_load_print_meta: n_expert_used    = 0
0.00.094.735 I llm_load_print_meta: causal attn      = 1
0.00.094.735 I llm_load_print_meta: pooling type     = 0
0.00.094.737 I llm_load_print_meta: rope type        = 2
0.00.094.737 I llm_load_print_meta: rope scaling     = linear
0.00.094.738 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.738 I llm_load_print_meta: freq_scale_train = 1
0.00.094.738 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.739 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.739 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.739 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.739 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.739 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.739 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.751 I llm_load_print_meta: model type       = 1.4B
0.00.094.751 I llm_load_print_meta: model ftype      = Q5_0
0.00.094.752 I llm_load_print_meta: model params     = 1.41 B
0.00.094.753 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.094.753 I llm_load_print_meta: general.name     = 1.4B
0.00.094.753 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.754 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.754 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.754 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.754 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.094.755 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.756 I llm_load_print_meta: max token length = 1024
0.00.097.051 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.051 I llm_load_tensors: offloading output layer to GPU
0.00.097.052 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.062 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.097.063 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.098.385 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.386 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.386 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.386 I llama_new_context_with_model: n_batch       = 2048
0.00.098.387 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.387 I llama_new_context_with_model: flash_attn    = 0
0.00.098.388 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.388 I llama_new_context_with_model: freq_scale    = 1
0.00.098.389 I ggml_metal_init: allocating
0.00.098.397 I ggml_metal_init: found device: Apple M4
0.00.098.400 I ggml_metal_init: picking default device: Apple M4
0.00.099.144 I ggml_metal_init: using embedded metal library
0.00.103.154 I ggml_metal_init: GPU name:   Apple M4
0.00.103.157 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.103.157 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.103.158 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.103.158 I ggml_metal_init: simdgroup reduction   = true
0.00.103.158 I ggml_metal_init: simdgroup matrix mul. = true
0.00.103.158 I ggml_metal_init: has bfloat            = true
0.00.103.158 I ggml_metal_init: use bfloat            = true
0.00.103.159 I ggml_metal_init: hasUnifiedMemory      = true
0.00.103.162 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.135.754 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.763 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.781 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.136.772 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.136.773 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.136.774 I llama_new_context_with_model: graph nodes  = 967
0.00.136.774 I llama_new_context_with_model: graph splits = 2
0.00.136.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.628.969 I main: llama threadpool init, n_threads = 4
0.01.629.021 I 
0.01.629.041 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.629.042 I 
0.01.629.254 I sampler seed: 1234
0.01.629.258 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.629.312 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.629.333 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.629.334 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.02.421.708 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55949.57 tokens per second)
0.02.421.708 I llama_perf_context_print:        load time =    1610.15 ms
0.02.421.709 I llama_perf_context_print: prompt eval time =      37.01 ms /     7 tokens (    5.29 ms per token,   189.16 tokens per second)
0.02.421.710 I llama_perf_context_print:        eval time =     752.48 ms /    63 runs   (   11.94 ms per token,    83.72 tokens per second)
0.02.421.710 I llama_perf_context_print:       total time =     792.74 ms /    70 tokens
0.02.421.886 I ggml_metal_free: deallocating

real	0m2.452s
user	0m0.141s
sys	0m0.234s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.691 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.353 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.357 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.359 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.359 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.364 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.364 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.364 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.365 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.365 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.366 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.366 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.366 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.367 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.367 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.370 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.370 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.370 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.262 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.091 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.092 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.092 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.092 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.093 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.093 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.093 I llama_model_loader: - type  f32:  194 tensors
0.00.025.094 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.094 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.526 I llm_load_vocab: special tokens cache size = 25
0.00.051.573 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.576 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.576 I llm_load_print_meta: arch             = gptneox
0.00.051.577 I llm_load_print_meta: vocab type       = BPE
0.00.051.577 I llm_load_print_meta: n_vocab          = 50304
0.00.051.577 I llm_load_print_meta: n_merges         = 50009
0.00.051.577 I llm_load_print_meta: vocab_only       = 0
0.00.051.578 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.578 I llm_load_print_meta: n_embd           = 2048
0.00.051.578 I llm_load_print_meta: n_layer          = 24
0.00.051.581 I llm_load_print_meta: n_head           = 16
0.00.051.581 I llm_load_print_meta: n_head_kv        = 16
0.00.051.582 I llm_load_print_meta: n_rot            = 32
0.00.051.582 I llm_load_print_meta: n_swa            = 0
0.00.051.582 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.582 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.584 I llm_load_print_meta: n_gqa            = 1
0.00.051.584 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.585 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.586 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.586 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.586 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.588 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.589 I llm_load_print_meta: n_ff             = 8192
0.00.051.589 I llm_load_print_meta: n_expert         = 0
0.00.051.589 I llm_load_print_meta: n_expert_used    = 0
0.00.051.589 I llm_load_print_meta: causal attn      = 1
0.00.051.589 I llm_load_print_meta: pooling type     = 0
0.00.051.589 I llm_load_print_meta: rope type        = 2
0.00.051.591 I llm_load_print_meta: rope scaling     = linear
0.00.051.591 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.592 I llm_load_print_meta: freq_scale_train = 1
0.00.051.592 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.592 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.592 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.592 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.593 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.593 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.593 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.604 I llm_load_print_meta: model type       = 1.4B
0.00.051.604 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.604 I llm_load_print_meta: model params     = 1.41 B
0.00.051.606 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.606 I llm_load_print_meta: general.name     = 1.4B
0.00.051.606 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.606 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.606 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.607 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.607 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.607 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.607 I llm_load_print_meta: max token length = 1024
0.00.053.194 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.194 I llm_load_tensors: offloading output layer to GPU
0.00.053.194 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.203 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.053.204 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.031 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.032 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.032 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.033 I llama_new_context_with_model: n_batch       = 2048
0.00.054.033 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.033 I llama_new_context_with_model: flash_attn    = 0
0.00.054.033 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.034 I llama_new_context_with_model: freq_scale    = 1
0.00.054.034 I ggml_metal_init: allocating
0.00.054.037 I ggml_metal_init: found device: Apple M4
0.00.054.039 I ggml_metal_init: picking default device: Apple M4
0.00.054.646 I ggml_metal_init: using embedded metal library
0.00.056.866 I ggml_metal_init: GPU name:   Apple M4
0.00.056.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.869 I ggml_metal_init: simdgroup reduction   = true
0.00.056.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.869 I ggml_metal_init: has bfloat            = true
0.00.056.869 I ggml_metal_init: use bfloat            = true
0.00.056.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.136 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.144 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.165 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.344 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.346 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.346 I llama_new_context_with_model: graph nodes  = 967
0.00.086.348 I llama_new_context_with_model: graph splits = 2
0.00.086.361 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.870.397 I main: llama threadpool init, n_threads = 4
0.00.870.434 I 
0.00.870.451 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.870.451 I 
0.00.870.582 I sampler seed: 1234
0.00.870.586 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.870.596 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.870.596 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.870.596 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, literature and art.

If one's life is not happy, one can have no other way but

0.01.709.831 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60374.15 tokens per second)
0.01.709.831 I llama_perf_context_print:        load time =     861.70 ms
0.01.709.833 I llama_perf_context_print: prompt eval time =      36.56 ms /     7 tokens (    5.22 ms per token,   191.44 tokens per second)
0.01.709.833 I llama_perf_context_print:        eval time =     799.68 ms /    63 runs   (   12.69 ms per token,    78.78 tokens per second)
0.01.709.834 I llama_perf_context_print:       total time =     839.44 ms /    70 tokens
0.01.710.000 I ggml_metal_free: deallocating

real	0m1.726s
user	0m0.108s
sys	0m0.190s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.069 I main: load the model and apply lora adapter, if any
0.00.009.797 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.415 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.420 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.425 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.426 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.426 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.426 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.427 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.428 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.430 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.431 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.431 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.431 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.433 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.433 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.434 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.222 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.957 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.959 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.959 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.959 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.960 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.960 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.960 I llama_model_loader: - type  f32:  194 tensors
0.00.025.961 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.961 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.961 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.390 I llm_load_vocab: special tokens cache size = 25
0.00.052.495 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.498 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.498 I llm_load_print_meta: arch             = gptneox
0.00.052.498 I llm_load_print_meta: vocab type       = BPE
0.00.052.499 I llm_load_print_meta: n_vocab          = 50304
0.00.052.499 I llm_load_print_meta: n_merges         = 50009
0.00.052.499 I llm_load_print_meta: vocab_only       = 0
0.00.052.499 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.499 I llm_load_print_meta: n_embd           = 2048
0.00.052.500 I llm_load_print_meta: n_layer          = 24
0.00.052.503 I llm_load_print_meta: n_head           = 16
0.00.052.503 I llm_load_print_meta: n_head_kv        = 16
0.00.052.504 I llm_load_print_meta: n_rot            = 32
0.00.052.504 I llm_load_print_meta: n_swa            = 0
0.00.052.504 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.506 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.507 I llm_load_print_meta: n_gqa            = 1
0.00.052.508 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.509 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.509 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.509 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.510 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.510 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.510 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.511 I llm_load_print_meta: n_ff             = 8192
0.00.052.511 I llm_load_print_meta: n_expert         = 0
0.00.052.511 I llm_load_print_meta: n_expert_used    = 0
0.00.052.511 I llm_load_print_meta: causal attn      = 1
0.00.052.511 I llm_load_print_meta: pooling type     = 0
0.00.052.511 I llm_load_print_meta: rope type        = 2
0.00.052.512 I llm_load_print_meta: rope scaling     = linear
0.00.052.512 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.512 I llm_load_print_meta: freq_scale_train = 1
0.00.052.513 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.513 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.513 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.513 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.513 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.513 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.514 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.525 I llm_load_print_meta: model type       = 1.4B
0.00.052.525 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.525 I llm_load_print_meta: model params     = 1.41 B
0.00.052.526 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.527 I llm_load_print_meta: general.name     = 1.4B
0.00.052.527 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.527 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.528 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.528 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.528 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.528 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.528 I llm_load_print_meta: max token length = 1024
0.00.054.086 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.086 I llm_load_tensors: offloading output layer to GPU
0.00.054.086 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.096 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.054.097 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.055.005 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.006 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.006 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.007 I llama_new_context_with_model: n_batch       = 2048
0.00.055.007 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.007 I llama_new_context_with_model: flash_attn    = 0
0.00.055.007 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.008 I llama_new_context_with_model: freq_scale    = 1
0.00.055.008 I ggml_metal_init: allocating
0.00.055.014 I ggml_metal_init: found device: Apple M4
0.00.055.016 I ggml_metal_init: picking default device: Apple M4
0.00.055.565 I ggml_metal_init: using embedded metal library
0.00.057.513 I ggml_metal_init: GPU name:   Apple M4
0.00.057.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.514 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.515 I ggml_metal_init: simdgroup reduction   = true
0.00.057.515 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.515 I ggml_metal_init: has bfloat            = true
0.00.057.515 I ggml_metal_init: use bfloat            = true
0.00.057.516 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.517 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.253 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.257 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.276 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.381 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.383 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.383 I llama_new_context_with_model: graph nodes  = 967
0.00.087.383 I llama_new_context_with_model: graph splits = 2
0.00.087.396 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.443 I main: llama threadpool init, n_threads = 4
0.00.552.478 I 
0.00.552.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.552.495 I 
0.00.552.629 I sampler seed: 1234
0.00.552.634 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.552.649 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.552.649 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.552.649 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.233.582 I llama_perf_sampler_print:    sampling time =       1.14 ms /    71 runs   (    0.02 ms per token, 62445.03 tokens per second)
0.01.233.582 I llama_perf_context_print:        load time =     542.64 ms
0.01.233.584 I llama_perf_context_print: prompt eval time =      35.77 ms /     7 tokens (    5.11 ms per token,   195.67 tokens per second)
0.01.233.585 I llama_perf_context_print:        eval time =     642.24 ms /    63 runs   (   10.19 ms per token,    98.09 tokens per second)
0.01.233.585 I llama_perf_context_print:       total time =     681.14 ms /    70 tokens
0.01.233.783 I ggml_metal_free: deallocating

real	0m1.249s
user	0m0.109s
sys	0m0.137s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.008.833 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.085 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.090 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.092 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.093 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.093 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.095 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.095 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.095 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.096 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.096 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.097 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.097 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.099 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.099 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.099 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.942 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.007 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.850 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.851 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.851 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.851 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.852 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.852 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.853 I llama_model_loader: - type  f32:  194 tensors
0.00.025.853 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.853 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.853 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.854 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.058 I llm_load_vocab: special tokens cache size = 25
0.00.053.226 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.229 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.229 I llm_load_print_meta: arch             = gptneox
0.00.053.229 I llm_load_print_meta: vocab type       = BPE
0.00.053.230 I llm_load_print_meta: n_vocab          = 50304
0.00.053.230 I llm_load_print_meta: n_merges         = 50009
0.00.053.230 I llm_load_print_meta: vocab_only       = 0
0.00.053.230 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.230 I llm_load_print_meta: n_embd           = 2048
0.00.053.230 I llm_load_print_meta: n_layer          = 24
0.00.053.233 I llm_load_print_meta: n_head           = 16
0.00.053.234 I llm_load_print_meta: n_head_kv        = 16
0.00.053.236 I llm_load_print_meta: n_rot            = 32
0.00.053.236 I llm_load_print_meta: n_swa            = 0
0.00.053.236 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.236 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.237 I llm_load_print_meta: n_gqa            = 1
0.00.053.238 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.239 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.239 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.239 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.240 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.240 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.240 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.240 I llm_load_print_meta: n_ff             = 8192
0.00.053.241 I llm_load_print_meta: n_expert         = 0
0.00.053.242 I llm_load_print_meta: n_expert_used    = 0
0.00.053.242 I llm_load_print_meta: causal attn      = 1
0.00.053.242 I llm_load_print_meta: pooling type     = 0
0.00.053.242 I llm_load_print_meta: rope type        = 2
0.00.053.242 I llm_load_print_meta: rope scaling     = linear
0.00.053.243 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.243 I llm_load_print_meta: freq_scale_train = 1
0.00.053.243 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.244 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.244 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.244 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.244 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.244 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.244 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.255 I llm_load_print_meta: model type       = 1.4B
0.00.053.256 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.053.256 I llm_load_print_meta: model params     = 1.41 B
0.00.053.257 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.053.257 I llm_load_print_meta: general.name     = 1.4B
0.00.053.257 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.257 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.257 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.257 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.258 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.053.258 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.258 I llm_load_print_meta: max token length = 1024
0.00.054.870 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.870 I llm_load_tensors: offloading output layer to GPU
0.00.054.870 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.880 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.881 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.747 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.748 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.748 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.748 I llama_new_context_with_model: n_batch       = 2048
0.00.055.749 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.749 I llama_new_context_with_model: flash_attn    = 0
0.00.055.749 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.750 I llama_new_context_with_model: freq_scale    = 1
0.00.055.750 I ggml_metal_init: allocating
0.00.055.756 I ggml_metal_init: found device: Apple M4
0.00.055.758 I ggml_metal_init: picking default device: Apple M4
0.00.056.296 I ggml_metal_init: using embedded metal library
0.00.058.254 I ggml_metal_init: GPU name:   Apple M4
0.00.058.257 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.258 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.258 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.259 I ggml_metal_init: simdgroup reduction   = true
0.00.058.259 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.260 I ggml_metal_init: has bfloat            = true
0.00.058.263 I ggml_metal_init: use bfloat            = true
0.00.058.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.264 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.086.188 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.201 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.223 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.160 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.161 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.162 I llama_new_context_with_model: graph nodes  = 967
0.00.087.162 I llama_new_context_with_model: graph splits = 2
0.00.087.174 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.688.678 I main: llama threadpool init, n_threads = 4
0.00.688.719 I 
0.00.688.736 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.688.736 I 
0.00.688.960 I sampler seed: 1234
0.00.688.963 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.688.974 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.688.974 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.688.974 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do that is most useful to my fellow men?'"

-Albert Einstein

"The way

0.01.432.749 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61471.86 tokens per second)
0.01.432.749 I llama_perf_context_print:        load time =     679.84 ms
0.01.432.750 I llama_perf_context_print: prompt eval time =      35.65 ms /     7 tokens (    5.09 ms per token,   196.36 tokens per second)
0.01.432.751 I llama_perf_context_print:        eval time =     705.18 ms /    63 runs   (   11.19 ms per token,    89.34 tokens per second)
0.01.432.751 I llama_perf_context_print:       total time =     744.07 ms /    70 tokens
0.01.432.921 I ggml_metal_free: deallocating

real	0m1.446s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.204 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.685 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.690 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.692 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.696 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.697 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.700 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.700 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.700 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.701 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.701 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.701 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.702 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.703 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.703 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.703 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.652 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.526 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.528 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.528 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.529 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.529 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.529 I llama_model_loader: - type  f32:  194 tensors
0.00.025.530 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.530 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.530 I llama_model_loader: - type q6_K:   13 tensors
0.00.046.231 I llm_load_vocab: special tokens cache size = 25
0.00.052.168 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.170 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.171 I llm_load_print_meta: arch             = gptneox
0.00.052.171 I llm_load_print_meta: vocab type       = BPE
0.00.052.171 I llm_load_print_meta: n_vocab          = 50304
0.00.052.171 I llm_load_print_meta: n_merges         = 50009
0.00.052.172 I llm_load_print_meta: vocab_only       = 0
0.00.052.172 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.172 I llm_load_print_meta: n_embd           = 2048
0.00.052.172 I llm_load_print_meta: n_layer          = 24
0.00.052.174 I llm_load_print_meta: n_head           = 16
0.00.052.175 I llm_load_print_meta: n_head_kv        = 16
0.00.052.177 I llm_load_print_meta: n_rot            = 32
0.00.052.177 I llm_load_print_meta: n_swa            = 0
0.00.052.178 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.178 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.179 I llm_load_print_meta: n_gqa            = 1
0.00.052.179 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.180 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.181 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.181 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.181 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.181 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.182 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.182 I llm_load_print_meta: n_ff             = 8192
0.00.052.182 I llm_load_print_meta: n_expert         = 0
0.00.052.183 I llm_load_print_meta: n_expert_used    = 0
0.00.052.183 I llm_load_print_meta: causal attn      = 1
0.00.052.183 I llm_load_print_meta: pooling type     = 0
0.00.052.183 I llm_load_print_meta: rope type        = 2
0.00.052.183 I llm_load_print_meta: rope scaling     = linear
0.00.052.184 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.184 I llm_load_print_meta: freq_scale_train = 1
0.00.052.184 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.184 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.185 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.185 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.185 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.185 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.185 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.199 I llm_load_print_meta: model type       = 1.4B
0.00.052.199 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.052.200 I llm_load_print_meta: model params     = 1.41 B
0.00.052.200 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.052.200 I llm_load_print_meta: general.name     = 1.4B
0.00.052.200 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.200 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.202 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.202 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.202 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.202 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.203 I llm_load_print_meta: max token length = 1024
0.00.054.191 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.191 I llm_load_tensors: offloading output layer to GPU
0.00.054.191 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.201 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.054.202 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.055.123 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.124 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.124 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.125 I llama_new_context_with_model: n_batch       = 2048
0.00.055.125 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.125 I llama_new_context_with_model: flash_attn    = 0
0.00.055.125 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.126 I llama_new_context_with_model: freq_scale    = 1
0.00.055.126 I ggml_metal_init: allocating
0.00.055.133 I ggml_metal_init: found device: Apple M4
0.00.055.135 I ggml_metal_init: picking default device: Apple M4
0.00.055.704 I ggml_metal_init: using embedded metal library
0.00.057.652 I ggml_metal_init: GPU name:   Apple M4
0.00.057.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.653 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.654 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.654 I ggml_metal_init: simdgroup reduction   = true
0.00.057.654 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.654 I ggml_metal_init: has bfloat            = true
0.00.057.655 I ggml_metal_init: use bfloat            = true
0.00.057.655 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.656 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.693 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.700 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.724 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.600 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.601 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.601 I llama_new_context_with_model: graph nodes  = 967
0.00.086.601 I llama_new_context_with_model: graph splits = 2
0.00.086.609 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.424 I main: llama threadpool init, n_threads = 4
0.00.663.460 I 
0.00.663.478 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.663.478 I 
0.00.663.613 I sampler seed: 1234
0.00.663.617 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.627 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.628 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.628 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the greatest power to create the future we want to see. I believe that we are in a wonderful time when people from all over the world are coming together to create a better world. I believe that we are going

0.01.429.022 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55642.63 tokens per second)
0.01.429.022 I llama_perf_context_print:        load time =     653.22 ms
0.01.429.023 I llama_perf_context_print: prompt eval time =      36.48 ms /     7 tokens (    5.21 ms per token,   191.89 tokens per second)
0.01.429.024 I llama_perf_context_print:        eval time =     725.85 ms /    63 runs   (   11.52 ms per token,    86.79 tokens per second)
0.01.429.024 I llama_perf_context_print:       total time =     765.60 ms /    70 tokens
0.01.429.208 I ggml_metal_free: deallocating

real	0m1.445s
user	0m0.109s
sys	0m0.155s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.010.434 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.068 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.075 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.075 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.076 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.076 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.077 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.077 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.078 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.078 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.079 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.938 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.768 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.770 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.770 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.770 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.770 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.771 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.771 I llama_model_loader: - type  f32:  194 tensors
0.00.025.772 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.772 I llama_model_loader: - type q6_K:   37 tensors
0.00.046.272 I llm_load_vocab: special tokens cache size = 25
0.00.052.263 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.265 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.266 I llm_load_print_meta: arch             = gptneox
0.00.052.266 I llm_load_print_meta: vocab type       = BPE
0.00.052.266 I llm_load_print_meta: n_vocab          = 50304
0.00.052.266 I llm_load_print_meta: n_merges         = 50009
0.00.052.267 I llm_load_print_meta: vocab_only       = 0
0.00.052.267 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.267 I llm_load_print_meta: n_embd           = 2048
0.00.052.267 I llm_load_print_meta: n_layer          = 24
0.00.052.269 I llm_load_print_meta: n_head           = 16
0.00.052.270 I llm_load_print_meta: n_head_kv        = 16
0.00.052.270 I llm_load_print_meta: n_rot            = 32
0.00.052.270 I llm_load_print_meta: n_swa            = 0
0.00.052.270 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.271 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.271 I llm_load_print_meta: n_gqa            = 1
0.00.052.272 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.273 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.273 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.274 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.274 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.274 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.274 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.275 I llm_load_print_meta: n_ff             = 8192
0.00.052.275 I llm_load_print_meta: n_expert         = 0
0.00.052.275 I llm_load_print_meta: n_expert_used    = 0
0.00.052.275 I llm_load_print_meta: causal attn      = 1
0.00.052.276 I llm_load_print_meta: pooling type     = 0
0.00.052.276 I llm_load_print_meta: rope type        = 2
0.00.052.276 I llm_load_print_meta: rope scaling     = linear
0.00.052.276 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.277 I llm_load_print_meta: freq_scale_train = 1
0.00.052.277 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.279 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.280 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.280 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.280 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.280 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.286 I llm_load_print_meta: model type       = 1.4B
0.00.052.286 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.287 I llm_load_print_meta: model params     = 1.41 B
0.00.052.288 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.288 I llm_load_print_meta: general.name     = 1.4B
0.00.052.289 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.289 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.289 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.290 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.290 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.291 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.291 I llm_load_print_meta: max token length = 1024
0.00.053.866 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.867 I llm_load_tensors: offloading output layer to GPU
0.00.053.867 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.871 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.872 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.712 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.713 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.713 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.713 I llama_new_context_with_model: n_batch       = 2048
0.00.054.713 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.714 I llama_new_context_with_model: flash_attn    = 0
0.00.054.714 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.715 I llama_new_context_with_model: freq_scale    = 1
0.00.054.715 I ggml_metal_init: allocating
0.00.054.721 I ggml_metal_init: found device: Apple M4
0.00.054.724 I ggml_metal_init: picking default device: Apple M4
0.00.055.269 I ggml_metal_init: using embedded metal library
0.00.057.204 I ggml_metal_init: GPU name:   Apple M4
0.00.057.206 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.206 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.206 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.207 I ggml_metal_init: simdgroup reduction   = true
0.00.057.207 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.207 I ggml_metal_init: has bfloat            = true
0.00.057.207 I ggml_metal_init: use bfloat            = true
0.00.057.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.374 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.378 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.404 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.308 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.310 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.310 I llama_new_context_with_model: graph nodes  = 967
0.00.086.310 I llama_new_context_with_model: graph splits = 2
0.00.086.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.921.074 I main: llama threadpool init, n_threads = 4
0.00.921.132 I 
0.00.921.159 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.921.162 I 
0.00.921.394 I sampler seed: 1234
0.00.921.399 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.921.424 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.921.426 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.921.426 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.762.533 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54996.13 tokens per second)
0.01.762.534 I llama_perf_context_print:        load time =     910.63 ms
0.01.762.535 I llama_perf_context_print: prompt eval time =      39.16 ms /     7 tokens (    5.59 ms per token,   178.77 tokens per second)
0.01.762.536 I llama_perf_context_print:        eval time =     798.93 ms /    63 runs   (   12.68 ms per token,    78.86 tokens per second)
0.01.762.536 I llama_perf_context_print:       total time =     841.46 ms /    70 tokens
0.01.762.711 I ggml_metal_free: deallocating

real	0m1.778s
user	0m0.115s
sys	0m0.218s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.009.566 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.743 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.748 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.750 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.751 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.751 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.753 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.754 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.756 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.756 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.756 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.751 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.621 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.622 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.622 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.623 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.623 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.623 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.624 I llama_model_loader: - type  f32:  194 tensors
0.00.025.624 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.187 I llm_load_vocab: special tokens cache size = 25
0.00.051.999 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.002 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.002 I llm_load_print_meta: arch             = gptneox
0.00.052.003 I llm_load_print_meta: vocab type       = BPE
0.00.052.003 I llm_load_print_meta: n_vocab          = 50304
0.00.052.003 I llm_load_print_meta: n_merges         = 50009
0.00.052.003 I llm_load_print_meta: vocab_only       = 0
0.00.052.003 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.003 I llm_load_print_meta: n_embd           = 2048
0.00.052.004 I llm_load_print_meta: n_layer          = 24
0.00.052.006 I llm_load_print_meta: n_head           = 16
0.00.052.007 I llm_load_print_meta: n_head_kv        = 16
0.00.052.007 I llm_load_print_meta: n_rot            = 32
0.00.052.007 I llm_load_print_meta: n_swa            = 0
0.00.052.008 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.008 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.008 I llm_load_print_meta: n_gqa            = 1
0.00.052.009 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.010 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.010 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.011 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.011 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.013 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.014 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.014 I llm_load_print_meta: n_ff             = 8192
0.00.052.014 I llm_load_print_meta: n_expert         = 0
0.00.052.015 I llm_load_print_meta: n_expert_used    = 0
0.00.052.015 I llm_load_print_meta: causal attn      = 1
0.00.052.015 I llm_load_print_meta: pooling type     = 0
0.00.052.015 I llm_load_print_meta: rope type        = 2
0.00.052.015 I llm_load_print_meta: rope scaling     = linear
0.00.052.016 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.016 I llm_load_print_meta: freq_scale_train = 1
0.00.052.016 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.017 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.017 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.017 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.017 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.028 I llm_load_print_meta: model type       = 1.4B
0.00.052.028 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.029 I llm_load_print_meta: model params     = 1.41 B
0.00.052.029 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.029 I llm_load_print_meta: general.name     = 1.4B
0.00.052.030 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.030 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.030 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.030 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.030 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.030 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.031 I llm_load_print_meta: max token length = 1024
0.00.053.646 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.646 I llm_load_tensors: offloading output layer to GPU
0.00.053.646 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.656 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.657 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.564 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.565 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.565 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.565 I llama_new_context_with_model: n_batch       = 2048
0.00.054.565 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.565 I llama_new_context_with_model: flash_attn    = 0
0.00.054.566 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.566 I llama_new_context_with_model: freq_scale    = 1
0.00.054.566 I ggml_metal_init: allocating
0.00.054.573 I ggml_metal_init: found device: Apple M4
0.00.054.575 I ggml_metal_init: picking default device: Apple M4
0.00.055.130 I ggml_metal_init: using embedded metal library
0.00.057.340 I ggml_metal_init: GPU name:   Apple M4
0.00.057.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.342 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.342 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.342 I ggml_metal_init: simdgroup reduction   = true
0.00.057.342 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.343 I ggml_metal_init: has bfloat            = true
0.00.057.343 I ggml_metal_init: use bfloat            = true
0.00.057.343 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.344 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.097.332 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.337 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.355 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.412 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.414 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.414 I llama_new_context_with_model: graph nodes  = 967
0.00.098.414 I llama_new_context_with_model: graph splits = 2
0.00.098.427 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.836.303 I main: llama threadpool init, n_threads = 4
0.00.836.339 I 
0.00.836.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.836.355 I 
0.00.836.549 I sampler seed: 1234
0.00.836.553 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.836.563 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.836.564 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.836.564 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.708.554 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59563.76 tokens per second)
0.01.708.555 I llama_perf_context_print:        load time =     826.73 ms
0.01.708.557 I llama_perf_context_print: prompt eval time =      38.49 ms /     7 tokens (    5.50 ms per token,   181.84 tokens per second)
0.01.708.557 I llama_perf_context_print:        eval time =     830.52 ms /    63 runs   (   13.18 ms per token,    75.86 tokens per second)
0.01.708.558 I llama_perf_context_print:       total time =     872.25 ms /    70 tokens
0.01.708.725 I ggml_metal_free: deallocating

real	0m1.723s
user	0m0.108s
sys	0m0.188s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.733 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.779 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.687 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.701 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.703 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.706 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.706 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.708 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.710 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.710 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.711 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.712 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.722 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.723 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.723 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.296 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.636 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.807 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.807 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.809 I llama_model_loader: - type  f32:  194 tensors
0.00.055.809 I llama_model_loader: - type  f16:   98 tensors
0.00.087.020 I llm_load_vocab: special tokens cache size = 25
0.00.093.999 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.002 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.002 I llm_load_print_meta: arch             = gptneox
0.00.094.003 I llm_load_print_meta: vocab type       = BPE
0.00.094.003 I llm_load_print_meta: n_vocab          = 50304
0.00.094.003 I llm_load_print_meta: n_merges         = 50009
0.00.094.003 I llm_load_print_meta: vocab_only       = 0
0.00.094.003 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.003 I llm_load_print_meta: n_embd           = 2048
0.00.094.004 I llm_load_print_meta: n_layer          = 24
0.00.094.006 I llm_load_print_meta: n_head           = 16
0.00.094.007 I llm_load_print_meta: n_head_kv        = 16
0.00.094.007 I llm_load_print_meta: n_rot            = 32
0.00.094.008 I llm_load_print_meta: n_swa            = 0
0.00.094.008 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.010 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.010 I llm_load_print_meta: n_gqa            = 1
0.00.094.011 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.012 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.012 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.013 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.013 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.013 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.013 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.014 I llm_load_print_meta: n_ff             = 8192
0.00.094.014 I llm_load_print_meta: n_expert         = 0
0.00.094.014 I llm_load_print_meta: n_expert_used    = 0
0.00.094.014 I llm_load_print_meta: causal attn      = 1
0.00.094.015 I llm_load_print_meta: pooling type     = 0
0.00.094.015 I llm_load_print_meta: rope type        = 2
0.00.094.015 I llm_load_print_meta: rope scaling     = linear
0.00.094.015 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.016 I llm_load_print_meta: freq_scale_train = 1
0.00.094.016 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.016 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.016 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.017 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.017 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.017 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.017 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.023 I llm_load_print_meta: model type       = 1.4B
0.00.094.024 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.094.024 I llm_load_print_meta: model params     = 1.41 B
0.00.094.025 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.094.025 I llm_load_print_meta: general.name     = 1.4B
0.00.094.025 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.026 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.026 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.026 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.026 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.094.026 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.028 I llm_load_print_meta: max token length = 1024
0.00.096.146 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.146 I llm_load_tensors: offloading output layer to GPU
0.00.096.147 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.152 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.096.153 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.097.128 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.129 I llama_new_context_with_model: n_ctx         = 128
0.00.097.129 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.097.129 I llama_new_context_with_model: n_batch       = 128
0.00.097.129 I llama_new_context_with_model: n_ubatch      = 128
0.00.097.129 I llama_new_context_with_model: flash_attn    = 0
0.00.097.130 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.130 I llama_new_context_with_model: freq_scale    = 1
0.00.097.130 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.097.131 I ggml_metal_init: allocating
0.00.097.138 I ggml_metal_init: found device: Apple M4
0.00.097.140 I ggml_metal_init: picking default device: Apple M4
0.00.097.767 I ggml_metal_init: using embedded metal library
0.00.099.889 I ggml_metal_init: GPU name:   Apple M4
0.00.099.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.099.891 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.099.891 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.099.892 I ggml_metal_init: simdgroup reduction   = true
0.00.099.892 I ggml_metal_init: simdgroup matrix mul. = true
0.00.099.892 I ggml_metal_init: has bfloat            = true
0.00.099.892 I ggml_metal_init: use bfloat            = true
0.00.099.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.099.894 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.307 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.109.310 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.109.323 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.110.180 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.110.181 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.110.181 I llama_new_context_with_model: graph nodes  = 967
0.00.110.181 I llama_new_context_with_model: graph splits = 2
0.00.110.188 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.387.146 I 
0.01.387.198 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.01.387.261 I perplexity: tokenizing the input ..
0.01.402.319 I perplexity: tokenization took 15.054 ms
0.01.402.325 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.524.150 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.525.996 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.526.032 I llama_perf_context_print:        load time =    1362.35 ms
0.01.526.033 I llama_perf_context_print: prompt eval time =     120.87 ms /   128 tokens (    0.94 ms per token,  1059.00 tokens per second)
0.01.526.034 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.526.037 I llama_perf_context_print:       total time =     138.89 ms /   129 tokens
0.01.526.577 I ggml_metal_free: deallocating

real	0m1.710s
user	0m0.126s
sys	0m0.249s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.279 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.118 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.033 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.047 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.047 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.048 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.048 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.049 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.049 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.050 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.050 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.051 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.051 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.052 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.054 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.055 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.055 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.956 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.262 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.263 I llama_model_loader: - type  f32:  194 tensors
0.00.034.263 I llama_model_loader: - type q8_0:   98 tensors
0.00.060.664 I llm_load_vocab: special tokens cache size = 25
0.00.067.541 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.543 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.544 I llm_load_print_meta: arch             = gptneox
0.00.067.544 I llm_load_print_meta: vocab type       = BPE
0.00.067.544 I llm_load_print_meta: n_vocab          = 50304
0.00.067.544 I llm_load_print_meta: n_merges         = 50009
0.00.067.544 I llm_load_print_meta: vocab_only       = 0
0.00.067.545 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.545 I llm_load_print_meta: n_embd           = 2048
0.00.067.545 I llm_load_print_meta: n_layer          = 24
0.00.067.548 I llm_load_print_meta: n_head           = 16
0.00.067.548 I llm_load_print_meta: n_head_kv        = 16
0.00.067.549 I llm_load_print_meta: n_rot            = 32
0.00.067.549 I llm_load_print_meta: n_swa            = 0
0.00.067.549 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.549 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.550 I llm_load_print_meta: n_gqa            = 1
0.00.067.551 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.551 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.552 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.552 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.552 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.552 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.552 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.553 I llm_load_print_meta: n_ff             = 8192
0.00.067.553 I llm_load_print_meta: n_expert         = 0
0.00.067.553 I llm_load_print_meta: n_expert_used    = 0
0.00.067.554 I llm_load_print_meta: causal attn      = 1
0.00.067.554 I llm_load_print_meta: pooling type     = 0
0.00.067.554 I llm_load_print_meta: rope type        = 2
0.00.067.554 I llm_load_print_meta: rope scaling     = linear
0.00.067.555 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.555 I llm_load_print_meta: freq_scale_train = 1
0.00.067.555 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.556 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.556 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.556 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.557 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.557 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.557 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.568 I llm_load_print_meta: model type       = 1.4B
0.00.067.568 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.569 I llm_load_print_meta: model params     = 1.41 B
0.00.067.569 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.569 I llm_load_print_meta: general.name     = 1.4B
0.00.067.569 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.570 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.570 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.570 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.570 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.067.572 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.572 I llm_load_print_meta: max token length = 1024
0.00.069.228 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.069.228 I llm_load_tensors: offloading output layer to GPU
0.00.069.228 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.069.238 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.069.239 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.149 I llama_new_context_with_model: n_ctx         = 128
0.00.070.149 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.070.149 I llama_new_context_with_model: n_batch       = 128
0.00.070.150 I llama_new_context_with_model: n_ubatch      = 128
0.00.070.150 I llama_new_context_with_model: flash_attn    = 0
0.00.070.150 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.150 I llama_new_context_with_model: freq_scale    = 1
0.00.070.151 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.070.151 I ggml_metal_init: allocating
0.00.070.154 I ggml_metal_init: found device: Apple M4
0.00.070.157 I ggml_metal_init: picking default device: Apple M4
0.00.070.739 I ggml_metal_init: using embedded metal library
0.00.072.788 I ggml_metal_init: GPU name:   Apple M4
0.00.072.790 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.072.790 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.072.790 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.072.790 I ggml_metal_init: simdgroup reduction   = true
0.00.072.791 I ggml_metal_init: simdgroup matrix mul. = true
0.00.072.791 I ggml_metal_init: has bfloat            = true
0.00.072.791 I ggml_metal_init: use bfloat            = true
0.00.072.791 I ggml_metal_init: hasUnifiedMemory      = true
0.00.072.792 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.083.186 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.189 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.203 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.192 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.193 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.194 I llama_new_context_with_model: graph nodes  = 967
0.00.084.194 I llama_new_context_with_model: graph splits = 2
0.00.084.206 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.996.032 I 
0.00.996.048 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.996.059 I perplexity: tokenizing the input ..
0.01.003.097 I perplexity: tokenization took 7.038 ms
0.01.003.101 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.124.459 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.125.658 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.125.676 I llama_perf_context_print:        load time =     983.91 ms
0.01.125.677 I llama_perf_context_print: prompt eval time =     121.13 ms /   128 tokens (    0.95 ms per token,  1056.72 tokens per second)
0.01.125.678 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.125.679 I llama_perf_context_print:       total time =     129.65 ms /   129 tokens
0.01.126.068 I ggml_metal_free: deallocating

real	0m1.143s
user	0m0.094s
sys	0m0.160s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.246 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.532 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.016.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.413 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.413 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.414 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.415 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.415 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.415 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.416 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.416 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.417 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.418 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.418 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.419 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.153 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.218 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.043 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.044 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.044 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.045 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.045 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.045 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.046 I llama_model_loader: - type  f32:  194 tensors
0.00.025.046 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.046 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.523 I llm_load_vocab: special tokens cache size = 25
0.00.051.460 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.462 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.463 I llm_load_print_meta: arch             = gptneox
0.00.051.463 I llm_load_print_meta: vocab type       = BPE
0.00.051.463 I llm_load_print_meta: n_vocab          = 50304
0.00.051.463 I llm_load_print_meta: n_merges         = 50009
0.00.051.463 I llm_load_print_meta: vocab_only       = 0
0.00.051.464 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.464 I llm_load_print_meta: n_embd           = 2048
0.00.051.464 I llm_load_print_meta: n_layer          = 24
0.00.051.467 I llm_load_print_meta: n_head           = 16
0.00.051.468 I llm_load_print_meta: n_head_kv        = 16
0.00.051.468 I llm_load_print_meta: n_rot            = 32
0.00.051.468 I llm_load_print_meta: n_swa            = 0
0.00.051.468 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.468 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.469 I llm_load_print_meta: n_gqa            = 1
0.00.051.470 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.470 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.477 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.477 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.477 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.477 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.477 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.478 I llm_load_print_meta: n_ff             = 8192
0.00.051.478 I llm_load_print_meta: n_expert         = 0
0.00.051.478 I llm_load_print_meta: n_expert_used    = 0
0.00.051.480 I llm_load_print_meta: causal attn      = 1
0.00.051.480 I llm_load_print_meta: pooling type     = 0
0.00.051.480 I llm_load_print_meta: rope type        = 2
0.00.051.481 I llm_load_print_meta: rope scaling     = linear
0.00.051.481 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.481 I llm_load_print_meta: freq_scale_train = 1
0.00.051.482 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.482 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.482 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.482 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.482 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.483 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.483 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.494 I llm_load_print_meta: model type       = 1.4B
0.00.051.494 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.494 I llm_load_print_meta: model params     = 1.41 B
0.00.051.495 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.495 I llm_load_print_meta: general.name     = 1.4B
0.00.051.495 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.495 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.496 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.496 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.496 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.496 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.496 I llm_load_print_meta: max token length = 1024
0.00.053.092 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.092 I llm_load_tensors: offloading output layer to GPU
0.00.053.093 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.102 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.103 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.919 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.920 I llama_new_context_with_model: n_ctx         = 128
0.00.053.921 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.921 I llama_new_context_with_model: n_batch       = 128
0.00.053.921 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.921 I llama_new_context_with_model: flash_attn    = 0
0.00.053.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.922 I llama_new_context_with_model: freq_scale    = 1
0.00.053.922 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.923 I ggml_metal_init: allocating
0.00.053.926 I ggml_metal_init: found device: Apple M4
0.00.053.928 I ggml_metal_init: picking default device: Apple M4
0.00.054.466 I ggml_metal_init: using embedded metal library
0.00.056.437 I ggml_metal_init: GPU name:   Apple M4
0.00.056.439 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.439 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.439 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.440 I ggml_metal_init: simdgroup reduction   = true
0.00.056.440 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.440 I ggml_metal_init: has bfloat            = true
0.00.056.440 I ggml_metal_init: use bfloat            = true
0.00.056.440 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.441 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.280 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.283 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.297 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.178 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.179 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.179 I llama_new_context_with_model: graph nodes  = 967
0.00.067.180 I llama_new_context_with_model: graph splits = 2
0.00.067.191 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.670.195 I 
0.00.670.212 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.670.225 I perplexity: tokenizing the input ..
0.00.677.276 I perplexity: tokenization took 7.05 ms
0.00.677.279 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.386 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.800.584 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.800.595 I llama_perf_context_print:        load time =     659.66 ms
0.00.800.596 I llama_perf_context_print: prompt eval time =     121.88 ms /   128 tokens (    0.95 ms per token,  1050.21 tokens per second)
0.00.800.597 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.800.597 I llama_perf_context_print:       total time =     130.40 ms /   129 tokens
0.00.801.018 I ggml_metal_free: deallocating

real	0m0.814s
user	0m0.076s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.350 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.270 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.274 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.279 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.280 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.280 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.281 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.281 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.282 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.282 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.282 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.283 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.283 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.283 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.284 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.285 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.285 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.286 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.071 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.104 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.921 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.922 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.922 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.922 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.922 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.923 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.022.923 I llama_model_loader: - type  f32:  194 tensors
0.00.022.923 I llama_model_loader: - type q4_1:   97 tensors
0.00.022.924 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.262 I llm_load_vocab: special tokens cache size = 25
0.00.049.293 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.296 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.296 I llm_load_print_meta: arch             = gptneox
0.00.049.297 I llm_load_print_meta: vocab type       = BPE
0.00.049.297 I llm_load_print_meta: n_vocab          = 50304
0.00.049.297 I llm_load_print_meta: n_merges         = 50009
0.00.049.297 I llm_load_print_meta: vocab_only       = 0
0.00.049.297 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.298 I llm_load_print_meta: n_embd           = 2048
0.00.049.298 I llm_load_print_meta: n_layer          = 24
0.00.049.300 I llm_load_print_meta: n_head           = 16
0.00.049.301 I llm_load_print_meta: n_head_kv        = 16
0.00.049.303 I llm_load_print_meta: n_rot            = 32
0.00.049.303 I llm_load_print_meta: n_swa            = 0
0.00.049.304 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.304 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.304 I llm_load_print_meta: n_gqa            = 1
0.00.049.305 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.306 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.307 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.307 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.307 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.307 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.307 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.308 I llm_load_print_meta: n_ff             = 8192
0.00.049.308 I llm_load_print_meta: n_expert         = 0
0.00.049.308 I llm_load_print_meta: n_expert_used    = 0
0.00.049.308 I llm_load_print_meta: causal attn      = 1
0.00.049.309 I llm_load_print_meta: pooling type     = 0
0.00.049.309 I llm_load_print_meta: rope type        = 2
0.00.049.309 I llm_load_print_meta: rope scaling     = linear
0.00.049.314 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.314 I llm_load_print_meta: freq_scale_train = 1
0.00.049.314 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.315 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.315 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.315 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.315 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.315 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.315 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.326 I llm_load_print_meta: model type       = 1.4B
0.00.049.327 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.327 I llm_load_print_meta: model params     = 1.41 B
0.00.049.328 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.328 I llm_load_print_meta: general.name     = 1.4B
0.00.049.328 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.328 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.328 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.328 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.329 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.329 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.329 I llm_load_print_meta: max token length = 1024
0.00.050.923 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.923 I llm_load_tensors: offloading output layer to GPU
0.00.050.923 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.050.933 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.050.934 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.051.749 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.750 I llama_new_context_with_model: n_ctx         = 128
0.00.051.750 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.750 I llama_new_context_with_model: n_batch       = 128
0.00.051.751 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.751 I llama_new_context_with_model: flash_attn    = 0
0.00.051.751 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.751 I llama_new_context_with_model: freq_scale    = 1
0.00.051.752 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.752 I ggml_metal_init: allocating
0.00.051.758 I ggml_metal_init: found device: Apple M4
0.00.051.760 I ggml_metal_init: picking default device: Apple M4
0.00.052.292 I ggml_metal_init: using embedded metal library
0.00.054.265 I ggml_metal_init: GPU name:   Apple M4
0.00.054.267 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.267 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.268 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.268 I ggml_metal_init: simdgroup reduction   = true
0.00.054.268 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.268 I ggml_metal_init: has bfloat            = true
0.00.054.268 I ggml_metal_init: use bfloat            = true
0.00.054.269 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.674 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.678 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.693 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.597 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.598 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.599 I llama_new_context_with_model: graph nodes  = 967
0.00.064.599 I llama_new_context_with_model: graph splits = 2
0.00.064.610 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.723.970 I 
0.00.723.990 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.724.012 I perplexity: tokenizing the input ..
0.00.731.338 I perplexity: tokenization took 7.324 ms
0.00.731.341 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.758 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.855.029 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.855.044 I llama_perf_context_print:        load time =     715.62 ms
0.00.855.045 I llama_perf_context_print: prompt eval time =     122.18 ms /   128 tokens (    0.95 ms per token,  1047.63 tokens per second)
0.00.855.046 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.855.047 I llama_perf_context_print:       total time =     131.07 ms /   129 tokens
0.00.855.419 I ggml_metal_free: deallocating

real	0m0.870s
user	0m0.076s
sys	0m0.125s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.638 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.698 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.704 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.705 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.706 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.708 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.709 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.709 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.710 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.712 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.713 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.713 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.486 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.485 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.323 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.324 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.325 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.326 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.326 I llama_model_loader: - type  f32:  194 tensors
0.00.024.326 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.327 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.859 I llm_load_vocab: special tokens cache size = 25
0.00.050.879 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.882 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.882 I llm_load_print_meta: arch             = gptneox
0.00.050.882 I llm_load_print_meta: vocab type       = BPE
0.00.050.883 I llm_load_print_meta: n_vocab          = 50304
0.00.050.883 I llm_load_print_meta: n_merges         = 50009
0.00.050.883 I llm_load_print_meta: vocab_only       = 0
0.00.050.883 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.883 I llm_load_print_meta: n_embd           = 2048
0.00.050.883 I llm_load_print_meta: n_layer          = 24
0.00.050.885 I llm_load_print_meta: n_head           = 16
0.00.050.886 I llm_load_print_meta: n_head_kv        = 16
0.00.050.886 I llm_load_print_meta: n_rot            = 32
0.00.050.887 I llm_load_print_meta: n_swa            = 0
0.00.050.887 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.887 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.888 I llm_load_print_meta: n_gqa            = 1
0.00.050.888 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.889 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.891 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.891 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.892 I llm_load_print_meta: n_ff             = 8192
0.00.050.892 I llm_load_print_meta: n_expert         = 0
0.00.050.893 I llm_load_print_meta: n_expert_used    = 0
0.00.050.893 I llm_load_print_meta: causal attn      = 1
0.00.050.893 I llm_load_print_meta: pooling type     = 0
0.00.050.893 I llm_load_print_meta: rope type        = 2
0.00.050.894 I llm_load_print_meta: rope scaling     = linear
0.00.050.895 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.897 I llm_load_print_meta: freq_scale_train = 1
0.00.050.897 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.897 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.897 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.897 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.898 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.898 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.898 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.904 I llm_load_print_meta: model type       = 1.4B
0.00.050.904 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.905 I llm_load_print_meta: model params     = 1.41 B
0.00.050.905 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.906 I llm_load_print_meta: general.name     = 1.4B
0.00.050.907 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.907 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.907 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.907 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.907 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.908 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.908 I llm_load_print_meta: max token length = 1024
0.00.052.453 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.453 I llm_load_tensors: offloading output layer to GPU
0.00.052.453 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.457 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.458 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.347 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.347 I llama_new_context_with_model: n_ctx         = 128
0.00.053.347 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.348 I llama_new_context_with_model: n_batch       = 128
0.00.053.348 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.348 I llama_new_context_with_model: flash_attn    = 0
0.00.053.349 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.349 I llama_new_context_with_model: freq_scale    = 1
0.00.053.349 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.350 I ggml_metal_init: allocating
0.00.053.353 I ggml_metal_init: found device: Apple M4
0.00.053.355 I ggml_metal_init: picking default device: Apple M4
0.00.053.879 I ggml_metal_init: using embedded metal library
0.00.055.829 I ggml_metal_init: GPU name:   Apple M4
0.00.055.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.832 I ggml_metal_init: simdgroup reduction   = true
0.00.055.832 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.832 I ggml_metal_init: has bfloat            = true
0.00.055.832 I ggml_metal_init: use bfloat            = true
0.00.055.833 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.835 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.118 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.121 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.136 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.008 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.009 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.010 I llama_new_context_with_model: graph nodes  = 967
0.00.066.010 I llama_new_context_with_model: graph splits = 2
0.00.066.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.747.787 I 
0.00.747.803 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.747.813 I perplexity: tokenizing the input ..
0.00.754.924 I perplexity: tokenization took 7.11 ms
0.00.754.929 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.889.407 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.890.620 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.890.634 I llama_perf_context_print:        load time =     738.15 ms
0.00.890.635 I llama_perf_context_print: prompt eval time =     134.25 ms /   128 tokens (    1.05 ms per token,   953.41 tokens per second)
0.00.890.640 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.890.640 I llama_perf_context_print:       total time =     142.85 ms /   129 tokens
0.00.890.990 I ggml_metal_free: deallocating

real	0m0.904s
user	0m0.076s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.362 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.451 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.455 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.461 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.461 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.462 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.462 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.463 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.464 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.464 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.464 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.465 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.465 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.465 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.467 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.467 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.467 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.299 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.326 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.214 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.215 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.216 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.216 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.216 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.217 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.217 I llama_model_loader: - type  f32:  194 tensors
0.00.023.217 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.218 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.530 I llm_load_vocab: special tokens cache size = 25
0.00.050.580 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.584 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.584 I llm_load_print_meta: arch             = gptneox
0.00.050.585 I llm_load_print_meta: vocab type       = BPE
0.00.050.585 I llm_load_print_meta: n_vocab          = 50304
0.00.050.585 I llm_load_print_meta: n_merges         = 50009
0.00.050.585 I llm_load_print_meta: vocab_only       = 0
0.00.050.586 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.586 I llm_load_print_meta: n_embd           = 2048
0.00.050.586 I llm_load_print_meta: n_layer          = 24
0.00.050.589 I llm_load_print_meta: n_head           = 16
0.00.050.589 I llm_load_print_meta: n_head_kv        = 16
0.00.050.590 I llm_load_print_meta: n_rot            = 32
0.00.050.590 I llm_load_print_meta: n_swa            = 0
0.00.050.590 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.590 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.591 I llm_load_print_meta: n_gqa            = 1
0.00.050.592 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.592 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.593 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.593 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.594 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.594 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.594 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.597 I llm_load_print_meta: n_ff             = 8192
0.00.050.597 I llm_load_print_meta: n_expert         = 0
0.00.050.597 I llm_load_print_meta: n_expert_used    = 0
0.00.050.597 I llm_load_print_meta: causal attn      = 1
0.00.050.597 I llm_load_print_meta: pooling type     = 0
0.00.050.598 I llm_load_print_meta: rope type        = 2
0.00.050.598 I llm_load_print_meta: rope scaling     = linear
0.00.050.598 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.598 I llm_load_print_meta: freq_scale_train = 1
0.00.050.599 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.599 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.599 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.599 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.599 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.599 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.600 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.610 I llm_load_print_meta: model type       = 1.4B
0.00.050.611 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.611 I llm_load_print_meta: model params     = 1.41 B
0.00.050.612 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.612 I llm_load_print_meta: general.name     = 1.4B
0.00.050.613 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.614 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.615 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.615 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.615 I llm_load_print_meta: max token length = 1024
0.00.052.243 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.244 I llm_load_tensors: offloading output layer to GPU
0.00.052.244 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.253 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.254 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.154 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.155 I llama_new_context_with_model: n_ctx         = 128
0.00.053.156 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.156 I llama_new_context_with_model: n_batch       = 128
0.00.053.156 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.156 I llama_new_context_with_model: flash_attn    = 0
0.00.053.157 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.157 I llama_new_context_with_model: freq_scale    = 1
0.00.053.157 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.158 I ggml_metal_init: allocating
0.00.053.161 I ggml_metal_init: found device: Apple M4
0.00.053.163 I ggml_metal_init: picking default device: Apple M4
0.00.053.703 I ggml_metal_init: using embedded metal library
0.00.055.622 I ggml_metal_init: GPU name:   Apple M4
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.624 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.624 I ggml_metal_init: simdgroup reduction   = true
0.00.055.625 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.625 I ggml_metal_init: has bfloat            = true
0.00.055.625 I ggml_metal_init: use bfloat            = true
0.00.055.625 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.627 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.082 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.085 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.099 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.996 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.997 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.997 I llama_new_context_with_model: graph nodes  = 967
0.00.065.998 I llama_new_context_with_model: graph splits = 2
0.00.066.010 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.634 I 
0.00.811.648 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.811.663 I perplexity: tokenizing the input ..
0.00.818.696 I perplexity: tokenization took 7.033 ms
0.00.818.699 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.952.993 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.954.235 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.954.245 I llama_perf_context_print:        load time =     803.27 ms
0.00.954.246 I llama_perf_context_print: prompt eval time =     134.07 ms /   128 tokens (    1.05 ms per token,   954.70 tokens per second)
0.00.954.247 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.954.247 I llama_perf_context_print:       total time =     142.61 ms /   129 tokens
0.00.954.578 I ggml_metal_free: deallocating

real	0m0.965s
user	0m0.077s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.062 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.082 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.087 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.093 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.093 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.094 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.094 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.094 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.096 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.096 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.096 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.098 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.099 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.099 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.099 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.101 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.101 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.101 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.883 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.896 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.677 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.678 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.678 I llama_model_loader: - type  f32:  194 tensors
0.00.024.679 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.679 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.679 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.336 I llm_load_vocab: special tokens cache size = 25
0.00.051.314 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.317 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.317 I llm_load_print_meta: arch             = gptneox
0.00.051.318 I llm_load_print_meta: vocab type       = BPE
0.00.051.318 I llm_load_print_meta: n_vocab          = 50304
0.00.051.318 I llm_load_print_meta: n_merges         = 50009
0.00.051.318 I llm_load_print_meta: vocab_only       = 0
0.00.051.319 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.319 I llm_load_print_meta: n_embd           = 2048
0.00.051.319 I llm_load_print_meta: n_layer          = 24
0.00.051.321 I llm_load_print_meta: n_head           = 16
0.00.051.322 I llm_load_print_meta: n_head_kv        = 16
0.00.051.324 I llm_load_print_meta: n_rot            = 32
0.00.051.324 I llm_load_print_meta: n_swa            = 0
0.00.051.324 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.324 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.325 I llm_load_print_meta: n_gqa            = 1
0.00.051.326 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.327 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.327 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.328 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.328 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.328 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.328 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.329 I llm_load_print_meta: n_ff             = 8192
0.00.051.329 I llm_load_print_meta: n_expert         = 0
0.00.051.329 I llm_load_print_meta: n_expert_used    = 0
0.00.051.329 I llm_load_print_meta: causal attn      = 1
0.00.051.329 I llm_load_print_meta: pooling type     = 0
0.00.051.329 I llm_load_print_meta: rope type        = 2
0.00.051.335 I llm_load_print_meta: rope scaling     = linear
0.00.051.336 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.337 I llm_load_print_meta: freq_scale_train = 1
0.00.051.337 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.337 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.337 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.338 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.338 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.338 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.338 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.350 I llm_load_print_meta: model type       = 1.4B
0.00.051.351 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.351 I llm_load_print_meta: model params     = 1.41 B
0.00.051.351 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.352 I llm_load_print_meta: general.name     = 1.4B
0.00.051.352 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.352 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.352 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.352 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.353 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.354 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.354 I llm_load_print_meta: max token length = 1024
0.00.053.268 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.269 I llm_load_tensors: offloading output layer to GPU
0.00.053.269 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.279 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.280 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.234 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.235 I llama_new_context_with_model: n_ctx         = 128
0.00.054.235 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.235 I llama_new_context_with_model: n_batch       = 128
0.00.054.235 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.236 I llama_new_context_with_model: flash_attn    = 0
0.00.054.236 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.236 I llama_new_context_with_model: freq_scale    = 1
0.00.054.237 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.237 I ggml_metal_init: allocating
0.00.054.240 I ggml_metal_init: found device: Apple M4
0.00.054.242 I ggml_metal_init: picking default device: Apple M4
0.00.054.788 I ggml_metal_init: using embedded metal library
0.00.056.717 I ggml_metal_init: GPU name:   Apple M4
0.00.056.718 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.719 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.719 I ggml_metal_init: simdgroup reduction   = true
0.00.056.719 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.719 I ggml_metal_init: has bfloat            = true
0.00.056.720 I ggml_metal_init: use bfloat            = true
0.00.056.720 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.722 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.026 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.031 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.045 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.952 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.953 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.953 I llama_new_context_with_model: graph nodes  = 967
0.00.066.953 I llama_new_context_with_model: graph splits = 2
0.00.066.965 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.494.619 I 
0.00.494.642 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.494.663 I perplexity: tokenizing the input ..
0.00.502.205 I perplexity: tokenization took 7.541 ms
0.00.502.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.634.239 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.635.515 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.635.531 I llama_perf_context_print:        load time =     484.55 ms
0.00.635.532 I llama_perf_context_print: prompt eval time =     131.79 ms /   128 tokens (    1.03 ms per token,   971.23 tokens per second)
0.00.635.533 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.635.534 I llama_perf_context_print:       total time =     140.91 ms /   129 tokens
0.00.635.893 I ggml_metal_free: deallocating

real	0m0.649s
user	0m0.076s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.236 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.208 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.213 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.214 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.215 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.215 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.216 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.217 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.217 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.218 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.221 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.224 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.224 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.028 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.086 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.883 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.884 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.884 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.885 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.885 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.885 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.886 I llama_model_loader: - type  f32:  194 tensors
0.00.022.886 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.886 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.887 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.887 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.270 I llm_load_vocab: special tokens cache size = 25
0.00.049.388 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.390 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.391 I llm_load_print_meta: arch             = gptneox
0.00.049.391 I llm_load_print_meta: vocab type       = BPE
0.00.049.391 I llm_load_print_meta: n_vocab          = 50304
0.00.049.391 I llm_load_print_meta: n_merges         = 50009
0.00.049.391 I llm_load_print_meta: vocab_only       = 0
0.00.049.392 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.392 I llm_load_print_meta: n_embd           = 2048
0.00.049.392 I llm_load_print_meta: n_layer          = 24
0.00.049.395 I llm_load_print_meta: n_head           = 16
0.00.049.395 I llm_load_print_meta: n_head_kv        = 16
0.00.049.396 I llm_load_print_meta: n_rot            = 32
0.00.049.396 I llm_load_print_meta: n_swa            = 0
0.00.049.396 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.396 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.397 I llm_load_print_meta: n_gqa            = 1
0.00.049.397 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.398 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.399 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.399 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.399 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.399 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.399 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.400 I llm_load_print_meta: n_ff             = 8192
0.00.049.400 I llm_load_print_meta: n_expert         = 0
0.00.049.400 I llm_load_print_meta: n_expert_used    = 0
0.00.049.401 I llm_load_print_meta: causal attn      = 1
0.00.049.401 I llm_load_print_meta: pooling type     = 0
0.00.049.402 I llm_load_print_meta: rope type        = 2
0.00.049.403 I llm_load_print_meta: rope scaling     = linear
0.00.049.404 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.404 I llm_load_print_meta: freq_scale_train = 1
0.00.049.404 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.404 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.405 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.405 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.405 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.405 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.405 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.416 I llm_load_print_meta: model type       = 1.4B
0.00.049.416 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.417 I llm_load_print_meta: model params     = 1.41 B
0.00.049.417 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.417 I llm_load_print_meta: general.name     = 1.4B
0.00.049.418 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.418 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.418 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.418 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.418 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.419 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.419 I llm_load_print_meta: max token length = 1024
0.00.050.993 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.050.993 I llm_load_tensors: offloading output layer to GPU
0.00.050.993 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.003 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.004 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.051.839 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.840 I llama_new_context_with_model: n_ctx         = 128
0.00.051.840 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.840 I llama_new_context_with_model: n_batch       = 128
0.00.051.840 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.840 I llama_new_context_with_model: flash_attn    = 0
0.00.051.841 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.841 I llama_new_context_with_model: freq_scale    = 1
0.00.051.842 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.842 I ggml_metal_init: allocating
0.00.051.846 I ggml_metal_init: found device: Apple M4
0.00.051.848 I ggml_metal_init: picking default device: Apple M4
0.00.052.416 I ggml_metal_init: using embedded metal library
0.00.054.339 I ggml_metal_init: GPU name:   Apple M4
0.00.054.341 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.341 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.341 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.343 I ggml_metal_init: simdgroup reduction   = true
0.00.054.343 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.343 I ggml_metal_init: has bfloat            = true
0.00.054.344 I ggml_metal_init: use bfloat            = true
0.00.054.344 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.345 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.728 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.063.731 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.063.746 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.064.648 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.064.650 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.064.650 I llama_new_context_with_model: graph nodes  = 967
0.00.064.650 I llama_new_context_with_model: graph splits = 2
0.00.064.662 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.604.676 I 
0.00.604.691 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.604.705 I perplexity: tokenizing the input ..
0.00.611.701 I perplexity: tokenization took 6.995 ms
0.00.611.704 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.399 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.744.589 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.744.605 I llama_perf_context_print:        load time =     596.44 ms
0.00.744.606 I llama_perf_context_print: prompt eval time =     131.47 ms /   128 tokens (    1.03 ms per token,   973.57 tokens per second)
0.00.744.607 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.744.608 I llama_perf_context_print:       total time =     139.93 ms /   129 tokens
0.00.745.019 I ggml_metal_free: deallocating

real	0m0.756s
user	0m0.076s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.084 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.398 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.836 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.840 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.846 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.851 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.852 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.852 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.632 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.696 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.436 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.437 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.438 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.438 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.439 I llama_model_loader: - type  f32:  194 tensors
0.00.024.439 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.440 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.440 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.862 I llm_load_vocab: special tokens cache size = 25
0.00.050.958 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.961 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.961 I llm_load_print_meta: arch             = gptneox
0.00.050.961 I llm_load_print_meta: vocab type       = BPE
0.00.050.961 I llm_load_print_meta: n_vocab          = 50304
0.00.050.962 I llm_load_print_meta: n_merges         = 50009
0.00.050.962 I llm_load_print_meta: vocab_only       = 0
0.00.050.962 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.962 I llm_load_print_meta: n_embd           = 2048
0.00.050.962 I llm_load_print_meta: n_layer          = 24
0.00.050.965 I llm_load_print_meta: n_head           = 16
0.00.050.966 I llm_load_print_meta: n_head_kv        = 16
0.00.050.966 I llm_load_print_meta: n_rot            = 32
0.00.050.966 I llm_load_print_meta: n_swa            = 0
0.00.050.966 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.966 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.967 I llm_load_print_meta: n_gqa            = 1
0.00.050.968 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.968 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.969 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.969 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.971 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.971 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.972 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.972 I llm_load_print_meta: n_ff             = 8192
0.00.050.973 I llm_load_print_meta: n_expert         = 0
0.00.050.973 I llm_load_print_meta: n_expert_used    = 0
0.00.050.974 I llm_load_print_meta: causal attn      = 1
0.00.050.974 I llm_load_print_meta: pooling type     = 0
0.00.050.975 I llm_load_print_meta: rope type        = 2
0.00.050.975 I llm_load_print_meta: rope scaling     = linear
0.00.050.975 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.976 I llm_load_print_meta: freq_scale_train = 1
0.00.050.976 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.976 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.976 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.977 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.977 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.978 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.978 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.989 I llm_load_print_meta: model type       = 1.4B
0.00.050.989 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.990 I llm_load_print_meta: model params     = 1.41 B
0.00.050.990 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.991 I llm_load_print_meta: general.name     = 1.4B
0.00.050.991 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.991 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.992 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.992 I llm_load_print_meta: max token length = 1024
0.00.052.607 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.607 I llm_load_tensors: offloading output layer to GPU
0.00.052.607 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.616 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.618 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.531 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.532 I llama_new_context_with_model: n_ctx         = 128
0.00.053.532 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.532 I llama_new_context_with_model: n_batch       = 128
0.00.053.532 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.532 I llama_new_context_with_model: flash_attn    = 0
0.00.053.533 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.533 I llama_new_context_with_model: freq_scale    = 1
0.00.053.534 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.534 I ggml_metal_init: allocating
0.00.053.541 I ggml_metal_init: found device: Apple M4
0.00.053.543 I ggml_metal_init: picking default device: Apple M4
0.00.054.092 I ggml_metal_init: using embedded metal library
0.00.055.996 I ggml_metal_init: GPU name:   Apple M4
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.997 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.998 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.998 I ggml_metal_init: simdgroup reduction   = true
0.00.055.998 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.998 I ggml_metal_init: has bfloat            = true
0.00.055.998 I ggml_metal_init: use bfloat            = true
0.00.055.999 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.999 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.153 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.156 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.170 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.068 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.069 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.070 I llama_new_context_with_model: graph nodes  = 967
0.00.066.070 I llama_new_context_with_model: graph splits = 2
0.00.066.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.396 I 
0.00.619.413 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.619.428 I perplexity: tokenizing the input ..
0.00.626.575 I perplexity: tokenization took 7.146 ms
0.00.626.578 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.760.609 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.761.954 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.761.969 I llama_perf_context_print:        load time =     610.00 ms
0.00.761.970 I llama_perf_context_print: prompt eval time =     133.81 ms /   128 tokens (    1.05 ms per token,   956.56 tokens per second)
0.00.761.983 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.761.984 I llama_perf_context_print:       total time =     142.57 ms /   129 tokens
0.00.762.397 I ggml_metal_free: deallocating

real	0m0.776s
user	0m0.076s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.081 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.503 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.456 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.458 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.459 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.459 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.460 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.461 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.461 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.462 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.462 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.462 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.463 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.465 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.466 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.466 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.271 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.321 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.193 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.194 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.194 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.195 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.195 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.195 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.196 I llama_model_loader: - type  f32:  194 tensors
0.00.023.196 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.196 I llama_model_loader: - type q6_K:   37 tensors
0.00.043.736 I llm_load_vocab: special tokens cache size = 25
0.00.049.709 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.712 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.712 I llm_load_print_meta: arch             = gptneox
0.00.049.713 I llm_load_print_meta: vocab type       = BPE
0.00.049.713 I llm_load_print_meta: n_vocab          = 50304
0.00.049.713 I llm_load_print_meta: n_merges         = 50009
0.00.049.713 I llm_load_print_meta: vocab_only       = 0
0.00.049.714 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.714 I llm_load_print_meta: n_embd           = 2048
0.00.049.714 I llm_load_print_meta: n_layer          = 24
0.00.049.716 I llm_load_print_meta: n_head           = 16
0.00.049.717 I llm_load_print_meta: n_head_kv        = 16
0.00.049.717 I llm_load_print_meta: n_rot            = 32
0.00.049.717 I llm_load_print_meta: n_swa            = 0
0.00.049.718 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.718 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.718 I llm_load_print_meta: n_gqa            = 1
0.00.049.719 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.720 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.720 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.721 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.721 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.721 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.721 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.722 I llm_load_print_meta: n_ff             = 8192
0.00.049.722 I llm_load_print_meta: n_expert         = 0
0.00.049.722 I llm_load_print_meta: n_expert_used    = 0
0.00.049.722 I llm_load_print_meta: causal attn      = 1
0.00.049.722 I llm_load_print_meta: pooling type     = 0
0.00.049.723 I llm_load_print_meta: rope type        = 2
0.00.049.723 I llm_load_print_meta: rope scaling     = linear
0.00.049.724 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.725 I llm_load_print_meta: freq_scale_train = 1
0.00.049.725 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.725 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.725 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.725 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.725 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.725 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.726 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.737 I llm_load_print_meta: model type       = 1.4B
0.00.049.737 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.737 I llm_load_print_meta: model params     = 1.41 B
0.00.049.738 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.738 I llm_load_print_meta: general.name     = 1.4B
0.00.049.738 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.739 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.739 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.741 I llm_load_print_meta: max token length = 1024
0.00.051.416 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.417 I llm_load_tensors: offloading output layer to GPU
0.00.051.417 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.426 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.051.428 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.304 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.305 I llama_new_context_with_model: n_ctx         = 128
0.00.052.305 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.305 I llama_new_context_with_model: n_batch       = 128
0.00.052.305 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.305 I llama_new_context_with_model: flash_attn    = 0
0.00.052.306 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.306 I llama_new_context_with_model: freq_scale    = 1
0.00.052.307 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.307 I ggml_metal_init: allocating
0.00.052.313 I ggml_metal_init: found device: Apple M4
0.00.052.316 I ggml_metal_init: picking default device: Apple M4
0.00.053.095 I ggml_metal_init: using embedded metal library
0.00.055.043 I ggml_metal_init: GPU name:   Apple M4
0.00.055.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.045 I ggml_metal_init: simdgroup reduction   = true
0.00.055.046 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.046 I ggml_metal_init: has bfloat            = true
0.00.055.047 I ggml_metal_init: use bfloat            = true
0.00.055.047 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.048 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.410 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.064.415 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.064.444 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.065.334 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.065.335 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.065.335 I llama_new_context_with_model: graph nodes  = 967
0.00.065.336 I llama_new_context_with_model: graph splits = 2
0.00.065.347 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.479 I 
0.00.700.495 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.700.506 I perplexity: tokenizing the input ..
0.00.707.699 I perplexity: tokenization took 7.192 ms
0.00.707.702 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.848.671 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.849.987 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.850.007 I llama_perf_context_print:        load time =     691.97 ms
0.00.850.008 I llama_perf_context_print: prompt eval time =     140.75 ms /   128 tokens (    1.10 ms per token,   909.44 tokens per second)
0.00.850.009 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.850.009 I llama_perf_context_print:       total time =     149.53 ms /   129 tokens
0.00.850.446 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.076s
sys	0m0.130s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.831 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.456 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.460 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.462 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.462 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.462 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.463 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.463 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.464 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.465 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.469 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.469 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.304 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.065 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.066 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.067 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.067 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.067 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.068 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.068 I llama_model_loader: - type  f32:  194 tensors
0.00.026.068 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.478 I llm_load_vocab: special tokens cache size = 25
0.00.052.416 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.419 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.419 I llm_load_print_meta: arch             = gptneox
0.00.052.420 I llm_load_print_meta: vocab type       = BPE
0.00.052.420 I llm_load_print_meta: n_vocab          = 50304
0.00.052.420 I llm_load_print_meta: n_merges         = 50009
0.00.052.420 I llm_load_print_meta: vocab_only       = 0
0.00.052.421 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.421 I llm_load_print_meta: n_embd           = 2048
0.00.052.421 I llm_load_print_meta: n_layer          = 24
0.00.052.424 I llm_load_print_meta: n_head           = 16
0.00.052.424 I llm_load_print_meta: n_head_kv        = 16
0.00.052.424 I llm_load_print_meta: n_rot            = 32
0.00.052.425 I llm_load_print_meta: n_swa            = 0
0.00.052.425 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.425 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.426 I llm_load_print_meta: n_gqa            = 1
0.00.052.426 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.427 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.428 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.428 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.428 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.428 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.428 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.429 I llm_load_print_meta: n_ff             = 8192
0.00.052.430 I llm_load_print_meta: n_expert         = 0
0.00.052.432 I llm_load_print_meta: n_expert_used    = 0
0.00.052.432 I llm_load_print_meta: causal attn      = 1
0.00.052.432 I llm_load_print_meta: pooling type     = 0
0.00.052.432 I llm_load_print_meta: rope type        = 2
0.00.052.433 I llm_load_print_meta: rope scaling     = linear
0.00.052.433 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.433 I llm_load_print_meta: freq_scale_train = 1
0.00.052.433 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.433 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.434 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.434 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.434 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.434 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.434 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.445 I llm_load_print_meta: model type       = 1.4B
0.00.052.445 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.446 I llm_load_print_meta: model params     = 1.41 B
0.00.052.448 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.448 I llm_load_print_meta: general.name     = 1.4B
0.00.052.449 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.449 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.449 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.449 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.449 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.450 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.450 I llm_load_print_meta: max token length = 1024
0.00.053.995 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.995 I llm_load_tensors: offloading output layer to GPU
0.00.053.995 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.004 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.006 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.906 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.907 I llama_new_context_with_model: n_ctx         = 128
0.00.054.907 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.907 I llama_new_context_with_model: n_batch       = 128
0.00.054.907 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.907 I llama_new_context_with_model: flash_attn    = 0
0.00.054.908 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.908 I llama_new_context_with_model: freq_scale    = 1
0.00.054.908 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.909 I ggml_metal_init: allocating
0.00.054.915 I ggml_metal_init: found device: Apple M4
0.00.054.918 I ggml_metal_init: picking default device: Apple M4
0.00.055.480 I ggml_metal_init: using embedded metal library
0.00.057.415 I ggml_metal_init: GPU name:   Apple M4
0.00.057.417 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.418 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.418 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.418 I ggml_metal_init: simdgroup reduction   = true
0.00.057.419 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.419 I ggml_metal_init: has bfloat            = true
0.00.057.419 I ggml_metal_init: use bfloat            = true
0.00.057.419 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.420 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.843 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.847 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.871 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.769 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.770 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.771 I llama_new_context_with_model: graph nodes  = 967
0.00.067.771 I llama_new_context_with_model: graph splits = 2
0.00.067.783 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.648 I 
0.00.647.662 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.647.672 I perplexity: tokenizing the input ..
0.00.654.943 I perplexity: tokenization took 7.269 ms
0.00.654.947 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.135 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.796.438 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.796.455 I llama_perf_context_print:        load time =     635.81 ms
0.00.796.456 I llama_perf_context_print: prompt eval time =     139.97 ms /   128 tokens (    1.09 ms per token,   914.49 tokens per second)
0.00.796.457 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.457 I llama_perf_context_print:       total time =     148.81 ms /   129 tokens
0.00.796.901 I ggml_metal_free: deallocating

real	0m0.812s
user	0m0.076s
sys	0m0.121s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.270 I build: 4159 (7dc6ae57) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.034.460 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.046.547 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.559 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.567 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.568 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.569 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.569 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.574 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.575 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.582 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.582 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.583 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.869 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.057.310 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.027 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.065.030 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.031 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.031 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.031 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.032 I llama_model_loader: - type  f32:  194 tensors
0.00.065.033 I llama_model_loader: - type  f16:   98 tensors
0.00.096.569 I llm_load_vocab: special tokens cache size = 25
0.00.103.396 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.103.398 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.103.399 I llm_load_print_meta: arch             = gptneox
0.00.103.399 I llm_load_print_meta: vocab type       = BPE
0.00.103.399 I llm_load_print_meta: n_vocab          = 50304
0.00.103.399 I llm_load_print_meta: n_merges         = 50009
0.00.103.399 I llm_load_print_meta: vocab_only       = 0
0.00.103.400 I llm_load_print_meta: n_ctx_train      = 2048
0.00.103.400 I llm_load_print_meta: n_embd           = 2048
0.00.103.400 I llm_load_print_meta: n_layer          = 24
0.00.103.402 I llm_load_print_meta: n_head           = 16
0.00.103.403 I llm_load_print_meta: n_head_kv        = 16
0.00.103.403 I llm_load_print_meta: n_rot            = 32
0.00.103.403 I llm_load_print_meta: n_swa            = 0
0.00.103.404 I llm_load_print_meta: n_embd_head_k    = 128
0.00.103.404 I llm_load_print_meta: n_embd_head_v    = 128
0.00.103.405 I llm_load_print_meta: n_gqa            = 1
0.00.103.405 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.103.406 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.103.407 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.103.408 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.103.408 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.103.408 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.103.408 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.103.409 I llm_load_print_meta: n_ff             = 8192
0.00.103.409 I llm_load_print_meta: n_expert         = 0
0.00.103.409 I llm_load_print_meta: n_expert_used    = 0
0.00.103.409 I llm_load_print_meta: causal attn      = 1
0.00.103.409 I llm_load_print_meta: pooling type     = 0
0.00.103.409 I llm_load_print_meta: rope type        = 2
0.00.103.410 I llm_load_print_meta: rope scaling     = linear
0.00.103.410 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.103.410 I llm_load_print_meta: freq_scale_train = 1
0.00.103.411 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.103.413 I llm_load_print_meta: rope_finetuned   = unknown
0.00.103.413 I llm_load_print_meta: ssm_d_conv       = 0
0.00.103.413 I llm_load_print_meta: ssm_d_inner      = 0
0.00.103.413 I llm_load_print_meta: ssm_d_state      = 0
0.00.103.413 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.103.413 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.103.425 I llm_load_print_meta: model type       = 1.4B
0.00.103.427 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.103.427 I llm_load_print_meta: model params     = 1.41 B
0.00.103.427 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.103.428 I llm_load_print_meta: general.name     = 1.4B
0.00.103.428 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.103.428 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.103.428 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.103.428 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.103.429 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.103.429 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.103.429 I llm_load_print_meta: max token length = 1024
0.00.105.300 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.105.300 I llm_load_tensors: offloading output layer to GPU
0.00.105.300 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.105.310 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.311 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.106.248 I llama_new_context_with_model: n_seq_max     = 1
0.00.106.249 I llama_new_context_with_model: n_ctx         = 128
0.00.106.250 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.106.250 I llama_new_context_with_model: n_batch       = 128
0.00.106.250 I llama_new_context_with_model: n_ubatch      = 128
0.00.106.250 I llama_new_context_with_model: flash_attn    = 0
0.00.106.251 I llama_new_context_with_model: freq_base     = 10000.0
0.00.106.251 I llama_new_context_with_model: freq_scale    = 1
0.00.106.251 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.106.252 I ggml_metal_init: allocating
0.00.106.255 I ggml_metal_init: found device: Apple M4
0.00.106.257 I ggml_metal_init: picking default device: Apple M4
0.00.106.869 I ggml_metal_init: using embedded metal library
0.00.109.284 I ggml_metal_init: GPU name:   Apple M4
0.00.109.285 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.109.286 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.109.286 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.109.286 I ggml_metal_init: simdgroup reduction   = true
0.00.109.287 I ggml_metal_init: simdgroup matrix mul. = true
0.00.109.287 I ggml_metal_init: has bfloat            = true
0.00.109.287 I ggml_metal_init: use bfloat            = true
0.00.109.287 I ggml_metal_init: hasUnifiedMemory      = true
0.00.109.290 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.118.265 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.118.268 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.118.284 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.119.201 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.119.202 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.119.202 I llama_new_context_with_model: graph nodes  = 967
0.00.119.202 I llama_new_context_with_model: graph splits = 2
0.00.119.214 I 
0.00.119.234 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 0 | NEON = 1 | SVE = 0 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | RISCV_VECT = 0 | WASM_SIMD = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 1 | LLAMAFILE = 1 | 
0.00.119.236 I compute_imatrix: tokenizing the input ..
0.00.126.369 I compute_imatrix: tokenization took 7.133 ms
0.00.126.370 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.563.176 I compute_imatrix: 1.44 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.566.160 I llama_perf_context_print:        load time =    1528.71 ms
0.01.566.161 I llama_perf_context_print: prompt eval time =    1436.37 ms /   128 tokens (   11.22 ms per token,    89.11 tokens per second)
0.01.566.162 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.566.162 I llama_perf_context_print:       total time =    1531.69 ms /   129 tokens
0.01.566.606 I ggml_metal_free: deallocating

real	0m1.773s
user	0m0.171s
sys	0m0.246s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4159 (7dc6ae57)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x105a0a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x105a0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x105a0add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x105a0b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x105a0b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x105a0bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x105a0c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x105a0ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x105a0cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x105a0d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x105a0d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x105a0def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x105a0ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x105a0f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x105a0f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x105a100f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x105a10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x105a10f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x105a11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x105a11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x105a12540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x105a12c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x105a13380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x105a13c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x105a14340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x105a14600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x105a14c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x105a15880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x105a15dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x105a16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x105a16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x105a167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x105a17070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x105a175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x105a17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x105a17d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x105a181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x105a18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x105a18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x105a18f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x105a19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x105a198d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x105a19d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x105a1a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x105a1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x105a1aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x105a1b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x105a1ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x105a1c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x105a1c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x105a1cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x105a1d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x105a1d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x105a1de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x105a1e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x105a1eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x105a1efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x105a1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x105a1f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x105a20060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x105a20320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x105a207c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x105a20c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x105a21100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x105a215a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x105a21a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x105a21ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x105a22380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x105a22820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x105a22cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x105a23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x105a23600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x105a23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x105a23f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x105a243e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x105a24880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x105a24d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x105a251c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x105a25660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x105a25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x105a25fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x105a26440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x105a268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x105a26d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x105a27220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x105a276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x105a27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x105a28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x105a284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x105a28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x105a28de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x105a29280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x105a29720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x105a29bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x105a2a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x105a2a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x105a2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x105a1b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x105a2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x105a2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x105a2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x105a2bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x105a2c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x105a2c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x105a2cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x105a2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x105a2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x105a2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x105a2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x105a2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x105a2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x105a2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x105a2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x105a2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x105a2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x105a2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x105a30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x105a307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x105a30c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x105a31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x105a315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x105a31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x105a31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x105a32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x105a32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x105a32cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x105a33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x105a33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x105a33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x105a33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x105a343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x105a34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x105a34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x105a351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x105a35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x105a35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x105a35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x105a36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x105a368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x105a36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x105a37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x105a376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x105a37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x105a38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x105a384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x105a38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x105a38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x105a39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x105a39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x105a39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x105a3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x105a3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x105a3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x105a3af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x105a3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x105a3b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x105a3bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x105a3c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x105a3c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x105a3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x105a3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x105a3d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x105a3e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x105a3e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x105a3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x105a3f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x105a3f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x105a3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x105a402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x105a40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x105a40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x105a412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x105a41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x105a41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x105a422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x105a42800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x105a42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x105a432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x105a437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x105a43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x105a44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x105a447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x105a44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x105a45280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x105a457d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x105a45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x105a46270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x105a467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x105a46d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x105a47260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x105a477b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x105a47d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x105a48250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x105a487a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x105a48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x105a49240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x105a49790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x105a49ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x105a4a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x105a4a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x105a4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x105a4b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x105a4b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x105a4bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x105a4c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x105a4c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x105a4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x105a4d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x105a4d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105a4dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105a4e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105a4e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105a4ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105a4f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105a4f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105a4fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105a501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105a50720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105a50c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105a511c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105a51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105a51c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105a521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105a52700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105a52ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105a53040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105a534e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105a53980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105a53e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105a542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105a54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105a54c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105a550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105a55540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105a559e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105a55e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105a56320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105a56870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105a56f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105a576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105a57dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105a584f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105a587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105a58dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105a593d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.154.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108f07360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108f077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108f07e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108f08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108f09120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108f09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x108f0a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x108f0a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x108f0ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x108f0b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108f0bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108f0c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x108f0cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x108f0d2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108f0da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108f0e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x108f0e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108f0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108f0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108f0ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108f0f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108f0f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108f0fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108f10200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108f104c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108f10930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108f10da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108f11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108f11680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108f11af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108f11f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108f123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108f12840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108f12cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108f13120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108f13590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108f13a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108f13e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108f142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108f14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108f14bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108f15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108f154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108f15910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108f15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108f161f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108f16660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108f16bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108f170d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108f17540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108f179b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108f17e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108f18290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108f18700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108f18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108f18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108f19450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108f198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108f19d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108f1a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108f1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x108f1aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108f1aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108f1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108f1b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108f1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108f1c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108f1c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108f1c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108f1ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108f1d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108f1d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108f1db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108f1dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108f1e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108f1e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108f1ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108f1f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x108f1f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108f1fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108f1fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x108f20340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108f207b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108f20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108f21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108f21500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x108f21970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108f21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108f22250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108f226c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108f22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108f22fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108f23410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108f23880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108f23cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108f24160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108f245d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108f24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108f24eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108f25320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108f25790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108f25c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108f26070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108f264e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108f26950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108f26dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108f27230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108f276a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108f27b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108f27f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108f283f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108f28860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108f28cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108f29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108f295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108f29a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108f29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108f2a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108f2a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108f2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108f2b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108f2b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108f2b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108f2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108f2c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108f2c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108f2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108f2cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108f2d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108f2d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108f2dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108f2e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108f2e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108f2ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x108f2ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108f2f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108f2f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x108f2fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108f30030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x108f304a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108f30910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108f30d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108f311f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108f31660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x108f31ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108f31f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108f323b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108f32820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108f32c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108f33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108f33570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108f339e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108f33e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108f342c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108f34730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108f34ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108f35010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108f35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108f36010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108f362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108f36590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108f36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108f36e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108f372e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108f37750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108f37bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108f38030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108f384a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108f38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108f38d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108f391f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108f39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108f39ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108f39f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108f3a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108f3a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108f3ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108f3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108f3b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108f3b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108f3be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108f3c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x108f3c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108f3cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108f3d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108f3d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108f3d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108f3dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108f3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x108f3e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108f3eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108f3ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108f3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108f3f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108f3fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x108f400e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108f40550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108f409c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x108f40e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108f412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108f41710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x108f41b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108f41ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108f42460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108f428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x108f42d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108f431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108f43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108f43a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108f43f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108f44370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108f447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108f44c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108f450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108f45530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108f459a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108f45e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108f46280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108f466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108f46b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108f46fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108f47440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108f478b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108f47d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108f48190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108f48600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108f48a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108f48ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108f49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108f49e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108f4a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108f4acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108f4b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x108f4b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108f4b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x108f4bde0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108f072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108f07740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108f07d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108f08610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108f08d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108f09570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x108f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x108f0a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x108f0aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x108f0b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108f0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108f0c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x108f0c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x108f0cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108f0d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108f0dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x108f0e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108f0e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108f0eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108f0ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108f0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108f0f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108f0fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108f100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108f103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108f10810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108f10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108f110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108f11560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108f119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108f11e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108f122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108f12720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108f12b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108f13000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108f13470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108f138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108f13d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108f14630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108f14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108f14f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108f15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108f157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108f15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108f160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108f16540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108f169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108f16e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108f17290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108f17700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108f17b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108f17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108f188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108f18d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108f191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108f19610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108f19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108f19ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108f1a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x108f1a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108f1ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108f1b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108f1b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108f1b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108f1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108f1c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108f1c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108f1cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108f1cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108f1d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108f1d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108f1dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108f1e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108f1e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108f1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108f1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x108f1f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108f1f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108f1fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x108f20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108f20500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108f20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108f20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108f21250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x108f216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108f21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108f21fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108f22410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108f22880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108f22cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108f23160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108f235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108f23a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108f23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108f24320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108f24790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108f24c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108f25070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108f254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108f25950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108f25dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108f26230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108f266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108f26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108f26f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108f273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108f27860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108f27cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108f28140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108f285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108f28a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108f28e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108f29300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108f29770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108f29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108f2a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108f2a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108f2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108f2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108f2b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108f2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108f2baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108f2bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108f2c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108f2c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108f2ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108f2d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108f2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108f2da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108f2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108f2e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108f2e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x108f2ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108f2f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108f2f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x108f2f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108f2fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x108f301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108f30660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108f30ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108f30f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108f313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x108f31820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108f31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108f32100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108f32570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108f329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108f32e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108f332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108f33730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108f33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108f34010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108f34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108f348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108f34d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108f351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108f35950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108f35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108f36230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108f366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108f36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108f36f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108f373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108f37860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108f37cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108f38140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108f385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108f38a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108f38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108f39300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108f39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108f39be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108f3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108f3a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108f3a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108f3ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108f3b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108f3b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108f3baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108f3bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x108f3c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108f3c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108f3ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108f3d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108f3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108f3da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108f3de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x108f3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108f3e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108f3ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108f3f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108f3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x108f3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108f401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108f40660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x108f40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108f40f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x105a0a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x105a49360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x105a497d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x105a49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x105a4a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x105a4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x105a4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x105a4ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x105a4b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x105a4b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x105a4bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x105a4bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x105a4c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x105a4c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x105a4cd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x105a4d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x105a4d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x105a4da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x105a4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x105a4e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x105a4e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x105a4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x105a4f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x105a4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x105a4f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x105a4fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x105a50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x105a506c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x105a50b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x105a51390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x105a51a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x105a52170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x105a52860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x105a52cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x105a53140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x105a535b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.851s
user	0m0.290s
sys	0m0.294s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4159 (7dc6ae57)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13b60bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13b60c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13b60caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13b60d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13b60d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13b60dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13b60e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13b60e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13b60ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13b60f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13b60f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13b60fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13b6106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13b610e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13b6116a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13b611dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13b6124e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13b612c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13b613320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13b613af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13b614210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13b614930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13b615050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13b6158f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13b616010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13b6162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13b6168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13b617550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13b617a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13b617d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13b6181f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13b6184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13b618d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13b619280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13b619540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13b6199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13b619e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13b61a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13b61a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13b61ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13b61b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13b61b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13b61ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13b61bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13b61c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13b61c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13b61cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13b61d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13b61dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13b61e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13b61e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13b61ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13b61f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13b61fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13b620330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13b6207d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13b620c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13b620f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13b621540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13b621d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13b621ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13b622490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13b622930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13b622dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13b623270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13b623710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13b623bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13b624050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13b6244f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13b624990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13b624e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13b6252d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13b625770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13b625c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13b6260b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13b626550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13b6269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13b626e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13b627330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13b6277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13b627c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13b628110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13b6285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13b628a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13b628ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13b629390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13b629830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13b629cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13b62a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13b62a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13b62aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13b62af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13b62b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13b62b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13b62bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13b62c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13b62c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13b61d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13b62ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13b62d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13b62d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13b62daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13b62df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13b62e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13b62e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13b62ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13b62f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13b62f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13b62fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13b62ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13b630440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13b6308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13b630d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13b631220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13b6316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13b631b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13b632000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13b6324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13b632940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13b632de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13b633280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13b633720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13b633bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13b634060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13b634500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13b6349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13b634e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13b6352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13b635780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13b635c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13b6360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13b636560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13b636a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13b636ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13b637340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13b6377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13b637c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13b638120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13b6385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13b638a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13b638f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13b6393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13b639840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13b639ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13b63a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13b63a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13b63aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13b63af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13b63b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13b63b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13b63bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13b63c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13b63c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13b63cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13b63d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13b63d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13b63dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13b63de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13b63e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13b63eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13b63f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13b63f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13b63fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13b6404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13b640960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13b640e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13b6412a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13b641a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13b641fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13b6424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13b642a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13b642f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13b6434e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13b643a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13b643f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13b6444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13b644a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13b644f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13b6454c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13b645a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13b645f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13b6464b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13b646a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13b646f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13b6474a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13b6479f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13b647f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13b648490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13b6489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13b648f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13b649480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13b6499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13b649f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13b64a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13b64a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13b64af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13b64b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13b64b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13b64bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13b64c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13b64c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13b64cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13b64d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13b64d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13b64dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13b64e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b64e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b64eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b64f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b64f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b64fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b650410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b650960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b650eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b651400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b651950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b651ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b6523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b652940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b652e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b6533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b653930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b653e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b6543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b654870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b654d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b6551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b655650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b655af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b655f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b656430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b6568d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b656d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b657210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b6576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b657b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b657ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b658540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b658c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b659380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b659aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b65a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b65a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b65aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b65b0a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.088.962 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c807e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c808940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c8090f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c809900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c80a020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c80a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c80ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c80b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c80bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c80c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c80cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c80d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c80d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c80e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c80e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c80e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c80eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c80ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c80f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c80f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c80fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c8101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c810490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c810900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c810d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c8111e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c811650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c811ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c811f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c8123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c812810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c812c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c8130f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c813560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c8139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c813e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c8142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c814720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c814b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c815000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c815470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c8158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c815d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c8161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c816630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c816ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c8170a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c817510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c817980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c817df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c818260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c8186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c818b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c818fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c819420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c819890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c819d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c81a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c81a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c81aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c81aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c81b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c81b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c81bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c81c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c81c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c81c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c81cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c81d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c81db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c81df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c81e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c81e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c81ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c81f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c81f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c81fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c81fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c820310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c820780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c820bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c821060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c8214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c821940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c821db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c822220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c822690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c822b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c822f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c8233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c823850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c823cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c824130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c8245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c824a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c824e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c8252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c825760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c825bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c826040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c8264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c826920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c826d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c827200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c827670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c827ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c8283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c828830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c828ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c829580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c8299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c829e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c82a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c82a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c82abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c82b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c82b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c82b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c82bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c82c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c82c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c82cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c82cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c82d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c82d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c82dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c82e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c82e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c82e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c82ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c82f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c82f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c82fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c830000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c830470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c8308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c830d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c8311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c831630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c831aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c831f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c832380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c8327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c832c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c8330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c833540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c8339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c833e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c834290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c834700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c834b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c834fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c835450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c835fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c8362a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c836560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c8369d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c836e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c8372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c837720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c837b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c838000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c838470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c8388e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c838d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c8391c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c839630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c839aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c839f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c83a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c83a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c83ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c83b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c83b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c83b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c83be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c83c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c83c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c83cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c83cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c83d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c83d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c83dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c83e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c83e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c83ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c83eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c83f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c83f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c83fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c8400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c840520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c840990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c840e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c841270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c8416e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c841b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c841fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c842430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c8428a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c842d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c843180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c8435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c843a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c843ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c844340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c8447b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c844c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c845090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c845500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c845970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c845de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c8466c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c846b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c846fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c847410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c847880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c847cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c848160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c8485d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c848a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c848eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c849320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c849e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c84a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c84aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c84b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c84b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c84b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c84bdb0 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c804ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c804f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c8053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c805830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c805ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c806110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c806580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c8069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c806e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c8072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c807740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c807d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c808610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c808d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c809570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c809c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c80a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c80aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c80b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c80bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c80c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c80c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c80cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c80d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c80dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c80e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c80e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c80eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c80ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c80f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c80f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c80fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c8100e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c8103a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c810810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c810c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c8110f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c811560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c8119d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c811e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c8122b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c812720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c812b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c813000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c813470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c8138e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c813d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c8141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c814630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c814aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c814f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c815380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c8157f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c815c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c8160d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c816540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c8169b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c816e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c817290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c817700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c817b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c817fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c818450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c8188c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c818d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c8191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c819610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c819a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c819ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c81a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c81a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c81ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c81b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c81b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c81b990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c81be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c81c270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c81c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c81cb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c81cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c81d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c81d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c81dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c81e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c81e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c81ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c81eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c81f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c81f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c81fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c820090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c820500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c820970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c820de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c821250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c8216c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c821b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c821fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c822410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c822880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c822cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c823160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c8235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c823a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c823eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c824320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c824790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c824c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c825070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c8254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c825950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c825dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c826230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c8266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c826b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c826f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c8273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c827860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c827cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c828140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c8285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c828a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c828e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c829300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c829770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c829be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c82a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c82a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c82a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c82ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c82b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c82b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c82baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c82bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c82c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c82c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c82ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c82d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c82d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c82da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c82de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c82e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c82e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c82ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c82f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c82f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c82f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c82fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c8301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c830660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c830ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c830f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c8313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c831820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c831c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c832100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c832570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c8329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c832e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c8332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c833730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c833ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c834010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c834480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c8348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c8351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c835950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c835dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c836230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c8366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c836b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c836f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c8373f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c837860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c837cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c838140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c8385b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c838a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c838e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c839300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c839770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c839be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c83a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c83a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c83a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c83ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c83b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c83b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c83baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c83bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c83c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c83c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c83ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c83d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c83d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c83da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c83de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c83e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c83e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c83ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c83f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c83f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c83f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c83fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c8401f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13b7096a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13b709b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13b709f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13b70a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13b70a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13b70acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13b70b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13b70b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13b70ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13b70be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13b70c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13b70c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13b70cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13b70d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13b70d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13b70d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13b70dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13b70e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13b70e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13b70eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13b70ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13b70f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13b70f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13b70fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13b710120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13b710590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13b710a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13b710e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13b7112e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13b711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13b711bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13b712030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13b712ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13b7131f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13b713910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13b714030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13b7142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13b7145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13b714a20 | th_max = 1024 | th_width =   32
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


second run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He


single seq run: The quick brown fox jumps over the lazy Dog." "Our old friend, Zorro." "He

real	0m0.941s
user	0m0.240s
sys	0m0.125s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
