### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.26 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.09 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.44 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.22 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.22 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.22 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.21 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.24 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.48 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.29 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.09 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.28 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.92 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.04 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  192.37 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.86 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.92 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 253.36 sec*proc (29 tests)

Total Test time (real) = 253.37 sec

real	4m13.406s
user	8m38.651s
sys	0m7.168s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.75 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.17 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.35 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.94 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.37 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.10 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  54.73 sec*proc (29 tests)

Total Test time (real) =  54.74 sec

real	0m54.749s
user	1m17.581s
sys	0m6.313s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.133 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.525 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.021.132 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.021.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.143 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.021.145 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.145 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.021.146 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.021.151 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.021.152 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.021.152 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.021.153 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.021.154 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.021.154 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.021.157 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.021.158 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.021.159 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.021.159 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.021.160 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.021.160 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.021.161 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.025.997 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.027.241 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.244 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.027.244 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.027.245 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.027.245 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.027.246 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.027.246 I llama_model_loader: - type  f32:  124 tensors
0.00.027.247 I llama_model_loader: - type  f16:   73 tensors
0.00.027.248 I print_info: file format = GGUF V3 (latest)
0.00.027.249 I print_info: file type   = F16
0.00.027.250 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.031.915 I load: special tokens cache size = 5
0.00.034.102 I load: token to piece cache size = 0.2032 MB
0.00.034.106 I print_info: arch             = bert
0.00.034.106 I print_info: vocab_only       = 0
0.00.034.106 I print_info: n_ctx_train      = 512
0.00.034.107 I print_info: n_embd           = 384
0.00.034.107 I print_info: n_layer          = 12
0.00.034.110 I print_info: n_head           = 12
0.00.034.112 I print_info: n_head_kv        = 12
0.00.034.112 I print_info: n_rot            = 32
0.00.034.112 I print_info: n_swa            = 0
0.00.034.113 I print_info: n_embd_head_k    = 32
0.00.034.113 I print_info: n_embd_head_v    = 32
0.00.034.114 I print_info: n_gqa            = 1
0.00.034.115 I print_info: n_embd_k_gqa     = 384
0.00.034.115 I print_info: n_embd_v_gqa     = 384
0.00.034.116 I print_info: f_norm_eps       = 1.0e-12
0.00.034.117 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.034.117 I print_info: f_clamp_kqv      = 0.0e+00
0.00.034.117 I print_info: f_max_alibi_bias = 0.0e+00
0.00.034.118 I print_info: f_logit_scale    = 0.0e+00
0.00.034.123 I print_info: n_ff             = 1536
0.00.034.123 I print_info: n_expert         = 0
0.00.034.124 I print_info: n_expert_used    = 0
0.00.034.124 I print_info: causal attn      = 0
0.00.034.124 I print_info: pooling type     = 2
0.00.034.124 I print_info: rope type        = 2
0.00.034.125 I print_info: rope scaling     = linear
0.00.034.125 I print_info: freq_base_train  = 10000.0
0.00.034.125 I print_info: freq_scale_train = 1
0.00.034.126 I print_info: n_ctx_orig_yarn  = 512
0.00.034.126 I print_info: rope_finetuned   = unknown
0.00.034.126 I print_info: ssm_d_conv       = 0
0.00.034.126 I print_info: ssm_d_inner      = 0
0.00.034.127 I print_info: ssm_d_state      = 0
0.00.034.127 I print_info: ssm_dt_rank      = 0
0.00.034.127 I print_info: ssm_dt_b_c_rms   = 0
0.00.034.127 I print_info: model type       = 33M
0.00.034.128 I print_info: model params     = 33.21 M
0.00.034.128 I print_info: general.name     = Bge Small
0.00.034.129 I print_info: vocab type       = WPM
0.00.034.129 I print_info: n_vocab          = 30522
0.00.034.135 I print_info: n_merges         = 0
0.00.034.135 I print_info: BOS token        = 101 '[CLS]'
0.00.034.136 I print_info: UNK token        = 100 '[UNK]'
0.00.034.136 I print_info: SEP token        = 102 '[SEP]'
0.00.034.136 I print_info: PAD token        = 0 '[PAD]'
0.00.034.137 I print_info: MASK token       = 103 '[MASK]'
0.00.034.137 I print_info: LF token         = 0 '[PAD]'
0.00.034.137 I print_info: max token length = 21
0.00.034.138 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.037.414 I load_tensors: offloading 12 repeating layers to GPU
0.00.037.416 I load_tensors: offloading output layer to GPU
0.00.037.416 I load_tensors: offloaded 13/13 layers to GPU
0.00.037.440 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.037.442 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.037.723 I llama_init_from_model: n_seq_max     = 1
0.00.037.725 I llama_init_from_model: n_ctx         = 512
0.00.037.725 I llama_init_from_model: n_ctx_per_seq = 512
0.00.037.725 I llama_init_from_model: n_batch       = 2048
0.00.037.726 I llama_init_from_model: n_ubatch      = 2048
0.00.037.726 I llama_init_from_model: flash_attn    = 0
0.00.037.726 I llama_init_from_model: freq_base     = 10000.0
0.00.037.727 I llama_init_from_model: freq_scale    = 1
0.00.037.728 I ggml_metal_init: allocating
0.00.037.741 I ggml_metal_init: found device: Apple M4
0.00.037.749 I ggml_metal_init: picking default device: Apple M4
0.00.038.520 I ggml_metal_init: using embedded metal library
0.00.042.487 I ggml_metal_init: GPU name:   Apple M4
0.00.042.490 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.042.491 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.042.491 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.042.492 I ggml_metal_init: simdgroup reduction   = true
0.00.042.492 I ggml_metal_init: simdgroup matrix mul. = true
0.00.042.492 I ggml_metal_init: has residency sets    = true
0.00.042.492 I ggml_metal_init: has bfloat            = true
0.00.042.492 I ggml_metal_init: use bfloat            = true
0.00.042.493 I ggml_metal_init: hasUnifiedMemory      = true
0.00.042.493 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.054.524 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.055.188 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.055.190 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.055.211 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.056.420 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.056.421 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.056.421 I llama_init_from_model: graph nodes  = 429
0.00.056.422 I llama_init_from_model: graph splits = 2
0.00.056.423 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.056.423 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.062.170 I 
0.00.062.184 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.062.854 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.068.061 I llama_perf_context_print:        load time =      46.63 ms
0.00.068.062 I llama_perf_context_print: prompt eval time =       5.06 ms /     9 tokens (    0.56 ms per token,  1779.71 tokens per second)
0.00.068.063 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.068.064 I llama_perf_context_print:       total time =       5.89 ms /    10 tokens
0.00.068.209 I ggml_metal_free: deallocating

real	0m0.248s
user	0m0.049s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.123 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.808 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.812 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.813 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.814 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.815 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.816 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.816 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.817 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.817 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.817 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.818 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.818 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.820 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.820 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.821 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.821 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.821 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.822 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.215 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.876 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.878 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.878 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.878 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.879 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.879 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.879 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.880 I llama_model_loader: - type  f32:  124 tensors
0.00.014.880 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.881 I print_info: file format = GGUF V3 (latest)
0.00.014.881 I print_info: file type   = Q8_0
0.00.014.882 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.263 I load: special tokens cache size = 5
0.00.018.575 I load: token to piece cache size = 0.2032 MB
0.00.018.578 I print_info: arch             = bert
0.00.018.578 I print_info: vocab_only       = 0
0.00.018.578 I print_info: n_ctx_train      = 512
0.00.018.579 I print_info: n_embd           = 384
0.00.018.579 I print_info: n_layer          = 12
0.00.018.582 I print_info: n_head           = 12
0.00.018.582 I print_info: n_head_kv        = 12
0.00.018.583 I print_info: n_rot            = 32
0.00.018.583 I print_info: n_swa            = 0
0.00.018.583 I print_info: n_embd_head_k    = 32
0.00.018.585 I print_info: n_embd_head_v    = 32
0.00.018.586 I print_info: n_gqa            = 1
0.00.018.587 I print_info: n_embd_k_gqa     = 384
0.00.018.587 I print_info: n_embd_v_gqa     = 384
0.00.018.588 I print_info: f_norm_eps       = 1.0e-12
0.00.018.588 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.592 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.594 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.594 I print_info: f_logit_scale    = 0.0e+00
0.00.018.594 I print_info: n_ff             = 1536
0.00.018.595 I print_info: n_expert         = 0
0.00.018.595 I print_info: n_expert_used    = 0
0.00.018.595 I print_info: causal attn      = 0
0.00.018.595 I print_info: pooling type     = 2
0.00.018.595 I print_info: rope type        = 2
0.00.018.595 I print_info: rope scaling     = linear
0.00.018.595 I print_info: freq_base_train  = 10000.0
0.00.018.596 I print_info: freq_scale_train = 1
0.00.018.596 I print_info: n_ctx_orig_yarn  = 512
0.00.018.596 I print_info: rope_finetuned   = unknown
0.00.018.596 I print_info: ssm_d_conv       = 0
0.00.018.596 I print_info: ssm_d_inner      = 0
0.00.018.597 I print_info: ssm_d_state      = 0
0.00.018.597 I print_info: ssm_dt_rank      = 0
0.00.018.597 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.597 I print_info: model type       = 33M
0.00.018.597 I print_info: model params     = 33.21 M
0.00.018.597 I print_info: general.name     = Bge Small
0.00.018.598 I print_info: vocab type       = WPM
0.00.018.598 I print_info: n_vocab          = 30522
0.00.018.598 I print_info: n_merges         = 0
0.00.018.599 I print_info: BOS token        = 101 '[CLS]'
0.00.018.599 I print_info: UNK token        = 100 '[UNK]'
0.00.018.599 I print_info: SEP token        = 102 '[SEP]'
0.00.018.599 I print_info: PAD token        = 0 '[PAD]'
0.00.018.599 I print_info: MASK token       = 103 '[MASK]'
0.00.018.599 I print_info: LF token         = 0 '[PAD]'
0.00.018.599 I print_info: max token length = 21
0.00.018.600 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.323 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.324 I load_tensors: offloading output layer to GPU
0.00.020.324 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.331 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.331 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.522 I llama_init_from_model: n_seq_max     = 1
0.00.020.523 I llama_init_from_model: n_ctx         = 512
0.00.020.524 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.524 I llama_init_from_model: n_batch       = 2048
0.00.020.524 I llama_init_from_model: n_ubatch      = 2048
0.00.020.524 I llama_init_from_model: flash_attn    = 0
0.00.020.525 I llama_init_from_model: freq_base     = 10000.0
0.00.020.525 I llama_init_from_model: freq_scale    = 1
0.00.020.525 I ggml_metal_init: allocating
0.00.020.529 I ggml_metal_init: found device: Apple M4
0.00.020.533 I ggml_metal_init: picking default device: Apple M4
0.00.021.046 I ggml_metal_init: using embedded metal library
0.00.023.587 I ggml_metal_init: GPU name:   Apple M4
0.00.023.590 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.590 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.590 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.591 I ggml_metal_init: simdgroup reduction   = true
0.00.023.591 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.591 I ggml_metal_init: has residency sets    = true
0.00.023.591 I ggml_metal_init: has bfloat            = true
0.00.023.591 I ggml_metal_init: use bfloat            = true
0.00.023.592 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.592 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.866 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.462 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.464 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.476 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.439 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.441 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.441 I llama_init_from_model: graph nodes  = 429
0.00.035.441 I llama_init_from_model: graph splits = 2
0.00.035.443 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.443 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.525 I 
0.00.039.542 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.057 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.452 I llama_perf_context_print:        load time =      30.39 ms
0.00.044.453 I llama_perf_context_print: prompt eval time =       4.27 ms /     9 tokens (    0.47 ms per token,  2109.21 tokens per second)
0.00.044.454 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.454 I llama_perf_context_print:       total time =       4.93 ms /    10 tokens
0.00.044.661 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.030s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.318 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.402 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.077 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.082 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.085 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.086 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.087 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.087 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.088 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.089 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.090 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.091 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.091 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.092 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.095 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.095 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.096 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.097 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.097 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.041.953 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.043.914 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.252 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.048.254 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.254 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.048.255 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.048.255 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.048.255 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.048.256 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.048.256 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.048.256 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.048.257 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.048.257 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.048.258 I llama_model_loader: - type  f32:   40 tensors
0.00.048.258 I llama_model_loader: - type  f16:   30 tensors
0.00.048.259 I print_info: file format = GGUF V3 (latest)
0.00.048.259 I print_info: file type   = F16
0.00.048.261 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.052.535 W load: empty token at index 5
0.00.057.594 W load: model vocab missing newline token, using special_pad_id instead
0.00.059.108 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.059.141 I load: special tokens cache size = 5
0.00.327.617 I load: token to piece cache size = 1.5060 MB
0.00.327.624 I print_info: arch             = jina-bert-v2
0.00.327.624 I print_info: vocab_only       = 0
0.00.327.624 I print_info: n_ctx_train      = 8192
0.00.327.625 I print_info: n_embd           = 384
0.00.327.625 I print_info: n_layer          = 4
0.00.327.629 I print_info: n_head           = 12
0.00.327.630 I print_info: n_head_kv        = 12
0.00.327.630 I print_info: n_rot            = 32
0.00.327.630 I print_info: n_swa            = 0
0.00.327.630 I print_info: n_embd_head_k    = 32
0.00.327.630 I print_info: n_embd_head_v    = 32
0.00.327.631 I print_info: n_gqa            = 1
0.00.327.631 I print_info: n_embd_k_gqa     = 384
0.00.327.631 I print_info: n_embd_v_gqa     = 384
0.00.327.632 I print_info: f_norm_eps       = 1.0e-12
0.00.327.633 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.327.634 I print_info: f_clamp_kqv      = 0.0e+00
0.00.327.634 I print_info: f_max_alibi_bias = 8.0e+00
0.00.327.635 I print_info: f_logit_scale    = 0.0e+00
0.00.327.635 I print_info: n_ff             = 1536
0.00.327.635 I print_info: n_expert         = 0
0.00.327.640 I print_info: n_expert_used    = 0
0.00.327.640 I print_info: causal attn      = 0
0.00.327.641 I print_info: pooling type     = -1
0.00.327.642 I print_info: rope type        = -1
0.00.327.643 I print_info: rope scaling     = linear
0.00.327.643 I print_info: freq_base_train  = 10000.0
0.00.327.643 I print_info: freq_scale_train = 1
0.00.327.643 I print_info: n_ctx_orig_yarn  = 8192
0.00.327.644 I print_info: rope_finetuned   = unknown
0.00.327.644 I print_info: ssm_d_conv       = 0
0.00.327.644 I print_info: ssm_d_inner      = 0
0.00.327.644 I print_info: ssm_d_state      = 0
0.00.327.644 I print_info: ssm_dt_rank      = 0
0.00.327.644 I print_info: ssm_dt_b_c_rms   = 0
0.00.327.644 I print_info: model type       = 33M
0.00.327.645 I print_info: model params     = 32.90 M
0.00.327.645 I print_info: general.name     = Jina Bert Implementation
0.00.327.646 I print_info: vocab type       = BPE
0.00.327.646 I print_info: n_vocab          = 61056
0.00.327.646 I print_info: n_merges         = 39382
0.00.327.646 I print_info: BOS token        = 0 '<s>'
0.00.327.646 I print_info: EOS token        = 2 '</s>'
0.00.327.647 I print_info: UNK token        = 3 '<unk>'
0.00.327.647 I print_info: SEP token        = 2 '</s>'
0.00.327.647 I print_info: PAD token        = 1 '<pad>'
0.00.327.647 I print_info: MASK token       = 4 '<mask>'
0.00.327.647 I print_info: EOG token        = 2 '</s>'
0.00.327.647 I print_info: max token length = 45
0.00.327.648 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.328.817 I load_tensors: offloading 4 repeating layers to GPU
0.00.328.818 I load_tensors: offloading output layer to GPU
0.00.328.818 I load_tensors: offloaded 5/5 layers to GPU
0.00.328.833 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.328.834 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.328.995 I llama_init_from_model: n_seq_max     = 1
0.00.328.996 I llama_init_from_model: n_ctx         = 8192
0.00.328.996 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.328.996 I llama_init_from_model: n_batch       = 2048
0.00.328.996 I llama_init_from_model: n_ubatch      = 2048
0.00.328.997 I llama_init_from_model: flash_attn    = 0
0.00.328.997 I llama_init_from_model: freq_base     = 10000.0
0.00.328.997 I llama_init_from_model: freq_scale    = 1
0.00.328.998 I ggml_metal_init: allocating
0.00.329.001 I ggml_metal_init: found device: Apple M4
0.00.329.004 I ggml_metal_init: picking default device: Apple M4
0.00.329.566 I ggml_metal_init: using embedded metal library
0.00.332.310 I ggml_metal_init: GPU name:   Apple M4
0.00.332.311 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.312 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.312 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.313 I ggml_metal_init: simdgroup reduction   = true
0.00.332.313 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.313 I ggml_metal_init: has residency sets    = true
0.00.332.313 I ggml_metal_init: has bfloat            = true
0.00.332.313 I ggml_metal_init: use bfloat            = true
0.00.332.314 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.315 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.341.360 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.344.368 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.344.378 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.344.396 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.351.158 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.351.161 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.351.161 I llama_init_from_model: graph nodes  = 154
0.00.351.162 I llama_init_from_model: graph splits = 2
0.00.351.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.358.263 I 
0.00.358.282 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.358.377 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.358.378 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.358.381 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.358.381 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.358.386 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.358.386 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.358.897 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.362.382 I llama_perf_context_print:        load time =     335.83 ms
0.00.362.383 I llama_perf_context_print: prompt eval time =       3.48 ms /    62 tokens (    0.06 ms per token, 17836.59 tokens per second)
0.00.362.387 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.362.387 I llama_perf_context_print:       total time =       4.12 ms /    63 tokens
0.00.362.596 I ggml_metal_free: deallocating

real	0m1.071s
user	0m0.334s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.106 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.257 I main: llama backend init
0.00.000.263 I main: load the model and apply lora adapter, if any
0.00.057.234 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.069.847 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.069.864 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.069.867 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.069.868 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.069.869 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.069.870 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.069.870 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.069.873 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.069.879 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.069.880 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.069.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.069.881 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.069.882 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.069.883 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.069.886 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.069.887 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.069.887 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.077.006 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.079.199 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.086.131 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.086.140 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.086.140 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.086.141 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.086.141 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.086.143 I llama_model_loader: - type  f32:  194 tensors
0.00.086.144 I llama_model_loader: - type  f16:   98 tensors
0.00.086.146 I print_info: file format = GGUF V3 (latest)
0.00.086.148 I print_info: file type   = all F32 (guessed)
0.00.086.150 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.098.439 I load: special tokens cache size = 25
0.00.105.683 I load: token to piece cache size = 0.2984 MB
0.00.105.687 I print_info: arch             = gptneox
0.00.105.687 I print_info: vocab_only       = 0
0.00.105.687 I print_info: n_ctx_train      = 2048
0.00.105.687 I print_info: n_embd           = 2048
0.00.105.688 I print_info: n_layer          = 24
0.00.105.692 I print_info: n_head           = 16
0.00.105.692 I print_info: n_head_kv        = 16
0.00.105.693 I print_info: n_rot            = 32
0.00.105.694 I print_info: n_swa            = 0
0.00.105.694 I print_info: n_embd_head_k    = 128
0.00.105.694 I print_info: n_embd_head_v    = 128
0.00.105.695 I print_info: n_gqa            = 1
0.00.105.696 I print_info: n_embd_k_gqa     = 2048
0.00.105.696 I print_info: n_embd_v_gqa     = 2048
0.00.105.697 I print_info: f_norm_eps       = 1.0e-05
0.00.105.697 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.105.697 I print_info: f_clamp_kqv      = 0.0e+00
0.00.105.698 I print_info: f_max_alibi_bias = 0.0e+00
0.00.105.698 I print_info: f_logit_scale    = 0.0e+00
0.00.105.698 I print_info: n_ff             = 8192
0.00.105.699 I print_info: n_expert         = 0
0.00.105.699 I print_info: n_expert_used    = 0
0.00.105.699 I print_info: causal attn      = 1
0.00.105.699 I print_info: pooling type     = 0
0.00.105.699 I print_info: rope type        = 2
0.00.105.701 I print_info: rope scaling     = linear
0.00.105.701 I print_info: freq_base_train  = 10000.0
0.00.105.701 I print_info: freq_scale_train = 1
0.00.105.702 I print_info: n_ctx_orig_yarn  = 2048
0.00.105.702 I print_info: rope_finetuned   = unknown
0.00.105.702 I print_info: ssm_d_conv       = 0
0.00.105.702 I print_info: ssm_d_inner      = 0
0.00.105.702 I print_info: ssm_d_state      = 0
0.00.105.702 I print_info: ssm_dt_rank      = 0
0.00.105.702 I print_info: ssm_dt_b_c_rms   = 0
0.00.105.704 I print_info: model type       = 1.4B
0.00.105.704 I print_info: model params     = 1.41 B
0.00.105.704 I print_info: general.name     = 1.4B
0.00.105.705 I print_info: vocab type       = BPE
0.00.105.705 I print_info: n_vocab          = 50304
0.00.105.705 I print_info: n_merges         = 50009
0.00.105.706 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.105.706 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.105.706 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.105.706 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.105.706 I print_info: LF token         = 187 ''
0.00.105.707 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.105.707 I print_info: max token length = 1024
0.00.105.707 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.153.259 I load_tensors: offloading 24 repeating layers to GPU
0.00.153.263 I load_tensors: offloading output layer to GPU
0.00.153.263 I load_tensors: offloaded 25/25 layers to GPU
0.00.153.288 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.153.289 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.153.872 I llama_init_from_model: n_seq_max     = 1
0.00.153.873 I llama_init_from_model: n_ctx         = 2048
0.00.153.874 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.153.874 I llama_init_from_model: n_batch       = 2048
0.00.153.874 I llama_init_from_model: n_ubatch      = 512
0.00.153.874 I llama_init_from_model: flash_attn    = 0
0.00.153.875 I llama_init_from_model: freq_base     = 10000.0
0.00.153.875 I llama_init_from_model: freq_scale    = 1
0.00.153.876 I ggml_metal_init: allocating
0.00.153.895 I ggml_metal_init: found device: Apple M4
0.00.153.899 I ggml_metal_init: picking default device: Apple M4
0.00.154.538 I ggml_metal_init: using embedded metal library
0.00.194.471 I ggml_metal_init: GPU name:   Apple M4
0.00.194.475 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.194.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.194.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.194.476 I ggml_metal_init: simdgroup reduction   = true
0.00.194.476 I ggml_metal_init: simdgroup matrix mul. = true
0.00.194.477 I ggml_metal_init: has residency sets    = true
0.00.194.477 I ggml_metal_init: has bfloat            = true
0.00.194.477 I ggml_metal_init: use bfloat            = true
0.00.194.478 I ggml_metal_init: hasUnifiedMemory      = true
0.00.194.481 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.437.723 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.482.866 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.482.873 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.482.916 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.486.404 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.486.405 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.486.406 I llama_init_from_model: graph nodes  = 967
0.00.486.406 I llama_init_from_model: graph splits = 2
0.00.486.411 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.486.539 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.486.540 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.552.270 I main: llama threadpool init, n_threads = 4
0.00.552.313 I 
0.00.552.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.552.329 I 
0.00.552.501 I sampler seed: 1234
0.00.552.506 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.552.530 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.552.532 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.552.532 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.392.125 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58244.46 tokens per second)
0.02.392.126 I llama_perf_context_print:        load time =     494.19 ms
0.02.392.127 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.48 tokens per second)
0.02.392.127 I llama_perf_context_print:        eval time =    1793.16 ms /    63 runs   (   28.46 ms per token,    35.13 tokens per second)
0.02.392.128 I llama_perf_context_print:       total time =    1840.67 ms /    70 tokens
0.02.392.401 I ggml_metal_free: deallocating

real	0m2.715s
user	0m0.131s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.779 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.009 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.037.465 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.469 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.472 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.472 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.478 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.478 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.479 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.480 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.480 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.481 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.481 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.481 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.482 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.484 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.484 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.485 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.044.990 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.047.043 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.416 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.053.418 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.419 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.419 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.420 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.420 I llama_model_loader: - type  f32:  194 tensors
0.00.053.420 I llama_model_loader: - type  f16:   98 tensors
0.00.053.421 I print_info: file format = GGUF V3 (latest)
0.00.053.422 I print_info: file type   = all F32 (guessed)
0.00.053.423 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.065.505 I load: special tokens cache size = 25
0.00.073.468 I load: token to piece cache size = 0.2984 MB
0.00.073.471 I print_info: arch             = gptneox
0.00.073.471 I print_info: vocab_only       = 0
0.00.073.472 I print_info: n_ctx_train      = 2048
0.00.073.472 I print_info: n_embd           = 2048
0.00.073.472 I print_info: n_layer          = 24
0.00.073.475 I print_info: n_head           = 16
0.00.073.476 I print_info: n_head_kv        = 16
0.00.073.476 I print_info: n_rot            = 32
0.00.073.476 I print_info: n_swa            = 0
0.00.073.476 I print_info: n_embd_head_k    = 128
0.00.073.478 I print_info: n_embd_head_v    = 128
0.00.073.479 I print_info: n_gqa            = 1
0.00.073.480 I print_info: n_embd_k_gqa     = 2048
0.00.073.480 I print_info: n_embd_v_gqa     = 2048
0.00.073.483 I print_info: f_norm_eps       = 1.0e-05
0.00.073.483 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.073.483 I print_info: f_clamp_kqv      = 0.0e+00
0.00.073.483 I print_info: f_max_alibi_bias = 0.0e+00
0.00.073.483 I print_info: f_logit_scale    = 0.0e+00
0.00.073.484 I print_info: n_ff             = 8192
0.00.073.484 I print_info: n_expert         = 0
0.00.073.485 I print_info: n_expert_used    = 0
0.00.073.485 I print_info: causal attn      = 1
0.00.073.485 I print_info: pooling type     = 0
0.00.073.485 I print_info: rope type        = 2
0.00.073.485 I print_info: rope scaling     = linear
0.00.073.486 I print_info: freq_base_train  = 10000.0
0.00.073.486 I print_info: freq_scale_train = 1
0.00.073.486 I print_info: n_ctx_orig_yarn  = 2048
0.00.073.486 I print_info: rope_finetuned   = unknown
0.00.073.487 I print_info: ssm_d_conv       = 0
0.00.073.487 I print_info: ssm_d_inner      = 0
0.00.073.487 I print_info: ssm_d_state      = 0
0.00.073.487 I print_info: ssm_dt_rank      = 0
0.00.073.487 I print_info: ssm_dt_b_c_rms   = 0
0.00.073.487 I print_info: model type       = 1.4B
0.00.073.488 I print_info: model params     = 1.41 B
0.00.073.488 I print_info: general.name     = 1.4B
0.00.073.489 I print_info: vocab type       = BPE
0.00.073.489 I print_info: n_vocab          = 50304
0.00.073.489 I print_info: n_merges         = 50009
0.00.073.489 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.073.489 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.073.489 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.073.490 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.073.490 I print_info: LF token         = 187 ''
0.00.073.490 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.073.490 I print_info: max token length = 1024
0.00.073.491 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.431.087 I load_tensors: offloading 24 repeating layers to GPU
0.01.431.095 I load_tensors: offloading output layer to GPU
0.01.431.096 I load_tensors: offloaded 25/25 layers to GPU
0.01.431.119 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.431.121 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.432.019 I llama_init_from_model: n_seq_max     = 1
0.01.432.020 I llama_init_from_model: n_ctx         = 128
0.01.432.021 I llama_init_from_model: n_ctx_per_seq = 128
0.01.432.021 I llama_init_from_model: n_batch       = 128
0.01.432.021 I llama_init_from_model: n_ubatch      = 128
0.01.432.021 I llama_init_from_model: flash_attn    = 0
0.01.432.022 I llama_init_from_model: freq_base     = 10000.0
0.01.432.022 I llama_init_from_model: freq_scale    = 1
0.01.432.023 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.432.024 I ggml_metal_init: allocating
0.01.432.052 I ggml_metal_init: found device: Apple M4
0.01.432.058 I ggml_metal_init: picking default device: Apple M4
0.01.433.020 I ggml_metal_init: using embedded metal library
0.01.436.810 I ggml_metal_init: GPU name:   Apple M4
0.01.436.812 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.436.812 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.436.813 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.436.813 I ggml_metal_init: simdgroup reduction   = true
0.01.436.814 I ggml_metal_init: simdgroup matrix mul. = true
0.01.436.814 I ggml_metal_init: has residency sets    = true
0.01.436.814 I ggml_metal_init: has bfloat            = true
0.01.436.814 I ggml_metal_init: use bfloat            = true
0.01.436.814 I ggml_metal_init: hasUnifiedMemory      = true
0.01.436.815 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.447.138 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.448.848 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.448.850 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.448.875 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.450.631 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.450.632 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.450.633 I llama_init_from_model: graph nodes  = 967
0.01.450.633 I llama_init_from_model: graph splits = 2
0.01.450.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.450.634 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.485.451 I 
0.01.485.479 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.485.482 I perplexity: tokenizing the input ..
0.01.490.526 I perplexity: tokenization took 5.042 ms
0.01.490.530 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.622.474 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.623.889 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.623.921 I llama_perf_context_print:        load time =    1463.42 ms
0.01.623.923 I llama_perf_context_print: prompt eval time =     131.63 ms /   128 tokens (    1.03 ms per token,   972.39 tokens per second)
0.01.623.924 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.623.924 I llama_perf_context_print:       total time =     138.47 ms /   129 tokens
0.01.624.289 I ggml_metal_free: deallocating

real	0m1.810s
user	0m0.096s
sys	0m0.261s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.093 I main: load the model and apply lora adapter, if any
0.00.010.370 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.026.728 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.026.736 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.026.740 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.026.741 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.026.741 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.026.741 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.026.742 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.026.743 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.026.743 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.026.743 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.026.744 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.026.744 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.026.744 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.026.745 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.026.749 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.026.749 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.026.750 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.643 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.703 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.546 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.035.548 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.548 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.549 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.549 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.549 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.035.550 I llama_model_loader: - type  f32:  194 tensors
0.00.035.550 I llama_model_loader: - type q8_0:   98 tensors
0.00.035.551 I print_info: file format = GGUF V3 (latest)
0.00.035.552 I print_info: file type   = Q8_0
0.00.035.553 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.043.825 I load: special tokens cache size = 25
0.00.049.857 I load: token to piece cache size = 0.2984 MB
0.00.049.861 I print_info: arch             = gptneox
0.00.049.862 I print_info: vocab_only       = 0
0.00.049.862 I print_info: n_ctx_train      = 2048
0.00.049.862 I print_info: n_embd           = 2048
0.00.049.862 I print_info: n_layer          = 24
0.00.049.869 I print_info: n_head           = 16
0.00.049.870 I print_info: n_head_kv        = 16
0.00.049.870 I print_info: n_rot            = 32
0.00.049.870 I print_info: n_swa            = 0
0.00.049.870 I print_info: n_embd_head_k    = 128
0.00.049.873 I print_info: n_embd_head_v    = 128
0.00.049.874 I print_info: n_gqa            = 1
0.00.049.874 I print_info: n_embd_k_gqa     = 2048
0.00.049.875 I print_info: n_embd_v_gqa     = 2048
0.00.049.876 I print_info: f_norm_eps       = 1.0e-05
0.00.049.876 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.876 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.877 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.877 I print_info: f_logit_scale    = 0.0e+00
0.00.049.878 I print_info: n_ff             = 8192
0.00.049.878 I print_info: n_expert         = 0
0.00.049.878 I print_info: n_expert_used    = 0
0.00.049.878 I print_info: causal attn      = 1
0.00.049.880 I print_info: pooling type     = 0
0.00.049.880 I print_info: rope type        = 2
0.00.049.881 I print_info: rope scaling     = linear
0.00.049.881 I print_info: freq_base_train  = 10000.0
0.00.049.881 I print_info: freq_scale_train = 1
0.00.049.881 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.882 I print_info: rope_finetuned   = unknown
0.00.049.882 I print_info: ssm_d_conv       = 0
0.00.049.882 I print_info: ssm_d_inner      = 0
0.00.049.882 I print_info: ssm_d_state      = 0
0.00.049.882 I print_info: ssm_dt_rank      = 0
0.00.049.882 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.883 I print_info: model type       = 1.4B
0.00.049.883 I print_info: model params     = 1.41 B
0.00.049.883 I print_info: general.name     = 1.4B
0.00.049.884 I print_info: vocab type       = BPE
0.00.049.884 I print_info: n_vocab          = 50304
0.00.049.884 I print_info: n_merges         = 50009
0.00.049.884 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.884 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.885 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.885 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.889 I print_info: LF token         = 187 ''
0.00.049.890 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.890 I print_info: max token length = 1024
0.00.049.890 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.970.849 I load_tensors: offloading 24 repeating layers to GPU
0.00.970.854 I load_tensors: offloading output layer to GPU
0.00.970.855 I load_tensors: offloaded 25/25 layers to GPU
0.00.970.883 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.970.886 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.972.044 I llama_init_from_model: n_seq_max     = 1
0.00.972.045 I llama_init_from_model: n_ctx         = 2048
0.00.972.046 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.972.046 I llama_init_from_model: n_batch       = 2048
0.00.972.047 I llama_init_from_model: n_ubatch      = 512
0.00.972.047 I llama_init_from_model: flash_attn    = 0
0.00.972.048 I llama_init_from_model: freq_base     = 10000.0
0.00.972.048 I llama_init_from_model: freq_scale    = 1
0.00.972.049 I ggml_metal_init: allocating
0.00.972.064 I ggml_metal_init: found device: Apple M4
0.00.972.073 I ggml_metal_init: picking default device: Apple M4
0.00.973.377 I ggml_metal_init: using embedded metal library
0.00.978.835 I ggml_metal_init: GPU name:   Apple M4
0.00.978.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.978.842 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.978.843 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.978.843 I ggml_metal_init: simdgroup reduction   = true
0.00.978.844 I ggml_metal_init: simdgroup matrix mul. = true
0.00.978.844 I ggml_metal_init: has residency sets    = true
0.00.978.844 I ggml_metal_init: has bfloat            = true
0.00.978.844 I ggml_metal_init: use bfloat            = true
0.00.978.845 I ggml_metal_init: hasUnifiedMemory      = true
0.00.978.846 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.994.254 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.042.800 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.042.808 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.042.891 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.047.380 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.047.382 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.047.382 I llama_init_from_model: graph nodes  = 967
0.01.047.382 I llama_init_from_model: graph splits = 2
0.01.047.387 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.047.520 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.047.521 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.104.100 I main: llama threadpool init, n_threads = 4
0.01.104.141 I 
0.01.104.158 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.104.158 I 
0.01.104.332 I sampler seed: 1234
0.01.104.337 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.104.348 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.104.348 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.104.348 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.196.866 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.02.196.867 I llama_perf_context_print:        load time =    1093.02 ms
0.02.196.868 I llama_perf_context_print: prompt eval time =      47.81 ms /     7 tokens (    6.83 ms per token,   146.42 tokens per second)
0.02.196.869 I llama_perf_context_print:        eval time =    1041.78 ms /    63 runs   (   16.54 ms per token,    60.47 tokens per second)
0.02.196.869 I llama_perf_context_print:       total time =    1093.47 ms /    70 tokens
0.02.197.185 I ggml_metal_free: deallocating

real	0m2.217s
user	0m0.107s
sys	0m0.258s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.982 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.585 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.587 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.670 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.795 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.750 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.752 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.753 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.754 I llama_model_loader: - type  f32:  194 tensors
0.00.025.755 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.755 I print_info: file format = GGUF V3 (latest)
0.00.025.756 I print_info: file type   = Q8_0
0.00.025.757 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.066 I load: special tokens cache size = 25
0.00.040.098 I load: token to piece cache size = 0.2984 MB
0.00.040.102 I print_info: arch             = gptneox
0.00.040.103 I print_info: vocab_only       = 0
0.00.040.103 I print_info: n_ctx_train      = 2048
0.00.040.103 I print_info: n_embd           = 2048
0.00.040.103 I print_info: n_layer          = 24
0.00.040.108 I print_info: n_head           = 16
0.00.040.109 I print_info: n_head_kv        = 16
0.00.040.111 I print_info: n_rot            = 32
0.00.040.112 I print_info: n_swa            = 0
0.00.040.112 I print_info: n_embd_head_k    = 128
0.00.040.112 I print_info: n_embd_head_v    = 128
0.00.040.112 I print_info: n_gqa            = 1
0.00.040.113 I print_info: n_embd_k_gqa     = 2048
0.00.040.114 I print_info: n_embd_v_gqa     = 2048
0.00.040.114 I print_info: f_norm_eps       = 1.0e-05
0.00.040.115 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.115 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.115 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.116 I print_info: f_logit_scale    = 0.0e+00
0.00.040.116 I print_info: n_ff             = 8192
0.00.040.117 I print_info: n_expert         = 0
0.00.040.117 I print_info: n_expert_used    = 0
0.00.040.118 I print_info: causal attn      = 1
0.00.040.118 I print_info: pooling type     = 0
0.00.040.118 I print_info: rope type        = 2
0.00.040.118 I print_info: rope scaling     = linear
0.00.040.118 I print_info: freq_base_train  = 10000.0
0.00.040.119 I print_info: freq_scale_train = 1
0.00.040.119 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.119 I print_info: rope_finetuned   = unknown
0.00.040.119 I print_info: ssm_d_conv       = 0
0.00.040.119 I print_info: ssm_d_inner      = 0
0.00.040.119 I print_info: ssm_d_state      = 0
0.00.040.120 I print_info: ssm_dt_rank      = 0
0.00.040.120 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.120 I print_info: model type       = 1.4B
0.00.040.120 I print_info: model params     = 1.41 B
0.00.040.120 I print_info: general.name     = 1.4B
0.00.040.121 I print_info: vocab type       = BPE
0.00.040.121 I print_info: n_vocab          = 50304
0.00.040.122 I print_info: n_merges         = 50009
0.00.040.122 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.128 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.129 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.129 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.130 I print_info: LF token         = 187 ''
0.00.040.132 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.132 I print_info: max token length = 1024
0.00.040.133 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.831.431 I load_tensors: offloading 24 repeating layers to GPU
0.00.831.439 I load_tensors: offloading output layer to GPU
0.00.831.439 I load_tensors: offloaded 25/25 layers to GPU
0.00.831.468 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.831.471 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.832.977 I llama_init_from_model: n_seq_max     = 1
0.00.832.979 I llama_init_from_model: n_ctx         = 128
0.00.832.980 I llama_init_from_model: n_ctx_per_seq = 128
0.00.832.980 I llama_init_from_model: n_batch       = 128
0.00.832.980 I llama_init_from_model: n_ubatch      = 128
0.00.832.981 I llama_init_from_model: flash_attn    = 0
0.00.832.982 I llama_init_from_model: freq_base     = 10000.0
0.00.832.982 I llama_init_from_model: freq_scale    = 1
0.00.832.983 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.832.984 I ggml_metal_init: allocating
0.00.833.039 I ggml_metal_init: found device: Apple M4
0.00.833.051 I ggml_metal_init: picking default device: Apple M4
0.00.834.372 I ggml_metal_init: using embedded metal library
0.00.839.708 I ggml_metal_init: GPU name:   Apple M4
0.00.839.712 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.839.713 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.839.713 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.839.714 I ggml_metal_init: simdgroup reduction   = true
0.00.839.714 I ggml_metal_init: simdgroup matrix mul. = true
0.00.839.715 I ggml_metal_init: has residency sets    = true
0.00.839.715 I ggml_metal_init: has bfloat            = true
0.00.839.715 I ggml_metal_init: use bfloat            = true
0.00.839.716 I ggml_metal_init: hasUnifiedMemory      = true
0.00.839.724 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.854.761 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.858.058 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.858.064 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.858.111 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.861.012 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.861.014 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.861.014 I llama_init_from_model: graph nodes  = 967
0.00.861.014 I llama_init_from_model: graph splits = 2
0.00.861.016 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.861.017 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.891.562 I 
0.00.891.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.891.628 I perplexity: tokenizing the input ..
0.00.898.681 I perplexity: tokenization took 7.049 ms
0.00.898.690 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.038.021 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.039.349 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.039.373 I llama_perf_context_print:        load time =     882.57 ms
0.01.039.374 I llama_perf_context_print: prompt eval time =     138.39 ms /   128 tokens (    1.08 ms per token,   924.94 tokens per second)
0.01.039.374 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.039.374 I llama_perf_context_print:       total time =     147.81 ms /   129 tokens
0.01.039.729 I ggml_metal_free: deallocating

real	0m1.053s
user	0m0.077s
sys	0m0.180s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.011.084 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.445 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.451 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.453 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.453 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.454 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.454 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.454 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.455 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.456 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.456 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.456 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.457 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.457 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.458 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.459 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.460 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.460 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.387 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.496 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.330 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.331 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.331 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.332 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.332 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.332 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.333 I llama_model_loader: - type  f32:  194 tensors
0.00.027.333 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.333 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.334 I print_info: file format = GGUF V3 (latest)
0.00.027.335 I print_info: file type   = Q4_0
0.00.027.336 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.701 I load: special tokens cache size = 25
0.00.041.582 I load: token to piece cache size = 0.2984 MB
0.00.041.585 I print_info: arch             = gptneox
0.00.041.586 I print_info: vocab_only       = 0
0.00.041.586 I print_info: n_ctx_train      = 2048
0.00.041.586 I print_info: n_embd           = 2048
0.00.041.586 I print_info: n_layer          = 24
0.00.041.592 I print_info: n_head           = 16
0.00.041.596 I print_info: n_head_kv        = 16
0.00.041.596 I print_info: n_rot            = 32
0.00.041.596 I print_info: n_swa            = 0
0.00.041.596 I print_info: n_embd_head_k    = 128
0.00.041.597 I print_info: n_embd_head_v    = 128
0.00.041.597 I print_info: n_gqa            = 1
0.00.041.598 I print_info: n_embd_k_gqa     = 2048
0.00.041.600 I print_info: n_embd_v_gqa     = 2048
0.00.041.601 I print_info: f_norm_eps       = 1.0e-05
0.00.041.601 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.601 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.601 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.602 I print_info: f_logit_scale    = 0.0e+00
0.00.041.602 I print_info: n_ff             = 8192
0.00.041.603 I print_info: n_expert         = 0
0.00.041.603 I print_info: n_expert_used    = 0
0.00.041.603 I print_info: causal attn      = 1
0.00.041.603 I print_info: pooling type     = 0
0.00.041.607 I print_info: rope type        = 2
0.00.041.608 I print_info: rope scaling     = linear
0.00.041.613 I print_info: freq_base_train  = 10000.0
0.00.041.614 I print_info: freq_scale_train = 1
0.00.041.614 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.614 I print_info: rope_finetuned   = unknown
0.00.041.614 I print_info: ssm_d_conv       = 0
0.00.041.615 I print_info: ssm_d_inner      = 0
0.00.041.615 I print_info: ssm_d_state      = 0
0.00.041.615 I print_info: ssm_dt_rank      = 0
0.00.041.615 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.615 I print_info: model type       = 1.4B
0.00.041.616 I print_info: model params     = 1.41 B
0.00.041.616 I print_info: general.name     = 1.4B
0.00.041.617 I print_info: vocab type       = BPE
0.00.041.617 I print_info: n_vocab          = 50304
0.00.041.617 I print_info: n_merges         = 50009
0.00.041.617 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.617 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.617 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.619 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.619 I print_info: LF token         = 187 ''
0.00.041.619 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.621 I print_info: max token length = 1024
0.00.041.621 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.555.374 I load_tensors: offloading 24 repeating layers to GPU
0.00.555.389 I load_tensors: offloading output layer to GPU
0.00.555.390 I load_tensors: offloaded 25/25 layers to GPU
0.00.555.423 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.555.425 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.557.190 I llama_init_from_model: n_seq_max     = 1
0.00.557.193 I llama_init_from_model: n_ctx         = 2048
0.00.557.194 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.557.195 I llama_init_from_model: n_batch       = 2048
0.00.557.195 I llama_init_from_model: n_ubatch      = 512
0.00.557.196 I llama_init_from_model: flash_attn    = 0
0.00.557.199 I llama_init_from_model: freq_base     = 10000.0
0.00.557.199 I llama_init_from_model: freq_scale    = 1
0.00.557.202 I ggml_metal_init: allocating
0.00.557.268 I ggml_metal_init: found device: Apple M4
0.00.557.281 I ggml_metal_init: picking default device: Apple M4
0.00.559.051 I ggml_metal_init: using embedded metal library
0.00.565.422 I ggml_metal_init: GPU name:   Apple M4
0.00.565.427 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.565.428 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.565.429 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.565.430 I ggml_metal_init: simdgroup reduction   = true
0.00.565.430 I ggml_metal_init: simdgroup matrix mul. = true
0.00.565.431 I ggml_metal_init: has residency sets    = true
0.00.565.431 I ggml_metal_init: has bfloat            = true
0.00.565.431 I ggml_metal_init: use bfloat            = true
0.00.565.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.565.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.584.522 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.780 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.642.787 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.642.824 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.647.435 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.647.437 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.647.438 I llama_init_from_model: graph nodes  = 967
0.00.647.438 I llama_init_from_model: graph splits = 2
0.00.647.443 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.647.578 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.647.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.029 I main: llama threadpool init, n_threads = 4
0.00.704.072 I 
0.00.704.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.098 I 
0.00.704.242 I sampler seed: 1234
0.00.704.247 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.704.258 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.704.259 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.704.259 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.385.415 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 51115.91 tokens per second)
0.01.385.416 I llama_perf_context_print:        load time =     692.24 ms
0.01.385.417 I llama_perf_context_print: prompt eval time =      48.02 ms /     7 tokens (    6.86 ms per token,   145.78 tokens per second)
0.01.385.417 I llama_perf_context_print:        eval time =     630.31 ms /    63 runs   (   10.00 ms per token,    99.95 tokens per second)
0.01.385.418 I llama_perf_context_print:       total time =     682.08 ms /    70 tokens
0.01.385.650 I ggml_metal_free: deallocating

real	0m1.403s
user	0m0.111s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.849 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.278 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.290 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.290 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.291 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.291 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.292 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.293 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.294 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.294 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.294 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.295 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.295 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.296 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.297 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.297 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.148 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.201 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.062 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.064 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.064 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.064 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.065 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.065 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.066 I llama_model_loader: - type  f32:  194 tensors
0.00.026.066 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.066 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.067 I print_info: file format = GGUF V3 (latest)
0.00.026.068 I print_info: file type   = Q4_0
0.00.026.069 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.506 I load: special tokens cache size = 25
0.00.040.719 I load: token to piece cache size = 0.2984 MB
0.00.040.723 I print_info: arch             = gptneox
0.00.040.724 I print_info: vocab_only       = 0
0.00.040.724 I print_info: n_ctx_train      = 2048
0.00.040.724 I print_info: n_embd           = 2048
0.00.040.724 I print_info: n_layer          = 24
0.00.040.728 I print_info: n_head           = 16
0.00.040.729 I print_info: n_head_kv        = 16
0.00.040.730 I print_info: n_rot            = 32
0.00.040.730 I print_info: n_swa            = 0
0.00.040.730 I print_info: n_embd_head_k    = 128
0.00.040.730 I print_info: n_embd_head_v    = 128
0.00.040.731 I print_info: n_gqa            = 1
0.00.040.732 I print_info: n_embd_k_gqa     = 2048
0.00.040.732 I print_info: n_embd_v_gqa     = 2048
0.00.040.734 I print_info: f_norm_eps       = 1.0e-05
0.00.040.736 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.736 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.736 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.737 I print_info: f_logit_scale    = 0.0e+00
0.00.040.737 I print_info: n_ff             = 8192
0.00.040.738 I print_info: n_expert         = 0
0.00.040.738 I print_info: n_expert_used    = 0
0.00.040.738 I print_info: causal attn      = 1
0.00.040.738 I print_info: pooling type     = 0
0.00.040.738 I print_info: rope type        = 2
0.00.040.738 I print_info: rope scaling     = linear
0.00.040.739 I print_info: freq_base_train  = 10000.0
0.00.040.740 I print_info: freq_scale_train = 1
0.00.040.740 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.740 I print_info: rope_finetuned   = unknown
0.00.040.740 I print_info: ssm_d_conv       = 0
0.00.040.741 I print_info: ssm_d_inner      = 0
0.00.040.741 I print_info: ssm_d_state      = 0
0.00.040.741 I print_info: ssm_dt_rank      = 0
0.00.040.741 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.742 I print_info: model type       = 1.4B
0.00.040.743 I print_info: model params     = 1.41 B
0.00.040.743 I print_info: general.name     = 1.4B
0.00.040.744 I print_info: vocab type       = BPE
0.00.040.744 I print_info: n_vocab          = 50304
0.00.040.744 I print_info: n_merges         = 50009
0.00.040.744 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: LF token         = 187 ''
0.00.040.745 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.745 I print_info: max token length = 1024
0.00.040.746 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.532.932 I load_tensors: offloading 24 repeating layers to GPU
0.00.532.949 I load_tensors: offloading output layer to GPU
0.00.532.949 I load_tensors: offloaded 25/25 layers to GPU
0.00.532.990 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.532.991 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.534.733 I llama_init_from_model: n_seq_max     = 1
0.00.534.735 I llama_init_from_model: n_ctx         = 128
0.00.534.736 I llama_init_from_model: n_ctx_per_seq = 128
0.00.534.737 I llama_init_from_model: n_batch       = 128
0.00.534.737 I llama_init_from_model: n_ubatch      = 128
0.00.534.738 I llama_init_from_model: flash_attn    = 0
0.00.534.740 I llama_init_from_model: freq_base     = 10000.0
0.00.534.741 I llama_init_from_model: freq_scale    = 1
0.00.534.741 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.534.744 I ggml_metal_init: allocating
0.00.534.830 I ggml_metal_init: found device: Apple M4
0.00.534.844 I ggml_metal_init: picking default device: Apple M4
0.00.536.664 I ggml_metal_init: using embedded metal library
0.00.542.969 I ggml_metal_init: GPU name:   Apple M4
0.00.542.979 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.542.980 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.542.981 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.542.982 I ggml_metal_init: simdgroup reduction   = true
0.00.542.982 I ggml_metal_init: simdgroup matrix mul. = true
0.00.542.982 I ggml_metal_init: has residency sets    = true
0.00.542.983 I ggml_metal_init: has bfloat            = true
0.00.542.983 I ggml_metal_init: use bfloat            = true
0.00.542.984 I ggml_metal_init: hasUnifiedMemory      = true
0.00.542.988 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.561.854 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.565.560 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.565.564 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.565.716 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.569.040 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.569.041 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.569.042 I llama_init_from_model: graph nodes  = 967
0.00.569.042 I llama_init_from_model: graph splits = 2
0.00.569.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.569.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.960 I 
0.00.593.023 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.593.033 I perplexity: tokenizing the input ..
0.00.600.013 I perplexity: tokenization took 6.976 ms
0.00.600.021 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.723.610 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.724.933 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.724.957 I llama_perf_context_print:        load time =     583.10 ms
0.00.724.958 I llama_perf_context_print: prompt eval time =     122.62 ms /   128 tokens (    0.96 ms per token,  1043.91 tokens per second)
0.00.724.959 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.724.959 I llama_perf_context_print:       total time =     132.00 ms /   129 tokens
0.00.725.340 I ggml_metal_free: deallocating

real	0m0.741s
user	0m0.081s
sys	0m0.110s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.214 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.136 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.140 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.141 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.142 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.142 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.142 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.143 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.147 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.147 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.148 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.148 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.149 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.150 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.151 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.154 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.155 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.155 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.886 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.897 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.629 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.630 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.631 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.631 I llama_model_loader: - type  f32:  194 tensors
0.00.025.631 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.631 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.632 I print_info: file format = GGUF V3 (latest)
0.00.025.632 I print_info: file type   = Q4_1
0.00.025.633 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.467 I load: special tokens cache size = 25
0.00.039.388 I load: token to piece cache size = 0.2984 MB
0.00.039.391 I print_info: arch             = gptneox
0.00.039.391 I print_info: vocab_only       = 0
0.00.039.391 I print_info: n_ctx_train      = 2048
0.00.039.392 I print_info: n_embd           = 2048
0.00.039.392 I print_info: n_layer          = 24
0.00.039.395 I print_info: n_head           = 16
0.00.039.395 I print_info: n_head_kv        = 16
0.00.039.395 I print_info: n_rot            = 32
0.00.039.396 I print_info: n_swa            = 0
0.00.039.396 I print_info: n_embd_head_k    = 128
0.00.039.396 I print_info: n_embd_head_v    = 128
0.00.039.397 I print_info: n_gqa            = 1
0.00.039.398 I print_info: n_embd_k_gqa     = 2048
0.00.039.398 I print_info: n_embd_v_gqa     = 2048
0.00.039.399 I print_info: f_norm_eps       = 1.0e-05
0.00.039.399 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.399 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.399 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.399 I print_info: f_logit_scale    = 0.0e+00
0.00.039.400 I print_info: n_ff             = 8192
0.00.039.400 I print_info: n_expert         = 0
0.00.039.401 I print_info: n_expert_used    = 0
0.00.039.401 I print_info: causal attn      = 1
0.00.039.401 I print_info: pooling type     = 0
0.00.039.402 I print_info: rope type        = 2
0.00.039.404 I print_info: rope scaling     = linear
0.00.039.405 I print_info: freq_base_train  = 10000.0
0.00.039.405 I print_info: freq_scale_train = 1
0.00.039.405 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.405 I print_info: rope_finetuned   = unknown
0.00.039.406 I print_info: ssm_d_conv       = 0
0.00.039.406 I print_info: ssm_d_inner      = 0
0.00.039.406 I print_info: ssm_d_state      = 0
0.00.039.406 I print_info: ssm_dt_rank      = 0
0.00.039.406 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.406 I print_info: model type       = 1.4B
0.00.039.407 I print_info: model params     = 1.41 B
0.00.039.407 I print_info: general.name     = 1.4B
0.00.039.409 I print_info: vocab type       = BPE
0.00.039.409 I print_info: n_vocab          = 50304
0.00.039.409 I print_info: n_merges         = 50009
0.00.039.409 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.410 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.410 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.410 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.410 I print_info: LF token         = 187 ''
0.00.039.410 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.411 I print_info: max token length = 1024
0.00.039.411 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.549 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.563 I load_tensors: offloading output layer to GPU
0.00.638.564 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.597 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.638.599 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.640.062 I llama_init_from_model: n_seq_max     = 1
0.00.640.064 I llama_init_from_model: n_ctx         = 2048
0.00.640.065 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.640.066 I llama_init_from_model: n_batch       = 2048
0.00.640.066 I llama_init_from_model: n_ubatch      = 512
0.00.640.067 I llama_init_from_model: flash_attn    = 0
0.00.640.068 I llama_init_from_model: freq_base     = 10000.0
0.00.640.069 I llama_init_from_model: freq_scale    = 1
0.00.640.071 I ggml_metal_init: allocating
0.00.640.156 I ggml_metal_init: found device: Apple M4
0.00.640.170 I ggml_metal_init: picking default device: Apple M4
0.00.642.047 I ggml_metal_init: using embedded metal library
0.00.648.181 I ggml_metal_init: GPU name:   Apple M4
0.00.648.187 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.187 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.188 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.189 I ggml_metal_init: simdgroup reduction   = true
0.00.648.190 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.190 I ggml_metal_init: has residency sets    = true
0.00.648.190 I ggml_metal_init: has bfloat            = true
0.00.648.190 I ggml_metal_init: use bfloat            = true
0.00.648.191 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.193 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.667.189 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.724.771 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.724.782 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.724.818 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.729.650 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.729.652 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.729.653 I llama_init_from_model: graph nodes  = 967
0.00.729.653 I llama_init_from_model: graph splits = 2
0.00.729.664 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.729.796 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.729.797 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.785.886 I main: llama threadpool init, n_threads = 4
0.00.785.931 I 
0.00.785.946 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.785.947 I 
0.00.786.087 I sampler seed: 1234
0.00.786.092 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.786.102 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.786.103 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.786.103 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.508.613 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57489.88 tokens per second)
0.01.508.614 I llama_perf_context_print:        load time =     775.98 ms
0.01.508.616 I llama_perf_context_print: prompt eval time =      47.48 ms /     7 tokens (    6.78 ms per token,   147.44 tokens per second)
0.01.508.617 I llama_perf_context_print:        eval time =     672.39 ms /    63 runs   (   10.67 ms per token,    93.70 tokens per second)
0.01.508.617 I llama_perf_context_print:       total time =     723.42 ms /    70 tokens
0.01.508.840 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.109s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.866 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.347 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.353 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.355 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.356 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.358 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.359 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.359 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.360 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.361 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.362 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.362 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.040 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.115 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.992 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.994 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.994 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.994 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.995 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.995 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.996 I llama_model_loader: - type  f32:  194 tensors
0.00.024.996 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.996 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.997 I print_info: file format = GGUF V3 (latest)
0.00.024.998 I print_info: file type   = Q4_1
0.00.025.002 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.929 I load: special tokens cache size = 25
0.00.039.064 I load: token to piece cache size = 0.2984 MB
0.00.039.069 I print_info: arch             = gptneox
0.00.039.069 I print_info: vocab_only       = 0
0.00.039.069 I print_info: n_ctx_train      = 2048
0.00.039.070 I print_info: n_embd           = 2048
0.00.039.070 I print_info: n_layer          = 24
0.00.039.074 I print_info: n_head           = 16
0.00.039.075 I print_info: n_head_kv        = 16
0.00.039.075 I print_info: n_rot            = 32
0.00.039.076 I print_info: n_swa            = 0
0.00.039.076 I print_info: n_embd_head_k    = 128
0.00.039.076 I print_info: n_embd_head_v    = 128
0.00.039.077 I print_info: n_gqa            = 1
0.00.039.077 I print_info: n_embd_k_gqa     = 2048
0.00.039.078 I print_info: n_embd_v_gqa     = 2048
0.00.039.078 I print_info: f_norm_eps       = 1.0e-05
0.00.039.079 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.081 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.082 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.082 I print_info: f_logit_scale    = 0.0e+00
0.00.039.082 I print_info: n_ff             = 8192
0.00.039.082 I print_info: n_expert         = 0
0.00.039.083 I print_info: n_expert_used    = 0
0.00.039.084 I print_info: causal attn      = 1
0.00.039.084 I print_info: pooling type     = 0
0.00.039.084 I print_info: rope type        = 2
0.00.039.084 I print_info: rope scaling     = linear
0.00.039.084 I print_info: freq_base_train  = 10000.0
0.00.039.085 I print_info: freq_scale_train = 1
0.00.039.085 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.086 I print_info: rope_finetuned   = unknown
0.00.039.086 I print_info: ssm_d_conv       = 0
0.00.039.086 I print_info: ssm_d_inner      = 0
0.00.039.086 I print_info: ssm_d_state      = 0
0.00.039.086 I print_info: ssm_dt_rank      = 0
0.00.039.086 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.087 I print_info: model type       = 1.4B
0.00.039.087 I print_info: model params     = 1.41 B
0.00.039.087 I print_info: general.name     = 1.4B
0.00.039.088 I print_info: vocab type       = BPE
0.00.039.088 I print_info: n_vocab          = 50304
0.00.039.089 I print_info: n_merges         = 50009
0.00.039.090 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.090 I print_info: LF token         = 187 ''
0.00.039.090 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.091 I print_info: max token length = 1024
0.00.039.091 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.257 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.270 I load_tensors: offloading output layer to GPU
0.00.641.271 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.301 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.641.302 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.642.558 I llama_init_from_model: n_seq_max     = 1
0.00.642.562 I llama_init_from_model: n_ctx         = 128
0.00.642.563 I llama_init_from_model: n_ctx_per_seq = 128
0.00.642.564 I llama_init_from_model: n_batch       = 128
0.00.642.564 I llama_init_from_model: n_ubatch      = 128
0.00.642.564 I llama_init_from_model: flash_attn    = 0
0.00.642.566 I llama_init_from_model: freq_base     = 10000.0
0.00.642.566 I llama_init_from_model: freq_scale    = 1
0.00.642.567 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.642.569 I ggml_metal_init: allocating
0.00.642.633 I ggml_metal_init: found device: Apple M4
0.00.642.647 I ggml_metal_init: picking default device: Apple M4
0.00.644.310 I ggml_metal_init: using embedded metal library
0.00.650.617 I ggml_metal_init: GPU name:   Apple M4
0.00.650.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.650.627 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.650.628 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.650.628 I ggml_metal_init: simdgroup reduction   = true
0.00.650.629 I ggml_metal_init: simdgroup matrix mul. = true
0.00.650.629 I ggml_metal_init: has residency sets    = true
0.00.650.629 I ggml_metal_init: has bfloat            = true
0.00.650.629 I ggml_metal_init: use bfloat            = true
0.00.650.631 I ggml_metal_init: hasUnifiedMemory      = true
0.00.650.633 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.669.281 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.891 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.672.898 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.672.953 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.676.092 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.676.094 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.676.094 I llama_init_from_model: graph nodes  = 967
0.00.676.095 I llama_init_from_model: graph splits = 2
0.00.676.098 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.676.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.701.514 I 
0.00.701.567 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.701.580 I perplexity: tokenizing the input ..
0.00.707.599 I perplexity: tokenization took 6.018 ms
0.00.707.602 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.835.532 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.836.884 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.836.906 I llama_perf_context_print:        load time =     692.63 ms
0.00.836.908 I llama_perf_context_print: prompt eval time =     127.70 ms /   128 tokens (    1.00 ms per token,  1002.33 tokens per second)
0.00.836.908 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.836.909 I llama_perf_context_print:       total time =     135.40 ms /   129 tokens
0.00.837.302 I ggml_metal_free: deallocating

real	0m0.851s
user	0m0.077s
sys	0m0.133s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.030 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.945 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.950 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.951 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.951 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.955 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.956 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.961 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.964 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.964 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.965 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.965 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.966 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.966 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.967 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.970 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.970 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.972 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.786 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.767 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.652 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.653 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.653 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.654 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.654 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.655 I llama_model_loader: - type  f32:  194 tensors
0.00.025.655 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.656 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.656 I print_info: file format = GGUF V3 (latest)
0.00.025.657 I print_info: file type   = Q5_0
0.00.025.658 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.257 I load: special tokens cache size = 25
0.00.039.422 I load: token to piece cache size = 0.2984 MB
0.00.039.425 I print_info: arch             = gptneox
0.00.039.425 I print_info: vocab_only       = 0
0.00.039.425 I print_info: n_ctx_train      = 2048
0.00.039.425 I print_info: n_embd           = 2048
0.00.039.425 I print_info: n_layer          = 24
0.00.039.427 I print_info: n_head           = 16
0.00.039.428 I print_info: n_head_kv        = 16
0.00.039.428 I print_info: n_rot            = 32
0.00.039.428 I print_info: n_swa            = 0
0.00.039.428 I print_info: n_embd_head_k    = 128
0.00.039.428 I print_info: n_embd_head_v    = 128
0.00.039.429 I print_info: n_gqa            = 1
0.00.039.430 I print_info: n_embd_k_gqa     = 2048
0.00.039.430 I print_info: n_embd_v_gqa     = 2048
0.00.039.431 I print_info: f_norm_eps       = 1.0e-05
0.00.039.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.432 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.432 I print_info: f_logit_scale    = 0.0e+00
0.00.039.432 I print_info: n_ff             = 8192
0.00.039.433 I print_info: n_expert         = 0
0.00.039.433 I print_info: n_expert_used    = 0
0.00.039.433 I print_info: causal attn      = 1
0.00.039.433 I print_info: pooling type     = 0
0.00.039.434 I print_info: rope type        = 2
0.00.039.436 I print_info: rope scaling     = linear
0.00.039.437 I print_info: freq_base_train  = 10000.0
0.00.039.437 I print_info: freq_scale_train = 1
0.00.039.437 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.437 I print_info: rope_finetuned   = unknown
0.00.039.438 I print_info: ssm_d_conv       = 0
0.00.039.438 I print_info: ssm_d_inner      = 0
0.00.039.438 I print_info: ssm_d_state      = 0
0.00.039.438 I print_info: ssm_dt_rank      = 0
0.00.039.438 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.438 I print_info: model type       = 1.4B
0.00.039.440 I print_info: model params     = 1.41 B
0.00.039.440 I print_info: general.name     = 1.4B
0.00.039.441 I print_info: vocab type       = BPE
0.00.039.441 I print_info: n_vocab          = 50304
0.00.039.441 I print_info: n_merges         = 50009
0.00.039.441 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.442 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.442 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.442 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.442 I print_info: LF token         = 187 ''
0.00.039.442 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.443 I print_info: max token length = 1024
0.00.039.443 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.670.012 I load_tensors: offloading 24 repeating layers to GPU
0.00.670.028 I load_tensors: offloading output layer to GPU
0.00.670.029 I load_tensors: offloaded 25/25 layers to GPU
0.00.670.064 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.670.066 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.671.180 I llama_init_from_model: n_seq_max     = 1
0.00.671.182 I llama_init_from_model: n_ctx         = 2048
0.00.671.183 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.671.183 I llama_init_from_model: n_batch       = 2048
0.00.671.184 I llama_init_from_model: n_ubatch      = 512
0.00.671.184 I llama_init_from_model: flash_attn    = 0
0.00.671.186 I llama_init_from_model: freq_base     = 10000.0
0.00.671.187 I llama_init_from_model: freq_scale    = 1
0.00.671.189 I ggml_metal_init: allocating
0.00.671.282 I ggml_metal_init: found device: Apple M4
0.00.671.296 I ggml_metal_init: picking default device: Apple M4
0.00.673.133 I ggml_metal_init: using embedded metal library
0.00.679.490 I ggml_metal_init: GPU name:   Apple M4
0.00.679.494 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.679.494 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.679.495 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.679.496 I ggml_metal_init: simdgroup reduction   = true
0.00.679.496 I ggml_metal_init: simdgroup matrix mul. = true
0.00.679.496 I ggml_metal_init: has residency sets    = true
0.00.679.496 I ggml_metal_init: has bfloat            = true
0.00.679.497 I ggml_metal_init: use bfloat            = true
0.00.679.497 I ggml_metal_init: hasUnifiedMemory      = true
0.00.679.499 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.696.680 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.752.088 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.752.095 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.752.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.756.342 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.756.344 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.756.344 I llama_init_from_model: graph nodes  = 967
0.00.756.344 I llama_init_from_model: graph splits = 2
0.00.756.349 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.756.479 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.756.480 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.849 I main: llama threadpool init, n_threads = 4
0.00.814.895 I 
0.00.814.911 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.911 I 
0.00.815.087 I sampler seed: 1234
0.00.815.092 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.815.112 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.815.113 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.815.113 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.601.540 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.601.541 I llama_perf_context_print:        load time =     805.12 ms
0.01.601.541 I llama_perf_context_print: prompt eval time =      52.01 ms /     7 tokens (    7.43 ms per token,   134.58 tokens per second)
0.01.601.542 I llama_perf_context_print:        eval time =     731.56 ms /    63 runs   (   11.61 ms per token,    86.12 tokens per second)
0.01.601.542 I llama_perf_context_print:       total time =     787.39 ms /    70 tokens
0.01.601.793 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.108s
sys	0m0.228s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.839 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.651 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.663 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.664 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.664 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.665 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.666 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.666 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.667 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.667 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.669 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.669 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.670 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.672 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.672 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.672 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.584 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.667 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.605 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.606 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.607 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.607 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.607 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.608 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.608 I llama_model_loader: - type  f32:  194 tensors
0.00.025.609 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.609 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.610 I print_info: file format = GGUF V3 (latest)
0.00.025.610 I print_info: file type   = Q5_0
0.00.025.615 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.516 I load: special tokens cache size = 25
0.00.039.549 I load: token to piece cache size = 0.2984 MB
0.00.039.555 I print_info: arch             = gptneox
0.00.039.557 I print_info: vocab_only       = 0
0.00.039.557 I print_info: n_ctx_train      = 2048
0.00.039.557 I print_info: n_embd           = 2048
0.00.039.557 I print_info: n_layer          = 24
0.00.039.562 I print_info: n_head           = 16
0.00.039.562 I print_info: n_head_kv        = 16
0.00.039.563 I print_info: n_rot            = 32
0.00.039.563 I print_info: n_swa            = 0
0.00.039.563 I print_info: n_embd_head_k    = 128
0.00.039.563 I print_info: n_embd_head_v    = 128
0.00.039.564 I print_info: n_gqa            = 1
0.00.039.564 I print_info: n_embd_k_gqa     = 2048
0.00.039.566 I print_info: n_embd_v_gqa     = 2048
0.00.039.567 I print_info: f_norm_eps       = 1.0e-05
0.00.039.567 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.568 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.568 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.568 I print_info: f_logit_scale    = 0.0e+00
0.00.039.568 I print_info: n_ff             = 8192
0.00.039.569 I print_info: n_expert         = 0
0.00.039.569 I print_info: n_expert_used    = 0
0.00.039.569 I print_info: causal attn      = 1
0.00.039.569 I print_info: pooling type     = 0
0.00.039.569 I print_info: rope type        = 2
0.00.039.569 I print_info: rope scaling     = linear
0.00.039.570 I print_info: freq_base_train  = 10000.0
0.00.039.570 I print_info: freq_scale_train = 1
0.00.039.570 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.571 I print_info: rope_finetuned   = unknown
0.00.039.571 I print_info: ssm_d_conv       = 0
0.00.039.571 I print_info: ssm_d_inner      = 0
0.00.039.572 I print_info: ssm_d_state      = 0
0.00.039.572 I print_info: ssm_dt_rank      = 0
0.00.039.572 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.572 I print_info: model type       = 1.4B
0.00.039.572 I print_info: model params     = 1.41 B
0.00.039.572 I print_info: general.name     = 1.4B
0.00.039.573 I print_info: vocab type       = BPE
0.00.039.573 I print_info: n_vocab          = 50304
0.00.039.573 I print_info: n_merges         = 50009
0.00.039.574 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.574 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.574 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.574 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.574 I print_info: LF token         = 187 ''
0.00.039.575 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.575 I print_info: max token length = 1024
0.00.039.575 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.658.866 I load_tensors: offloading 24 repeating layers to GPU
0.00.658.870 I load_tensors: offloading output layer to GPU
0.00.658.870 I load_tensors: offloaded 25/25 layers to GPU
0.00.658.891 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.658.892 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.659.741 I llama_init_from_model: n_seq_max     = 1
0.00.659.745 I llama_init_from_model: n_ctx         = 128
0.00.659.745 I llama_init_from_model: n_ctx_per_seq = 128
0.00.659.745 I llama_init_from_model: n_batch       = 128
0.00.659.746 I llama_init_from_model: n_ubatch      = 128
0.00.659.746 I llama_init_from_model: flash_attn    = 0
0.00.659.748 I llama_init_from_model: freq_base     = 10000.0
0.00.659.748 I llama_init_from_model: freq_scale    = 1
0.00.659.749 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.659.750 I ggml_metal_init: allocating
0.00.659.801 I ggml_metal_init: found device: Apple M4
0.00.659.812 I ggml_metal_init: picking default device: Apple M4
0.00.660.869 I ggml_metal_init: using embedded metal library
0.00.665.077 I ggml_metal_init: GPU name:   Apple M4
0.00.665.081 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.665.082 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.665.082 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.665.083 I ggml_metal_init: simdgroup reduction   = true
0.00.665.083 I ggml_metal_init: simdgroup matrix mul. = true
0.00.665.083 I ggml_metal_init: has residency sets    = true
0.00.665.084 I ggml_metal_init: has bfloat            = true
0.00.665.084 I ggml_metal_init: use bfloat            = true
0.00.665.085 I ggml_metal_init: hasUnifiedMemory      = true
0.00.665.088 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.680.462 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.682.087 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.682.089 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.682.116 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.683.791 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.683.792 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.683.792 I llama_init_from_model: graph nodes  = 967
0.00.683.793 I llama_init_from_model: graph splits = 2
0.00.683.794 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.683.794 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.907 I 
0.00.711.933 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.936 I perplexity: tokenizing the input ..
0.00.715.830 I perplexity: tokenization took 3.892 ms
0.00.715.834 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.860.191 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.861.423 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.861.444 I llama_perf_context_print:        load time =     703.06 ms
0.00.861.445 I llama_perf_context_print: prompt eval time =     144.13 ms /   128 tokens (    1.13 ms per token,   888.10 tokens per second)
0.00.861.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.861.446 I llama_perf_context_print:       total time =     149.54 ms /   129 tokens
0.00.861.866 I ggml_metal_free: deallocating

real	0m0.875s
user	0m0.069s
sys	0m0.103s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.835 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.959 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.963 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.965 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.966 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.966 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.966 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.967 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.968 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.968 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.968 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.969 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.969 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.969 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.972 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.975 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.976 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.977 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.771 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.526 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.527 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.527 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.527 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.528 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.528 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.528 I llama_model_loader: - type  f32:  194 tensors
0.00.026.529 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.529 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.530 I print_info: file format = GGUF V3 (latest)
0.00.026.530 I print_info: file type   = Q5_1
0.00.026.531 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.312 I load: special tokens cache size = 25
0.00.040.195 I load: token to piece cache size = 0.2984 MB
0.00.040.197 I print_info: arch             = gptneox
0.00.040.197 I print_info: vocab_only       = 0
0.00.040.198 I print_info: n_ctx_train      = 2048
0.00.040.198 I print_info: n_embd           = 2048
0.00.040.198 I print_info: n_layer          = 24
0.00.040.200 I print_info: n_head           = 16
0.00.040.201 I print_info: n_head_kv        = 16
0.00.040.201 I print_info: n_rot            = 32
0.00.040.202 I print_info: n_swa            = 0
0.00.040.203 I print_info: n_embd_head_k    = 128
0.00.040.204 I print_info: n_embd_head_v    = 128
0.00.040.204 I print_info: n_gqa            = 1
0.00.040.205 I print_info: n_embd_k_gqa     = 2048
0.00.040.210 I print_info: n_embd_v_gqa     = 2048
0.00.040.211 I print_info: f_norm_eps       = 1.0e-05
0.00.040.211 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.212 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.212 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.212 I print_info: f_logit_scale    = 0.0e+00
0.00.040.213 I print_info: n_ff             = 8192
0.00.040.213 I print_info: n_expert         = 0
0.00.040.213 I print_info: n_expert_used    = 0
0.00.040.213 I print_info: causal attn      = 1
0.00.040.213 I print_info: pooling type     = 0
0.00.040.215 I print_info: rope type        = 2
0.00.040.216 I print_info: rope scaling     = linear
0.00.040.217 I print_info: freq_base_train  = 10000.0
0.00.040.217 I print_info: freq_scale_train = 1
0.00.040.217 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.218 I print_info: rope_finetuned   = unknown
0.00.040.218 I print_info: ssm_d_conv       = 0
0.00.040.220 I print_info: ssm_d_inner      = 0
0.00.040.220 I print_info: ssm_d_state      = 0
0.00.040.220 I print_info: ssm_dt_rank      = 0
0.00.040.220 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.220 I print_info: model type       = 1.4B
0.00.040.220 I print_info: model params     = 1.41 B
0.00.040.221 I print_info: general.name     = 1.4B
0.00.040.221 I print_info: vocab type       = BPE
0.00.040.221 I print_info: n_vocab          = 50304
0.00.040.221 I print_info: n_merges         = 50009
0.00.040.222 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.222 I print_info: LF token         = 187 ''
0.00.040.222 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.223 I print_info: max token length = 1024
0.00.040.223 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.729.042 I load_tensors: offloading 24 repeating layers to GPU
0.00.729.056 I load_tensors: offloading output layer to GPU
0.00.729.057 I load_tensors: offloaded 25/25 layers to GPU
0.00.729.085 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.729.087 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.730.633 I llama_init_from_model: n_seq_max     = 1
0.00.730.636 I llama_init_from_model: n_ctx         = 2048
0.00.730.636 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.730.637 I llama_init_from_model: n_batch       = 2048
0.00.730.637 I llama_init_from_model: n_ubatch      = 512
0.00.730.638 I llama_init_from_model: flash_attn    = 0
0.00.730.639 I llama_init_from_model: freq_base     = 10000.0
0.00.730.639 I llama_init_from_model: freq_scale    = 1
0.00.730.641 I ggml_metal_init: allocating
0.00.730.658 I ggml_metal_init: found device: Apple M4
0.00.730.668 I ggml_metal_init: picking default device: Apple M4
0.00.732.064 I ggml_metal_init: using embedded metal library
0.00.738.290 I ggml_metal_init: GPU name:   Apple M4
0.00.738.293 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.738.294 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.738.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.738.296 I ggml_metal_init: simdgroup reduction   = true
0.00.738.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.738.296 I ggml_metal_init: has residency sets    = true
0.00.738.297 I ggml_metal_init: has bfloat            = true
0.00.738.297 I ggml_metal_init: use bfloat            = true
0.00.738.298 I ggml_metal_init: hasUnifiedMemory      = true
0.00.738.299 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.754.944 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.811.877 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.811.895 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.811.931 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.816.150 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.816.152 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.816.152 I llama_init_from_model: graph nodes  = 967
0.00.816.153 I llama_init_from_model: graph splits = 2
0.00.816.158 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.816.279 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.816.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.876.735 I main: llama threadpool init, n_threads = 4
0.00.876.775 I 
0.00.876.788 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.876.788 I 
0.00.876.935 I sampler seed: 1234
0.00.876.939 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.876.984 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.876.987 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.876.988 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.723.008 I llama_perf_sampler_print:    sampling time =       1.50 ms /    71 runs   (    0.02 ms per token, 47428.19 tokens per second)
0.01.723.009 I llama_perf_context_print:        load time =     866.20 ms
0.01.723.010 I llama_perf_context_print: prompt eval time =      52.05 ms /     7 tokens (    7.44 ms per token,   134.48 tokens per second)
0.01.723.010 I llama_perf_context_print:        eval time =     791.46 ms /    63 runs   (   12.56 ms per token,    79.60 tokens per second)
0.01.723.011 I llama_perf_context_print:       total time =     846.97 ms /    70 tokens
0.01.723.282 I ggml_metal_free: deallocating

real	0m1.741s
user	0m0.107s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.528 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.696 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.700 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.702 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.703 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.703 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.704 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.705 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.705 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.706 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.706 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.708 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.708 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.710 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.710 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.710 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.454 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.418 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.142 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.143 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.143 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.144 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.144 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.144 I llama_model_loader: - type  f32:  194 tensors
0.00.025.145 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.145 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.145 I print_info: file format = GGUF V3 (latest)
0.00.025.146 I print_info: file type   = Q5_1
0.00.025.146 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.873 I load: special tokens cache size = 25
0.00.038.955 I load: token to piece cache size = 0.2984 MB
0.00.038.958 I print_info: arch             = gptneox
0.00.038.958 I print_info: vocab_only       = 0
0.00.038.958 I print_info: n_ctx_train      = 2048
0.00.038.959 I print_info: n_embd           = 2048
0.00.038.959 I print_info: n_layer          = 24
0.00.038.961 I print_info: n_head           = 16
0.00.038.962 I print_info: n_head_kv        = 16
0.00.038.962 I print_info: n_rot            = 32
0.00.038.962 I print_info: n_swa            = 0
0.00.038.963 I print_info: n_embd_head_k    = 128
0.00.038.963 I print_info: n_embd_head_v    = 128
0.00.038.963 I print_info: n_gqa            = 1
0.00.038.964 I print_info: n_embd_k_gqa     = 2048
0.00.038.965 I print_info: n_embd_v_gqa     = 2048
0.00.038.966 I print_info: f_norm_eps       = 1.0e-05
0.00.038.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.966 I print_info: f_logit_scale    = 0.0e+00
0.00.038.967 I print_info: n_ff             = 8192
0.00.038.967 I print_info: n_expert         = 0
0.00.038.967 I print_info: n_expert_used    = 0
0.00.038.968 I print_info: causal attn      = 1
0.00.038.968 I print_info: pooling type     = 0
0.00.038.968 I print_info: rope type        = 2
0.00.038.968 I print_info: rope scaling     = linear
0.00.038.971 I print_info: freq_base_train  = 10000.0
0.00.038.971 I print_info: freq_scale_train = 1
0.00.038.971 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.972 I print_info: rope_finetuned   = unknown
0.00.038.974 I print_info: ssm_d_conv       = 0
0.00.038.974 I print_info: ssm_d_inner      = 0
0.00.038.974 I print_info: ssm_d_state      = 0
0.00.038.974 I print_info: ssm_dt_rank      = 0
0.00.038.974 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.974 I print_info: model type       = 1.4B
0.00.038.975 I print_info: model params     = 1.41 B
0.00.038.975 I print_info: general.name     = 1.4B
0.00.038.975 I print_info: vocab type       = BPE
0.00.038.975 I print_info: n_vocab          = 50304
0.00.038.976 I print_info: n_merges         = 50009
0.00.038.976 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.977 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.977 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.978 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.978 I print_info: LF token         = 187 ''
0.00.038.979 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: max token length = 1024
0.00.038.979 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.729.815 I load_tensors: offloading 24 repeating layers to GPU
0.00.729.830 I load_tensors: offloading output layer to GPU
0.00.729.831 I load_tensors: offloaded 25/25 layers to GPU
0.00.729.871 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.729.873 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.731.653 I llama_init_from_model: n_seq_max     = 1
0.00.731.656 I llama_init_from_model: n_ctx         = 128
0.00.731.656 I llama_init_from_model: n_ctx_per_seq = 128
0.00.731.657 I llama_init_from_model: n_batch       = 128
0.00.731.657 I llama_init_from_model: n_ubatch      = 128
0.00.731.658 I llama_init_from_model: flash_attn    = 0
0.00.731.660 I llama_init_from_model: freq_base     = 10000.0
0.00.731.661 I llama_init_from_model: freq_scale    = 1
0.00.731.661 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.731.663 I ggml_metal_init: allocating
0.00.731.781 I ggml_metal_init: found device: Apple M4
0.00.731.795 I ggml_metal_init: picking default device: Apple M4
0.00.733.696 I ggml_metal_init: using embedded metal library
0.00.740.268 I ggml_metal_init: GPU name:   Apple M4
0.00.740.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.740.274 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.740.275 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.740.275 I ggml_metal_init: simdgroup reduction   = true
0.00.740.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.740.276 I ggml_metal_init: has residency sets    = true
0.00.740.276 I ggml_metal_init: has bfloat            = true
0.00.740.276 I ggml_metal_init: use bfloat            = true
0.00.740.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.740.278 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.757.685 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.761.165 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.761.169 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.761.234 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.764.425 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.764.427 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.764.427 I llama_init_from_model: graph nodes  = 967
0.00.764.427 I llama_init_from_model: graph splits = 2
0.00.764.430 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.764.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.792.392 I 
0.00.792.453 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.792.460 I perplexity: tokenizing the input ..
0.00.799.790 I perplexity: tokenization took 7.327 ms
0.00.799.798 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.935.012 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.936.349 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.936.372 I llama_perf_context_print:        load time =     782.85 ms
0.00.936.373 I llama_perf_context_print: prompt eval time =     134.67 ms /   128 tokens (    1.05 ms per token,   950.49 tokens per second)
0.00.936.374 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.936.374 I llama_perf_context_print:       total time =     143.99 ms /   129 tokens
0.00.936.720 I ggml_metal_free: deallocating

real	0m0.952s
user	0m0.079s
sys	0m0.151s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.837 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.105 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.111 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.113 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.114 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.114 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.116 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.117 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.118 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.118 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.118 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.119 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.119 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.119 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.120 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.123 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.124 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.124 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.034 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.053 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.871 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.872 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.872 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.873 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.873 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.874 I llama_model_loader: - type  f32:  194 tensors
0.00.025.874 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.875 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.875 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.876 I print_info: file format = GGUF V3 (latest)
0.00.025.876 I print_info: file type   = Q2_K - Medium
0.00.025.877 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.034.040 I load: special tokens cache size = 25
0.00.040.055 I load: token to piece cache size = 0.2984 MB
0.00.040.059 I print_info: arch             = gptneox
0.00.040.060 I print_info: vocab_only       = 0
0.00.040.060 I print_info: n_ctx_train      = 2048
0.00.040.060 I print_info: n_embd           = 2048
0.00.040.060 I print_info: n_layer          = 24
0.00.040.065 I print_info: n_head           = 16
0.00.040.066 I print_info: n_head_kv        = 16
0.00.040.066 I print_info: n_rot            = 32
0.00.040.066 I print_info: n_swa            = 0
0.00.040.066 I print_info: n_embd_head_k    = 128
0.00.040.066 I print_info: n_embd_head_v    = 128
0.00.040.067 I print_info: n_gqa            = 1
0.00.040.068 I print_info: n_embd_k_gqa     = 2048
0.00.040.068 I print_info: n_embd_v_gqa     = 2048
0.00.040.069 I print_info: f_norm_eps       = 1.0e-05
0.00.040.069 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.069 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.070 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.070 I print_info: f_logit_scale    = 0.0e+00
0.00.040.070 I print_info: n_ff             = 8192
0.00.040.071 I print_info: n_expert         = 0
0.00.040.071 I print_info: n_expert_used    = 0
0.00.040.071 I print_info: causal attn      = 1
0.00.040.073 I print_info: pooling type     = 0
0.00.040.075 I print_info: rope type        = 2
0.00.040.075 I print_info: rope scaling     = linear
0.00.040.075 I print_info: freq_base_train  = 10000.0
0.00.040.077 I print_info: freq_scale_train = 1
0.00.040.077 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.078 I print_info: rope_finetuned   = unknown
0.00.040.078 I print_info: ssm_d_conv       = 0
0.00.040.078 I print_info: ssm_d_inner      = 0
0.00.040.078 I print_info: ssm_d_state      = 0
0.00.040.079 I print_info: ssm_dt_rank      = 0
0.00.040.079 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.079 I print_info: model type       = 1.4B
0.00.040.080 I print_info: model params     = 1.41 B
0.00.040.080 I print_info: general.name     = 1.4B
0.00.040.081 I print_info: vocab type       = BPE
0.00.040.081 I print_info: n_vocab          = 50304
0.00.040.081 I print_info: n_merges         = 50009
0.00.040.082 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.082 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: LF token         = 187 ''
0.00.040.083 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.083 I print_info: max token length = 1024
0.00.040.084 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.734.756 I load_tensors: offloading 24 repeating layers to GPU
0.00.734.762 I load_tensors: offloading output layer to GPU
0.00.734.763 I load_tensors: offloaded 25/25 layers to GPU
0.00.734.780 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.734.781 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.735.634 I llama_init_from_model: n_seq_max     = 1
0.00.735.638 I llama_init_from_model: n_ctx         = 2048
0.00.735.638 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.735.639 I llama_init_from_model: n_batch       = 2048
0.00.735.639 I llama_init_from_model: n_ubatch      = 512
0.00.735.639 I llama_init_from_model: flash_attn    = 0
0.00.735.640 I llama_init_from_model: freq_base     = 10000.0
0.00.735.641 I llama_init_from_model: freq_scale    = 1
0.00.735.642 I ggml_metal_init: allocating
0.00.735.669 I ggml_metal_init: found device: Apple M4
0.00.735.681 I ggml_metal_init: picking default device: Apple M4
0.00.736.706 I ggml_metal_init: using embedded metal library
0.00.748.764 I ggml_metal_init: GPU name:   Apple M4
0.00.748.770 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.748.771 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.748.771 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.748.772 I ggml_metal_init: simdgroup reduction   = true
0.00.748.772 I ggml_metal_init: simdgroup matrix mul. = true
0.00.748.772 I ggml_metal_init: has residency sets    = true
0.00.748.773 I ggml_metal_init: has bfloat            = true
0.00.748.773 I ggml_metal_init: use bfloat            = true
0.00.748.774 I ggml_metal_init: hasUnifiedMemory      = true
0.00.748.776 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.774.099 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.812.553 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.812.563 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.812.603 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.816.781 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.816.783 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.816.784 I llama_init_from_model: graph nodes  = 967
0.00.816.784 I llama_init_from_model: graph splits = 2
0.00.816.789 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.816.902 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.816.903 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.877.775 I main: llama threadpool init, n_threads = 4
0.00.877.818 I 
0.00.877.832 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.877.833 I 
0.00.877.971 I sampler seed: 1234
0.00.877.975 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.877.986 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.877.986 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.877.986 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.558.525 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50533.81 tokens per second)
0.01.558.525 I llama_perf_context_print:        load time =     868.24 ms
0.01.558.526 I llama_perf_context_print: prompt eval time =      44.20 ms /     7 tokens (    6.31 ms per token,   158.38 tokens per second)
0.01.558.527 I llama_perf_context_print:        eval time =     633.69 ms /    63 runs   (   10.06 ms per token,    99.42 tokens per second)
0.01.558.528 I llama_perf_context_print:       total time =     681.44 ms /    70 tokens
0.01.558.759 I ggml_metal_free: deallocating

real	0m1.584s
user	0m0.108s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.951 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.297 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.304 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.305 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.305 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.306 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.306 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.307 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.307 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.308 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.308 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.308 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.309 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.309 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.312 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.312 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.312 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.145 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.214 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.164 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.165 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.165 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.166 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.166 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.167 I llama_model_loader: - type  f32:  194 tensors
0.00.025.167 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.168 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.168 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.169 I print_info: file format = GGUF V3 (latest)
0.00.025.169 I print_info: file type   = Q2_K - Medium
0.00.025.170 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.612 I load: special tokens cache size = 25
0.00.039.446 I load: token to piece cache size = 0.2984 MB
0.00.039.451 I print_info: arch             = gptneox
0.00.039.451 I print_info: vocab_only       = 0
0.00.039.451 I print_info: n_ctx_train      = 2048
0.00.039.451 I print_info: n_embd           = 2048
0.00.039.451 I print_info: n_layer          = 24
0.00.039.456 I print_info: n_head           = 16
0.00.039.456 I print_info: n_head_kv        = 16
0.00.039.459 I print_info: n_rot            = 32
0.00.039.459 I print_info: n_swa            = 0
0.00.039.460 I print_info: n_embd_head_k    = 128
0.00.039.460 I print_info: n_embd_head_v    = 128
0.00.039.460 I print_info: n_gqa            = 1
0.00.039.461 I print_info: n_embd_k_gqa     = 2048
0.00.039.462 I print_info: n_embd_v_gqa     = 2048
0.00.039.462 I print_info: f_norm_eps       = 1.0e-05
0.00.039.463 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.463 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.463 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.463 I print_info: f_logit_scale    = 0.0e+00
0.00.039.464 I print_info: n_ff             = 8192
0.00.039.464 I print_info: n_expert         = 0
0.00.039.464 I print_info: n_expert_used    = 0
0.00.039.464 I print_info: causal attn      = 1
0.00.039.464 I print_info: pooling type     = 0
0.00.039.465 I print_info: rope type        = 2
0.00.039.465 I print_info: rope scaling     = linear
0.00.039.465 I print_info: freq_base_train  = 10000.0
0.00.039.465 I print_info: freq_scale_train = 1
0.00.039.466 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.466 I print_info: rope_finetuned   = unknown
0.00.039.466 I print_info: ssm_d_conv       = 0
0.00.039.466 I print_info: ssm_d_inner      = 0
0.00.039.491 I print_info: ssm_d_state      = 0
0.00.039.492 I print_info: ssm_dt_rank      = 0
0.00.039.492 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.493 I print_info: model type       = 1.4B
0.00.039.493 I print_info: model params     = 1.41 B
0.00.039.495 I print_info: general.name     = 1.4B
0.00.039.496 I print_info: vocab type       = BPE
0.00.039.496 I print_info: n_vocab          = 50304
0.00.039.496 I print_info: n_merges         = 50009
0.00.039.496 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.496 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.496 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.497 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.497 I print_info: LF token         = 187 ''
0.00.039.497 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.497 I print_info: max token length = 1024
0.00.039.498 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.363.988 I load_tensors: offloading 24 repeating layers to GPU
0.00.364.003 I load_tensors: offloading output layer to GPU
0.00.364.004 I load_tensors: offloaded 25/25 layers to GPU
0.00.364.037 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.364.038 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.365.782 I llama_init_from_model: n_seq_max     = 1
0.00.365.787 I llama_init_from_model: n_ctx         = 128
0.00.365.788 I llama_init_from_model: n_ctx_per_seq = 128
0.00.365.789 I llama_init_from_model: n_batch       = 128
0.00.365.789 I llama_init_from_model: n_ubatch      = 128
0.00.365.789 I llama_init_from_model: flash_attn    = 0
0.00.365.791 I llama_init_from_model: freq_base     = 10000.0
0.00.365.792 I llama_init_from_model: freq_scale    = 1
0.00.365.792 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.365.794 I ggml_metal_init: allocating
0.00.365.875 I ggml_metal_init: found device: Apple M4
0.00.365.888 I ggml_metal_init: picking default device: Apple M4
0.00.367.667 I ggml_metal_init: using embedded metal library
0.00.373.030 I ggml_metal_init: GPU name:   Apple M4
0.00.373.051 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.373.051 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.373.052 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.373.053 I ggml_metal_init: simdgroup reduction   = true
0.00.373.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.373.054 I ggml_metal_init: has residency sets    = true
0.00.373.054 I ggml_metal_init: has bfloat            = true
0.00.373.054 I ggml_metal_init: use bfloat            = true
0.00.373.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.373.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.394.321 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.398.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.398.098 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.398.144 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.401.449 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.401.451 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.401.452 I llama_init_from_model: graph nodes  = 967
0.00.401.452 I llama_init_from_model: graph splits = 2
0.00.401.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.401.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.470 I 
0.00.434.526 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.535 I perplexity: tokenizing the input ..
0.00.441.146 I perplexity: tokenization took 6.609 ms
0.00.441.149 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.585.447 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.586.784 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.586.807 I llama_perf_context_print:        load time =     425.51 ms
0.00.586.808 I llama_perf_context_print: prompt eval time =     144.07 ms /   128 tokens (    1.13 ms per token,   888.44 tokens per second)
0.00.586.808 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.586.809 I llama_perf_context_print:       total time =     152.34 ms /   129 tokens
0.00.587.197 I ggml_metal_free: deallocating

real	0m0.601s
user	0m0.080s
sys	0m0.090s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.894 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.407 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.412 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.414 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.414 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.420 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.420 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.420 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.423 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.423 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.423 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.424 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.424 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.424 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.428 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.430 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.432 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.432 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.252 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.280 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.115 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.116 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.117 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.117 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.117 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.118 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.118 I llama_model_loader: - type  f32:  194 tensors
0.00.025.118 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.119 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.119 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.119 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.120 I print_info: file format = GGUF V3 (latest)
0.00.025.120 I print_info: file type   = Q3_K - Medium
0.00.025.121 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.256 I load: special tokens cache size = 25
0.00.039.230 I load: token to piece cache size = 0.2984 MB
0.00.039.233 I print_info: arch             = gptneox
0.00.039.233 I print_info: vocab_only       = 0
0.00.039.233 I print_info: n_ctx_train      = 2048
0.00.039.233 I print_info: n_embd           = 2048
0.00.039.233 I print_info: n_layer          = 24
0.00.039.236 I print_info: n_head           = 16
0.00.039.237 I print_info: n_head_kv        = 16
0.00.039.237 I print_info: n_rot            = 32
0.00.039.237 I print_info: n_swa            = 0
0.00.039.238 I print_info: n_embd_head_k    = 128
0.00.039.238 I print_info: n_embd_head_v    = 128
0.00.039.239 I print_info: n_gqa            = 1
0.00.039.239 I print_info: n_embd_k_gqa     = 2048
0.00.039.240 I print_info: n_embd_v_gqa     = 2048
0.00.039.241 I print_info: f_norm_eps       = 1.0e-05
0.00.039.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.241 I print_info: f_logit_scale    = 0.0e+00
0.00.039.244 I print_info: n_ff             = 8192
0.00.039.244 I print_info: n_expert         = 0
0.00.039.244 I print_info: n_expert_used    = 0
0.00.039.246 I print_info: causal attn      = 1
0.00.039.247 I print_info: pooling type     = 0
0.00.039.247 I print_info: rope type        = 2
0.00.039.248 I print_info: rope scaling     = linear
0.00.039.248 I print_info: freq_base_train  = 10000.0
0.00.039.248 I print_info: freq_scale_train = 1
0.00.039.249 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.249 I print_info: rope_finetuned   = unknown
0.00.039.249 I print_info: ssm_d_conv       = 0
0.00.039.249 I print_info: ssm_d_inner      = 0
0.00.039.249 I print_info: ssm_d_state      = 0
0.00.039.250 I print_info: ssm_dt_rank      = 0
0.00.039.250 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.250 I print_info: model type       = 1.4B
0.00.039.250 I print_info: model params     = 1.41 B
0.00.039.250 I print_info: general.name     = 1.4B
0.00.039.251 I print_info: vocab type       = BPE
0.00.039.251 I print_info: n_vocab          = 50304
0.00.039.252 I print_info: n_merges         = 50009
0.00.039.252 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.252 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: LF token         = 187 ''
0.00.039.253 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.253 I print_info: max token length = 1024
0.00.039.254 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.441.341 I load_tensors: offloading 24 repeating layers to GPU
0.00.441.351 I load_tensors: offloading output layer to GPU
0.00.441.351 I load_tensors: offloaded 25/25 layers to GPU
0.00.441.391 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.441.395 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.442.859 I llama_init_from_model: n_seq_max     = 1
0.00.442.862 I llama_init_from_model: n_ctx         = 2048
0.00.442.863 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.442.863 I llama_init_from_model: n_batch       = 2048
0.00.442.864 I llama_init_from_model: n_ubatch      = 512
0.00.442.864 I llama_init_from_model: flash_attn    = 0
0.00.442.866 I llama_init_from_model: freq_base     = 10000.0
0.00.442.866 I llama_init_from_model: freq_scale    = 1
0.00.442.870 I ggml_metal_init: allocating
0.00.442.916 I ggml_metal_init: found device: Apple M4
0.00.442.928 I ggml_metal_init: picking default device: Apple M4
0.00.444.575 I ggml_metal_init: using embedded metal library
0.00.450.102 I ggml_metal_init: GPU name:   Apple M4
0.00.450.115 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.450.115 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.450.116 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.450.117 I ggml_metal_init: simdgroup reduction   = true
0.00.450.117 I ggml_metal_init: simdgroup matrix mul. = true
0.00.450.117 I ggml_metal_init: has residency sets    = true
0.00.450.118 I ggml_metal_init: has bfloat            = true
0.00.450.118 I ggml_metal_init: use bfloat            = true
0.00.450.122 I ggml_metal_init: hasUnifiedMemory      = true
0.00.450.126 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.471.792 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.537.162 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.537.171 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.537.213 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.542.945 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.542.947 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.542.947 I llama_init_from_model: graph nodes  = 967
0.00.542.947 I llama_init_from_model: graph splits = 2
0.00.542.957 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.543.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.543.081 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.809 I main: llama threadpool init, n_threads = 4
0.00.600.851 I 
0.00.600.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.866 I 
0.00.601.028 I sampler seed: 1234
0.00.601.033 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.601.044 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.601.044 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.601.046 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.348.905 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52985.07 tokens per second)
0.01.348.905 I llama_perf_context_print:        load time =     591.22 ms
0.01.348.906 I llama_perf_context_print: prompt eval time =      50.08 ms /     7 tokens (    7.15 ms per token,   139.79 tokens per second)
0.01.348.908 I llama_perf_context_print:        eval time =     694.91 ms /    63 runs   (   11.03 ms per token,    90.66 tokens per second)
0.01.348.908 I llama_perf_context_print:       total time =     748.79 ms /    70 tokens
0.01.349.122 I ggml_metal_free: deallocating

real	0m1.366s
user	0m0.112s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.782 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.791 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.792 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.792 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.793 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.794 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.794 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.794 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.795 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.795 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.796 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.798 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.799 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.799 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.652 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.769 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.710 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.711 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.711 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.712 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.713 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.713 I llama_model_loader: - type  f32:  194 tensors
0.00.024.714 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.714 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.714 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.714 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.715 I print_info: file format = GGUF V3 (latest)
0.00.024.716 I print_info: file type   = Q3_K - Medium
0.00.024.717 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.080 I load: special tokens cache size = 25
0.00.039.221 I load: token to piece cache size = 0.2984 MB
0.00.039.224 I print_info: arch             = gptneox
0.00.039.225 I print_info: vocab_only       = 0
0.00.039.225 I print_info: n_ctx_train      = 2048
0.00.039.225 I print_info: n_embd           = 2048
0.00.039.225 I print_info: n_layer          = 24
0.00.039.228 I print_info: n_head           = 16
0.00.039.229 I print_info: n_head_kv        = 16
0.00.039.229 I print_info: n_rot            = 32
0.00.039.229 I print_info: n_swa            = 0
0.00.039.230 I print_info: n_embd_head_k    = 128
0.00.039.230 I print_info: n_embd_head_v    = 128
0.00.039.230 I print_info: n_gqa            = 1
0.00.039.231 I print_info: n_embd_k_gqa     = 2048
0.00.039.233 I print_info: n_embd_v_gqa     = 2048
0.00.039.234 I print_info: f_norm_eps       = 1.0e-05
0.00.039.234 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.236 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.236 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.236 I print_info: f_logit_scale    = 0.0e+00
0.00.039.237 I print_info: n_ff             = 8192
0.00.039.237 I print_info: n_expert         = 0
0.00.039.238 I print_info: n_expert_used    = 0
0.00.039.238 I print_info: causal attn      = 1
0.00.039.238 I print_info: pooling type     = 0
0.00.039.238 I print_info: rope type        = 2
0.00.039.238 I print_info: rope scaling     = linear
0.00.039.239 I print_info: freq_base_train  = 10000.0
0.00.039.239 I print_info: freq_scale_train = 1
0.00.039.239 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.240 I print_info: rope_finetuned   = unknown
0.00.039.240 I print_info: ssm_d_conv       = 0
0.00.039.240 I print_info: ssm_d_inner      = 0
0.00.039.241 I print_info: ssm_d_state      = 0
0.00.039.241 I print_info: ssm_dt_rank      = 0
0.00.039.242 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.242 I print_info: model type       = 1.4B
0.00.039.242 I print_info: model params     = 1.41 B
0.00.039.242 I print_info: general.name     = 1.4B
0.00.039.243 I print_info: vocab type       = BPE
0.00.039.243 I print_info: n_vocab          = 50304
0.00.039.244 I print_info: n_merges         = 50009
0.00.039.244 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.244 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.244 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.244 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: LF token         = 187 ''
0.00.039.245 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.245 I print_info: max token length = 1024
0.00.039.245 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.439.783 I load_tensors: offloading 24 repeating layers to GPU
0.00.439.797 I load_tensors: offloading output layer to GPU
0.00.439.798 I load_tensors: offloaded 25/25 layers to GPU
0.00.439.830 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.439.831 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.441.389 I llama_init_from_model: n_seq_max     = 1
0.00.441.393 I llama_init_from_model: n_ctx         = 128
0.00.441.393 I llama_init_from_model: n_ctx_per_seq = 128
0.00.441.394 I llama_init_from_model: n_batch       = 128
0.00.441.394 I llama_init_from_model: n_ubatch      = 128
0.00.441.395 I llama_init_from_model: flash_attn    = 0
0.00.441.397 I llama_init_from_model: freq_base     = 10000.0
0.00.441.397 I llama_init_from_model: freq_scale    = 1
0.00.441.398 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.441.410 I ggml_metal_init: allocating
0.00.441.500 I ggml_metal_init: found device: Apple M4
0.00.441.518 I ggml_metal_init: picking default device: Apple M4
0.00.443.357 I ggml_metal_init: using embedded metal library
0.00.449.722 I ggml_metal_init: GPU name:   Apple M4
0.00.449.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.449.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.449.729 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.449.729 I ggml_metal_init: simdgroup reduction   = true
0.00.449.730 I ggml_metal_init: simdgroup matrix mul. = true
0.00.449.730 I ggml_metal_init: has residency sets    = true
0.00.449.730 I ggml_metal_init: has bfloat            = true
0.00.449.731 I ggml_metal_init: use bfloat            = true
0.00.449.732 I ggml_metal_init: hasUnifiedMemory      = true
0.00.449.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.468.253 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.471.806 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.471.810 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.471.851 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.475.292 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.475.293 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.475.294 I llama_init_from_model: graph nodes  = 967
0.00.475.294 I llama_init_from_model: graph splits = 2
0.00.475.298 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.475.298 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.502.854 I 
0.00.502.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.502.923 I perplexity: tokenizing the input ..
0.00.509.950 I perplexity: tokenization took 7.023 ms
0.00.509.962 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.652.392 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.653.808 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.653.827 I llama_perf_context_print:        load time =     494.06 ms
0.00.653.828 I llama_perf_context_print: prompt eval time =     141.54 ms /   128 tokens (    1.11 ms per token,   904.34 tokens per second)
0.00.653.828 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.653.829 I llama_perf_context_print:       total time =     150.98 ms /   129 tokens
0.00.654.123 I ggml_metal_free: deallocating

real	0m0.667s
user	0m0.079s
sys	0m0.107s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.824 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.096 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.101 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.105 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.105 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.106 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.108 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.108 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.109 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.109 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.110 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.110 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.111 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.114 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.115 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.117 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.117 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.117 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.962 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.934 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.637 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.638 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.638 I llama_model_loader: - type  f32:  194 tensors
0.00.025.639 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.639 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.639 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.639 I print_info: file format = GGUF V3 (latest)
0.00.025.640 I print_info: file type   = Q4_K - Medium
0.00.025.640 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.454 I load: special tokens cache size = 25
0.00.039.424 I load: token to piece cache size = 0.2984 MB
0.00.039.426 I print_info: arch             = gptneox
0.00.039.427 I print_info: vocab_only       = 0
0.00.039.427 I print_info: n_ctx_train      = 2048
0.00.039.427 I print_info: n_embd           = 2048
0.00.039.427 I print_info: n_layer          = 24
0.00.039.430 I print_info: n_head           = 16
0.00.039.431 I print_info: n_head_kv        = 16
0.00.039.431 I print_info: n_rot            = 32
0.00.039.431 I print_info: n_swa            = 0
0.00.039.431 I print_info: n_embd_head_k    = 128
0.00.039.433 I print_info: n_embd_head_v    = 128
0.00.039.434 I print_info: n_gqa            = 1
0.00.039.435 I print_info: n_embd_k_gqa     = 2048
0.00.039.435 I print_info: n_embd_v_gqa     = 2048
0.00.039.436 I print_info: f_norm_eps       = 1.0e-05
0.00.039.436 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.436 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.437 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.437 I print_info: f_logit_scale    = 0.0e+00
0.00.039.437 I print_info: n_ff             = 8192
0.00.039.438 I print_info: n_expert         = 0
0.00.039.438 I print_info: n_expert_used    = 0
0.00.039.438 I print_info: causal attn      = 1
0.00.039.442 I print_info: pooling type     = 0
0.00.039.442 I print_info: rope type        = 2
0.00.039.442 I print_info: rope scaling     = linear
0.00.039.443 I print_info: freq_base_train  = 10000.0
0.00.039.443 I print_info: freq_scale_train = 1
0.00.039.443 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.443 I print_info: rope_finetuned   = unknown
0.00.039.444 I print_info: ssm_d_conv       = 0
0.00.039.444 I print_info: ssm_d_inner      = 0
0.00.039.446 I print_info: ssm_d_state      = 0
0.00.039.446 I print_info: ssm_dt_rank      = 0
0.00.039.446 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.446 I print_info: model type       = 1.4B
0.00.039.447 I print_info: model params     = 1.41 B
0.00.039.447 I print_info: general.name     = 1.4B
0.00.039.447 I print_info: vocab type       = BPE
0.00.039.448 I print_info: n_vocab          = 50304
0.00.039.448 I print_info: n_merges         = 50009
0.00.039.448 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.448 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.449 I print_info: LF token         = 187 ''
0.00.039.449 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.449 I print_info: max token length = 1024
0.00.039.450 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.672 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.686 I load_tensors: offloading output layer to GPU
0.00.528.686 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.720 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.721 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.530.191 I llama_init_from_model: n_seq_max     = 1
0.00.530.194 I llama_init_from_model: n_ctx         = 2048
0.00.530.194 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.530.195 I llama_init_from_model: n_batch       = 2048
0.00.530.196 I llama_init_from_model: n_ubatch      = 512
0.00.530.196 I llama_init_from_model: flash_attn    = 0
0.00.530.199 I llama_init_from_model: freq_base     = 10000.0
0.00.530.210 I llama_init_from_model: freq_scale    = 1
0.00.530.214 I ggml_metal_init: allocating
0.00.530.283 I ggml_metal_init: found device: Apple M4
0.00.530.331 I ggml_metal_init: picking default device: Apple M4
0.00.532.107 I ggml_metal_init: using embedded metal library
0.00.538.509 I ggml_metal_init: GPU name:   Apple M4
0.00.538.514 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.514 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.515 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.516 I ggml_metal_init: simdgroup reduction   = true
0.00.538.516 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.517 I ggml_metal_init: has residency sets    = true
0.00.538.517 I ggml_metal_init: has bfloat            = true
0.00.538.517 I ggml_metal_init: use bfloat            = true
0.00.538.518 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.519 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.675 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.612.081 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.612.089 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.612.124 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.616.238 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.616.241 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.616.241 I llama_init_from_model: graph nodes  = 967
0.00.616.241 I llama_init_from_model: graph splits = 2
0.00.616.248 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.616.370 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.616.370 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.534 I main: llama threadpool init, n_threads = 4
0.00.676.575 I 
0.00.676.592 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.592 I 
0.00.676.744 I sampler seed: 1234
0.00.676.748 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.676.769 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.676.769 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.676.769 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.445.244 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50678.09 tokens per second)
0.01.445.245 I llama_perf_context_print:        load time =     665.96 ms
0.01.445.245 I llama_perf_context_print: prompt eval time =      55.46 ms /     7 tokens (    7.92 ms per token,   126.22 tokens per second)
0.01.445.246 I llama_perf_context_print:        eval time =     710.01 ms /    63 runs   (   11.27 ms per token,    88.73 tokens per second)
0.01.445.246 I llama_perf_context_print:       total time =     769.46 ms /    70 tokens
0.01.445.528 I ggml_metal_free: deallocating

real	0m1.463s
user	0m0.108s
sys	0m0.199s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.826 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.833 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.839 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.841 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.843 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.843 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.843 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.843 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.844 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.844 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.846 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.846 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.847 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.716 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.700 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.504 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.506 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.506 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.507 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.507 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.508 I llama_model_loader: - type  f32:  194 tensors
0.00.025.509 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.509 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.509 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.510 I print_info: file format = GGUF V3 (latest)
0.00.025.514 I print_info: file type   = Q4_K - Medium
0.00.025.515 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.907 I load: special tokens cache size = 25
0.00.039.724 I load: token to piece cache size = 0.2984 MB
0.00.039.728 I print_info: arch             = gptneox
0.00.039.728 I print_info: vocab_only       = 0
0.00.039.728 I print_info: n_ctx_train      = 2048
0.00.039.728 I print_info: n_embd           = 2048
0.00.039.729 I print_info: n_layer          = 24
0.00.039.732 I print_info: n_head           = 16
0.00.039.732 I print_info: n_head_kv        = 16
0.00.039.733 I print_info: n_rot            = 32
0.00.039.733 I print_info: n_swa            = 0
0.00.039.733 I print_info: n_embd_head_k    = 128
0.00.039.733 I print_info: n_embd_head_v    = 128
0.00.039.734 I print_info: n_gqa            = 1
0.00.039.735 I print_info: n_embd_k_gqa     = 2048
0.00.039.737 I print_info: n_embd_v_gqa     = 2048
0.00.039.738 I print_info: f_norm_eps       = 1.0e-05
0.00.039.738 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.740 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.740 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.740 I print_info: f_logit_scale    = 0.0e+00
0.00.039.741 I print_info: n_ff             = 8192
0.00.039.741 I print_info: n_expert         = 0
0.00.039.741 I print_info: n_expert_used    = 0
0.00.039.741 I print_info: causal attn      = 1
0.00.039.742 I print_info: pooling type     = 0
0.00.039.742 I print_info: rope type        = 2
0.00.039.742 I print_info: rope scaling     = linear
0.00.039.742 I print_info: freq_base_train  = 10000.0
0.00.039.743 I print_info: freq_scale_train = 1
0.00.039.743 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.743 I print_info: rope_finetuned   = unknown
0.00.039.743 I print_info: ssm_d_conv       = 0
0.00.039.743 I print_info: ssm_d_inner      = 0
0.00.039.745 I print_info: ssm_d_state      = 0
0.00.039.745 I print_info: ssm_dt_rank      = 0
0.00.039.745 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.745 I print_info: model type       = 1.4B
0.00.039.746 I print_info: model params     = 1.41 B
0.00.039.746 I print_info: general.name     = 1.4B
0.00.039.747 I print_info: vocab type       = BPE
0.00.039.747 I print_info: n_vocab          = 50304
0.00.039.747 I print_info: n_merges         = 50009
0.00.039.747 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.747 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.748 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.748 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.751 I print_info: LF token         = 187 ''
0.00.039.751 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.751 I print_info: max token length = 1024
0.00.039.752 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.553.885 I load_tensors: offloading 24 repeating layers to GPU
0.00.553.900 I load_tensors: offloading output layer to GPU
0.00.553.901 I load_tensors: offloaded 25/25 layers to GPU
0.00.553.936 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.553.937 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.555.567 I llama_init_from_model: n_seq_max     = 1
0.00.555.570 I llama_init_from_model: n_ctx         = 128
0.00.555.570 I llama_init_from_model: n_ctx_per_seq = 128
0.00.555.571 I llama_init_from_model: n_batch       = 128
0.00.555.571 I llama_init_from_model: n_ubatch      = 128
0.00.555.571 I llama_init_from_model: flash_attn    = 0
0.00.555.573 I llama_init_from_model: freq_base     = 10000.0
0.00.555.574 I llama_init_from_model: freq_scale    = 1
0.00.555.574 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.555.577 I ggml_metal_init: allocating
0.00.555.659 I ggml_metal_init: found device: Apple M4
0.00.555.673 I ggml_metal_init: picking default device: Apple M4
0.00.557.423 I ggml_metal_init: using embedded metal library
0.00.563.921 I ggml_metal_init: GPU name:   Apple M4
0.00.563.925 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.563.926 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.563.927 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.563.927 I ggml_metal_init: simdgroup reduction   = true
0.00.563.928 I ggml_metal_init: simdgroup matrix mul. = true
0.00.563.928 I ggml_metal_init: has residency sets    = true
0.00.563.928 I ggml_metal_init: has bfloat            = true
0.00.563.928 I ggml_metal_init: use bfloat            = true
0.00.563.929 I ggml_metal_init: hasUnifiedMemory      = true
0.00.563.931 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.581.457 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.584.836 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.584.843 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.584.900 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.588.100 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.588.102 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.588.103 I llama_init_from_model: graph nodes  = 967
0.00.588.104 I llama_init_from_model: graph splits = 2
0.00.588.107 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.588.108 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.616.749 I 
0.00.616.804 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.616.810 I perplexity: tokenizing the input ..
0.00.621.278 I perplexity: tokenization took 4.466 ms
0.00.621.281 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.754.654 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.755.991 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.756.013 I llama_perf_context_print:        load time =     606.80 ms
0.00.756.014 I llama_perf_context_print: prompt eval time =     133.14 ms /   128 tokens (    1.04 ms per token,   961.38 tokens per second)
0.00.756.016 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.756.016 I llama_perf_context_print:       total time =     139.27 ms /   129 tokens
0.00.756.393 I ggml_metal_free: deallocating

real	0m0.771s
user	0m0.076s
sys	0m0.150s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.808 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.356 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.358 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.358 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.360 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.360 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.361 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.361 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.362 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.364 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.364 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.118 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.137 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.854 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.856 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.856 I llama_model_loader: - type  f32:  194 tensors
0.00.023.857 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.857 I llama_model_loader: - type q6_K:   37 tensors
0.00.023.857 I print_info: file format = GGUF V3 (latest)
0.00.023.858 I print_info: file type   = Q5_K - Medium
0.00.023.860 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.031.616 I load: special tokens cache size = 25
0.00.037.566 I load: token to piece cache size = 0.2984 MB
0.00.037.569 I print_info: arch             = gptneox
0.00.037.569 I print_info: vocab_only       = 0
0.00.037.569 I print_info: n_ctx_train      = 2048
0.00.037.569 I print_info: n_embd           = 2048
0.00.037.569 I print_info: n_layer          = 24
0.00.037.573 I print_info: n_head           = 16
0.00.037.574 I print_info: n_head_kv        = 16
0.00.037.574 I print_info: n_rot            = 32
0.00.037.575 I print_info: n_swa            = 0
0.00.037.575 I print_info: n_embd_head_k    = 128
0.00.037.576 I print_info: n_embd_head_v    = 128
0.00.037.577 I print_info: n_gqa            = 1
0.00.037.578 I print_info: n_embd_k_gqa     = 2048
0.00.037.578 I print_info: n_embd_v_gqa     = 2048
0.00.037.583 I print_info: f_norm_eps       = 1.0e-05
0.00.037.584 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.584 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.584 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.584 I print_info: f_logit_scale    = 0.0e+00
0.00.037.585 I print_info: n_ff             = 8192
0.00.037.585 I print_info: n_expert         = 0
0.00.037.585 I print_info: n_expert_used    = 0
0.00.037.586 I print_info: causal attn      = 1
0.00.037.586 I print_info: pooling type     = 0
0.00.037.586 I print_info: rope type        = 2
0.00.037.586 I print_info: rope scaling     = linear
0.00.037.587 I print_info: freq_base_train  = 10000.0
0.00.037.587 I print_info: freq_scale_train = 1
0.00.037.587 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.589 I print_info: rope_finetuned   = unknown
0.00.037.589 I print_info: ssm_d_conv       = 0
0.00.037.589 I print_info: ssm_d_inner      = 0
0.00.037.589 I print_info: ssm_d_state      = 0
0.00.037.589 I print_info: ssm_dt_rank      = 0
0.00.037.589 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.589 I print_info: model type       = 1.4B
0.00.037.590 I print_info: model params     = 1.41 B
0.00.037.590 I print_info: general.name     = 1.4B
0.00.037.590 I print_info: vocab type       = BPE
0.00.037.591 I print_info: n_vocab          = 50304
0.00.037.592 I print_info: n_merges         = 50009
0.00.037.592 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.592 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.592 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.592 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.592 I print_info: LF token         = 187 ''
0.00.037.593 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.593 I print_info: max token length = 1024
0.00.037.593 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.596.449 I load_tensors: offloading 24 repeating layers to GPU
0.00.596.462 I load_tensors: offloading output layer to GPU
0.00.596.463 I load_tensors: offloaded 25/25 layers to GPU
0.00.596.495 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.596.497 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.598.166 I llama_init_from_model: n_seq_max     = 1
0.00.598.169 I llama_init_from_model: n_ctx         = 2048
0.00.598.169 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.598.170 I llama_init_from_model: n_batch       = 2048
0.00.598.170 I llama_init_from_model: n_ubatch      = 512
0.00.598.171 I llama_init_from_model: flash_attn    = 0
0.00.598.172 I llama_init_from_model: freq_base     = 10000.0
0.00.598.172 I llama_init_from_model: freq_scale    = 1
0.00.598.173 I ggml_metal_init: allocating
0.00.598.211 I ggml_metal_init: found device: Apple M4
0.00.598.221 I ggml_metal_init: picking default device: Apple M4
0.00.599.753 I ggml_metal_init: using embedded metal library
0.00.605.908 I ggml_metal_init: GPU name:   Apple M4
0.00.605.911 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.605.912 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.605.913 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.605.913 I ggml_metal_init: simdgroup reduction   = true
0.00.605.914 I ggml_metal_init: simdgroup matrix mul. = true
0.00.605.914 I ggml_metal_init: has residency sets    = true
0.00.605.914 I ggml_metal_init: has bfloat            = true
0.00.605.914 I ggml_metal_init: use bfloat            = true
0.00.605.915 I ggml_metal_init: hasUnifiedMemory      = true
0.00.605.917 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.622.972 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.681.656 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.681.664 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.681.698 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.686.676 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.686.678 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.686.679 I llama_init_from_model: graph nodes  = 967
0.00.686.679 I llama_init_from_model: graph splits = 2
0.00.686.685 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.686.810 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.686.811 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.746.208 I main: llama threadpool init, n_threads = 4
0.00.746.252 I 
0.00.746.268 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.746.269 I 
0.00.746.432 I sampler seed: 1234
0.00.746.437 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.447 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.447 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.448 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.596.167 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52906.11 tokens per second)
0.01.596.168 I llama_perf_context_print:        load time =     736.70 ms
0.01.596.172 I llama_perf_context_print: prompt eval time =      51.24 ms /     7 tokens (    7.32 ms per token,   136.61 tokens per second)
0.01.596.173 I llama_perf_context_print:        eval time =     795.53 ms /    63 runs   (   12.63 ms per token,    79.19 tokens per second)
0.01.596.173 I llama_perf_context_print:       total time =     850.66 ms /    70 tokens
0.01.596.429 I ggml_metal_free: deallocating

real	0m1.613s
user	0m0.108s
sys	0m0.219s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.108 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.958 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.883 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.889 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.891 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.892 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.892 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.892 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.893 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.894 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.894 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.895 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.895 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.895 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.896 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.896 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.898 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.898 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.898 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.744 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.781 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.536 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.537 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.537 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.538 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.539 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.539 I llama_model_loader: - type  f32:  194 tensors
0.00.024.540 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.540 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.541 I print_info: file format = GGUF V3 (latest)
0.00.024.541 I print_info: file type   = Q5_K - Medium
0.00.024.543 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.815 I load: special tokens cache size = 25
0.00.038.953 I load: token to piece cache size = 0.2984 MB
0.00.038.958 I print_info: arch             = gptneox
0.00.038.958 I print_info: vocab_only       = 0
0.00.038.959 I print_info: n_ctx_train      = 2048
0.00.038.959 I print_info: n_embd           = 2048
0.00.038.959 I print_info: n_layer          = 24
0.00.038.963 I print_info: n_head           = 16
0.00.038.964 I print_info: n_head_kv        = 16
0.00.038.964 I print_info: n_rot            = 32
0.00.038.964 I print_info: n_swa            = 0
0.00.038.964 I print_info: n_embd_head_k    = 128
0.00.038.965 I print_info: n_embd_head_v    = 128
0.00.038.965 I print_info: n_gqa            = 1
0.00.038.966 I print_info: n_embd_k_gqa     = 2048
0.00.038.967 I print_info: n_embd_v_gqa     = 2048
0.00.038.967 I print_info: f_norm_eps       = 1.0e-05
0.00.038.968 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.968 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.968 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.970 I print_info: f_logit_scale    = 0.0e+00
0.00.038.971 I print_info: n_ff             = 8192
0.00.038.971 I print_info: n_expert         = 0
0.00.038.971 I print_info: n_expert_used    = 0
0.00.038.971 I print_info: causal attn      = 1
0.00.038.971 I print_info: pooling type     = 0
0.00.038.971 I print_info: rope type        = 2
0.00.038.973 I print_info: rope scaling     = linear
0.00.038.973 I print_info: freq_base_train  = 10000.0
0.00.038.973 I print_info: freq_scale_train = 1
0.00.038.973 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.976 I print_info: rope_finetuned   = unknown
0.00.038.976 I print_info: ssm_d_conv       = 0
0.00.038.976 I print_info: ssm_d_inner      = 0
0.00.038.976 I print_info: ssm_d_state      = 0
0.00.038.976 I print_info: ssm_dt_rank      = 0
0.00.038.976 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.976 I print_info: model type       = 1.4B
0.00.038.977 I print_info: model params     = 1.41 B
0.00.038.977 I print_info: general.name     = 1.4B
0.00.038.978 I print_info: vocab type       = BPE
0.00.038.978 I print_info: n_vocab          = 50304
0.00.038.978 I print_info: n_merges         = 50009
0.00.038.979 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.979 I print_info: LF token         = 187 ''
0.00.038.980 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.980 I print_info: max token length = 1024
0.00.038.980 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.594.080 I load_tensors: offloading 24 repeating layers to GPU
0.00.594.094 I load_tensors: offloading output layer to GPU
0.00.594.095 I load_tensors: offloaded 25/25 layers to GPU
0.00.594.125 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.594.127 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.595.967 I llama_init_from_model: n_seq_max     = 1
0.00.595.971 I llama_init_from_model: n_ctx         = 128
0.00.595.972 I llama_init_from_model: n_ctx_per_seq = 128
0.00.595.972 I llama_init_from_model: n_batch       = 128
0.00.595.972 I llama_init_from_model: n_ubatch      = 128
0.00.595.973 I llama_init_from_model: flash_attn    = 0
0.00.595.974 I llama_init_from_model: freq_base     = 10000.0
0.00.595.975 I llama_init_from_model: freq_scale    = 1
0.00.595.976 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.595.978 I ggml_metal_init: allocating
0.00.596.072 I ggml_metal_init: found device: Apple M4
0.00.596.086 I ggml_metal_init: picking default device: Apple M4
0.00.597.678 I ggml_metal_init: using embedded metal library
0.00.604.019 I ggml_metal_init: GPU name:   Apple M4
0.00.604.023 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.604.024 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.604.025 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.604.025 I ggml_metal_init: simdgroup reduction   = true
0.00.604.026 I ggml_metal_init: simdgroup matrix mul. = true
0.00.604.026 I ggml_metal_init: has residency sets    = true
0.00.604.026 I ggml_metal_init: has bfloat            = true
0.00.604.026 I ggml_metal_init: use bfloat            = true
0.00.604.027 I ggml_metal_init: hasUnifiedMemory      = true
0.00.604.030 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.620.788 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.624.254 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.624.258 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.624.299 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.627.769 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.627.771 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.627.771 I llama_init_from_model: graph nodes  = 967
0.00.627.772 I llama_init_from_model: graph splits = 2
0.00.627.775 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.627.775 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.663.720 I 
0.00.663.791 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.663.805 I perplexity: tokenizing the input ..
0.00.670.820 I perplexity: tokenization took 7.01 ms
0.00.670.828 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.817.047 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.818.380 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.818.401 I llama_perf_context_print:        load time =     654.75 ms
0.00.818.402 I llama_perf_context_print: prompt eval time =     145.72 ms /   128 tokens (    1.14 ms per token,   878.39 tokens per second)
0.00.818.405 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.818.405 I llama_perf_context_print:       total time =     154.69 ms /   129 tokens
0.00.818.784 I ggml_metal_free: deallocating

real	0m0.832s
user	0m0.078s
sys	0m0.141s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.279 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.284 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.285 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.286 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.286 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.287 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.287 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.288 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.288 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.289 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.289 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.289 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.290 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.290 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.292 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.292 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.292 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.053 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.099 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.879 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.880 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.881 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.882 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.882 I llama_model_loader: - type  f32:  194 tensors
0.00.023.883 I llama_model_loader: - type q6_K:   98 tensors
0.00.023.883 I print_info: file format = GGUF V3 (latest)
0.00.023.884 I print_info: file type   = Q6_K
0.00.023.884 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.643 I load: special tokens cache size = 25
0.00.037.318 I load: token to piece cache size = 0.2984 MB
0.00.037.321 I print_info: arch             = gptneox
0.00.037.322 I print_info: vocab_only       = 0
0.00.037.322 I print_info: n_ctx_train      = 2048
0.00.037.322 I print_info: n_embd           = 2048
0.00.037.322 I print_info: n_layer          = 24
0.00.037.325 I print_info: n_head           = 16
0.00.037.326 I print_info: n_head_kv        = 16
0.00.037.326 I print_info: n_rot            = 32
0.00.037.326 I print_info: n_swa            = 0
0.00.037.326 I print_info: n_embd_head_k    = 128
0.00.037.327 I print_info: n_embd_head_v    = 128
0.00.037.327 I print_info: n_gqa            = 1
0.00.037.328 I print_info: n_embd_k_gqa     = 2048
0.00.037.330 I print_info: n_embd_v_gqa     = 2048
0.00.037.330 I print_info: f_norm_eps       = 1.0e-05
0.00.037.332 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.332 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.333 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.333 I print_info: f_logit_scale    = 0.0e+00
0.00.037.333 I print_info: n_ff             = 8192
0.00.037.333 I print_info: n_expert         = 0
0.00.037.334 I print_info: n_expert_used    = 0
0.00.037.334 I print_info: causal attn      = 1
0.00.037.334 I print_info: pooling type     = 0
0.00.037.334 I print_info: rope type        = 2
0.00.037.334 I print_info: rope scaling     = linear
0.00.037.335 I print_info: freq_base_train  = 10000.0
0.00.037.335 I print_info: freq_scale_train = 1
0.00.037.335 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.336 I print_info: rope_finetuned   = unknown
0.00.037.336 I print_info: ssm_d_conv       = 0
0.00.037.336 I print_info: ssm_d_inner      = 0
0.00.037.336 I print_info: ssm_d_state      = 0
0.00.037.336 I print_info: ssm_dt_rank      = 0
0.00.037.336 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.337 I print_info: model type       = 1.4B
0.00.037.337 I print_info: model params     = 1.41 B
0.00.037.337 I print_info: general.name     = 1.4B
0.00.037.338 I print_info: vocab type       = BPE
0.00.037.338 I print_info: n_vocab          = 50304
0.00.037.338 I print_info: n_merges         = 50009
0.00.037.339 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.339 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.341 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.341 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.341 I print_info: LF token         = 187 ''
0.00.037.341 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.342 I print_info: max token length = 1024
0.00.037.342 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.638.707 I load_tensors: offloading 24 repeating layers to GPU
0.00.638.723 I load_tensors: offloading output layer to GPU
0.00.638.724 I load_tensors: offloaded 25/25 layers to GPU
0.00.638.758 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.638.759 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.640.492 I llama_init_from_model: n_seq_max     = 1
0.00.640.494 I llama_init_from_model: n_ctx         = 2048
0.00.640.494 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.640.495 I llama_init_from_model: n_batch       = 2048
0.00.640.495 I llama_init_from_model: n_ubatch      = 512
0.00.640.496 I llama_init_from_model: flash_attn    = 0
0.00.640.497 I llama_init_from_model: freq_base     = 10000.0
0.00.640.497 I llama_init_from_model: freq_scale    = 1
0.00.640.499 I ggml_metal_init: allocating
0.00.640.543 I ggml_metal_init: found device: Apple M4
0.00.640.554 I ggml_metal_init: picking default device: Apple M4
0.00.642.048 I ggml_metal_init: using embedded metal library
0.00.648.062 I ggml_metal_init: GPU name:   Apple M4
0.00.648.066 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.648.067 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.648.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.648.068 I ggml_metal_init: simdgroup reduction   = true
0.00.648.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.648.068 I ggml_metal_init: has residency sets    = true
0.00.648.069 I ggml_metal_init: has bfloat            = true
0.00.648.069 I ggml_metal_init: use bfloat            = true
0.00.648.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.648.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.664.490 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.717.084 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.717.090 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.717.170 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.721.807 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.721.810 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.721.810 I llama_init_from_model: graph nodes  = 967
0.00.721.810 I llama_init_from_model: graph splits = 2
0.00.721.815 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.721.942 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.721.943 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.255 I main: llama threadpool init, n_threads = 4
0.00.784.304 I 
0.00.784.321 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.321 I 
0.00.784.483 I sampler seed: 1234
0.00.784.487 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.508 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.509 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.509 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.665.375 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53504.14 tokens per second)
0.01.665.375 I llama_perf_context_print:        load time =     774.69 ms
0.01.665.376 I llama_perf_context_print: prompt eval time =      54.00 ms /     7 tokens (    7.71 ms per token,   129.62 tokens per second)
0.01.665.377 I llama_perf_context_print:        eval time =     823.90 ms /    63 runs   (   13.08 ms per token,    76.47 tokens per second)
0.01.665.378 I llama_perf_context_print:       total time =     881.83 ms /    70 tokens
0.01.665.644 I ggml_metal_free: deallocating

real	0m1.683s
user	0m0.107s
sys	0m0.209s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4711 (04045bb8) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.096 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.582 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.588 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.590 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.590 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.591 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.596 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.596 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.597 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.597 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.597 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.598 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.600 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.600 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.600 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.334 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.083 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.085 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.085 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.085 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.087 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.090 I llama_model_loader: - type  f32:  194 tensors
0.00.024.091 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.091 I print_info: file format = GGUF V3 (latest)
0.00.024.092 I print_info: file type   = Q6_K
0.00.024.093 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.059 I load: special tokens cache size = 25
0.00.038.167 I load: token to piece cache size = 0.2984 MB
0.00.038.171 I print_info: arch             = gptneox
0.00.038.171 I print_info: vocab_only       = 0
0.00.038.171 I print_info: n_ctx_train      = 2048
0.00.038.172 I print_info: n_embd           = 2048
0.00.038.172 I print_info: n_layer          = 24
0.00.038.176 I print_info: n_head           = 16
0.00.038.176 I print_info: n_head_kv        = 16
0.00.038.176 I print_info: n_rot            = 32
0.00.038.177 I print_info: n_swa            = 0
0.00.038.179 I print_info: n_embd_head_k    = 128
0.00.038.180 I print_info: n_embd_head_v    = 128
0.00.038.180 I print_info: n_gqa            = 1
0.00.038.181 I print_info: n_embd_k_gqa     = 2048
0.00.038.182 I print_info: n_embd_v_gqa     = 2048
0.00.038.183 I print_info: f_norm_eps       = 1.0e-05
0.00.038.183 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.183 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.183 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.183 I print_info: f_logit_scale    = 0.0e+00
0.00.038.184 I print_info: n_ff             = 8192
0.00.038.184 I print_info: n_expert         = 0
0.00.038.184 I print_info: n_expert_used    = 0
0.00.038.185 I print_info: causal attn      = 1
0.00.038.185 I print_info: pooling type     = 0
0.00.038.185 I print_info: rope type        = 2
0.00.038.192 I print_info: rope scaling     = linear
0.00.038.194 I print_info: freq_base_train  = 10000.0
0.00.038.194 I print_info: freq_scale_train = 1
0.00.038.196 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.196 I print_info: rope_finetuned   = unknown
0.00.038.196 I print_info: ssm_d_conv       = 0
0.00.038.197 I print_info: ssm_d_inner      = 0
0.00.038.197 I print_info: ssm_d_state      = 0
0.00.038.198 I print_info: ssm_dt_rank      = 0
0.00.038.198 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.198 I print_info: model type       = 1.4B
0.00.038.198 I print_info: model params     = 1.41 B
0.00.038.198 I print_info: general.name     = 1.4B
0.00.038.199 I print_info: vocab type       = BPE
0.00.038.199 I print_info: n_vocab          = 50304
0.00.038.199 I print_info: n_merges         = 50009
0.00.038.200 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.200 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.200 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.200 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.200 I print_info: LF token         = 187 ''
0.00.038.201 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.201 I print_info: max token length = 1024
0.00.038.201 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.576.387 I load_tensors: offloading 24 repeating layers to GPU
0.00.576.390 I load_tensors: offloading output layer to GPU
0.00.576.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.576.421 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.576.423 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.578.001 I llama_init_from_model: n_seq_max     = 1
0.00.578.003 I llama_init_from_model: n_ctx         = 128
0.00.578.003 I llama_init_from_model: n_ctx_per_seq = 128
0.00.578.004 I llama_init_from_model: n_batch       = 128
0.00.578.004 I llama_init_from_model: n_ubatch      = 128
0.00.578.005 I llama_init_from_model: flash_attn    = 0
0.00.578.005 I llama_init_from_model: freq_base     = 10000.0
0.00.578.006 I llama_init_from_model: freq_scale    = 1
0.00.578.007 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.578.012 I ggml_metal_init: allocating
0.00.578.077 I ggml_metal_init: found device: Apple M4
0.00.578.088 I ggml_metal_init: picking default device: Apple M4
0.00.579.550 I ggml_metal_init: using embedded metal library
0.00.585.583 I ggml_metal_init: GPU name:   Apple M4
0.00.585.587 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.585.587 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.585.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.585.589 I ggml_metal_init: simdgroup reduction   = true
0.00.585.589 I ggml_metal_init: simdgroup matrix mul. = true
0.00.585.589 I ggml_metal_init: has residency sets    = true
0.00.585.589 I ggml_metal_init: has bfloat            = true
0.00.585.590 I ggml_metal_init: use bfloat            = true
0.00.585.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.585.599 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.602.150 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.605.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.605.576 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.605.636 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.608.727 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.608.729 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.608.729 I llama_init_from_model: graph nodes  = 967
0.00.608.730 I llama_init_from_model: graph splits = 2
0.00.608.733 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.608.733 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.680 I 
0.00.645.730 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.738 I perplexity: tokenizing the input ..
0.00.652.281 I perplexity: tokenization took 6.54 ms
0.00.652.291 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.799.820 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.801.273 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.801.296 I llama_perf_context_print:        load time =     636.57 ms
0.00.801.297 I llama_perf_context_print: prompt eval time =     146.81 ms /   128 tokens (    1.15 ms per token,   871.89 tokens per second)
0.00.801.298 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.801.299 I llama_perf_context_print:       total time =     155.62 ms /   129 tokens
0.00.801.641 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.076s
sys	0m0.130s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4711 (04045bb8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123605260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1236085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123608a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123608ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123609340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1236097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123609c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12360a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12360a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12360a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12360ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12360b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12360bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12360c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12360cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12360d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12360dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12360e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12360ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12360f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12360fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x1236101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123610910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1236111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1236118d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123611b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123611e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1236122c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1236129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123612e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123613410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123613920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123613d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123614050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1236144c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123614930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123614e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123615390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123615890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123615d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123616290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123616790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123616c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123617190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123617690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123617b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123617f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1236183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123618b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123618fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123619450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1236198c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123619d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12361a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12361a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12361ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12361b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12361b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12361ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12361c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12361c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12361c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12361ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12361d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12361d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12361dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12361e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12361e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12361ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12361eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12361f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12361f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12361fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1236201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123620740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123620c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1236211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123621730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123621c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1236221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123622720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123622c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1236231c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123623710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123623c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1236241b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123624700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123624c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1236256f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123625c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123626190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1236266e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123626c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123627180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1236276d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123627c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1236186a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123628090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123628840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123628d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1236292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123629830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123629d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12362a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12362a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12362ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12362b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12362b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12362bd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12362c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12362c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12362cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12362d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12362d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12362db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12362dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12362e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12362e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12362edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12362f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12362f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12362fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1236302a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123630560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123630a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123630f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123631460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123631960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123631e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123632360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123632860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123632d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123633260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123633760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123633c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123634160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123634660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123634b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123635060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123635560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123635a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123635f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123636460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123636960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123636e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123637360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123637860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123637d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123638260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123638760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123638c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123639160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123639660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123639b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12363a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12363a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12363aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12363af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12363b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12363b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12363be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12363c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12363c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12363cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12363d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12363d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12363dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12363e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12363e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12363eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12363f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12363f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12363fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12363ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123640460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123640960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123640e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123641360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123641860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123641d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123642260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123642760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123642c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123643160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123643660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123643b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123644060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123644560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123644a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123644f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123645460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123645960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123645e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123646410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1236469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123646f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123647520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123647b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123648140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123648750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123648f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1236493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1236496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123649cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12364a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12364aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12364af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12364b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12364b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12364c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12364c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12364cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12364d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12364d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12364dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12364e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12364e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12364eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12364f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12364f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12364fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123650000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123650550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123650aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123650ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123651540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123651a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123651fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123652530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123652a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123652fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123653520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123653a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123653fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123654510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123654a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123654fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123655500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123655a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123655fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1236564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123656a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123656f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1236574e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123657a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123657f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1236584d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123658a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123658f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1236594c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123659a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123659f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12365a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12365aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12365af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12365b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12365b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12365bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12365c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12365c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12365cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12365d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12365d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12365df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12365e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12365e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12365ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12365f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12365f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12365fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1236600e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123660580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123660a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123660ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123661360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123661800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123661ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123662140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1236625e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123662a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123662f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123663470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123663b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1236642b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1236649d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1236650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1236653b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123665ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123665e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123666470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.689.638 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.689.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123646120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123647df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123649960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123666120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1236477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123648400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123649f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123610e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12360b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12361a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12361be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123628350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12364a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123612580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123666c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123666f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1236671c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123667480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123667740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123667a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123667cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123667f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123668240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123668500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1236687c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123668a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123668d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123669000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1236692c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123669580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123669840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123669b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123669dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12366a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12366a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12366a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12366a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12366ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12366ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12366b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12366b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12366b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12366b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12366bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12366bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12366c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12366c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12366c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12366c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12366cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12366cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12366d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12366d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12366d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12366da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12366dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12366dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12366e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12366e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12366e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12366eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12366ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12366f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12366f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12366f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12366f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12366fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12366fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1236700c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123670380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123670640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123670900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123670bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123670e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123671140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123671400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1236716c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123671980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123671c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123671f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1236721c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123672480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123672740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123672a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123672cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123672f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123673240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123673500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236737c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123673a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123673d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123674000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1236742c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123674580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123674840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123674b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123674dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123675080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123675340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123675600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1236758c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123675b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123675e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123676100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1236763c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123676680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123676940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123676c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123676ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123677180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123677440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123677700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1236779c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123677c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123677f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123678200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1236784c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123678780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123678a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123678d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123678fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123679280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123679540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123679800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123679ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123679d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12367a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12367a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12367a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12367a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12367ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12367ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12367b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12367b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12367b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12367b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12367bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12367be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12367c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12367c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12367c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12367c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12367cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12367cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12367d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12367d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12367d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12367da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12367dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12367df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12367e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12367e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12367e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12367ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12367ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12367f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12367f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12367f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12367f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12367fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12367fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123680080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123680340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123680600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1236808c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123680b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123680e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123681100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1236813c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123681680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123681940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123681c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123681ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123682180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123682440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123682700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1236829c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123682c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123682f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123683200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1236834c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123683780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123683a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123683d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123683fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123684280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123684540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123684800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123684ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123684d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123685040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123685300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1236855c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123685880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123685b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123685e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1236860c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123686380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123686640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123686900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123686bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123686e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123687140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123687400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1236876c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123687980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123687c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123687f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1236881c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123688480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123688a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123688d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123688fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123689290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123689550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123689810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123689d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12368a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12368a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12368ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12368b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12368b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12368bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12368c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12368c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12368cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12368d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12368d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12368dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12368e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12368e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12368ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12368f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12368f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12368fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123690250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1236907a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123690cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123691240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123691790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123691ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123692230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123692780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123692cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123693220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123693770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123693cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123694210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123694760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123694cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123695200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123695750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123695ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1236961f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123696740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123696c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1236971e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123697730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123697c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1236981d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123698720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123698c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1236991c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123699710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123699c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12369a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11b304080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11b3044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11b304ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11b304fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11b3054d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11b3059d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11b305ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11b3063d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11b3068d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11b306dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11b3072d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11b3077d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11b307cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11b3081d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11b3086d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11b308bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11b3090d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11b309ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11b30a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11b30a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11b30b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11b30b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11b30baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11b30bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11b30c3c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12369a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12369a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12369a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12369ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12369b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12369b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12369b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12369b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12369bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12369be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12369c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12369c3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12369c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12369cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12369d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12369d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12369daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12369ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12369e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12369e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12369e5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12369e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12369eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12369ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12369f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12369f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12369f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12369f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12369fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12369feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1236a0170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1236a0430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1236a06f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1236a09b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1236a0c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1236a0f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1236a11f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1236a14b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1236a1770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1236a1a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1236a1cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1236a1fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1236a2270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1236a2530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1236a27f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1236a2ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1236a2d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1236a3030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1236a32f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1236a35b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1236a3870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1236a3b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1236a3df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1236a40b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1236a4370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1236a4630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1236a48f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1236a4bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1236a4e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1236a5130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1236a53f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x1236a56b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1236a5970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1236a5c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1236a5ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1236a61b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1236a6470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1236a6730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1236a69f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x1236a6cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1236a6f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1236a7230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1236a74f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1236a77b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1236a7a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1236a7d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1236a7ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1236a82b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1236a8570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1236a8830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1236a8af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x1236a8db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1236a9070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1236a9330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1236a95f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1236a98b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1236a9b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1236a9e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1236aa0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1236aa3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1236aa670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1236aa930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1236aabf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1236aaeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1236ab170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1236ab430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1236ab6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1236ab9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1236abc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1236abf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1236ac1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1236ac4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1236ac770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1236aca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1236accf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1236acfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1236ad270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1236ad530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1236ad7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1236adab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1236add70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1236ae030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1236ae2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1236ae5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1236ae870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1236aeb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1236aedf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1236af0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1236af370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1236af630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1236af8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1236afbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1236afe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1236b0130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1236b03f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1236b06b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1236b0970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1236b0c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1236b0ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1236b11b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1236b1470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1236b1730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1236b19f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1236b1cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1236b1f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1236b2230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1236b24f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x1236b27b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1236b2a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1236b2d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1236b2ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1236b32b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1236b3570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1236b3830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1236b3af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1236b3db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1236b4070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1236b4330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1236b45f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1236b48b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1236b4b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1236b4e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1236b50f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1236b53b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1236b5670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1236b5930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1236b5bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1236b5eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1236b6170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1236b6430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1236b66f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1236b69b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1236b6c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1236b6f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1236b71f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1236b74b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1236b7770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1236b7a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1236b7cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1236b7fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1236b8270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1236b8530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1236b87f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1236b8ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1236b8d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1236b9030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1236b92f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1236b95b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1236b9870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1236b9b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1236b9df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1236ba0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1236ba370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1236ba630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1236ba8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1236babb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1236bae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1236bb130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1236bb3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1236bb6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1236bb970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1236bbc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1236bbef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1236bc1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1236bc470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1236bc730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1236bc9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1236bccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1236bcf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1236bd230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1236bd4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1236bd7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1236bda70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1236bdd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1236bdff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1236be2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1236be570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1236be830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1236beaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1236bedb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1236bf380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1236bf640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1236bf900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1236bfbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1236bfe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1236c0140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1236c0400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1236c06c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1236c0980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1236c0c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1236c0f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1236c11c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1236c1480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1236c1740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1236c1a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1236c1cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1236c1f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1236c2240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1236c2500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1236c27c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1236c2a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1236c2d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1236c3000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1236c32c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1236c3580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1236c3840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1236c3b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1236c3dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1236c4080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1236c4340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1236c4600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1236c48c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1236c4b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1236c4e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1236c5100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1236c53c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1236c5680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1236c5940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1236c5c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1236c5ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1236c6180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1236c6440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1236c6700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1236c69c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1236c6c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1236c71d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1236c7720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1236c7c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1236c81c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1236c8710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1236c8c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1236c91b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1236c9700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1236c9c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1236ca1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1236ca6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1236cac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1236caf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1236cb1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1236cb6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1236cbbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1236cc0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1236cc5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1236ccac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1236ccfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1236cd4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1236cd9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1236cdec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1236ce3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1236ce8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1236cedc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1236cf2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1236cf7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1236d01d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1236d08f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1236d1010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1236d1730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1236d19f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1236d21e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1236d24a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1236d2ab0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.732s
user	0m0.279s
sys	0m0.323s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4711 (04045bb8)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e0e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e0edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e0f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e0f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e0fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e10470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e10a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e10fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e11580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e11a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e11f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e12480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e12fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e13750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e13f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e14680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e14da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e163b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e16ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e171f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e17910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e181b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e188d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e18b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e191a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e19e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e1a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e1a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e1ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e1bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e1be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e1c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e1c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e1cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e1d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e1d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e1d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e1de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e1e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e1e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e1f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e1f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e1ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e205b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e20bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e211d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e217e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e21df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e22400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e22bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e23090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e23530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e23e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e245f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e248b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e24d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e251f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e25690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e25b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e25fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e26470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e26910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e26db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e27250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e276f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e28030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e28580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e29570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e2e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e1fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e30bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e31120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e31670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e31bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e32110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e32660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e32bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e33100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e340f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e34640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e34b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e35ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e36360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e36800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e36ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e37140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e375e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e37a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e37f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e383c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e38860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e38d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e391a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e39640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e39ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e39f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e3a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e3b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e3bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e3bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e3c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e3c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e3cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e3d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e3d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e3dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e3e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e3e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e3e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e3ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e3f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e3f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e3fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e40540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e409e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e40e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e41320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e417c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e41c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e42100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e425a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e42a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e42ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e43cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e44160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e44aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e44f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e453e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e45880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e45d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e46b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e46fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e47440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e478e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e47d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e48220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e48b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e494a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e49940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e49de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e4a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e4a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e4abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e4b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e4be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e4c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e4c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e4cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e4d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e4d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e4dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e4e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e4f500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e4f9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e4fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e50270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e51070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e519b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e51e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e52600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e530a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e535f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e53b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e54090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e54b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e55080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e555d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e55b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e56070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e565c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e56b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e575b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e57b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e58050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e585a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e58af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e59040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e59590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e59ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e5a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e5a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e5aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e5b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e5b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e5bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e5c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e5c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e5cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e5d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e5d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e5daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e5dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e5e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e5ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e5efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e5f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e5fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e5ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e60520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e60fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e61510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e61a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e61fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e62500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e62a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e62fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e634f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e63a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e63f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e644e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e64a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e64f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e65420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e658c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e65d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e66200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e666a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e66b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e66fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e67480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e67920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e67dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e68260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e68700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126e68ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126e69040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126e694e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126e69a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126e6a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126e6a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126e6af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126e6b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126e6b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126e6c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126e6c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126e6ca30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.428 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126f07d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126f081c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126f08630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126f08aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126f08f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126f097f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126f09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126f0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126f0a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126f0a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126f0b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126f0bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126f0c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126f0cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126f0d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126f0d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126f0e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126f0e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126f0ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126f0f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126f0fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126f10490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126f10bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126f112d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126f11590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126f11850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126f11cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126f12130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126f125a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126f12a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126f12f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126f133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126f13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126f13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126f13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126f143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126f14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126f14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126f15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126f15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126f159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126f15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126f162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126f16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126f16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126f17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126f17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126f17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126f17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126f181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126f18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126f18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126f18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126f193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126f19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126f19d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126f1a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126f1a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126f1ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126f1afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126f1b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126f1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126f1bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126f1c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126f1c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126f1ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126f1cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126f1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126f1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126f1dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126f1e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126f1e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126f1e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126f1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126f1f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126f1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126f1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126f1ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126f20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126f20890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126f20d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126f21170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126f215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126f21a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126f21ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126f22330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126f227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126f22c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126f23080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126f234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126f23960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126f23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126f24240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126f246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126f24b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126f24f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126f25400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126f25870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126f25ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126f26150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126f265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126f26a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126f26ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126f27310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126f27780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126f27bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126f28060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126f284d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126f28940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126f28db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126f29220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126f29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126f29b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126f29f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126f2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126f2a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126f2acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126f2b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126f2b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126f2ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126f2be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126f2c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126f2c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126f2cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126f2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126f2d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126f2d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126f2dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126f2e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126f2e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126f2eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126f2ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126f2f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126f2f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126f2fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126f30110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126f30580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126f309f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126f30e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126f312d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126f31740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126f31bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126f32020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126f32490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126f32900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126f32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126f331e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126f33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126f33ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126f33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126f343a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126f34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126f34c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126f350f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126f35560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126f359d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126f35e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126f362b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126f36720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126f36b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126f37000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126f37470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126f378e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126f37d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126f381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126f38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126f390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126f39370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126f397e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126f39c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126f3a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126f3a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126f3a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126f3ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126f3b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126f3b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126f3bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126f3bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126f3c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126f3c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126f3cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126f3d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126f3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126f3da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126f3dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126f3e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126f3e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126f3ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126f3f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126f3f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126f3f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126f3fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126f40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126f406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126f40b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126f40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126f41420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126f41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126f41d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126f42170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126f425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126f42b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126f43050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126f434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126f43930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126f43da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126f44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126f44730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126f44c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126f457b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126f45a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126f46030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126f465f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126f46bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126f47170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126f47730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126f47cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126f482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126f48870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126f48e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126f493f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126f499b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126f49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126f4a530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126f4aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126f4b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126f4b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126f4bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126f4c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126f4c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126f4cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126f4d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126f4d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126f4deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126f4e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126f4ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126f4eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126f4f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126f4fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126f50130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126f506f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126f50cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126f51270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126f51830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126f51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126f523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126f52970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126f52f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126f534f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126f53ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126f54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126f54630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126f54bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126f551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126f55770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126f55d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126f562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126f568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126f56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126f57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126f579f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126f57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126f58570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126f58b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126f590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126f596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126f59c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126f5a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126f5a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126f5ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126f5b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126f5b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126f5ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126f5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126f5c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126f5c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126f5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126f5d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126f5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126f5dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126f5e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126f5e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126f5f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126f5f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126f5ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126f606e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126f609a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126f61190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126f61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126f61a60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x126e6c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x126e4e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x126e4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x126e4e9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x126e21aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x126e21490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x126e23ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x126e50530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x126e18e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x126e1f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x126e20260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x126e20870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x126e1ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x126e20e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x126e17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x126e240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x126e306e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x126e6bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x126e1b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x126e1b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x126e50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x126e4efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x126e19460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x126e19720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x126e199e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x126e6ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x126e6d150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x126e6d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x126e6d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x126e6d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x126e6dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x126e6df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x126e6e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x126e6e490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x126e6e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x126e6ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x126e6ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x126e6ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x126e6f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x126e6f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x126e6f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x126e6fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x126e6fd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x126e70010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x126e702d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x126e70590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x126e70850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x126e70b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x126e70dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x126e71090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x126e71350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x126e71610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x126e718d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x126e71b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x126e71e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x126e72110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x126e723d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x126e72690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x126e72950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x126e72c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x126e72ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x126e73190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x126e73450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x126e73710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x126e739d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x126e73c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x126e73f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x126e74210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x126e744d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x126e74790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x126e74a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x126e74d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x126e74fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x126e75290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x126e75550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x126e75810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x126e75ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x126e75d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x126e76050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x126e76310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x126e765d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x126e76890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x126e76b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x126e76e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x126e770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x126e77390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x126e77650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x126e77910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x126e77bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x126e77e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x126e78150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x126e78410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x126e786d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x126e78990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x126e78c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x126e78f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x126e791d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x126e79490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x126e79750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x126e79a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x126e79cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x126e79f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x126e7a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x126e7a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x126e7a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x126e7aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x126e7ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x126e7b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x126e7b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x126e7b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x126e7b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x126e7bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x126e7bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x126e7c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x126e7c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x126e7c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x126e7c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x126e7cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x126e7ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x126e7d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x126e7d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x126e7d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x126e7d950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x126e7dc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x126e7ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x126e7e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x126e7e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x126e7e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x126e7e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x126e7ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x126e7ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x126e7f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x126e7f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x126e7f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x126e7fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x126e7fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x126e7ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x126e80290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x126e80550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x126e80810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x126e80ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x126e80d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x126e81050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x126e81310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x126e815d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x126e81890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x126e81b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x126e81e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x126e820d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x126e82390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x126e82650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x126e82910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x126e82bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x126e82e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x126e83150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x126e83410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x126e836d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x126e83990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x126e83c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x126e83f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x126e841d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x126e84490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x126e84750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x126e84a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x126e84cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x126e84f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x126e85250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x126e85510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x126e857d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x126e85a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x126e85d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x126e86010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x126e862d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x126e86590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x126e86850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x126e86b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x126e86dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x126e87090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x126e87350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x126e87610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x126e878d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x126e87b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x126e87e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x126e88110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x126e883d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x126e88690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x126e88950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x126e88c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x126e88ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x126e89190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x126e89450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x126e89710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x126e899d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x126e89c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x126e89f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x126e8a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x126e8a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x126e8a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x126e8aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x126e8ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x126e8afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x126e8b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x126e8b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x126e8b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x126e8bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x126e8bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x126e8c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x126e8c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x126e8c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x126e8cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x126e8d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x126e8d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x126e8d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x126e8dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x126e8e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x126e8e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x126e8eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x126e8ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x126e8f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x126e8f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x126e8fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x126e900d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x126e90540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x126e909b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x126e90e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x126e91290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x126e91700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x126e91b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x126e91fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x126e92450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x126e928c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x126e92d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x126e931a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x126e93610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x126e93a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x126e93ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x126e94360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x126e947d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x126e94c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x126e950b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x126e95520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x126e95990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x126e95e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x126e96270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x126e966e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x126e96b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x126e96fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x126e97430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x126e978a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x126e97d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x126e98180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x126e985f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x126e98a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x126e98ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x126e99340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x126e997b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x126e99c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x126e9a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x126e9a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x126e9a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x126e9ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x126e9b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x126e9b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x126e9bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x126e9bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x126e9c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x126e9c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x126e9ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x126e9d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x126e9d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x126e9da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x126e9deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x126e9e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x126e9e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x126e9ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x126e9f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x126e9f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x126e9f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x126e9fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126ea0230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x126ea06a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x126ea0b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x126ea0f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x126ea19f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x126ea2110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x126ea2830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x126ea2f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x126ea3210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x126ea3a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x126ea3cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x126ea42d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.947s
user	0m0.230s
sys	0m0.183s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.43 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.17 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.60 sec*proc (2 tests)

Total Test time (real) =   1.62 sec
        1.64 real         0.51 user         0.20 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.29 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.54 real         0.12 user         0.08 sys
```
