Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:301 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.553s
user	0m0.895s
sys	0m1.204s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  1%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  6%] Linking CXX shared library libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  8%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 14%] Linking C shared library libggml-metal.dylib
[ 14%] Built target ggml-metal
[ 14%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 15%] Linking CXX shared library libggml.dylib
[ 15%] Built target ggml
[ 15%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 15%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 26%] Linking CXX shared library libllama.dylib
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 26%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 27%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Built target llava
[ 35%] Linking C executable ../bin/test-c
[ 35%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 37%] Linking CXX static library libcommon.a
[ 37%] Built target llama-simple-chat
[ 37%] Built target llama-quantize-stats
[ 37%] Built target test-c
[ 37%] Built target llama-simple
[ 37%] Built target llava_static
[ 37%] Built target common
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-sampling
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-llama-grammar
[ 49%] Linking CXX executable ../bin/test-grammar-parser
[ 50%] Linking CXX executable ../bin/test-log
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-tokenizer-1-bpe
[ 50%] Built target test-tokenizer-1-spm
[ 50%] Built target test-tokenizer-0
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Built target test-grammar-integration
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Built target test-log
[ 50%] Built target test-grammar-parser
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 51%] Built target test-arg-parser
[ 52%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Linking CXX executable ../bin/test-chat-template
[ 53%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 56%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Built target test-chat-template
[ 61%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-gguf
[ 63%] Built target test-backend-ops
[ 63%] Built target test-autorelease
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-barrier
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-quantize-perf
[ 64%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 65%] Built target test-rope
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Built target llama-batched-bench
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 67%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-embedding
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Built target llama-batched
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Built target llama-embedding
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 74%] Built target llama-gguf-split
[ 74%] Built target llama-gritlm
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Built target llama-lookahead
[ 77%] Built target llama-imatrix
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Built target llama-bench
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup-merge
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 81%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 81%] Linking CXX executable ../../bin/llama-parallel
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-cli
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-lookup-merge
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 83%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Built target llama-lookup-stats
[ 84%] Generating loading.html.hpp
[ 84%] Built target llama-parallel
[ 84%] Built target llama-cli
[ 85%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 85%] Built target llama-passkey
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 86%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 86%] Built target llama-quantize
[ 86%] Linking CXX executable ../../bin/llama-run
[ 86%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 86%] Built target llama-perplexity
[ 86%] Built target llama-retrieval
[ 86%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Built target llama-run
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 91%] Built target llama-save-load-state
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Built target llama-speculative
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Built target llama-speculative-simple
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Built target llama-tokenize
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 97%] Built target llama-cvector-generator
[ 97%] Built target llama-convert-llama2c-to-ggml
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 98%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-llava-cli
[ 99%] Linking CXX executable ../../bin/llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.047s
user	0m6.167s
sys	0m9.557s

main: quantize time =  3119.02 ms
main:    total time =  3119.02 ms

main: quantize time =  1350.56 ms
main:    total time =  1350.56 ms

main: quantize time =  1309.52 ms
main:    total time =  1309.52 ms

main: quantize time =  1462.65 ms
main:    total time =  1462.65 ms

main: quantize time =  2066.55 ms
main:    total time =  2066.55 ms

main: quantize time =  4987.23 ms
main:    total time =  4987.23 ms

main: quantize time =  5723.35 ms
main:    total time =  5723.35 ms

main: quantize time =  6727.83 ms
main:    total time =  6727.83 ms

main: quantize time =  5709.72 ms
main:    total time =  5709.72 ms

main: quantize time =  4418.86 ms
main:    total time =  4418.86 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.173 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.287 I main: llama backend init
0.00.000.292 I main: load the model and apply lora adapter, if any
0.00.039.219 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.052.228 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.052.247 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.052.253 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.052.258 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.052.269 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.052.270 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.052.270 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.052.273 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.052.274 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.052.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.052.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.052.276 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.052.277 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.052.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.052.284 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.052.285 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.052.286 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.061.047 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.063.371 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.071.362 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.071.366 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.071.367 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.071.367 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.071.368 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.071.368 I llama_model_loader: - type  f32:  194 tensors
0.00.071.369 I llama_model_loader: - type  f16:   98 tensors
0.00.071.378 I print_info: file format = GGUF V3 (latest)
0.00.071.379 I print_info: file type   = all F32 (guessed)
0.00.071.380 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.101.448 I load: special tokens cache size = 25
0.00.108.519 I load: token to piece cache size = 0.2984 MB
0.00.108.522 I print_info: arch             = gptneox
0.00.108.522 I print_info: vocab_only       = 0
0.00.108.522 I print_info: n_ctx_train      = 2048
0.00.108.523 I print_info: n_embd           = 2048
0.00.108.523 I print_info: n_layer          = 24
0.00.108.526 I print_info: n_head           = 16
0.00.108.527 I print_info: n_head_kv        = 16
0.00.108.527 I print_info: n_rot            = 32
0.00.108.527 I print_info: n_swa            = 0
0.00.108.527 I print_info: n_embd_head_k    = 128
0.00.108.527 I print_info: n_embd_head_v    = 128
0.00.108.528 I print_info: n_gqa            = 1
0.00.108.529 I print_info: n_embd_k_gqa     = 2048
0.00.108.529 I print_info: n_embd_v_gqa     = 2048
0.00.108.530 I print_info: f_norm_eps       = 1.0e-05
0.00.108.530 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.108.530 I print_info: f_clamp_kqv      = 0.0e+00
0.00.108.531 I print_info: f_max_alibi_bias = 0.0e+00
0.00.108.531 I print_info: f_logit_scale    = 0.0e+00
0.00.108.531 I print_info: n_ff             = 8192
0.00.108.532 I print_info: n_expert         = 0
0.00.108.532 I print_info: n_expert_used    = 0
0.00.108.532 I print_info: causal attn      = 1
0.00.108.532 I print_info: pooling type     = 0
0.00.108.534 I print_info: rope type        = 2
0.00.108.534 I print_info: rope scaling     = linear
0.00.108.535 I print_info: freq_base_train  = 10000.0
0.00.108.535 I print_info: freq_scale_train = 1
0.00.108.535 I print_info: n_ctx_orig_yarn  = 2048
0.00.108.535 I print_info: rope_finetuned   = unknown
0.00.108.536 I print_info: ssm_d_conv       = 0
0.00.108.536 I print_info: ssm_d_inner      = 0
0.00.108.536 I print_info: ssm_d_state      = 0
0.00.108.536 I print_info: ssm_dt_rank      = 0
0.00.108.537 I print_info: ssm_dt_b_c_rms   = 0
0.00.108.538 I print_info: model type       = 1.4B
0.00.108.538 I print_info: model params     = 1.41 B
0.00.108.538 I print_info: general.name     = 1.4B
0.00.108.539 I print_info: vocab type       = BPE
0.00.108.539 I print_info: n_vocab          = 50304
0.00.108.539 I print_info: n_merges         = 50009
0.00.108.539 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.108.540 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.108.540 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.108.540 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.108.544 I print_info: LF token         = 128 'Ä'
0.00.108.544 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.108.544 I print_info: max token length = 1024
0.00.111.212 I load_tensors: offloading 24 repeating layers to GPU
0.00.111.212 I load_tensors: offloading output layer to GPU
0.00.111.213 I load_tensors: offloaded 25/25 layers to GPU
0.00.111.231 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.111.232 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.111.529 I llama_init_from_model: n_seq_max     = 1
0.00.111.530 I llama_init_from_model: n_ctx         = 2048
0.00.111.530 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.111.531 I llama_init_from_model: n_batch       = 2048
0.00.111.531 I llama_init_from_model: n_ubatch      = 512
0.00.111.531 I llama_init_from_model: flash_attn    = 0
0.00.111.531 I llama_init_from_model: freq_base     = 10000.0
0.00.111.532 I llama_init_from_model: freq_scale    = 1
0.00.111.532 I ggml_metal_init: allocating
0.00.111.535 I ggml_metal_init: found device: Apple M4
0.00.111.537 I ggml_metal_init: picking default device: Apple M4
0.00.112.227 I ggml_metal_init: using embedded metal library
0.00.121.664 I ggml_metal_init: GPU name:   Apple M4
0.00.121.666 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.121.666 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.121.667 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.121.667 I ggml_metal_init: simdgroup reduction   = true
0.00.121.667 I ggml_metal_init: simdgroup matrix mul. = true
0.00.121.667 I ggml_metal_init: has bfloat            = true
0.00.121.667 I ggml_metal_init: use bfloat            = true
0.00.121.668 I ggml_metal_init: hasUnifiedMemory      = true
0.00.121.668 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.145.352 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.166.524 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.166.533 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.166.557 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.167.611 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.167.613 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.167.614 I llama_init_from_model: graph nodes  = 967
0.00.167.614 I llama_init_from_model: graph splits = 2
0.00.167.618 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.167.744 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.167.744 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.248.379 I main: llama threadpool init, n_threads = 4
0.00.248.422 I 
0.00.248.448 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.248.448 I 
0.00.248.518 I sampler seed: 1234
0.00.248.523 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.248.548 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.248.551 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.248.551 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.090.232 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58436.21 tokens per second)
0.02.090.233 I llama_perf_context_print:        load time =     209.15 ms
0.02.090.234 I llama_perf_context_print: prompt eval time =      43.62 ms /     7 tokens (    6.23 ms per token,   160.48 tokens per second)
0.02.090.234 I llama_perf_context_print:        eval time =    1795.12 ms /    63 runs   (   28.49 ms per token,    35.10 tokens per second)
0.02.090.235 I llama_perf_context_print:       total time =    1841.86 ms /    70 tokens
0.02.090.465 I ggml_metal_free: deallocating

real	0m2.434s
user	0m0.145s
sys	0m0.104s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.945 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.856 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.862 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.865 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.865 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.865 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.866 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.866 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.867 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.867 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.868 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.868 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.868 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.869 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.869 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.872 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.872 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.872 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.781 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.883 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.748 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.749 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.750 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.750 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.750 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.751 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.040.751 I llama_model_loader: - type  f32:  194 tensors
0.00.040.752 I llama_model_loader: - type q8_0:   98 tensors
0.00.040.753 I print_info: file format = GGUF V3 (latest)
0.00.040.754 I print_info: file type   = Q8_0
0.00.040.755 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.063.430 I load: special tokens cache size = 25
0.00.071.108 I load: token to piece cache size = 0.2984 MB
0.00.071.113 I print_info: arch             = gptneox
0.00.071.114 I print_info: vocab_only       = 0
0.00.071.114 I print_info: n_ctx_train      = 2048
0.00.071.117 I print_info: n_embd           = 2048
0.00.071.117 I print_info: n_layer          = 24
0.00.071.124 I print_info: n_head           = 16
0.00.071.125 I print_info: n_head_kv        = 16
0.00.071.125 I print_info: n_rot            = 32
0.00.071.130 I print_info: n_swa            = 0
0.00.071.131 I print_info: n_embd_head_k    = 128
0.00.071.132 I print_info: n_embd_head_v    = 128
0.00.071.132 I print_info: n_gqa            = 1
0.00.071.133 I print_info: n_embd_k_gqa     = 2048
0.00.071.134 I print_info: n_embd_v_gqa     = 2048
0.00.071.135 I print_info: f_norm_eps       = 1.0e-05
0.00.071.135 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.071.135 I print_info: f_clamp_kqv      = 0.0e+00
0.00.071.135 I print_info: f_max_alibi_bias = 0.0e+00
0.00.071.137 I print_info: f_logit_scale    = 0.0e+00
0.00.071.138 I print_info: n_ff             = 8192
0.00.071.138 I print_info: n_expert         = 0
0.00.071.138 I print_info: n_expert_used    = 0
0.00.071.138 I print_info: causal attn      = 1
0.00.071.138 I print_info: pooling type     = 0
0.00.071.139 I print_info: rope type        = 2
0.00.071.139 I print_info: rope scaling     = linear
0.00.071.139 I print_info: freq_base_train  = 10000.0
0.00.071.140 I print_info: freq_scale_train = 1
0.00.071.140 I print_info: n_ctx_orig_yarn  = 2048
0.00.071.140 I print_info: rope_finetuned   = unknown
0.00.071.140 I print_info: ssm_d_conv       = 0
0.00.071.140 I print_info: ssm_d_inner      = 0
0.00.071.140 I print_info: ssm_d_state      = 0
0.00.071.140 I print_info: ssm_dt_rank      = 0
0.00.071.141 I print_info: ssm_dt_b_c_rms   = 0
0.00.071.141 I print_info: model type       = 1.4B
0.00.071.141 I print_info: model params     = 1.41 B
0.00.071.142 I print_info: general.name     = 1.4B
0.00.071.143 I print_info: vocab type       = BPE
0.00.071.143 I print_info: n_vocab          = 50304
0.00.071.143 I print_info: n_merges         = 50009
0.00.071.143 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.071.144 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.071.144 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.071.144 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.071.144 I print_info: LF token         = 128 'Ä'
0.00.071.145 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.071.145 I print_info: max token length = 1024
0.00.073.906 I load_tensors: offloading 24 repeating layers to GPU
0.00.073.907 I load_tensors: offloading output layer to GPU
0.00.073.907 I load_tensors: offloaded 25/25 layers to GPU
0.00.073.919 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.073.920 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.074.293 I llama_init_from_model: n_seq_max     = 1
0.00.074.294 I llama_init_from_model: n_ctx         = 2048
0.00.074.294 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.074.294 I llama_init_from_model: n_batch       = 2048
0.00.074.295 I llama_init_from_model: n_ubatch      = 512
0.00.074.295 I llama_init_from_model: flash_attn    = 0
0.00.074.295 I llama_init_from_model: freq_base     = 10000.0
0.00.074.295 I llama_init_from_model: freq_scale    = 1
0.00.074.296 I ggml_metal_init: allocating
0.00.074.300 I ggml_metal_init: found device: Apple M4
0.00.074.302 I ggml_metal_init: picking default device: Apple M4
0.00.075.127 I ggml_metal_init: using embedded metal library
0.00.078.179 I ggml_metal_init: GPU name:   Apple M4
0.00.078.181 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.078.181 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.078.182 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.078.182 I ggml_metal_init: simdgroup reduction   = true
0.00.078.182 I ggml_metal_init: simdgroup matrix mul. = true
0.00.078.182 I ggml_metal_init: has bfloat            = true
0.00.078.183 I ggml_metal_init: use bfloat            = true
0.00.078.183 I ggml_metal_init: hasUnifiedMemory      = true
0.00.078.184 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.090.666 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.116.811 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.116.827 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.116.860 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.118.066 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.118.068 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.118.068 I llama_init_from_model: graph nodes  = 967
0.00.118.069 I llama_init_from_model: graph splits = 2
0.00.118.073 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.118.201 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.118.202 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.390.869 I main: llama threadpool init, n_threads = 4
0.01.390.953 I 
0.01.391.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.391.010 I 
0.01.391.528 I sampler seed: 1234
0.01.391.534 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.391.567 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.391.569 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.391.569 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.485.505 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53747.16 tokens per second)
0.02.485.505 I llama_perf_context_print:        load time =    1380.92 ms
0.02.485.506 I llama_perf_context_print: prompt eval time =      49.87 ms /     7 tokens (    7.12 ms per token,   140.36 tokens per second)
0.02.485.507 I llama_perf_context_print:        eval time =    1041.03 ms /    63 runs   (   16.52 ms per token,    60.52 tokens per second)
0.02.485.507 I llama_perf_context_print:       total time =    1094.64 ms /    70 tokens
0.02.485.745 I ggml_metal_free: deallocating

real	0m2.504s
user	0m0.128s
sys	0m0.233s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.010.764 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.435 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.440 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.442 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.442 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.443 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.443 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.443 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.444 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.445 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.445 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.446 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.446 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.449 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.449 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.452 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.452 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.452 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.288 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.339 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.078 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.080 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.080 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.081 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.081 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.082 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.082 I llama_model_loader: - type  f32:  194 tensors
0.00.027.083 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.083 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.084 I print_info: file format = GGUF V3 (latest)
0.00.027.084 I print_info: file type   = Q4_0
0.00.027.085 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.046.341 I load: special tokens cache size = 25
0.00.052.317 I load: token to piece cache size = 0.2984 MB
0.00.052.321 I print_info: arch             = gptneox
0.00.052.322 I print_info: vocab_only       = 0
0.00.052.322 I print_info: n_ctx_train      = 2048
0.00.052.322 I print_info: n_embd           = 2048
0.00.052.322 I print_info: n_layer          = 24
0.00.052.328 I print_info: n_head           = 16
0.00.052.331 I print_info: n_head_kv        = 16
0.00.052.331 I print_info: n_rot            = 32
0.00.052.331 I print_info: n_swa            = 0
0.00.052.332 I print_info: n_embd_head_k    = 128
0.00.052.333 I print_info: n_embd_head_v    = 128
0.00.052.334 I print_info: n_gqa            = 1
0.00.052.335 I print_info: n_embd_k_gqa     = 2048
0.00.052.335 I print_info: n_embd_v_gqa     = 2048
0.00.052.336 I print_info: f_norm_eps       = 1.0e-05
0.00.052.336 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.337 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.337 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.337 I print_info: f_logit_scale    = 0.0e+00
0.00.052.339 I print_info: n_ff             = 8192
0.00.052.339 I print_info: n_expert         = 0
0.00.052.340 I print_info: n_expert_used    = 0
0.00.052.340 I print_info: causal attn      = 1
0.00.052.340 I print_info: pooling type     = 0
0.00.052.340 I print_info: rope type        = 2
0.00.052.341 I print_info: rope scaling     = linear
0.00.052.341 I print_info: freq_base_train  = 10000.0
0.00.052.341 I print_info: freq_scale_train = 1
0.00.052.342 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.347 I print_info: rope_finetuned   = unknown
0.00.052.347 I print_info: ssm_d_conv       = 0
0.00.052.347 I print_info: ssm_d_inner      = 0
0.00.052.347 I print_info: ssm_d_state      = 0
0.00.052.348 I print_info: ssm_dt_rank      = 0
0.00.052.348 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.348 I print_info: model type       = 1.4B
0.00.052.348 I print_info: model params     = 1.41 B
0.00.052.349 I print_info: general.name     = 1.4B
0.00.052.349 I print_info: vocab type       = BPE
0.00.052.349 I print_info: n_vocab          = 50304
0.00.052.350 I print_info: n_merges         = 50009
0.00.052.350 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.350 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.350 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.350 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.351 I print_info: LF token         = 128 'Ä'
0.00.052.351 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.351 I print_info: max token length = 1024
0.00.054.625 I load_tensors: offloading 24 repeating layers to GPU
0.00.054.626 I load_tensors: offloading output layer to GPU
0.00.054.626 I load_tensors: offloaded 25/25 layers to GPU
0.00.054.637 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.639 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.055.125 I llama_init_from_model: n_seq_max     = 1
0.00.055.126 I llama_init_from_model: n_ctx         = 2048
0.00.055.126 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.055.126 I llama_init_from_model: n_batch       = 2048
0.00.055.126 I llama_init_from_model: n_ubatch      = 512
0.00.055.126 I llama_init_from_model: flash_attn    = 0
0.00.055.127 I llama_init_from_model: freq_base     = 10000.0
0.00.055.127 I llama_init_from_model: freq_scale    = 1
0.00.055.128 I ggml_metal_init: allocating
0.00.055.130 I ggml_metal_init: found device: Apple M4
0.00.055.132 I ggml_metal_init: picking default device: Apple M4
0.00.055.869 I ggml_metal_init: using embedded metal library
0.00.058.428 I ggml_metal_init: GPU name:   Apple M4
0.00.058.430 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.430 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.431 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.431 I ggml_metal_init: simdgroup reduction   = true
0.00.058.431 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.431 I ggml_metal_init: has bfloat            = true
0.00.058.431 I ggml_metal_init: use bfloat            = true
0.00.058.432 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.631 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.093.154 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.093.161 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.093.182 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.094.305 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.094.306 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.094.307 I llama_init_from_model: graph nodes  = 967
0.00.094.307 I llama_init_from_model: graph splits = 2
0.00.094.312 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.094.451 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.094.452 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.147 I main: llama threadpool init, n_threads = 4
0.00.674.185 I 
0.00.674.216 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.217 I 
0.00.674.341 I sampler seed: 1234
0.00.674.349 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.674.360 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.674.361 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.674.361 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.345.143 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59068.22 tokens per second)
0.01.345.144 I llama_perf_context_print:        load time =     663.38 ms
0.01.345.144 I llama_perf_context_print: prompt eval time =      39.75 ms /     7 tokens (    5.68 ms per token,   176.11 tokens per second)
0.01.345.145 I llama_perf_context_print:        eval time =     628.08 ms /    63 runs   (    9.97 ms per token,   100.31 tokens per second)
0.01.345.145 I llama_perf_context_print:       total time =     671.00 ms /    70 tokens
0.01.345.386 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.109s
sys	0m0.150s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.833 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.787 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.792 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.797 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.798 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.799 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.800 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.801 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.801 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.802 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.802 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.803 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.806 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.630 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.635 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.437 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.438 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.438 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.439 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.439 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.440 I llama_model_loader: - type  f32:  194 tensors
0.00.025.440 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.440 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.441 I print_info: file format = GGUF V3 (latest)
0.00.025.441 I print_info: file type   = Q4_1
0.00.025.442 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.045.227 I load: special tokens cache size = 25
0.00.051.226 I load: token to piece cache size = 0.2984 MB
0.00.051.229 I print_info: arch             = gptneox
0.00.051.229 I print_info: vocab_only       = 0
0.00.051.229 I print_info: n_ctx_train      = 2048
0.00.051.229 I print_info: n_embd           = 2048
0.00.051.229 I print_info: n_layer          = 24
0.00.051.232 I print_info: n_head           = 16
0.00.051.233 I print_info: n_head_kv        = 16
0.00.051.233 I print_info: n_rot            = 32
0.00.051.233 I print_info: n_swa            = 0
0.00.051.234 I print_info: n_embd_head_k    = 128
0.00.051.234 I print_info: n_embd_head_v    = 128
0.00.051.235 I print_info: n_gqa            = 1
0.00.051.236 I print_info: n_embd_k_gqa     = 2048
0.00.051.236 I print_info: n_embd_v_gqa     = 2048
0.00.051.237 I print_info: f_norm_eps       = 1.0e-05
0.00.051.241 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.241 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.241 I print_info: f_logit_scale    = 0.0e+00
0.00.051.242 I print_info: n_ff             = 8192
0.00.051.242 I print_info: n_expert         = 0
0.00.051.242 I print_info: n_expert_used    = 0
0.00.051.244 I print_info: causal attn      = 1
0.00.051.244 I print_info: pooling type     = 0
0.00.051.244 I print_info: rope type        = 2
0.00.051.244 I print_info: rope scaling     = linear
0.00.051.245 I print_info: freq_base_train  = 10000.0
0.00.051.245 I print_info: freq_scale_train = 1
0.00.051.245 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.246 I print_info: rope_finetuned   = unknown
0.00.051.246 I print_info: ssm_d_conv       = 0
0.00.051.246 I print_info: ssm_d_inner      = 0
0.00.051.246 I print_info: ssm_d_state      = 0
0.00.051.246 I print_info: ssm_dt_rank      = 0
0.00.051.246 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.246 I print_info: model type       = 1.4B
0.00.051.248 I print_info: model params     = 1.41 B
0.00.051.248 I print_info: general.name     = 1.4B
0.00.051.248 I print_info: vocab type       = BPE
0.00.051.249 I print_info: n_vocab          = 50304
0.00.051.249 I print_info: n_merges         = 50009
0.00.051.249 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.249 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.249 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.250 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.250 I print_info: LF token         = 128 'Ä'
0.00.051.250 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.250 I print_info: max token length = 1024
0.00.053.337 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.338 I load_tensors: offloading output layer to GPU
0.00.053.338 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.349 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.053.350 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.736 I llama_init_from_model: n_seq_max     = 1
0.00.053.737 I llama_init_from_model: n_ctx         = 2048
0.00.053.737 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.737 I llama_init_from_model: n_batch       = 2048
0.00.053.737 I llama_init_from_model: n_ubatch      = 512
0.00.053.737 I llama_init_from_model: flash_attn    = 0
0.00.053.738 I llama_init_from_model: freq_base     = 10000.0
0.00.053.738 I llama_init_from_model: freq_scale    = 1
0.00.053.739 I ggml_metal_init: allocating
0.00.053.742 I ggml_metal_init: found device: Apple M4
0.00.053.744 I ggml_metal_init: picking default device: Apple M4
0.00.054.364 I ggml_metal_init: using embedded metal library
0.00.056.853 I ggml_metal_init: GPU name:   Apple M4
0.00.056.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.856 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.856 I ggml_metal_init: simdgroup reduction   = true
0.00.056.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.856 I ggml_metal_init: has bfloat            = true
0.00.056.856 I ggml_metal_init: use bfloat            = true
0.00.056.857 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.858 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.993 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.167 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.172 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.190 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.297 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.298 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.299 I llama_init_from_model: graph nodes  = 967
0.00.088.299 I llama_init_from_model: graph splits = 2
0.00.088.302 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.432 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.711.902 I main: llama threadpool init, n_threads = 4
0.00.711.944 I 
0.00.711.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.711.963 I 
0.00.712.192 I sampler seed: 1234
0.00.712.196 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.712.207 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.712.208 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.712.208 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.437.232 I llama_perf_sampler_print:    sampling time =       1.04 ms /    71 runs   (    0.01 ms per token, 68269.23 tokens per second)
0.01.437.233 I llama_perf_context_print:        load time =     703.06 ms
0.01.437.233 I llama_perf_context_print: prompt eval time =      42.91 ms /     7 tokens (    6.13 ms per token,   163.14 tokens per second)
0.01.437.234 I llama_perf_context_print:        eval time =     679.26 ms /    63 runs   (   10.78 ms per token,    92.75 tokens per second)
0.01.437.235 I llama_perf_context_print:       total time =     725.33 ms /    70 tokens
0.01.437.467 I ggml_metal_free: deallocating

real	0m1.453s
user	0m0.110s
sys	0m0.139s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.011.366 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.724 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.729 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.731 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.731 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.732 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.732 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.737 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.740 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.741 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.742 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.743 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.612 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.651 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.564 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.566 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.566 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.567 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.567 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.567 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.568 I llama_model_loader: - type  f32:  194 tensors
0.00.028.568 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.569 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.569 I print_info: file format = GGUF V3 (latest)
0.00.028.571 I print_info: file type   = Q5_0
0.00.028.572 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.049.035 I load: special tokens cache size = 25
0.00.055.364 I load: token to piece cache size = 0.2984 MB
0.00.055.368 I print_info: arch             = gptneox
0.00.055.368 I print_info: vocab_only       = 0
0.00.055.369 I print_info: n_ctx_train      = 2048
0.00.055.369 I print_info: n_embd           = 2048
0.00.055.369 I print_info: n_layer          = 24
0.00.055.374 I print_info: n_head           = 16
0.00.055.375 I print_info: n_head_kv        = 16
0.00.055.375 I print_info: n_rot            = 32
0.00.055.375 I print_info: n_swa            = 0
0.00.055.375 I print_info: n_embd_head_k    = 128
0.00.055.378 I print_info: n_embd_head_v    = 128
0.00.055.378 I print_info: n_gqa            = 1
0.00.055.379 I print_info: n_embd_k_gqa     = 2048
0.00.055.379 I print_info: n_embd_v_gqa     = 2048
0.00.055.380 I print_info: f_norm_eps       = 1.0e-05
0.00.055.382 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.055.382 I print_info: f_clamp_kqv      = 0.0e+00
0.00.055.382 I print_info: f_max_alibi_bias = 0.0e+00
0.00.055.382 I print_info: f_logit_scale    = 0.0e+00
0.00.055.383 I print_info: n_ff             = 8192
0.00.055.383 I print_info: n_expert         = 0
0.00.055.383 I print_info: n_expert_used    = 0
0.00.055.385 I print_info: causal attn      = 1
0.00.055.386 I print_info: pooling type     = 0
0.00.055.386 I print_info: rope type        = 2
0.00.055.387 I print_info: rope scaling     = linear
0.00.055.387 I print_info: freq_base_train  = 10000.0
0.00.055.387 I print_info: freq_scale_train = 1
0.00.055.413 I print_info: n_ctx_orig_yarn  = 2048
0.00.055.415 I print_info: rope_finetuned   = unknown
0.00.055.416 I print_info: ssm_d_conv       = 0
0.00.055.416 I print_info: ssm_d_inner      = 0
0.00.055.416 I print_info: ssm_d_state      = 0
0.00.055.416 I print_info: ssm_dt_rank      = 0
0.00.055.416 I print_info: ssm_dt_b_c_rms   = 0
0.00.055.417 I print_info: model type       = 1.4B
0.00.055.417 I print_info: model params     = 1.41 B
0.00.055.417 I print_info: general.name     = 1.4B
0.00.055.418 I print_info: vocab type       = BPE
0.00.055.418 I print_info: n_vocab          = 50304
0.00.055.418 I print_info: n_merges         = 50009
0.00.055.420 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.055.420 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.055.420 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.055.420 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.055.420 I print_info: LF token         = 128 'Ä'
0.00.055.423 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.055.423 I print_info: max token length = 1024
0.00.057.522 I load_tensors: offloading 24 repeating layers to GPU
0.00.057.523 I load_tensors: offloading output layer to GPU
0.00.057.523 I load_tensors: offloaded 25/25 layers to GPU
0.00.057.534 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.057.536 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.057.931 I llama_init_from_model: n_seq_max     = 1
0.00.057.932 I llama_init_from_model: n_ctx         = 2048
0.00.057.932 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.057.932 I llama_init_from_model: n_batch       = 2048
0.00.057.932 I llama_init_from_model: n_ubatch      = 512
0.00.057.933 I llama_init_from_model: flash_attn    = 0
0.00.057.933 I llama_init_from_model: freq_base     = 10000.0
0.00.057.933 I llama_init_from_model: freq_scale    = 1
0.00.057.934 I ggml_metal_init: allocating
0.00.057.937 I ggml_metal_init: found device: Apple M4
0.00.057.940 I ggml_metal_init: picking default device: Apple M4
0.00.058.612 I ggml_metal_init: using embedded metal library
0.00.060.972 I ggml_metal_init: GPU name:   Apple M4
0.00.060.974 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.060.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.060.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.060.976 I ggml_metal_init: simdgroup reduction   = true
0.00.060.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.060.976 I ggml_metal_init: has bfloat            = true
0.00.060.976 I ggml_metal_init: use bfloat            = true
0.00.060.977 I ggml_metal_init: hasUnifiedMemory      = true
0.00.060.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.071.249 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.090.960 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.965 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.988 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.091.982 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.091.984 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.091.984 I llama_init_from_model: graph nodes  = 967
0.00.091.985 I llama_init_from_model: graph splits = 2
0.00.091.987 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.092.116 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.092.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.758.733 I main: llama threadpool init, n_threads = 4
0.00.758.775 I 
0.00.758.802 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.758.804 I 
0.00.759.030 I sampler seed: 1234
0.00.759.035 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.759.078 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.759.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.759.078 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.575.644 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48168.25 tokens per second)
0.01.575.645 I llama_perf_context_print:        load time =     747.36 ms
0.01.575.649 I llama_perf_context_print: prompt eval time =      46.73 ms /     7 tokens (    6.68 ms per token,   149.79 tokens per second)
0.01.575.650 I llama_perf_context_print:        eval time =     767.12 ms /    63 runs   (   12.18 ms per token,    82.13 tokens per second)
0.01.575.651 I llama_perf_context_print:       total time =     816.91 ms /    70 tokens
0.01.575.890 I ggml_metal_free: deallocating

real	0m1.594s
user	0m0.111s
sys	0m0.149s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.243 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.758 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.023.763 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.765 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.765 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.766 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.766 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.766 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.771 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.772 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.772 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.772 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.773 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.773 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.775 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.775 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.775 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.575 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.590 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.399 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.400 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.400 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.401 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.401 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.401 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.032.402 I llama_model_loader: - type  f32:  194 tensors
0.00.032.402 I llama_model_loader: - type q5_1:   97 tensors
0.00.032.402 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.403 I print_info: file format = GGUF V3 (latest)
0.00.032.403 I print_info: file type   = Q5_1
0.00.032.404 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.052.402 I load: special tokens cache size = 25
0.00.058.570 I load: token to piece cache size = 0.2984 MB
0.00.058.573 I print_info: arch             = gptneox
0.00.058.573 I print_info: vocab_only       = 0
0.00.058.573 I print_info: n_ctx_train      = 2048
0.00.058.573 I print_info: n_embd           = 2048
0.00.058.574 I print_info: n_layer          = 24
0.00.058.577 I print_info: n_head           = 16
0.00.058.578 I print_info: n_head_kv        = 16
0.00.058.578 I print_info: n_rot            = 32
0.00.058.578 I print_info: n_swa            = 0
0.00.058.578 I print_info: n_embd_head_k    = 128
0.00.058.579 I print_info: n_embd_head_v    = 128
0.00.058.580 I print_info: n_gqa            = 1
0.00.058.580 I print_info: n_embd_k_gqa     = 2048
0.00.058.581 I print_info: n_embd_v_gqa     = 2048
0.00.058.581 I print_info: f_norm_eps       = 1.0e-05
0.00.058.582 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.058.583 I print_info: f_clamp_kqv      = 0.0e+00
0.00.058.583 I print_info: f_max_alibi_bias = 0.0e+00
0.00.058.583 I print_info: f_logit_scale    = 0.0e+00
0.00.058.584 I print_info: n_ff             = 8192
0.00.058.584 I print_info: n_expert         = 0
0.00.058.584 I print_info: n_expert_used    = 0
0.00.058.586 I print_info: causal attn      = 1
0.00.058.587 I print_info: pooling type     = 0
0.00.058.588 I print_info: rope type        = 2
0.00.058.588 I print_info: rope scaling     = linear
0.00.058.588 I print_info: freq_base_train  = 10000.0
0.00.058.589 I print_info: freq_scale_train = 1
0.00.058.589 I print_info: n_ctx_orig_yarn  = 2048
0.00.058.589 I print_info: rope_finetuned   = unknown
0.00.058.589 I print_info: ssm_d_conv       = 0
0.00.058.590 I print_info: ssm_d_inner      = 0
0.00.058.590 I print_info: ssm_d_state      = 0
0.00.058.590 I print_info: ssm_dt_rank      = 0
0.00.058.590 I print_info: ssm_dt_b_c_rms   = 0
0.00.058.592 I print_info: model type       = 1.4B
0.00.058.592 I print_info: model params     = 1.41 B
0.00.058.592 I print_info: general.name     = 1.4B
0.00.058.593 I print_info: vocab type       = BPE
0.00.058.593 I print_info: n_vocab          = 50304
0.00.058.594 I print_info: n_merges         = 50009
0.00.058.595 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.058.595 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.058.595 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.058.595 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.058.595 I print_info: LF token         = 128 'Ä'
0.00.058.596 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.058.596 I print_info: max token length = 1024
0.00.060.620 I load_tensors: offloading 24 repeating layers to GPU
0.00.060.620 I load_tensors: offloading output layer to GPU
0.00.060.620 I load_tensors: offloaded 25/25 layers to GPU
0.00.060.631 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.060.632 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.061.035 I llama_init_from_model: n_seq_max     = 1
0.00.061.036 I llama_init_from_model: n_ctx         = 2048
0.00.061.036 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.061.036 I llama_init_from_model: n_batch       = 2048
0.00.061.036 I llama_init_from_model: n_ubatch      = 512
0.00.061.036 I llama_init_from_model: flash_attn    = 0
0.00.061.037 I llama_init_from_model: freq_base     = 10000.0
0.00.061.037 I llama_init_from_model: freq_scale    = 1
0.00.061.037 I ggml_metal_init: allocating
0.00.061.041 I ggml_metal_init: found device: Apple M4
0.00.061.043 I ggml_metal_init: picking default device: Apple M4
0.00.061.672 I ggml_metal_init: using embedded metal library
0.00.064.198 I ggml_metal_init: GPU name:   Apple M4
0.00.064.199 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.201 I ggml_metal_init: simdgroup reduction   = true
0.00.064.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.201 I ggml_metal_init: has bfloat            = true
0.00.064.201 I ggml_metal_init: use bfloat            = true
0.00.064.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.336 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.525 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.533 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.553 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.095.642 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.095.643 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.095.643 I llama_init_from_model: graph nodes  = 967
0.00.095.644 I llama_init_from_model: graph splits = 2
0.00.095.647 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.778 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.779 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.312.674 I main: llama threadpool init, n_threads = 4
0.01.312.717 I 
0.01.312.739 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.312.740 I 
0.01.313.050 I sampler seed: 1234
0.01.313.059 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.313.076 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.313.077 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.313.078 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.02.152.193 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54615.38 tokens per second)
0.02.152.194 I llama_perf_context_print:        load time =    1303.42 ms
0.02.152.195 I llama_perf_context_print: prompt eval time =      42.40 ms /     7 tokens (    6.06 ms per token,   165.11 tokens per second)
0.02.152.196 I llama_perf_context_print:        eval time =     793.64 ms /    63 runs   (   12.60 ms per token,    79.38 tokens per second)
0.02.152.196 I llama_perf_context_print:       total time =     839.52 ms /    70 tokens
0.02.152.431 I ggml_metal_free: deallocating

real	0m2.169s
user	0m0.109s
sys	0m0.167s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.843 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.462 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.467 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.469 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.469 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.470 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.470 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.470 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.471 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.472 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.472 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.472 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.473 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.474 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.475 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.476 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.477 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.477 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.175 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.161 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.832 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.834 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.834 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.835 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.835 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.835 I llama_model_loader: - type  f32:  194 tensors
0.00.024.836 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.836 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.836 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.837 I print_info: file format = GGUF V3 (latest)
0.00.024.837 I print_info: file type   = Q2_K - Medium
0.00.024.842 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.044.033 I load: special tokens cache size = 25
0.00.049.786 I load: token to piece cache size = 0.2984 MB
0.00.049.789 I print_info: arch             = gptneox
0.00.049.789 I print_info: vocab_only       = 0
0.00.049.790 I print_info: n_ctx_train      = 2048
0.00.049.790 I print_info: n_embd           = 2048
0.00.049.790 I print_info: n_layer          = 24
0.00.049.793 I print_info: n_head           = 16
0.00.049.794 I print_info: n_head_kv        = 16
0.00.049.794 I print_info: n_rot            = 32
0.00.049.794 I print_info: n_swa            = 0
0.00.049.795 I print_info: n_embd_head_k    = 128
0.00.049.795 I print_info: n_embd_head_v    = 128
0.00.049.795 I print_info: n_gqa            = 1
0.00.049.796 I print_info: n_embd_k_gqa     = 2048
0.00.049.797 I print_info: n_embd_v_gqa     = 2048
0.00.049.797 I print_info: f_norm_eps       = 1.0e-05
0.00.049.798 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.798 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.802 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.803 I print_info: f_logit_scale    = 0.0e+00
0.00.049.805 I print_info: n_ff             = 8192
0.00.049.806 I print_info: n_expert         = 0
0.00.049.806 I print_info: n_expert_used    = 0
0.00.049.806 I print_info: causal attn      = 1
0.00.049.806 I print_info: pooling type     = 0
0.00.049.807 I print_info: rope type        = 2
0.00.049.807 I print_info: rope scaling     = linear
0.00.049.807 I print_info: freq_base_train  = 10000.0
0.00.049.811 I print_info: freq_scale_train = 1
0.00.049.811 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.811 I print_info: rope_finetuned   = unknown
0.00.049.811 I print_info: ssm_d_conv       = 0
0.00.049.811 I print_info: ssm_d_inner      = 0
0.00.049.812 I print_info: ssm_d_state      = 0
0.00.049.812 I print_info: ssm_dt_rank      = 0
0.00.049.812 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.812 I print_info: model type       = 1.4B
0.00.049.813 I print_info: model params     = 1.41 B
0.00.049.813 I print_info: general.name     = 1.4B
0.00.049.814 I print_info: vocab type       = BPE
0.00.049.814 I print_info: n_vocab          = 50304
0.00.049.814 I print_info: n_merges         = 50009
0.00.049.814 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.814 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.815 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.815 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.815 I print_info: LF token         = 128 'Ä'
0.00.049.815 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.815 I print_info: max token length = 1024
0.00.051.701 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.701 I load_tensors: offloading output layer to GPU
0.00.051.701 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.712 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.713 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.051.992 I llama_init_from_model: n_seq_max     = 1
0.00.051.993 I llama_init_from_model: n_ctx         = 2048
0.00.051.993 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.051.993 I llama_init_from_model: n_batch       = 2048
0.00.051.993 I llama_init_from_model: n_ubatch      = 512
0.00.051.993 I llama_init_from_model: flash_attn    = 0
0.00.051.994 I llama_init_from_model: freq_base     = 10000.0
0.00.051.994 I llama_init_from_model: freq_scale    = 1
0.00.051.995 I ggml_metal_init: allocating
0.00.051.998 I ggml_metal_init: found device: Apple M4
0.00.052.000 I ggml_metal_init: picking default device: Apple M4
0.00.052.591 I ggml_metal_init: using embedded metal library
0.00.054.997 I ggml_metal_init: GPU name:   Apple M4
0.00.054.999 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.999 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.999 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.000 I ggml_metal_init: simdgroup reduction   = true
0.00.055.000 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.000 I ggml_metal_init: has bfloat            = true
0.00.055.000 I ggml_metal_init: use bfloat            = true
0.00.055.001 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.001 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.878 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.328 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.333 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.352 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.085.476 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.085.478 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.085.478 I llama_init_from_model: graph nodes  = 967
0.00.085.478 I llama_init_from_model: graph splits = 2
0.00.085.482 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.622 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.622 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.431.417 I main: llama threadpool init, n_threads = 4
0.00.431.460 I 
0.00.431.484 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.431.485 I 
0.00.431.720 I sampler seed: 1234
0.00.431.726 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.431.762 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.431.766 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.431.767 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.109.547 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58823.53 tokens per second)
0.01.109.548 I llama_perf_context_print:        load time =     421.57 ms
0.01.109.549 I llama_perf_context_print: prompt eval time =      35.82 ms /     7 tokens (    5.12 ms per token,   195.44 tokens per second)
0.01.109.550 I llama_perf_context_print:        eval time =     639.26 ms /    63 runs   (   10.15 ms per token,    98.55 tokens per second)
0.01.109.550 I llama_perf_context_print:       total time =     678.13 ms /    70 tokens
0.01.109.820 I ggml_metal_free: deallocating

real	0m1.129s
user	0m0.109s
sys	0m0.108s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.830 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.477 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.489 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.490 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.490 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.491 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.494 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.333 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.332 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.073 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.075 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.075 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.076 I llama_model_loader: - type  f32:  194 tensors
0.00.025.076 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.076 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.076 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.077 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.077 I print_info: file format = GGUF V3 (latest)
0.00.025.078 I print_info: file type   = Q3_K - Medium
0.00.025.083 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.912 I load: special tokens cache size = 25
0.00.050.888 I load: token to piece cache size = 0.2984 MB
0.00.050.891 I print_info: arch             = gptneox
0.00.050.891 I print_info: vocab_only       = 0
0.00.050.891 I print_info: n_ctx_train      = 2048
0.00.050.891 I print_info: n_embd           = 2048
0.00.050.892 I print_info: n_layer          = 24
0.00.050.894 I print_info: n_head           = 16
0.00.050.895 I print_info: n_head_kv        = 16
0.00.050.895 I print_info: n_rot            = 32
0.00.050.895 I print_info: n_swa            = 0
0.00.050.895 I print_info: n_embd_head_k    = 128
0.00.050.896 I print_info: n_embd_head_v    = 128
0.00.050.896 I print_info: n_gqa            = 1
0.00.050.897 I print_info: n_embd_k_gqa     = 2048
0.00.050.898 I print_info: n_embd_v_gqa     = 2048
0.00.050.898 I print_info: f_norm_eps       = 1.0e-05
0.00.050.899 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.899 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.899 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.899 I print_info: f_logit_scale    = 0.0e+00
0.00.050.900 I print_info: n_ff             = 8192
0.00.050.900 I print_info: n_expert         = 0
0.00.050.901 I print_info: n_expert_used    = 0
0.00.050.901 I print_info: causal attn      = 1
0.00.050.901 I print_info: pooling type     = 0
0.00.050.901 I print_info: rope type        = 2
0.00.050.901 I print_info: rope scaling     = linear
0.00.050.902 I print_info: freq_base_train  = 10000.0
0.00.050.902 I print_info: freq_scale_train = 1
0.00.050.902 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.902 I print_info: rope_finetuned   = unknown
0.00.050.903 I print_info: ssm_d_conv       = 0
0.00.050.905 I print_info: ssm_d_inner      = 0
0.00.050.905 I print_info: ssm_d_state      = 0
0.00.050.905 I print_info: ssm_dt_rank      = 0
0.00.050.905 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.905 I print_info: model type       = 1.4B
0.00.050.906 I print_info: model params     = 1.41 B
0.00.050.906 I print_info: general.name     = 1.4B
0.00.050.906 I print_info: vocab type       = BPE
0.00.050.907 I print_info: n_vocab          = 50304
0.00.050.907 I print_info: n_merges         = 50009
0.00.050.907 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.908 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.908 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.908 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.908 I print_info: LF token         = 128 'Ä'
0.00.050.910 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.910 I print_info: max token length = 1024
0.00.052.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.874 I load_tensors: offloading output layer to GPU
0.00.052.875 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.885 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.886 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.053.181 I llama_init_from_model: n_seq_max     = 1
0.00.053.182 I llama_init_from_model: n_ctx         = 2048
0.00.053.183 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.053.183 I llama_init_from_model: n_batch       = 2048
0.00.053.183 I llama_init_from_model: n_ubatch      = 512
0.00.053.183 I llama_init_from_model: flash_attn    = 0
0.00.053.183 I llama_init_from_model: freq_base     = 10000.0
0.00.053.184 I llama_init_from_model: freq_scale    = 1
0.00.053.184 I ggml_metal_init: allocating
0.00.053.188 I ggml_metal_init: found device: Apple M4
0.00.053.190 I ggml_metal_init: picking default device: Apple M4
0.00.053.812 I ggml_metal_init: using embedded metal library
0.00.056.322 I ggml_metal_init: GPU name:   Apple M4
0.00.056.323 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.324 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.324 I ggml_metal_init: simdgroup reduction   = true
0.00.056.324 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.325 I ggml_metal_init: has bfloat            = true
0.00.056.325 I ggml_metal_init: use bfloat            = true
0.00.056.325 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.326 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.583 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.091 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.097 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.116 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.088.179 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.088.181 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.088.181 I llama_init_from_model: graph nodes  = 967
0.00.088.182 I llama_init_from_model: graph splits = 2
0.00.088.184 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.313 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.314 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.548 I main: llama threadpool init, n_threads = 4
0.00.532.588 I 
0.00.532.612 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.614 I 
0.00.532.840 I sampler seed: 1234
0.00.532.845 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.532.864 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.532.865 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.532.865 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.280.213 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59216.01 tokens per second)
0.01.280.213 I llama_perf_context_print:        load time =     523.71 ms
0.01.280.214 I llama_perf_context_print: prompt eval time =      40.51 ms /     7 tokens (    5.79 ms per token,   172.81 tokens per second)
0.01.280.215 I llama_perf_context_print:        eval time =     703.81 ms /    63 runs   (   11.17 ms per token,    89.51 tokens per second)
0.01.280.215 I llama_perf_context_print:       total time =     747.67 ms /    70 tokens
0.01.280.439 I ggml_metal_free: deallocating

real	0m1.298s
user	0m0.111s
sys	0m0.125s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.740 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.593 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.604 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.605 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.606 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.606 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.606 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.608 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.609 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.609 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.609 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.611 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.612 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.453 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.492 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.293 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.294 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.294 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.294 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.295 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.295 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.296 I llama_model_loader: - type  f32:  194 tensors
0.00.025.296 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.296 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.296 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.297 I print_info: file format = GGUF V3 (latest)
0.00.025.298 I print_info: file type   = Q4_K - Medium
0.00.025.298 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.457 I load: special tokens cache size = 25
0.00.050.532 I load: token to piece cache size = 0.2984 MB
0.00.050.536 I print_info: arch             = gptneox
0.00.050.536 I print_info: vocab_only       = 0
0.00.050.536 I print_info: n_ctx_train      = 2048
0.00.050.536 I print_info: n_embd           = 2048
0.00.050.537 I print_info: n_layer          = 24
0.00.050.539 I print_info: n_head           = 16
0.00.050.540 I print_info: n_head_kv        = 16
0.00.050.540 I print_info: n_rot            = 32
0.00.050.540 I print_info: n_swa            = 0
0.00.050.541 I print_info: n_embd_head_k    = 128
0.00.050.541 I print_info: n_embd_head_v    = 128
0.00.050.541 I print_info: n_gqa            = 1
0.00.050.542 I print_info: n_embd_k_gqa     = 2048
0.00.050.543 I print_info: n_embd_v_gqa     = 2048
0.00.050.544 I print_info: f_norm_eps       = 1.0e-05
0.00.050.544 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.546 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.546 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.546 I print_info: f_logit_scale    = 0.0e+00
0.00.050.547 I print_info: n_ff             = 8192
0.00.050.547 I print_info: n_expert         = 0
0.00.050.547 I print_info: n_expert_used    = 0
0.00.050.549 I print_info: causal attn      = 1
0.00.050.549 I print_info: pooling type     = 0
0.00.050.549 I print_info: rope type        = 2
0.00.050.549 I print_info: rope scaling     = linear
0.00.050.550 I print_info: freq_base_train  = 10000.0
0.00.050.550 I print_info: freq_scale_train = 1
0.00.050.550 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.550 I print_info: rope_finetuned   = unknown
0.00.050.550 I print_info: ssm_d_conv       = 0
0.00.050.551 I print_info: ssm_d_inner      = 0
0.00.050.551 I print_info: ssm_d_state      = 0
0.00.050.551 I print_info: ssm_dt_rank      = 0
0.00.050.551 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.551 I print_info: model type       = 1.4B
0.00.050.552 I print_info: model params     = 1.41 B
0.00.050.552 I print_info: general.name     = 1.4B
0.00.050.552 I print_info: vocab type       = BPE
0.00.050.553 I print_info: n_vocab          = 50304
0.00.050.553 I print_info: n_merges         = 50009
0.00.050.553 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.554 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.554 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.558 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.558 I print_info: LF token         = 128 'Ä'
0.00.050.558 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.558 I print_info: max token length = 1024
0.00.052.525 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.525 I load_tensors: offloading output layer to GPU
0.00.052.525 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.536 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.537 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.832 I llama_init_from_model: n_seq_max     = 1
0.00.052.832 I llama_init_from_model: n_ctx         = 2048
0.00.052.832 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.832 I llama_init_from_model: n_batch       = 2048
0.00.052.833 I llama_init_from_model: n_ubatch      = 512
0.00.052.833 I llama_init_from_model: flash_attn    = 0
0.00.052.833 I llama_init_from_model: freq_base     = 10000.0
0.00.052.834 I llama_init_from_model: freq_scale    = 1
0.00.052.834 I ggml_metal_init: allocating
0.00.052.837 I ggml_metal_init: found device: Apple M4
0.00.052.839 I ggml_metal_init: picking default device: Apple M4
0.00.053.473 I ggml_metal_init: using embedded metal library
0.00.055.853 I ggml_metal_init: GPU name:   Apple M4
0.00.055.855 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.855 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.855 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.856 I ggml_metal_init: simdgroup reduction   = true
0.00.055.856 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.856 I ggml_metal_init: has bfloat            = true
0.00.055.856 I ggml_metal_init: use bfloat            = true
0.00.055.856 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.772 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.452 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.457 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.475 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.428 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.430 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.430 I llama_init_from_model: graph nodes  = 967
0.00.086.431 I llama_init_from_model: graph splits = 2
0.00.086.433 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.562 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.562 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.606.337 I main: llama threadpool init, n_threads = 4
0.00.606.377 I 
0.00.606.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.606.424 I 
0.00.606.661 I sampler seed: 1234
0.00.606.667 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.606.689 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.606.689 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.606.689 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.368.178 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57350.57 tokens per second)
0.01.368.179 I llama_perf_context_print:        load time =     597.59 ms
0.01.368.180 I llama_perf_context_print: prompt eval time =      47.12 ms /     7 tokens (    6.73 ms per token,   148.54 tokens per second)
0.01.368.181 I llama_perf_context_print:        eval time =     711.28 ms /    63 runs   (   11.29 ms per token,    88.57 tokens per second)
0.01.368.181 I llama_perf_context_print:       total time =     761.85 ms /    70 tokens
0.01.368.411 I ggml_metal_free: deallocating

real	0m1.386s
user	0m0.110s
sys	0m0.137s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.956 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.678 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.683 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.685 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.686 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.686 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.686 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.687 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.688 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.688 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.688 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.689 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.689 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.691 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.692 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.693 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.694 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.694 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.471 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.171 I llama_model_loader: - type  f32:  194 tensors
0.00.025.172 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.172 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.172 I print_info: file format = GGUF V3 (latest)
0.00.025.173 I print_info: file type   = Q5_K - Medium
0.00.025.177 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.212 I load: special tokens cache size = 25
0.00.050.188 I load: token to piece cache size = 0.2984 MB
0.00.050.191 I print_info: arch             = gptneox
0.00.050.191 I print_info: vocab_only       = 0
0.00.050.191 I print_info: n_ctx_train      = 2048
0.00.050.192 I print_info: n_embd           = 2048
0.00.050.192 I print_info: n_layer          = 24
0.00.050.195 I print_info: n_head           = 16
0.00.050.195 I print_info: n_head_kv        = 16
0.00.050.196 I print_info: n_rot            = 32
0.00.050.197 I print_info: n_swa            = 0
0.00.050.197 I print_info: n_embd_head_k    = 128
0.00.050.198 I print_info: n_embd_head_v    = 128
0.00.050.198 I print_info: n_gqa            = 1
0.00.050.199 I print_info: n_embd_k_gqa     = 2048
0.00.050.200 I print_info: n_embd_v_gqa     = 2048
0.00.050.200 I print_info: f_norm_eps       = 1.0e-05
0.00.050.201 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.201 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.201 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.201 I print_info: f_logit_scale    = 0.0e+00
0.00.050.202 I print_info: n_ff             = 8192
0.00.050.202 I print_info: n_expert         = 0
0.00.050.202 I print_info: n_expert_used    = 0
0.00.050.202 I print_info: causal attn      = 1
0.00.050.202 I print_info: pooling type     = 0
0.00.050.203 I print_info: rope type        = 2
0.00.050.203 I print_info: rope scaling     = linear
0.00.050.203 I print_info: freq_base_train  = 10000.0
0.00.050.204 I print_info: freq_scale_train = 1
0.00.050.204 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.204 I print_info: rope_finetuned   = unknown
0.00.050.204 I print_info: ssm_d_conv       = 0
0.00.050.205 I print_info: ssm_d_inner      = 0
0.00.050.206 I print_info: ssm_d_state      = 0
0.00.050.208 I print_info: ssm_dt_rank      = 0
0.00.050.208 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.208 I print_info: model type       = 1.4B
0.00.050.208 I print_info: model params     = 1.41 B
0.00.050.208 I print_info: general.name     = 1.4B
0.00.050.209 I print_info: vocab type       = BPE
0.00.050.209 I print_info: n_vocab          = 50304
0.00.050.210 I print_info: n_merges         = 50009
0.00.050.210 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.211 I print_info: LF token         = 128 'Ä'
0.00.050.211 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.211 I print_info: max token length = 1024
0.00.052.247 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.247 I load_tensors: offloading output layer to GPU
0.00.052.247 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.258 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.259 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.532 I llama_init_from_model: n_seq_max     = 1
0.00.052.532 I llama_init_from_model: n_ctx         = 2048
0.00.052.532 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.533 I llama_init_from_model: n_batch       = 2048
0.00.052.533 I llama_init_from_model: n_ubatch      = 512
0.00.052.533 I llama_init_from_model: flash_attn    = 0
0.00.052.533 I llama_init_from_model: freq_base     = 10000.0
0.00.052.534 I llama_init_from_model: freq_scale    = 1
0.00.052.534 I ggml_metal_init: allocating
0.00.052.538 I ggml_metal_init: found device: Apple M4
0.00.052.540 I ggml_metal_init: picking default device: Apple M4
0.00.053.149 I ggml_metal_init: using embedded metal library
0.00.055.527 I ggml_metal_init: GPU name:   Apple M4
0.00.055.529 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.529 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.530 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.530 I ggml_metal_init: simdgroup reduction   = true
0.00.055.530 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.530 I ggml_metal_init: has bfloat            = true
0.00.055.530 I ggml_metal_init: use bfloat            = true
0.00.055.531 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.532 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.410 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.972 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.977 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.994 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.091 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.093 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.093 I llama_init_from_model: graph nodes  = 967
0.00.086.093 I llama_init_from_model: graph splits = 2
0.00.086.096 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.706 I main: llama threadpool init, n_threads = 4
0.00.684.739 I 
0.00.684.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.764 I 
0.00.685.004 I sampler seed: 1234
0.00.685.009 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.685.020 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.685.021 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.685.021 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.530.536 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57073.95 tokens per second)
0.01.530.538 I llama_perf_context_print:        load time =     674.75 ms
0.01.530.539 I llama_perf_context_print: prompt eval time =      51.67 ms /     7 tokens (    7.38 ms per token,   135.49 tokens per second)
0.01.530.540 I llama_perf_context_print:        eval time =     790.80 ms /    63 runs   (   12.55 ms per token,    79.67 tokens per second)
0.01.530.541 I llama_perf_context_print:       total time =     845.83 ms /    70 tokens
0.01.530.782 I ggml_metal_free: deallocating

real	0m1.549s
user	0m0.110s
sys	0m0.142s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.996 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.997 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.002 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.003 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.004 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.004 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.004 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.005 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.006 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.006 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.007 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.007 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.007 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.008 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.009 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.010 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.010 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.776 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.774 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.523 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.523 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.524 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.525 I llama_model_loader: - type  f32:  194 tensors
0.00.024.525 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.526 I print_info: file format = GGUF V3 (latest)
0.00.024.526 I print_info: file type   = Q6_K
0.00.024.527 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.668 I load: special tokens cache size = 25
0.00.049.771 I load: token to piece cache size = 0.2984 MB
0.00.049.773 I print_info: arch             = gptneox
0.00.049.773 I print_info: vocab_only       = 0
0.00.049.774 I print_info: n_ctx_train      = 2048
0.00.049.774 I print_info: n_embd           = 2048
0.00.049.774 I print_info: n_layer          = 24
0.00.049.777 I print_info: n_head           = 16
0.00.049.778 I print_info: n_head_kv        = 16
0.00.049.778 I print_info: n_rot            = 32
0.00.049.778 I print_info: n_swa            = 0
0.00.049.779 I print_info: n_embd_head_k    = 128
0.00.049.779 I print_info: n_embd_head_v    = 128
0.00.049.779 I print_info: n_gqa            = 1
0.00.049.780 I print_info: n_embd_k_gqa     = 2048
0.00.049.781 I print_info: n_embd_v_gqa     = 2048
0.00.049.781 I print_info: f_norm_eps       = 1.0e-05
0.00.049.782 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.783 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.783 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.784 I print_info: f_logit_scale    = 0.0e+00
0.00.049.784 I print_info: n_ff             = 8192
0.00.049.784 I print_info: n_expert         = 0
0.00.049.785 I print_info: n_expert_used    = 0
0.00.049.785 I print_info: causal attn      = 1
0.00.049.785 I print_info: pooling type     = 0
0.00.049.785 I print_info: rope type        = 2
0.00.049.785 I print_info: rope scaling     = linear
0.00.049.786 I print_info: freq_base_train  = 10000.0
0.00.049.786 I print_info: freq_scale_train = 1
0.00.049.786 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.787 I print_info: rope_finetuned   = unknown
0.00.049.787 I print_info: ssm_d_conv       = 0
0.00.049.787 I print_info: ssm_d_inner      = 0
0.00.049.787 I print_info: ssm_d_state      = 0
0.00.049.787 I print_info: ssm_dt_rank      = 0
0.00.049.787 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.789 I print_info: model type       = 1.4B
0.00.049.790 I print_info: model params     = 1.41 B
0.00.049.790 I print_info: general.name     = 1.4B
0.00.049.790 I print_info: vocab type       = BPE
0.00.049.791 I print_info: n_vocab          = 50304
0.00.049.791 I print_info: n_merges         = 50009
0.00.049.791 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.791 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.791 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.791 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.792 I print_info: LF token         = 128 'Ä'
0.00.049.792 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.792 I print_info: max token length = 1024
0.00.051.805 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.806 I load_tensors: offloading output layer to GPU
0.00.051.806 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.816 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.818 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.112 I llama_init_from_model: n_seq_max     = 1
0.00.052.112 I llama_init_from_model: n_ctx         = 2048
0.00.052.112 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.052.113 I llama_init_from_model: n_batch       = 2048
0.00.052.113 I llama_init_from_model: n_ubatch      = 512
0.00.052.113 I llama_init_from_model: flash_attn    = 0
0.00.052.113 I llama_init_from_model: freq_base     = 10000.0
0.00.052.114 I llama_init_from_model: freq_scale    = 1
0.00.052.114 I ggml_metal_init: allocating
0.00.052.117 I ggml_metal_init: found device: Apple M4
0.00.052.119 I ggml_metal_init: picking default device: Apple M4
0.00.052.709 I ggml_metal_init: using embedded metal library
0.00.055.077 I ggml_metal_init: GPU name:   Apple M4
0.00.055.079 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.079 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.080 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.080 I ggml_metal_init: simdgroup reduction   = true
0.00.055.080 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.080 I ggml_metal_init: has bfloat            = true
0.00.055.080 I ggml_metal_init: use bfloat            = true
0.00.055.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.081 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.097 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.239 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.245 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.270 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.086.246 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.086.247 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.086.247 I llama_init_from_model: graph nodes  = 967
0.00.086.247 I llama_init_from_model: graph splits = 2
0.00.086.250 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.379 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.379 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.754.662 I main: llama threadpool init, n_threads = 4
0.00.754.721 I 
0.00.754.773 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.754.775 I 
0.00.755.038 I sampler seed: 1234
0.00.755.044 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.755.082 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.755.085 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.755.085 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.633.540 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50714.29 tokens per second)
0.01.633.540 I llama_perf_context_print:        load time =     745.65 ms
0.01.633.541 I llama_perf_context_print: prompt eval time =      54.42 ms /     7 tokens (    7.77 ms per token,   128.63 tokens per second)
0.01.633.542 I llama_perf_context_print:        eval time =     821.26 ms /    63 runs   (   13.04 ms per token,    76.71 tokens per second)
0.01.633.542 I llama_perf_context_print:       total time =     878.89 ms /    70 tokens
0.01.633.803 I ggml_metal_free: deallocating

real	0m1.653s
user	0m0.110s
sys	0m0.166s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.607 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.233 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.811 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.817 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.819 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.820 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.822 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.822 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.822 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.824 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.825 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.825 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.826 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.827 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.829 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.830 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.045.858 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.588 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.590 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.591 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.591 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.591 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.592 I llama_model_loader: - type  f32:  194 tensors
0.00.055.593 I llama_model_loader: - type  f16:   98 tensors
0.00.055.593 I print_info: file format = GGUF V3 (latest)
0.00.055.594 I print_info: file type   = all F32 (guessed)
0.00.055.596 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.807 I load: special tokens cache size = 25
0.00.090.929 I load: token to piece cache size = 0.2984 MB
0.00.090.932 I print_info: arch             = gptneox
0.00.090.932 I print_info: vocab_only       = 0
0.00.090.932 I print_info: n_ctx_train      = 2048
0.00.090.932 I print_info: n_embd           = 2048
0.00.090.932 I print_info: n_layer          = 24
0.00.090.935 I print_info: n_head           = 16
0.00.090.936 I print_info: n_head_kv        = 16
0.00.090.938 I print_info: n_rot            = 32
0.00.090.938 I print_info: n_swa            = 0
0.00.090.938 I print_info: n_embd_head_k    = 128
0.00.090.938 I print_info: n_embd_head_v    = 128
0.00.090.939 I print_info: n_gqa            = 1
0.00.090.940 I print_info: n_embd_k_gqa     = 2048
0.00.090.940 I print_info: n_embd_v_gqa     = 2048
0.00.090.941 I print_info: f_norm_eps       = 1.0e-05
0.00.090.941 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.090.941 I print_info: f_clamp_kqv      = 0.0e+00
0.00.090.942 I print_info: f_max_alibi_bias = 0.0e+00
0.00.090.942 I print_info: f_logit_scale    = 0.0e+00
0.00.090.942 I print_info: n_ff             = 8192
0.00.090.942 I print_info: n_expert         = 0
0.00.090.943 I print_info: n_expert_used    = 0
0.00.090.943 I print_info: causal attn      = 1
0.00.090.943 I print_info: pooling type     = 0
0.00.090.943 I print_info: rope type        = 2
0.00.090.943 I print_info: rope scaling     = linear
0.00.090.944 I print_info: freq_base_train  = 10000.0
0.00.090.948 I print_info: freq_scale_train = 1
0.00.090.948 I print_info: n_ctx_orig_yarn  = 2048
0.00.090.948 I print_info: rope_finetuned   = unknown
0.00.090.948 I print_info: ssm_d_conv       = 0
0.00.090.949 I print_info: ssm_d_inner      = 0
0.00.090.950 I print_info: ssm_d_state      = 0
0.00.090.950 I print_info: ssm_dt_rank      = 0
0.00.090.950 I print_info: ssm_dt_b_c_rms   = 0
0.00.090.950 I print_info: model type       = 1.4B
0.00.090.950 I print_info: model params     = 1.41 B
0.00.090.951 I print_info: general.name     = 1.4B
0.00.090.951 I print_info: vocab type       = BPE
0.00.090.951 I print_info: n_vocab          = 50304
0.00.090.952 I print_info: n_merges         = 50009
0.00.090.952 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.090.952 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.090.952 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.090.952 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.090.953 I print_info: LF token         = 128 'Ä'
0.00.090.953 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.090.953 I print_info: max token length = 1024
0.00.093.435 I load_tensors: offloading 24 repeating layers to GPU
0.00.093.435 I load_tensors: offloading output layer to GPU
0.00.093.435 I load_tensors: offloaded 25/25 layers to GPU
0.00.093.446 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.447 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.093.743 I llama_init_from_model: n_seq_max     = 1
0.00.093.743 I llama_init_from_model: n_ctx         = 128
0.00.093.744 I llama_init_from_model: n_ctx_per_seq = 128
0.00.093.744 I llama_init_from_model: n_batch       = 128
0.00.093.744 I llama_init_from_model: n_ubatch      = 128
0.00.093.744 I llama_init_from_model: flash_attn    = 0
0.00.093.744 I llama_init_from_model: freq_base     = 10000.0
0.00.093.745 I llama_init_from_model: freq_scale    = 1
0.00.093.745 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.746 I ggml_metal_init: allocating
0.00.093.748 I ggml_metal_init: found device: Apple M4
0.00.093.750 I ggml_metal_init: picking default device: Apple M4
0.00.094.374 I ggml_metal_init: using embedded metal library
0.00.096.971 I ggml_metal_init: GPU name:   Apple M4
0.00.096.973 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.973 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.974 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.974 I ggml_metal_init: simdgroup reduction   = true
0.00.096.974 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.974 I ggml_metal_init: has bfloat            = true
0.00.096.974 I ggml_metal_init: use bfloat            = true
0.00.096.975 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.975 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.423 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.736 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.740 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.756 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.108.632 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.108.633 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.108.633 I llama_init_from_model: graph nodes  = 967
0.00.108.633 I llama_init_from_model: graph splits = 2
0.00.108.634 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.634 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.128.795 I 
0.01.128.825 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.128.848 I perplexity: tokenizing the input ..
0.01.141.022 I perplexity: tokenization took 12.172 ms
0.01.141.028 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.262.724 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.264.557 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.264.622 I llama_perf_context_print:        load time =    1104.55 ms
0.01.264.626 I llama_perf_context_print: prompt eval time =     121.30 ms /   128 tokens (    0.95 ms per token,  1055.28 tokens per second)
0.01.264.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.264.628 I llama_perf_context_print:       total time =     135.82 ms /   129 tokens
0.01.265.650 I ggml_metal_free: deallocating

real	0m1.463s
user	0m0.123s
sys	0m0.213s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.122 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.566 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.574 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.021.580 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.582 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.583 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.583 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.583 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.584 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.585 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.585 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.586 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.586 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.586 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.587 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.587 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.589 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.589 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.590 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.952 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.408 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.733 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.734 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.735 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.735 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.736 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.736 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.033.737 I llama_model_loader: - type  f32:  194 tensors
0.00.033.737 I llama_model_loader: - type q8_0:   98 tensors
0.00.033.738 I print_info: file format = GGUF V3 (latest)
0.00.033.744 I print_info: file type   = Q8_0
0.00.033.746 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.056.817 I load: special tokens cache size = 25
0.00.063.249 I load: token to piece cache size = 0.2984 MB
0.00.063.252 I print_info: arch             = gptneox
0.00.063.253 I print_info: vocab_only       = 0
0.00.063.253 I print_info: n_ctx_train      = 2048
0.00.063.253 I print_info: n_embd           = 2048
0.00.063.253 I print_info: n_layer          = 24
0.00.063.258 I print_info: n_head           = 16
0.00.063.258 I print_info: n_head_kv        = 16
0.00.063.258 I print_info: n_rot            = 32
0.00.063.259 I print_info: n_swa            = 0
0.00.063.261 I print_info: n_embd_head_k    = 128
0.00.063.261 I print_info: n_embd_head_v    = 128
0.00.063.262 I print_info: n_gqa            = 1
0.00.063.263 I print_info: n_embd_k_gqa     = 2048
0.00.063.263 I print_info: n_embd_v_gqa     = 2048
0.00.063.264 I print_info: f_norm_eps       = 1.0e-05
0.00.063.264 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.063.265 I print_info: f_clamp_kqv      = 0.0e+00
0.00.063.265 I print_info: f_max_alibi_bias = 0.0e+00
0.00.063.265 I print_info: f_logit_scale    = 0.0e+00
0.00.063.265 I print_info: n_ff             = 8192
0.00.063.267 I print_info: n_expert         = 0
0.00.063.267 I print_info: n_expert_used    = 0
0.00.063.267 I print_info: causal attn      = 1
0.00.063.267 I print_info: pooling type     = 0
0.00.063.267 I print_info: rope type        = 2
0.00.063.267 I print_info: rope scaling     = linear
0.00.063.268 I print_info: freq_base_train  = 10000.0
0.00.063.268 I print_info: freq_scale_train = 1
0.00.063.268 I print_info: n_ctx_orig_yarn  = 2048
0.00.063.268 I print_info: rope_finetuned   = unknown
0.00.063.269 I print_info: ssm_d_conv       = 0
0.00.063.269 I print_info: ssm_d_inner      = 0
0.00.063.269 I print_info: ssm_d_state      = 0
0.00.063.269 I print_info: ssm_dt_rank      = 0
0.00.063.269 I print_info: ssm_dt_b_c_rms   = 0
0.00.063.272 I print_info: model type       = 1.4B
0.00.063.272 I print_info: model params     = 1.41 B
0.00.063.273 I print_info: general.name     = 1.4B
0.00.063.273 I print_info: vocab type       = BPE
0.00.063.273 I print_info: n_vocab          = 50304
0.00.063.273 I print_info: n_merges         = 50009
0.00.063.274 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.063.274 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.063.274 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.063.275 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.063.275 I print_info: LF token         = 128 'Ä'
0.00.063.275 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.063.276 I print_info: max token length = 1024
0.00.065.089 I load_tensors: offloading 24 repeating layers to GPU
0.00.065.089 I load_tensors: offloading output layer to GPU
0.00.065.089 I load_tensors: offloaded 25/25 layers to GPU
0.00.065.100 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.101 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.065.426 I llama_init_from_model: n_seq_max     = 1
0.00.065.427 I llama_init_from_model: n_ctx         = 128
0.00.065.427 I llama_init_from_model: n_ctx_per_seq = 128
0.00.065.428 I llama_init_from_model: n_batch       = 128
0.00.065.428 I llama_init_from_model: n_ubatch      = 128
0.00.065.428 I llama_init_from_model: flash_attn    = 0
0.00.065.428 I llama_init_from_model: freq_base     = 10000.0
0.00.065.429 I llama_init_from_model: freq_scale    = 1
0.00.065.429 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.065.429 I ggml_metal_init: allocating
0.00.065.433 I ggml_metal_init: found device: Apple M4
0.00.065.435 I ggml_metal_init: picking default device: Apple M4
0.00.066.102 I ggml_metal_init: using embedded metal library
0.00.068.820 I ggml_metal_init: GPU name:   Apple M4
0.00.068.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.822 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.822 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.823 I ggml_metal_init: simdgroup reduction   = true
0.00.068.823 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.823 I ggml_metal_init: has bfloat            = true
0.00.068.823 I ggml_metal_init: use bfloat            = true
0.00.068.824 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.100 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.080.443 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.445 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.461 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.081.526 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.081.527 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.081.527 I llama_init_from_model: graph nodes  = 967
0.00.081.527 I llama_init_from_model: graph splits = 2
0.00.081.529 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.529 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.935.660 I 
0.00.935.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.935.694 I perplexity: tokenizing the input ..
0.00.943.950 I perplexity: tokenization took 8.255 ms
0.00.943.953 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.068.409 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.069.570 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.069.597 I llama_perf_context_print:        load time =     924.09 ms
0.01.069.598 I llama_perf_context_print: prompt eval time =     124.23 ms /   128 tokens (    0.97 ms per token,  1030.34 tokens per second)
0.01.069.599 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.069.599 I llama_perf_context_print:       total time =     133.94 ms /   129 tokens
0.01.070.107 I ggml_metal_free: deallocating

real	0m1.088s
user	0m0.092s
sys	0m0.150s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.137 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.184 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.189 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.195 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.196 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.196 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.196 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.197 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.198 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.198 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.198 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.199 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.199 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.199 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.200 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.201 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.201 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.202 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.964 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.949 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.721 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.723 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.723 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.723 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.724 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.724 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.025.725 I llama_model_loader: - type  f32:  194 tensors
0.00.025.725 I llama_model_loader: - type q4_0:   97 tensors
0.00.025.725 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.726 I print_info: file format = GGUF V3 (latest)
0.00.025.726 I print_info: file type   = Q4_0
0.00.025.727 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.617 I load: special tokens cache size = 25
0.00.050.616 I load: token to piece cache size = 0.2984 MB
0.00.050.619 I print_info: arch             = gptneox
0.00.050.619 I print_info: vocab_only       = 0
0.00.050.619 I print_info: n_ctx_train      = 2048
0.00.050.620 I print_info: n_embd           = 2048
0.00.050.620 I print_info: n_layer          = 24
0.00.050.622 I print_info: n_head           = 16
0.00.050.623 I print_info: n_head_kv        = 16
0.00.050.623 I print_info: n_rot            = 32
0.00.050.625 I print_info: n_swa            = 0
0.00.050.625 I print_info: n_embd_head_k    = 128
0.00.050.625 I print_info: n_embd_head_v    = 128
0.00.050.626 I print_info: n_gqa            = 1
0.00.050.627 I print_info: n_embd_k_gqa     = 2048
0.00.050.627 I print_info: n_embd_v_gqa     = 2048
0.00.050.628 I print_info: f_norm_eps       = 1.0e-05
0.00.050.628 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.628 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.629 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.629 I print_info: f_logit_scale    = 0.0e+00
0.00.050.629 I print_info: n_ff             = 8192
0.00.050.630 I print_info: n_expert         = 0
0.00.050.630 I print_info: n_expert_used    = 0
0.00.050.630 I print_info: causal attn      = 1
0.00.050.630 I print_info: pooling type     = 0
0.00.050.630 I print_info: rope type        = 2
0.00.050.630 I print_info: rope scaling     = linear
0.00.050.631 I print_info: freq_base_train  = 10000.0
0.00.050.631 I print_info: freq_scale_train = 1
0.00.050.631 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.632 I print_info: rope_finetuned   = unknown
0.00.050.632 I print_info: ssm_d_conv       = 0
0.00.050.632 I print_info: ssm_d_inner      = 0
0.00.050.632 I print_info: ssm_d_state      = 0
0.00.050.632 I print_info: ssm_dt_rank      = 0
0.00.050.633 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.633 I print_info: model type       = 1.4B
0.00.050.633 I print_info: model params     = 1.41 B
0.00.050.633 I print_info: general.name     = 1.4B
0.00.050.634 I print_info: vocab type       = BPE
0.00.050.634 I print_info: n_vocab          = 50304
0.00.050.634 I print_info: n_merges         = 50009
0.00.050.635 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.635 I print_info: LF token         = 128 'Ä'
0.00.050.636 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.636 I print_info: max token length = 1024
0.00.052.633 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.633 I load_tensors: offloading output layer to GPU
0.00.052.633 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.643 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.052.645 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.930 I llama_init_from_model: n_seq_max     = 1
0.00.052.931 I llama_init_from_model: n_ctx         = 128
0.00.052.931 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.932 I llama_init_from_model: n_batch       = 128
0.00.052.932 I llama_init_from_model: n_ubatch      = 128
0.00.052.932 I llama_init_from_model: flash_attn    = 0
0.00.052.932 I llama_init_from_model: freq_base     = 10000.0
0.00.052.933 I llama_init_from_model: freq_scale    = 1
0.00.052.933 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.934 I ggml_metal_init: allocating
0.00.052.936 I ggml_metal_init: found device: Apple M4
0.00.052.938 I ggml_metal_init: picking default device: Apple M4
0.00.053.488 I ggml_metal_init: using embedded metal library
0.00.055.864 I ggml_metal_init: GPU name:   Apple M4
0.00.055.865 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.866 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.866 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.866 I ggml_metal_init: simdgroup reduction   = true
0.00.055.866 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.867 I ggml_metal_init: has bfloat            = true
0.00.055.867 I ggml_metal_init: use bfloat            = true
0.00.055.867 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.868 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.784 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.290 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.293 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.307 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.176 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.177 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.177 I llama_init_from_model: graph nodes  = 967
0.00.068.178 I llama_init_from_model: graph splits = 2
0.00.068.179 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.405 I 
0.00.596.436 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.451 I perplexity: tokenizing the input ..
0.00.604.132 I perplexity: tokenization took 7.679 ms
0.00.604.135 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.727.153 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.728.379 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.728.407 I llama_perf_context_print:        load time =     586.26 ms
0.00.728.408 I llama_perf_context_print: prompt eval time =     122.79 ms /   128 tokens (    0.96 ms per token,  1042.43 tokens per second)
0.00.728.409 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.728.410 I llama_perf_context_print:       total time =     132.00 ms /   129 tokens
0.00.729.060 I ggml_metal_free: deallocating

real	0m0.745s
user	0m0.078s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.120 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.358 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.363 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.365 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.365 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.366 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.366 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.366 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.367 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.367 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.368 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.370 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.371 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.371 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.371 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.373 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.373 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.375 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.175 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.904 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.905 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.906 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.906 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.906 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.907 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.907 I llama_model_loader: - type  f32:  194 tensors
0.00.024.907 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.908 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.908 I print_info: file format = GGUF V3 (latest)
0.00.024.909 I print_info: file type   = Q4_1
0.00.024.909 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.044.833 I load: special tokens cache size = 25
0.00.050.632 I load: token to piece cache size = 0.2984 MB
0.00.050.635 I print_info: arch             = gptneox
0.00.050.635 I print_info: vocab_only       = 0
0.00.050.635 I print_info: n_ctx_train      = 2048
0.00.050.635 I print_info: n_embd           = 2048
0.00.050.636 I print_info: n_layer          = 24
0.00.050.639 I print_info: n_head           = 16
0.00.050.640 I print_info: n_head_kv        = 16
0.00.050.641 I print_info: n_rot            = 32
0.00.050.641 I print_info: n_swa            = 0
0.00.050.641 I print_info: n_embd_head_k    = 128
0.00.050.641 I print_info: n_embd_head_v    = 128
0.00.050.642 I print_info: n_gqa            = 1
0.00.050.643 I print_info: n_embd_k_gqa     = 2048
0.00.050.643 I print_info: n_embd_v_gqa     = 2048
0.00.050.644 I print_info: f_norm_eps       = 1.0e-05
0.00.050.644 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.646 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.646 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.646 I print_info: f_logit_scale    = 0.0e+00
0.00.050.647 I print_info: n_ff             = 8192
0.00.050.647 I print_info: n_expert         = 0
0.00.050.649 I print_info: n_expert_used    = 0
0.00.050.649 I print_info: causal attn      = 1
0.00.050.649 I print_info: pooling type     = 0
0.00.050.649 I print_info: rope type        = 2
0.00.050.649 I print_info: rope scaling     = linear
0.00.050.649 I print_info: freq_base_train  = 10000.0
0.00.050.650 I print_info: freq_scale_train = 1
0.00.050.650 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.650 I print_info: rope_finetuned   = unknown
0.00.050.650 I print_info: ssm_d_conv       = 0
0.00.050.651 I print_info: ssm_d_inner      = 0
0.00.050.651 I print_info: ssm_d_state      = 0
0.00.050.651 I print_info: ssm_dt_rank      = 0
0.00.050.651 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.651 I print_info: model type       = 1.4B
0.00.050.656 I print_info: model params     = 1.41 B
0.00.050.656 I print_info: general.name     = 1.4B
0.00.050.656 I print_info: vocab type       = BPE
0.00.050.656 I print_info: n_vocab          = 50304
0.00.050.657 I print_info: n_merges         = 50009
0.00.050.657 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.657 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.657 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.657 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.657 I print_info: LF token         = 128 'Ä'
0.00.050.658 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.658 I print_info: max token length = 1024
0.00.052.743 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.744 I load_tensors: offloading output layer to GPU
0.00.052.744 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.755 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.756 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.053.053 I llama_init_from_model: n_seq_max     = 1
0.00.053.053 I llama_init_from_model: n_ctx         = 128
0.00.053.054 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.054 I llama_init_from_model: n_batch       = 128
0.00.053.054 I llama_init_from_model: n_ubatch      = 128
0.00.053.054 I llama_init_from_model: flash_attn    = 0
0.00.053.054 I llama_init_from_model: freq_base     = 10000.0
0.00.053.055 I llama_init_from_model: freq_scale    = 1
0.00.053.055 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.056 I ggml_metal_init: allocating
0.00.053.059 I ggml_metal_init: found device: Apple M4
0.00.053.061 I ggml_metal_init: picking default device: Apple M4
0.00.053.651 I ggml_metal_init: using embedded metal library
0.00.056.073 I ggml_metal_init: GPU name:   Apple M4
0.00.056.075 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.075 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.075 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.076 I ggml_metal_init: simdgroup reduction   = true
0.00.056.076 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.076 I ggml_metal_init: has bfloat            = true
0.00.056.076 I ggml_metal_init: use bfloat            = true
0.00.056.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.077 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.097 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.374 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.379 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.395 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.301 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.303 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.303 I llama_init_from_model: graph nodes  = 967
0.00.068.303 I llama_init_from_model: graph splits = 2
0.00.068.304 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.305 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.650.078 I 
0.00.650.120 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.650.140 I perplexity: tokenizing the input ..
0.00.658.346 I perplexity: tokenization took 8.204 ms
0.00.658.350 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.781.188 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.782.383 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.782.408 I llama_perf_context_print:        load time =     640.95 ms
0.00.782.409 I llama_perf_context_print: prompt eval time =     122.61 ms /   128 tokens (    0.96 ms per token,  1043.94 tokens per second)
0.00.782.410 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.782.410 I llama_perf_context_print:       total time =     132.33 ms /   129 tokens
0.00.782.925 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.079s
sys	0m0.095s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.916 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.526 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.530 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.532 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.533 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.533 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.533 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.534 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.534 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.535 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.535 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.536 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.536 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.536 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.540 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.540 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.540 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.229 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.211 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.918 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.919 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.919 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.920 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.920 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.920 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.921 I llama_model_loader: - type  f32:  194 tensors
0.00.024.921 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.921 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.922 I print_info: file format = GGUF V3 (latest)
0.00.024.923 I print_info: file type   = Q5_0
0.00.024.923 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.043.853 I load: special tokens cache size = 25
0.00.050.001 I load: token to piece cache size = 0.2984 MB
0.00.050.004 I print_info: arch             = gptneox
0.00.050.005 I print_info: vocab_only       = 0
0.00.050.005 I print_info: n_ctx_train      = 2048
0.00.050.005 I print_info: n_embd           = 2048
0.00.050.005 I print_info: n_layer          = 24
0.00.050.008 I print_info: n_head           = 16
0.00.050.009 I print_info: n_head_kv        = 16
0.00.050.009 I print_info: n_rot            = 32
0.00.050.010 I print_info: n_swa            = 0
0.00.050.010 I print_info: n_embd_head_k    = 128
0.00.050.010 I print_info: n_embd_head_v    = 128
0.00.050.011 I print_info: n_gqa            = 1
0.00.050.011 I print_info: n_embd_k_gqa     = 2048
0.00.050.012 I print_info: n_embd_v_gqa     = 2048
0.00.050.013 I print_info: f_norm_eps       = 1.0e-05
0.00.050.013 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.015 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.015 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.015 I print_info: f_logit_scale    = 0.0e+00
0.00.050.016 I print_info: n_ff             = 8192
0.00.050.016 I print_info: n_expert         = 0
0.00.050.016 I print_info: n_expert_used    = 0
0.00.050.016 I print_info: causal attn      = 1
0.00.050.017 I print_info: pooling type     = 0
0.00.050.017 I print_info: rope type        = 2
0.00.050.017 I print_info: rope scaling     = linear
0.00.050.017 I print_info: freq_base_train  = 10000.0
0.00.050.018 I print_info: freq_scale_train = 1
0.00.050.018 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.018 I print_info: rope_finetuned   = unknown
0.00.050.019 I print_info: ssm_d_conv       = 0
0.00.050.019 I print_info: ssm_d_inner      = 0
0.00.050.019 I print_info: ssm_d_state      = 0
0.00.050.019 I print_info: ssm_dt_rank      = 0
0.00.050.019 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.020 I print_info: model type       = 1.4B
0.00.050.020 I print_info: model params     = 1.41 B
0.00.050.020 I print_info: general.name     = 1.4B
0.00.050.021 I print_info: vocab type       = BPE
0.00.050.021 I print_info: n_vocab          = 50304
0.00.050.023 I print_info: n_merges         = 50009
0.00.050.023 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.023 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.024 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.024 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.024 I print_info: LF token         = 128 'Ä'
0.00.050.024 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.024 I print_info: max token length = 1024
0.00.052.050 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.050 I load_tensors: offloading output layer to GPU
0.00.052.050 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.061 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.062 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.052.350 I llama_init_from_model: n_seq_max     = 1
0.00.052.351 I llama_init_from_model: n_ctx         = 128
0.00.052.351 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.351 I llama_init_from_model: n_batch       = 128
0.00.052.352 I llama_init_from_model: n_ubatch      = 128
0.00.052.352 I llama_init_from_model: flash_attn    = 0
0.00.052.352 I llama_init_from_model: freq_base     = 10000.0
0.00.052.352 I llama_init_from_model: freq_scale    = 1
0.00.052.353 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.353 I ggml_metal_init: allocating
0.00.052.356 I ggml_metal_init: found device: Apple M4
0.00.052.358 I ggml_metal_init: picking default device: Apple M4
0.00.052.922 I ggml_metal_init: using embedded metal library
0.00.055.345 I ggml_metal_init: GPU name:   Apple M4
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.346 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.347 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.347 I ggml_metal_init: simdgroup reduction   = true
0.00.055.347 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.347 I ggml_metal_init: has bfloat            = true
0.00.055.347 I ggml_metal_init: use bfloat            = true
0.00.055.348 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.348 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.151 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.396 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.400 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.417 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.304 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.305 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.305 I llama_init_from_model: graph nodes  = 967
0.00.067.305 I llama_init_from_model: graph splits = 2
0.00.067.307 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.307 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.692.542 I 
0.00.692.573 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.692.585 I perplexity: tokenizing the input ..
0.00.701.033 I perplexity: tokenization took 8.447 ms
0.00.701.036 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.836.375 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.837.712 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.837.739 I llama_perf_context_print:        load time =     682.62 ms
0.00.837.741 I llama_perf_context_print: prompt eval time =     135.09 ms /   128 tokens (    1.06 ms per token,   947.55 tokens per second)
0.00.837.742 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.837.742 I llama_perf_context_print:       total time =     145.20 ms /   129 tokens
0.00.838.226 I ggml_metal_free: deallocating

real	0m0.853s
user	0m0.077s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.189 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.193 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.200 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.200 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.201 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.202 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.202 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.202 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.203 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.203 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.203 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.204 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.207 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.207 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.207 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.987 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.779 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.781 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.781 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.781 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.782 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.782 I llama_model_loader: - type  f32:  194 tensors
0.00.024.783 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.783 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.784 I print_info: file format = GGUF V3 (latest)
0.00.024.784 I print_info: file type   = Q5_1
0.00.024.785 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.044.396 I load: special tokens cache size = 25
0.00.050.185 I load: token to piece cache size = 0.2984 MB
0.00.050.187 I print_info: arch             = gptneox
0.00.050.188 I print_info: vocab_only       = 0
0.00.050.188 I print_info: n_ctx_train      = 2048
0.00.050.188 I print_info: n_embd           = 2048
0.00.050.188 I print_info: n_layer          = 24
0.00.050.191 I print_info: n_head           = 16
0.00.050.192 I print_info: n_head_kv        = 16
0.00.050.192 I print_info: n_rot            = 32
0.00.050.192 I print_info: n_swa            = 0
0.00.050.193 I print_info: n_embd_head_k    = 128
0.00.050.193 I print_info: n_embd_head_v    = 128
0.00.050.194 I print_info: n_gqa            = 1
0.00.050.194 I print_info: n_embd_k_gqa     = 2048
0.00.050.196 I print_info: n_embd_v_gqa     = 2048
0.00.050.197 I print_info: f_norm_eps       = 1.0e-05
0.00.050.197 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.206 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.209 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.209 I print_info: f_logit_scale    = 0.0e+00
0.00.050.215 I print_info: n_ff             = 8192
0.00.050.215 I print_info: n_expert         = 0
0.00.050.215 I print_info: n_expert_used    = 0
0.00.050.215 I print_info: causal attn      = 1
0.00.050.216 I print_info: pooling type     = 0
0.00.050.216 I print_info: rope type        = 2
0.00.050.216 I print_info: rope scaling     = linear
0.00.050.216 I print_info: freq_base_train  = 10000.0
0.00.050.217 I print_info: freq_scale_train = 1
0.00.050.217 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.217 I print_info: rope_finetuned   = unknown
0.00.050.217 I print_info: ssm_d_conv       = 0
0.00.050.217 I print_info: ssm_d_inner      = 0
0.00.050.218 I print_info: ssm_d_state      = 0
0.00.050.218 I print_info: ssm_dt_rank      = 0
0.00.050.218 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.218 I print_info: model type       = 1.4B
0.00.050.218 I print_info: model params     = 1.41 B
0.00.050.219 I print_info: general.name     = 1.4B
0.00.050.219 I print_info: vocab type       = BPE
0.00.050.219 I print_info: n_vocab          = 50304
0.00.050.220 I print_info: n_merges         = 50009
0.00.050.220 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.220 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.220 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.220 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.221 I print_info: LF token         = 128 'Ä'
0.00.050.221 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.221 I print_info: max token length = 1024
0.00.052.057 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.057 I load_tensors: offloading output layer to GPU
0.00.052.057 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.063 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.064 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.052.348 I llama_init_from_model: n_seq_max     = 1
0.00.052.349 I llama_init_from_model: n_ctx         = 128
0.00.052.350 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.350 I llama_init_from_model: n_batch       = 128
0.00.052.350 I llama_init_from_model: n_ubatch      = 128
0.00.052.350 I llama_init_from_model: flash_attn    = 0
0.00.052.350 I llama_init_from_model: freq_base     = 10000.0
0.00.052.351 I llama_init_from_model: freq_scale    = 1
0.00.052.351 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.352 I ggml_metal_init: allocating
0.00.052.357 I ggml_metal_init: found device: Apple M4
0.00.052.359 I ggml_metal_init: picking default device: Apple M4
0.00.052.983 I ggml_metal_init: using embedded metal library
0.00.055.396 I ggml_metal_init: GPU name:   Apple M4
0.00.055.398 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.399 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.399 I ggml_metal_init: simdgroup reduction   = true
0.00.055.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.399 I ggml_metal_init: has bfloat            = true
0.00.055.399 I ggml_metal_init: use bfloat            = true
0.00.055.400 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.298 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.522 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.526 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.539 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.423 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.424 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.424 I llama_init_from_model: graph nodes  = 967
0.00.067.424 I llama_init_from_model: graph splits = 2
0.00.067.425 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.426 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.944 I 
0.00.646.986 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.001 I perplexity: tokenizing the input ..
0.00.655.084 I perplexity: tokenization took 8.08 ms
0.00.655.087 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.351 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.791.521 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.791.545 I llama_perf_context_print:        load time =     637.75 ms
0.00.791.546 I llama_perf_context_print: prompt eval time =     135.04 ms /   128 tokens (    1.05 ms per token,   947.89 tokens per second)
0.00.791.546 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.547 I llama_perf_context_print:       total time =     144.61 ms /   129 tokens
0.00.792.183 I ggml_metal_free: deallocating

real	0m0.806s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.099 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.886 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.853 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.858 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.860 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.860 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.861 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.861 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.861 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.862 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.863 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.863 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.863 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.864 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.864 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.865 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.866 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.866 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.867 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.720 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.717 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.498 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.498 I llama_model_loader: - type  f32:  194 tensors
0.00.025.498 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.499 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.499 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.500 I print_info: file format = GGUF V3 (latest)
0.00.025.500 I print_info: file type   = Q2_K - Medium
0.00.025.501 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.045.352 I load: special tokens cache size = 25
0.00.051.272 I load: token to piece cache size = 0.2984 MB
0.00.051.275 I print_info: arch             = gptneox
0.00.051.275 I print_info: vocab_only       = 0
0.00.051.275 I print_info: n_ctx_train      = 2048
0.00.051.276 I print_info: n_embd           = 2048
0.00.051.276 I print_info: n_layer          = 24
0.00.051.278 I print_info: n_head           = 16
0.00.051.279 I print_info: n_head_kv        = 16
0.00.051.279 I print_info: n_rot            = 32
0.00.051.279 I print_info: n_swa            = 0
0.00.051.282 I print_info: n_embd_head_k    = 128
0.00.051.282 I print_info: n_embd_head_v    = 128
0.00.051.283 I print_info: n_gqa            = 1
0.00.051.284 I print_info: n_embd_k_gqa     = 2048
0.00.051.285 I print_info: n_embd_v_gqa     = 2048
0.00.051.285 I print_info: f_norm_eps       = 1.0e-05
0.00.051.286 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.286 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.286 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.286 I print_info: f_logit_scale    = 0.0e+00
0.00.051.287 I print_info: n_ff             = 8192
0.00.051.287 I print_info: n_expert         = 0
0.00.051.287 I print_info: n_expert_used    = 0
0.00.051.288 I print_info: causal attn      = 1
0.00.051.288 I print_info: pooling type     = 0
0.00.051.288 I print_info: rope type        = 2
0.00.051.288 I print_info: rope scaling     = linear
0.00.051.289 I print_info: freq_base_train  = 10000.0
0.00.051.289 I print_info: freq_scale_train = 1
0.00.051.290 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.290 I print_info: rope_finetuned   = unknown
0.00.051.291 I print_info: ssm_d_conv       = 0
0.00.051.291 I print_info: ssm_d_inner      = 0
0.00.051.291 I print_info: ssm_d_state      = 0
0.00.051.291 I print_info: ssm_dt_rank      = 0
0.00.051.291 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.291 I print_info: model type       = 1.4B
0.00.051.292 I print_info: model params     = 1.41 B
0.00.051.292 I print_info: general.name     = 1.4B
0.00.051.294 I print_info: vocab type       = BPE
0.00.051.294 I print_info: n_vocab          = 50304
0.00.051.294 I print_info: n_merges         = 50009
0.00.051.294 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.294 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.295 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.295 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.295 I print_info: LF token         = 128 'Ä'
0.00.051.297 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.297 I print_info: max token length = 1024
0.00.053.191 I load_tensors: offloading 24 repeating layers to GPU
0.00.053.191 I load_tensors: offloading output layer to GPU
0.00.053.191 I load_tensors: offloaded 25/25 layers to GPU
0.00.053.202 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.203 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.053.488 I llama_init_from_model: n_seq_max     = 1
0.00.053.489 I llama_init_from_model: n_ctx         = 128
0.00.053.489 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.489 I llama_init_from_model: n_batch       = 128
0.00.053.490 I llama_init_from_model: n_ubatch      = 128
0.00.053.490 I llama_init_from_model: flash_attn    = 0
0.00.053.490 I llama_init_from_model: freq_base     = 10000.0
0.00.053.490 I llama_init_from_model: freq_scale    = 1
0.00.053.491 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.491 I ggml_metal_init: allocating
0.00.053.495 I ggml_metal_init: found device: Apple M4
0.00.053.498 I ggml_metal_init: picking default device: Apple M4
0.00.054.100 I ggml_metal_init: using embedded metal library
0.00.056.586 I ggml_metal_init: GPU name:   Apple M4
0.00.056.587 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.588 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.588 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.589 I ggml_metal_init: simdgroup reduction   = true
0.00.056.589 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.589 I ggml_metal_init: has bfloat            = true
0.00.056.589 I ggml_metal_init: use bfloat            = true
0.00.056.590 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.590 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.656 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.007 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.009 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.023 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.069.020 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.069.021 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.069.022 I llama_init_from_model: graph nodes  = 967
0.00.069.022 I llama_init_from_model: graph splits = 2
0.00.069.023 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.023 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.374.505 I 
0.00.374.536 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.374.547 I perplexity: tokenizing the input ..
0.00.382.110 I perplexity: tokenization took 7.561 ms
0.00.382.113 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.514.739 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.515.995 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.516.014 I llama_perf_context_print:        load time =     364.61 ms
0.00.516.015 I llama_perf_context_print: prompt eval time =     132.36 ms /   128 tokens (    1.03 ms per token,   967.03 tokens per second)
0.00.516.016 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.516.017 I llama_perf_context_print:       total time =     141.51 ms /   129 tokens
0.00.516.398 I ggml_metal_free: deallocating

real	0m0.530s
user	0m0.079s
sys	0m0.060s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.058 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.195 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.202 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.202 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.202 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.203 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.203 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.204 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.204 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.205 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.205 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.205 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.206 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.206 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.209 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.209 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.210 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.034 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.789 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.791 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.791 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.792 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.792 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.792 I llama_model_loader: - type  f32:  194 tensors
0.00.024.793 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.793 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.793 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.793 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.794 I print_info: file format = GGUF V3 (latest)
0.00.024.794 I print_info: file type   = Q3_K - Medium
0.00.024.795 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.043.823 I load: special tokens cache size = 25
0.00.049.921 I load: token to piece cache size = 0.2984 MB
0.00.049.924 I print_info: arch             = gptneox
0.00.049.924 I print_info: vocab_only       = 0
0.00.049.924 I print_info: n_ctx_train      = 2048
0.00.049.925 I print_info: n_embd           = 2048
0.00.049.925 I print_info: n_layer          = 24
0.00.049.928 I print_info: n_head           = 16
0.00.049.929 I print_info: n_head_kv        = 16
0.00.049.929 I print_info: n_rot            = 32
0.00.049.929 I print_info: n_swa            = 0
0.00.049.929 I print_info: n_embd_head_k    = 128
0.00.049.929 I print_info: n_embd_head_v    = 128
0.00.049.930 I print_info: n_gqa            = 1
0.00.049.931 I print_info: n_embd_k_gqa     = 2048
0.00.049.931 I print_info: n_embd_v_gqa     = 2048
0.00.049.932 I print_info: f_norm_eps       = 1.0e-05
0.00.049.932 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.932 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.935 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.935 I print_info: f_logit_scale    = 0.0e+00
0.00.049.936 I print_info: n_ff             = 8192
0.00.049.936 I print_info: n_expert         = 0
0.00.049.936 I print_info: n_expert_used    = 0
0.00.049.936 I print_info: causal attn      = 1
0.00.049.937 I print_info: pooling type     = 0
0.00.049.937 I print_info: rope type        = 2
0.00.049.938 I print_info: rope scaling     = linear
0.00.049.939 I print_info: freq_base_train  = 10000.0
0.00.049.939 I print_info: freq_scale_train = 1
0.00.049.939 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.939 I print_info: rope_finetuned   = unknown
0.00.049.939 I print_info: ssm_d_conv       = 0
0.00.049.940 I print_info: ssm_d_inner      = 0
0.00.049.940 I print_info: ssm_d_state      = 0
0.00.049.940 I print_info: ssm_dt_rank      = 0
0.00.049.940 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.940 I print_info: model type       = 1.4B
0.00.049.941 I print_info: model params     = 1.41 B
0.00.049.941 I print_info: general.name     = 1.4B
0.00.049.945 I print_info: vocab type       = BPE
0.00.049.945 I print_info: n_vocab          = 50304
0.00.049.946 I print_info: n_merges         = 50009
0.00.049.946 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.946 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.947 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.947 I print_info: LF token         = 128 'Ä'
0.00.049.947 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.947 I print_info: max token length = 1024
0.00.051.761 I load_tensors: offloading 24 repeating layers to GPU
0.00.051.762 I load_tensors: offloading output layer to GPU
0.00.051.762 I load_tensors: offloaded 25/25 layers to GPU
0.00.051.767 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.768 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.052.083 I llama_init_from_model: n_seq_max     = 1
0.00.052.084 I llama_init_from_model: n_ctx         = 128
0.00.052.084 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.085 I llama_init_from_model: n_batch       = 128
0.00.052.085 I llama_init_from_model: n_ubatch      = 128
0.00.052.085 I llama_init_from_model: flash_attn    = 0
0.00.052.085 I llama_init_from_model: freq_base     = 10000.0
0.00.052.086 I llama_init_from_model: freq_scale    = 1
0.00.052.086 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.087 I ggml_metal_init: allocating
0.00.052.090 I ggml_metal_init: found device: Apple M4
0.00.052.092 I ggml_metal_init: picking default device: Apple M4
0.00.052.677 I ggml_metal_init: using embedded metal library
0.00.055.043 I ggml_metal_init: GPU name:   Apple M4
0.00.055.044 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.044 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.045 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.045 I ggml_metal_init: simdgroup reduction   = true
0.00.055.045 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.045 I ggml_metal_init: has bfloat            = true
0.00.055.045 I ggml_metal_init: use bfloat            = true
0.00.055.046 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.046 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.916 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.170 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.174 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.191 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.097 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.098 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.098 I llama_init_from_model: graph nodes  = 967
0.00.067.098 I llama_init_from_model: graph splits = 2
0.00.067.099 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.100 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.469.319 I 
0.00.469.354 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.469.373 I perplexity: tokenizing the input ..
0.00.477.259 I perplexity: tokenization took 7.884 ms
0.00.477.262 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.609.568 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.610.720 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.610.758 I llama_perf_context_print:        load time =     460.25 ms
0.00.610.759 I llama_perf_context_print: prompt eval time =     132.07 ms /   128 tokens (    1.03 ms per token,   969.15 tokens per second)
0.00.610.760 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.610.761 I llama_perf_context_print:       total time =     141.44 ms /   129 tokens
0.00.611.100 I ggml_metal_free: deallocating

real	0m0.624s
user	0m0.078s
sys	0m0.080s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.868 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.393 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.398 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.400 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.401 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.401 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.401 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.403 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.406 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.406 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.406 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.407 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.407 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.407 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.408 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.409 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.410 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.410 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.231 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.268 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.048 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.049 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.050 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.050 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.050 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.051 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.051 I llama_model_loader: - type  f32:  194 tensors
0.00.025.052 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.052 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.052 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.053 I print_info: file format = GGUF V3 (latest)
0.00.025.053 I print_info: file type   = Q4_K - Medium
0.00.025.055 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.044.650 I load: special tokens cache size = 25
0.00.050.535 I load: token to piece cache size = 0.2984 MB
0.00.050.539 I print_info: arch             = gptneox
0.00.050.539 I print_info: vocab_only       = 0
0.00.050.539 I print_info: n_ctx_train      = 2048
0.00.050.539 I print_info: n_embd           = 2048
0.00.050.540 I print_info: n_layer          = 24
0.00.050.543 I print_info: n_head           = 16
0.00.050.544 I print_info: n_head_kv        = 16
0.00.050.544 I print_info: n_rot            = 32
0.00.050.544 I print_info: n_swa            = 0
0.00.050.544 I print_info: n_embd_head_k    = 128
0.00.050.544 I print_info: n_embd_head_v    = 128
0.00.050.546 I print_info: n_gqa            = 1
0.00.050.547 I print_info: n_embd_k_gqa     = 2048
0.00.050.548 I print_info: n_embd_v_gqa     = 2048
0.00.050.548 I print_info: f_norm_eps       = 1.0e-05
0.00.050.549 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.550 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.551 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.551 I print_info: f_logit_scale    = 0.0e+00
0.00.050.551 I print_info: n_ff             = 8192
0.00.050.552 I print_info: n_expert         = 0
0.00.050.552 I print_info: n_expert_used    = 0
0.00.050.552 I print_info: causal attn      = 1
0.00.050.552 I print_info: pooling type     = 0
0.00.050.552 I print_info: rope type        = 2
0.00.050.553 I print_info: rope scaling     = linear
0.00.050.553 I print_info: freq_base_train  = 10000.0
0.00.050.553 I print_info: freq_scale_train = 1
0.00.050.553 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.554 I print_info: rope_finetuned   = unknown
0.00.050.554 I print_info: ssm_d_conv       = 0
0.00.050.554 I print_info: ssm_d_inner      = 0
0.00.050.554 I print_info: ssm_d_state      = 0
0.00.050.554 I print_info: ssm_dt_rank      = 0
0.00.050.554 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.555 I print_info: model type       = 1.4B
0.00.050.555 I print_info: model params     = 1.41 B
0.00.050.555 I print_info: general.name     = 1.4B
0.00.050.560 I print_info: vocab type       = BPE
0.00.050.560 I print_info: n_vocab          = 50304
0.00.050.560 I print_info: n_merges         = 50009
0.00.050.560 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.560 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.561 I print_info: LF token         = 128 'Ä'
0.00.050.561 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.562 I print_info: max token length = 1024
0.00.052.567 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.567 I load_tensors: offloading output layer to GPU
0.00.052.568 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.578 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.580 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.052.868 I llama_init_from_model: n_seq_max     = 1
0.00.052.869 I llama_init_from_model: n_ctx         = 128
0.00.052.869 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.869 I llama_init_from_model: n_batch       = 128
0.00.052.870 I llama_init_from_model: n_ubatch      = 128
0.00.052.870 I llama_init_from_model: flash_attn    = 0
0.00.052.870 I llama_init_from_model: freq_base     = 10000.0
0.00.052.870 I llama_init_from_model: freq_scale    = 1
0.00.052.871 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.871 I ggml_metal_init: allocating
0.00.052.874 I ggml_metal_init: found device: Apple M4
0.00.052.876 I ggml_metal_init: picking default device: Apple M4
0.00.053.423 I ggml_metal_init: using embedded metal library
0.00.055.819 I ggml_metal_init: GPU name:   Apple M4
0.00.055.821 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.821 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.822 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.822 I ggml_metal_init: simdgroup reduction   = true
0.00.055.822 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.822 I ggml_metal_init: has bfloat            = true
0.00.055.823 I ggml_metal_init: use bfloat            = true
0.00.055.823 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.824 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.746 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.042 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.046 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.063 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.941 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.943 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.943 I llama_init_from_model: graph nodes  = 967
0.00.067.943 I llama_init_from_model: graph splits = 2
0.00.067.944 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.944 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.544.731 I 
0.00.544.775 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.544.792 I perplexity: tokenizing the input ..
0.00.552.800 I perplexity: tokenization took 8.007 ms
0.00.552.804 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.687.355 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.688.612 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.688.641 I llama_perf_context_print:        load time =     535.86 ms
0.00.688.642 I llama_perf_context_print: prompt eval time =     134.31 ms /   128 tokens (    1.05 ms per token,   953.00 tokens per second)
0.00.688.642 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.688.643 I llama_perf_context_print:       total time =     143.91 ms /   129 tokens
0.00.689.089 I ggml_metal_free: deallocating

real	0m0.704s
user	0m0.078s
sys	0m0.094s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.573 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.463 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.468 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.471 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.471 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.472 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.472 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.472 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.473 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.473 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.474 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.474 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.475 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.475 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.478 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.479 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.479 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.480 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.301 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.063 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.065 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.065 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.065 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.066 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.066 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.026.066 I llama_model_loader: - type  f32:  194 tensors
0.00.026.067 I llama_model_loader: - type q5_K:   61 tensors
0.00.026.067 I llama_model_loader: - type q6_K:   37 tensors
0.00.026.067 I print_info: file format = GGUF V3 (latest)
0.00.026.068 I print_info: file type   = Q5_K - Medium
0.00.026.072 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.045.067 I load: special tokens cache size = 25
0.00.051.150 I load: token to piece cache size = 0.2984 MB
0.00.051.153 I print_info: arch             = gptneox
0.00.051.154 I print_info: vocab_only       = 0
0.00.051.154 I print_info: n_ctx_train      = 2048
0.00.051.154 I print_info: n_embd           = 2048
0.00.051.154 I print_info: n_layer          = 24
0.00.051.157 I print_info: n_head           = 16
0.00.051.158 I print_info: n_head_kv        = 16
0.00.051.158 I print_info: n_rot            = 32
0.00.051.158 I print_info: n_swa            = 0
0.00.051.158 I print_info: n_embd_head_k    = 128
0.00.051.159 I print_info: n_embd_head_v    = 128
0.00.051.159 I print_info: n_gqa            = 1
0.00.051.160 I print_info: n_embd_k_gqa     = 2048
0.00.051.161 I print_info: n_embd_v_gqa     = 2048
0.00.051.163 I print_info: f_norm_eps       = 1.0e-05
0.00.051.163 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.164 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.164 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.164 I print_info: f_logit_scale    = 0.0e+00
0.00.051.166 I print_info: n_ff             = 8192
0.00.051.166 I print_info: n_expert         = 0
0.00.051.167 I print_info: n_expert_used    = 0
0.00.051.167 I print_info: causal attn      = 1
0.00.051.167 I print_info: pooling type     = 0
0.00.051.167 I print_info: rope type        = 2
0.00.051.167 I print_info: rope scaling     = linear
0.00.051.169 I print_info: freq_base_train  = 10000.0
0.00.051.171 I print_info: freq_scale_train = 1
0.00.051.171 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.171 I print_info: rope_finetuned   = unknown
0.00.051.171 I print_info: ssm_d_conv       = 0
0.00.051.172 I print_info: ssm_d_inner      = 0
0.00.051.172 I print_info: ssm_d_state      = 0
0.00.051.172 I print_info: ssm_dt_rank      = 0
0.00.051.172 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.172 I print_info: model type       = 1.4B
0.00.051.176 I print_info: model params     = 1.41 B
0.00.051.176 I print_info: general.name     = 1.4B
0.00.051.177 I print_info: vocab type       = BPE
0.00.051.177 I print_info: n_vocab          = 50304
0.00.051.177 I print_info: n_merges         = 50009
0.00.051.177 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.177 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.177 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.178 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.178 I print_info: LF token         = 128 'Ä'
0.00.051.178 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.178 I print_info: max token length = 1024
0.00.052.953 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.953 I load_tensors: offloading output layer to GPU
0.00.052.953 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.959 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.959 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.053.354 I llama_init_from_model: n_seq_max     = 1
0.00.053.355 I llama_init_from_model: n_ctx         = 128
0.00.053.355 I llama_init_from_model: n_ctx_per_seq = 128
0.00.053.355 I llama_init_from_model: n_batch       = 128
0.00.053.355 I llama_init_from_model: n_ubatch      = 128
0.00.053.355 I llama_init_from_model: flash_attn    = 0
0.00.053.355 I llama_init_from_model: freq_base     = 10000.0
0.00.053.356 I llama_init_from_model: freq_scale    = 1
0.00.053.356 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.357 I ggml_metal_init: allocating
0.00.053.359 I ggml_metal_init: found device: Apple M4
0.00.053.361 I ggml_metal_init: picking default device: Apple M4
0.00.053.944 I ggml_metal_init: using embedded metal library
0.00.056.271 I ggml_metal_init: GPU name:   Apple M4
0.00.056.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.274 I ggml_metal_init: simdgroup reduction   = true
0.00.056.274 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.274 I ggml_metal_init: has bfloat            = true
0.00.056.274 I ggml_metal_init: use bfloat            = true
0.00.056.275 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.275 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.050 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.303 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.310 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.327 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.068.187 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.068.188 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.068.188 I llama_init_from_model: graph nodes  = 967
0.00.068.189 I llama_init_from_model: graph splits = 2
0.00.068.190 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.190 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.619.827 I 
0.00.619.854 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.619.864 I perplexity: tokenizing the input ..
0.00.628.029 I perplexity: tokenization took 8.163 ms
0.00.628.034 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.769.024 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.770.217 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.770.248 I llama_perf_context_print:        load time =     609.25 ms
0.00.770.251 I llama_perf_context_print: prompt eval time =     140.77 ms /   128 tokens (    1.10 ms per token,   909.32 tokens per second)
0.00.770.251 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.770.252 I llama_perf_context_print:       total time =     150.42 ms /   129 tokens
0.00.770.753 I ggml_metal_free: deallocating

real	0m0.786s
user	0m0.078s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.024 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.540 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.544 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.546 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.547 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.547 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.547 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.548 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.548 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.549 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.552 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.553 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.553 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.553 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.554 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.557 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.557 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.557 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.375 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.415 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.232 I llama_model_loader: - type  f32:  194 tensors
0.00.024.232 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.232 I print_info: file format = GGUF V3 (latest)
0.00.024.233 I print_info: file type   = Q6_K
0.00.024.234 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.043.887 I load: special tokens cache size = 25
0.00.049.944 I load: token to piece cache size = 0.2984 MB
0.00.049.947 I print_info: arch             = gptneox
0.00.049.947 I print_info: vocab_only       = 0
0.00.049.947 I print_info: n_ctx_train      = 2048
0.00.049.948 I print_info: n_embd           = 2048
0.00.049.948 I print_info: n_layer          = 24
0.00.049.951 I print_info: n_head           = 16
0.00.049.951 I print_info: n_head_kv        = 16
0.00.049.951 I print_info: n_rot            = 32
0.00.049.952 I print_info: n_swa            = 0
0.00.049.952 I print_info: n_embd_head_k    = 128
0.00.049.952 I print_info: n_embd_head_v    = 128
0.00.049.953 I print_info: n_gqa            = 1
0.00.049.953 I print_info: n_embd_k_gqa     = 2048
0.00.049.955 I print_info: n_embd_v_gqa     = 2048
0.00.049.956 I print_info: f_norm_eps       = 1.0e-05
0.00.049.956 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.968 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.968 I print_info: f_logit_scale    = 0.0e+00
0.00.049.974 I print_info: n_ff             = 8192
0.00.049.975 I print_info: n_expert         = 0
0.00.049.975 I print_info: n_expert_used    = 0
0.00.049.975 I print_info: causal attn      = 1
0.00.049.975 I print_info: pooling type     = 0
0.00.049.975 I print_info: rope type        = 2
0.00.049.975 I print_info: rope scaling     = linear
0.00.049.976 I print_info: freq_base_train  = 10000.0
0.00.049.977 I print_info: freq_scale_train = 1
0.00.049.979 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.979 I print_info: rope_finetuned   = unknown
0.00.049.979 I print_info: ssm_d_conv       = 0
0.00.049.979 I print_info: ssm_d_inner      = 0
0.00.049.979 I print_info: ssm_d_state      = 0
0.00.049.979 I print_info: ssm_dt_rank      = 0
0.00.049.979 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.980 I print_info: model type       = 1.4B
0.00.049.981 I print_info: model params     = 1.41 B
0.00.049.983 I print_info: general.name     = 1.4B
0.00.049.983 I print_info: vocab type       = BPE
0.00.049.983 I print_info: n_vocab          = 50304
0.00.049.984 I print_info: n_merges         = 50009
0.00.049.984 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.987 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.987 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.987 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.988 I print_info: LF token         = 128 'Ä'
0.00.049.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.989 I print_info: max token length = 1024
0.00.052.026 I load_tensors: offloading 24 repeating layers to GPU
0.00.052.026 I load_tensors: offloading output layer to GPU
0.00.052.026 I load_tensors: offloaded 25/25 layers to GPU
0.00.052.037 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.038 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.052.317 I llama_init_from_model: n_seq_max     = 1
0.00.052.318 I llama_init_from_model: n_ctx         = 128
0.00.052.318 I llama_init_from_model: n_ctx_per_seq = 128
0.00.052.318 I llama_init_from_model: n_batch       = 128
0.00.052.318 I llama_init_from_model: n_ubatch      = 128
0.00.052.319 I llama_init_from_model: flash_attn    = 0
0.00.052.319 I llama_init_from_model: freq_base     = 10000.0
0.00.052.319 I llama_init_from_model: freq_scale    = 1
0.00.052.320 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.320 I ggml_metal_init: allocating
0.00.052.323 I ggml_metal_init: found device: Apple M4
0.00.052.325 I ggml_metal_init: picking default device: Apple M4
0.00.052.935 I ggml_metal_init: using embedded metal library
0.00.055.293 I ggml_metal_init: GPU name:   Apple M4
0.00.055.294 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.295 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.295 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.295 I ggml_metal_init: simdgroup reduction   = true
0.00.055.296 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.296 I ggml_metal_init: has bfloat            = true
0.00.055.296 I ggml_metal_init: use bfloat            = true
0.00.055.296 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.297 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.964 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.221 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.223 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.237 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.067.126 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.067.128 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.067.128 I llama_init_from_model: graph nodes  = 967
0.00.067.128 I llama_init_from_model: graph splits = 2
0.00.067.130 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.130 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.381.367 I 
0.00.381.415 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.381.433 I perplexity: tokenizing the input ..
0.00.389.238 I perplexity: tokenization took 7.803 ms
0.00.389.243 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.529.750 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.531.068 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.531.085 I llama_perf_context_print:        load time =     372.33 ms
0.00.531.086 I llama_perf_context_print: prompt eval time =     140.25 ms /   128 tokens (    1.10 ms per token,   912.66 tokens per second)
0.00.531.087 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.531.087 I llama_perf_context_print:       total time =     149.72 ms /   129 tokens
0.00.531.423 I ggml_metal_free: deallocating

real	0m0.545s
user	0m0.078s
sys	0m0.073s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.249 I build: 4468 (04a9ef8f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.686 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.042.750 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.042.757 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.042.759 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.042.760 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.042.761 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.042.761 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.042.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.042.763 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.042.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.042.766 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.042.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.042.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.042.767 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.042.768 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.042.770 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.042.770 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.042.771 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.624 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.057.625 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.057.626 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.057.627 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.057.627 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.057.627 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.057.628 I llama_model_loader: - type  f32:  194 tensors
0.00.057.628 I llama_model_loader: - type  f16:   98 tensors
0.00.057.629 I print_info: file format = GGUF V3 (latest)
0.00.057.630 I print_info: file type   = all F32 (guessed)
0.00.057.631 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.082.153 I load: special tokens cache size = 25
0.00.088.486 I load: token to piece cache size = 0.2984 MB
0.00.088.489 I print_info: arch             = gptneox
0.00.088.489 I print_info: vocab_only       = 0
0.00.088.489 I print_info: n_ctx_train      = 2048
0.00.088.490 I print_info: n_embd           = 2048
0.00.088.490 I print_info: n_layer          = 24
0.00.088.493 I print_info: n_head           = 16
0.00.088.495 I print_info: n_head_kv        = 16
0.00.088.495 I print_info: n_rot            = 32
0.00.088.495 I print_info: n_swa            = 0
0.00.088.495 I print_info: n_embd_head_k    = 128
0.00.088.495 I print_info: n_embd_head_v    = 128
0.00.088.496 I print_info: n_gqa            = 1
0.00.088.497 I print_info: n_embd_k_gqa     = 2048
0.00.088.497 I print_info: n_embd_v_gqa     = 2048
0.00.088.498 I print_info: f_norm_eps       = 1.0e-05
0.00.088.498 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.088.498 I print_info: f_clamp_kqv      = 0.0e+00
0.00.088.498 I print_info: f_max_alibi_bias = 0.0e+00
0.00.088.499 I print_info: f_logit_scale    = 0.0e+00
0.00.088.499 I print_info: n_ff             = 8192
0.00.088.499 I print_info: n_expert         = 0
0.00.088.500 I print_info: n_expert_used    = 0
0.00.088.501 I print_info: causal attn      = 1
0.00.088.502 I print_info: pooling type     = 0
0.00.088.502 I print_info: rope type        = 2
0.00.088.502 I print_info: rope scaling     = linear
0.00.088.502 I print_info: freq_base_train  = 10000.0
0.00.088.503 I print_info: freq_scale_train = 1
0.00.088.503 I print_info: n_ctx_orig_yarn  = 2048
0.00.088.503 I print_info: rope_finetuned   = unknown
0.00.088.503 I print_info: ssm_d_conv       = 0
0.00.088.503 I print_info: ssm_d_inner      = 0
0.00.088.504 I print_info: ssm_d_state      = 0
0.00.088.504 I print_info: ssm_dt_rank      = 0
0.00.088.504 I print_info: ssm_dt_b_c_rms   = 0
0.00.088.504 I print_info: model type       = 1.4B
0.00.088.504 I print_info: model params     = 1.41 B
0.00.088.505 I print_info: general.name     = 1.4B
0.00.088.505 I print_info: vocab type       = BPE
0.00.088.505 I print_info: n_vocab          = 50304
0.00.088.505 I print_info: n_merges         = 50009
0.00.088.507 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.088.507 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.088.507 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.088.507 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.088.508 I print_info: LF token         = 128 'Ä'
0.00.088.508 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.088.508 I print_info: max token length = 1024
0.00.090.898 I load_tensors: offloading 24 repeating layers to GPU
0.00.090.899 I load_tensors: offloading output layer to GPU
0.00.090.899 I load_tensors: offloaded 25/25 layers to GPU
0.00.090.909 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.090.911 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.091.193 I llama_init_from_model: n_seq_max     = 1
0.00.091.194 I llama_init_from_model: n_ctx         = 128
0.00.091.194 I llama_init_from_model: n_ctx_per_seq = 128
0.00.091.194 I llama_init_from_model: n_batch       = 128
0.00.091.195 I llama_init_from_model: n_ubatch      = 128
0.00.091.195 I llama_init_from_model: flash_attn    = 0
0.00.091.195 I llama_init_from_model: freq_base     = 10000.0
0.00.091.195 I llama_init_from_model: freq_scale    = 1
0.00.091.196 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.091.196 I ggml_metal_init: allocating
0.00.091.199 I ggml_metal_init: found device: Apple M4
0.00.091.201 I ggml_metal_init: picking default device: Apple M4
0.00.091.801 I ggml_metal_init: using embedded metal library
0.00.094.289 I ggml_metal_init: GPU name:   Apple M4
0.00.094.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.094.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.094.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.094.292 I ggml_metal_init: simdgroup reduction   = true
0.00.094.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.094.292 I ggml_metal_init: has bfloat            = true
0.00.094.292 I ggml_metal_init: use bfloat            = true
0.00.094.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.094.293 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.103.478 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.104.768 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.104.772 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.104.785 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.105.715 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.105.716 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.105.716 I llama_init_from_model: graph nodes  = 967
0.00.105.717 I llama_init_from_model: graph splits = 2
0.00.105.718 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.105.718 I 
0.00.105.744 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.105.746 I compute_imatrix: tokenizing the input ..
0.00.112.706 I compute_imatrix: tokenization took 6.96 ms
0.00.112.708 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.681.763 I compute_imatrix: 1.57 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.684.940 I llama_perf_context_print:        load time =    1659.08 ms
0.01.684.941 I llama_perf_context_print: prompt eval time =    1568.40 ms /   128 tokens (   12.25 ms per token,    81.61 tokens per second)
0.01.684.943 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.684.943 I llama_perf_context_print:       total time =    1662.24 ms /   129 tokens
0.01.685.964 I ggml_metal_free: deallocating

real	0m1.890s
user	0m0.169s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4468 (04a9ef8f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x123a0a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x123a0a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x123a0af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x123a0b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x123a0bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x123a0c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x123a0c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x123a0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x123a0d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x123a0d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x123a0dba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x123a0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x123a0ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x123a0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x123a0fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x123a102a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x123a109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x123a110e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x123a11800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x123a11fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x123a126f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x123a12e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x123a13530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x123a13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x123a144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x123a147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x123a14dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x123a15a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x123a15f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x123a16230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x123a166d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x123a16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x123a17220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x123a17760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x123a17a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x123a17ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x123a18360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x123a18800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x123a18ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x123a19140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x123a195e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x123a19a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x123a19f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x123a1a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x123a1a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x123a1ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x123a1b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x123a1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x123a1c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x123a1c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x123a1cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x123a1d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x123a1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x123a1e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x123a1e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x123a1ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x123a1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x123a1f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x123a1fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x123a20210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x123a204d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x123a20970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x123a20e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x123a212b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x123a21750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x123a21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x123a22090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x123a22530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x123a229d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x123a22e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123a23310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123a237b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123a23c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123a241a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123a246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123a24c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123a25190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123a256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123a25c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123a26180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123a266d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123a26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123a27170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123a276c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123a27c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123a28160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123a286b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123a28c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123a29150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123a296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123a29bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123a2a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123a2a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123a2abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123a2b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123a2b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123a2bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123a1b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123a2c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123a2c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123a2cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123a2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123a2d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123a2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123a2e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123a2e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123a2ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123a2f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123a2f7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123a2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123a30260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123a307b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123a30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123a311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123a31640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123a31ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123a31f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123a32420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123a328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123a32d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123a33200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123a336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123a33b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123a33fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123a34480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123a34920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123a34dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123a35260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123a35700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123a35ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123a36040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123a364e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123a36980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123a36e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123a372c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123a37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123a37c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123a380a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123a38540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123a389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123a38e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123a39320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123a397c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123a39c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123a3a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123a3a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123a3aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123a3aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123a3b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123a3b820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123a3bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123a3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123a3c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123a3caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123a3cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123a3d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123a3d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123a3dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123a3e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123a3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123a3eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123a3efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123a3f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123a3f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123a3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123a40220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123a406c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123a40b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123a41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123a414a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123a41940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123a41de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123a42280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123a42720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123a42bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123a43060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123a43500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123a439a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123a43e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123a442e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123a44780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123a44c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123a450c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123a45560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123a45a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123a45ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123a46340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123a467e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123a46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123a47120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123a475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123a47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123a47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123a48450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123a489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123a48ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123a49440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123a49700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123a49d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123a4a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123a4a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123a4b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123a4b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123a4b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123a4be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123a4c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123a4cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123a4d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123a4d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123a4da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123a4e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123a4e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123a4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123a4f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123a4f760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123a4fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123a50200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123a50750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123a50ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123a511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123a51740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123a51c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123a521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123a52730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123a52c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123a531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123a53720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123a53c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123a541c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123a54710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123a54c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123a551b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123a55700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123a55c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123a561a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123a566f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123a56c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123a57190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123a576e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123a57c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123a58180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123a586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123a58c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123a59170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123a596c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123a59c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123a5a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123a5a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123a5ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123a5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123a5b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123a5bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123a5c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123a5c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123a5cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123a5d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123a5d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123a5dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123a5e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123a5e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123a5ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123a5f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123a5f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123a5fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123a60100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123a60650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123a60ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123a61040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123a614e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123a61980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123a61e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123a622c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123a62760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123a62c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123a630a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123a63540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123a639e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123a63e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123a64320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123a647c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123a64c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123a65100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123a65650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123a65d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123a66490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123a66bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123a672d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123a67590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123a67d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123a68040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123a68650 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.140.456 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.140.461 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120a04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120a04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120a05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120a05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120a05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120a06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120a065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120a06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120a06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120a07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120a07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120a07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120a08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120a09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120a09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120a0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120a0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120a0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120a0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120a0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120a0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120a0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120a0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120a0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120a0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120a0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120a0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120a0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120a0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120a0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120a0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120a0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120a10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120a10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120a108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120a10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120a11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120a11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120a11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120a11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120a12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120a127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120a12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120a130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120a13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120a13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120a13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120a14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120a146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120a14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120a14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120a15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120a15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120a15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120a16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120a165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120a16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120a17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120a174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120a17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120a17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120a18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120a18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120a18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120a18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120a193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120a19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120a19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120a1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120a1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120a1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120a1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120a1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120a1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120a1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120a1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120a1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120a1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120a1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120a1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120a1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120a1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120a1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120a1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120a1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120a1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120a1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120a1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120a1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120a1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120a202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120a20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120a20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120a21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120a21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120a218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120a21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120a221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120a22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120a22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120a22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120a23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120a23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120a23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120a240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120a24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120a249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120a24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120a252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120a25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120a25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120a25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120a26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120a268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120a26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120a271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120a27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120a27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120a27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120a28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120a287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120a28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120a290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120a29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120a299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120a29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120a2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120a2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120a2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120a2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120a2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120a2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120a2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120a2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120a2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120a2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120a2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120a2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120a2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120a2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120a2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120a2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120a2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120a2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120a2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120a2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120a2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120a2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120a30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120a30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120a30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120a31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120a315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120a31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120a31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120a32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120a327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120a32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120a33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120a334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120a33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120a33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120a34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120a346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120a34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120a34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120a35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120a35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120a36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120a365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120a36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120a36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120a37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120a37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120a37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120a38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120a384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120a38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120a38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120a39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120a39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120a39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120a39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120a3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120a3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120a3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120a3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120a3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120a3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120a3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120a3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120a3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120a3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120a3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120a3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120a3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120a3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120a3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120a3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120a3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120a3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120a3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120a3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120a3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120a40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120a40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120a40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120a40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120a41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120a41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120a42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120a42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120a42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120a433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120a43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120a43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120a44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120a44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120a45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120a45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120a45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120a461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120a46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120a46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120a47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120a478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120a47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120a48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120a48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120a48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120a49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120a49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120a4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120a4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120a4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120a4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120a4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120a4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120a4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120a4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120a4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120a4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120a4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120a4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120a4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120a4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120a4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120a4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120a4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120a502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120a50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120a50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120a51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120a519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120a51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120a52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120a52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120a530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120a53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120a53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120a54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120a547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120a54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120a55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120a55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120a55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120a56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120a56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120a56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120a57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120a57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120a57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120a58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120a58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120a58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120a59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120a59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120a59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120a5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120a5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120a5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120a5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120a5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120a5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120a5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120a5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120a5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120a5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120a5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120a5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120a5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120a5b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120a4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120a4b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120a48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120a45900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120a55040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120a52800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120a50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120a4e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120a46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120a43c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120a48cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120a49e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120a4f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120a4c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120a53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120a47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120a51100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120a4a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120a4cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120a475c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120a55600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120a447c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120a430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120a45340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120a55bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120a4af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120a53380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120a49280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120a4bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120a4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120a47000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120a4ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120a516c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120a45ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120a544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120a51c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120a4d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120a56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120a44d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120a56180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120a44200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120a54a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120a4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120a50b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120a53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120a52240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120a4a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120a41cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120a04680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120a5da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120a0b890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120a5ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120a5f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120a5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120a5f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120a5fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120a5fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120a5ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120a60250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120a60510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120a607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120a60a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120a60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120a61010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120a612d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120a61590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120a61850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120a61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120a61dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x123a4bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x123a49fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x123a68300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x123a499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x123a4a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x123a1d6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x123a1d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x123a1f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x123a4c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x123a14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x123a1b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x123a1be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x123a1c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x123a1af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x123a1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x123a1dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x123a1caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x123a13a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x123a0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x123a1fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x123a2c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x123a67850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x123a16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x123a16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x123a4c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x123a4abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x123a15080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x123a15340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x123a15600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x123a68ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x123a68d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x123a69030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x123a692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x123a695b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x123a69870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x123a69b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x123a69df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x123a6a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x123a6a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x123a6a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x123a6a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x123a6abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x123a6ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x123a6b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x123a6b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x123a6b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x123a6b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x123a6bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x123a6bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x123a6c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x123a6c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x123a6c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x123a6c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x123a6ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x123a6cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x123a6d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x123a6d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x123a6d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x123a6da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x123a6dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x123a6dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x123a6e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x123a6e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x123a6e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x123a6eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x123a6edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x123a6f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x123a6f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x123a6f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x123a6f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x123a6fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x123a6fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x123a700f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x123a703b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x123a70670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x123a70930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x123a70bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x123a70eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x123a71170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x123a71430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x123a716f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x123a719b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x123a71c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x123a71f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x123a721f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x123a724b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x123a72770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x123a72a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x123a72cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x123a72fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x123a73270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x123a73530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x123a737f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x123a73ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x123a73d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x123a74030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x123a742f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x123a745b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x123a74870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x123a74b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x123a74df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x123a750b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x123a75370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x123a75630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x123a758f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x123a75bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x123a75e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x123a76130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x123a763f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x123a766b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x123a76970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x123a76c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x123a76ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x123a771b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x123a77470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x123a77730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x123a779f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x123a77cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x123a77f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x123a78230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x123a784f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x123a787b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x123a78a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x123a78d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x123a78ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x123a792b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x123a79570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x123a79830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x123a79af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x123a79db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x123a7a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x123a7a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x123a7a5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x123a7a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x123a7ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x123a7ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x123a7b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x123a7b3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x123a7b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x123a7b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x123a7bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x123a7c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x123a7c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x123a7c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x123a7ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x123a7ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x123a7cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x123a7d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x123a7d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x123a7d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x123a7da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x123a7dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x123a7e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x123a7e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x123a7e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x123a7e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x123a7eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x123a7edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x123a7f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x123a7f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x123a7f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x123a7f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x123a7fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x123a7fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x123a80100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x123a803c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x123a80680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x123a80940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x123a80c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x123a80ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x123a81180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x123a81440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x123a81700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x123a819c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x123a81c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x123a81f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x123a82200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x123a824c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x123a82780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x123a82a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x123a82d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x123a82fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x123a83280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x123a83540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x123a83800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x123a83ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x123a83d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x123a84040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x123a84300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x123a845c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x123a84880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x123a84b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x123a84e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x123a850c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x123a85380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x123a85640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x123a85900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x123a85bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x123a85e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x123a86140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x123a86400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x123a866c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x123a86980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x123a86c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x123a86f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x123a871c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x123a87480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x123a87740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x123a87a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x123a87cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x123a87f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x123a88240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x123a88500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x123a887c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x123a88a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x123a88d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x123a89000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x123a89540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x123a89800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x123a89ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x123a89d80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.815s
user	0m0.293s
sys	0m0.315s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4468 (04a9ef8f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155f0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155f0e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155f0ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155f0f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155f0f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155f0fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155f102e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155f10890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155f10e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155f11340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155f11840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155f11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155f12860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155f13010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155f13820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155f13f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155f14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155f14d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155f154a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155f15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155f16390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155f16ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155f171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155f17a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155f18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155f18450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155f18a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155f196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155f19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155f19ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155f1a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155f1a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155f1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155f1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155f1b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155f1bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155f1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155f1c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155f1cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155f1d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155f1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155f1dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155f1e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155f1e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155f1e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155f1ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155f1f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155f1fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155f20480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155f20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155f210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155f216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155f21cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155f224b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155f22950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155f22df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155f230b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155f236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155f23eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155f24170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155f24610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155f24ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155f24f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155f253f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155f25890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155f25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155f261d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155f26670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155f26b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155f26fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155f27450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155f278f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155f27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155f28390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155f288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155f28e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155f29380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155f298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155f29e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155f2a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155f2a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155f2ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155f2b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155f2be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155f2c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155f2c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155f2cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155f2d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155f2d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155f2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155f2e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155f2e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155f2edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155f2f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155f2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155f1f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155f2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155f30490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155f309e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155f30f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155f31480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155f319d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155f31f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x155f32470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155f329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155f32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155f33460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155f339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155f33f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155f34450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155f349a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155f34e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155f352e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155f35780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155f35c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155f360c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155f36560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155f36a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155f36ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155f37340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155f377e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155f37c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155f38120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155f385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155f38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155f38f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155f393a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155f39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155f39ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155f3a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155f3a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155f3aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155f3af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155f3b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155f3b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155f3bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155f3c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155f3c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155f3cb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155f3cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155f3d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155f3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155f3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155f3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155f3e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155f3eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155f3f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155f3f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155f3f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155f3fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155f402a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155f40740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155f40be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155f41080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155f41520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155f419c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155f41e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155f42300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155f427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155f42c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155f430e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155f43580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155f43a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155f43ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155f44360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155f44800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155f44ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155f45140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155f455e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155f45a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155f45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155f463c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155f46860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155f46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155f471a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155f47640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155f47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155f47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155f48420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155f488c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155f48d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155f49200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155f496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155f49fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155f4a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155f4a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155f4adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155f4b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155f4b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155f4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155f4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155f4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155f4cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155f4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155f4d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155f4d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155f4dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155f4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155f4edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155f4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155f4f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155f4fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155f50140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155f50930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155f50dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155f51270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155f51710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155f51ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155f52410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155f52960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155f52eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155f53400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155f53950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155f53ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155f543f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155f54940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155f54e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155f553e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155f55930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155f55e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155f563d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155f56920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155f56e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155f573c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155f57910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155f57e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155f583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155f58900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155f58e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155f593a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155f598f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155f59e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155f5a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155f5a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155f5ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155f5b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155f5b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155f5be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155f5c370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155f5c8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155f5ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155f5d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155f5d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155f5de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155f5e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155f5e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155f5edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155f5f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155f5f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155f5fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155f60330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155f60880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155f60dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155f61320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155f61870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155f61dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155f62310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155f62860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155f62db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155f63300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155f63850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155f63da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155f642f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155f64840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155f64ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155f65180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155f65620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155f65ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155f65f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155f66400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155f668a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155f66d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155f671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155f67680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155f67b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155f67fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155f68460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155f68900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155f68da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155f692f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155f69a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155f6a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155f6a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155f6af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155f6b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155f6ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155f6bce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155f6c2f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.086.491 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x155f6bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x155f4dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x155f4d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x155f4e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x155f21360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x155f20d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x155f23370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x155f4fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x155f18710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x155f1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x155f1fb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x155f20130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x155f1e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x155f20740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x155f17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x155f23980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x155f2ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x155f6b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x155f1a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x155f1abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x155f50400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x155f4e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x155f18d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x155f18fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x155f192a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x155f6c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x155f6ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x155f6ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x155f6cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x155f6d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x155f6d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x155f6d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x155f6da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x155f6dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x155f6e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x155f6e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x155f6e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x155f6e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x155f6eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x155f6edd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x155f6f090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x155f6f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x155f6f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x155f6f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x155f6fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x155f6fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x155f70110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x155f703d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x155f70690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x155f70950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x155f70c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x155f70ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x155f71190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x155f71450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x155f71710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x155f719d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x155f71c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x155f71f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x155f72210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x155f724d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x155f72790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x155f72a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x155f72d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x155f72fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x155f73290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x155f73550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x155f73810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x155f73ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x155f73d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x155f74050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x155f74310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x155f745d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x155f74890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x155f74b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x155f74e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x155f750d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x155f75390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x155f75650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x155f75910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x155f75bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x155f75e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x155f76150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x155f76410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x155f766d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x155f76990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x155f76c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x155f76f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x155f771d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x155f77490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x155f77750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x155f77a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x155f77cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x155f77f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x155f78250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x155f78510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x155f787d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x155f78a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x155f78d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x155f79010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x155f792d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x155f79590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x155f79850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x155f79b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x155f79dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x155f7a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x155f7a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x155f7a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x155f7a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x155f7ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x155f7ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x155f7b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x155f7b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x155f7b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x155f7b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x155f7bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x155f7bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x155f7c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x155f7c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x155f7c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x155f7c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x155f7cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x155f7cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x155f7d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x155f7d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x155f7d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x155f7da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x155f7dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x155f7dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x155f7e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x155f7e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x155f7e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x155f7ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x155f7ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x155f7f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x155f7f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x155f7f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x155f7f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x155f7fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x155f7fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x155f800d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x155f80390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x155f80650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x155f80910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x155f80bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x155f80e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x155f81150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x155f81410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x155f816d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x155f81990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x155f81c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x155f81f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x155f821d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x155f82490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x155f82750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x155f82a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x155f82cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x155f82f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x155f83250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x155f83510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x155f837d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x155f83a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x155f83d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x155f84010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x155f842d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x155f84590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x155f84850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x155f84b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x155f84dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x155f85090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x155f85350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x155f85610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x155f858d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x155f85b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x155f85e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x155f86110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x155f863d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x155f86690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x155f86950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x155f86c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x155f86ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x155f87190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x155f87450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x155f87710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x155f879d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x155f87c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x155f87f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x155f88210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x155f884d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x155f88790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x155f88a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x155f88d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x155f88fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x155f89290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x155f89550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x155f89810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x155f89ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x155f89d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x155f8a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x155f8a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x155f8a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x155f8a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x155f8ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x155f8ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x155f8b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x155f8b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x155f8b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x155f8b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x155f8bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x155f8be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x155f8c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x155f8c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x155f8c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x155f8cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x155f8d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x155f8d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x155f8d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x155f8de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x155f8e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x155f8e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x155f8ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x155f8f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x155f8f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x155f8f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x155f8fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x155f901e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x155f90650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x155f90ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x155f90f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x155f913a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x155f91810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x155f91c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x155f920f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x155f92560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x155f929d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x155f92e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x155f932b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x155f93720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x155f93b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x155f94000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x155f94470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x155f948e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x155f94d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x155f951c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x155f95630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x155f95aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x155f95f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x155f96380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x155f967f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x155f96c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x155f970d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x155f97540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x155f979b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x155f97e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x155f98290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x155f98700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x155f98b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x155f98fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x155f99450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x155f998c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x155f99d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x155f9a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x155f9a610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x155f9aa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x155f9aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x155f9b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x155f9b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x155f9bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x155f9c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x155f9c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x155f9c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x155f9ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x155f9d270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x155f9d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x155f9db50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x155f9dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x155f9e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x155f9e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x155f9ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x155f9f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x155f9f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x155f9fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x155f9fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x155fa0340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x155fa0db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x155fa14d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x155fa1bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x155fa2310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x155fa25d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x155fa2dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x155fa3080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x155fa3690 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1578046e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x157804b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x157804fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x157805430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1578058a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x157805d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x157806180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1578065f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x157806a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x157806fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x157807440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x157807ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1578085e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x157808d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1578095a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x157809cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15780a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15780ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15780b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15780b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15780c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15780c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15780cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15780d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15780dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15780e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15780e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15780e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15780ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15780f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15780f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15780fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15780fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x157810130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1578105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x157810a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x157810e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1578112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x157811760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x157811bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x157812040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1578124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x157812920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x157812d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x157813200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x157813670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x157813ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x157813f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1578143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x157814830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x157814ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x157815110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x157815580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1578159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x157815e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1578162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x157816840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x157816d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1578171b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x157817620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x157817a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x157817f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x157818370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1578187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x157818c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1578190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x157819530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1578199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x157819e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15781a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15781a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15781ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15781afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15781b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15781b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15781bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15781c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15781c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15781ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15781cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15781d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15781d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15781dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15781e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15781e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15781e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15781edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15781f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15781f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15781fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15781ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x157820420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x157820890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x157820d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x157821170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1578215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x157821a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x157821ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x157822330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1578227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x157822c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x157823080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1578234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x157823d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x157824040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x1578244b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x157824920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x157824d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x157825200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x157825670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x157825ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x157825f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1578263c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x157826830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x157826ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x157827110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x157827580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1578279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x157827e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1578282d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x157828740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x157828bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x157829020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x157829490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x157829900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x157829d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15782a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15782a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15782aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15782af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15782b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15782b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15782bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15782c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15782c560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15782c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15782ce40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15782d2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15782d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15782db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15782e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15782e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15782e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15782ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15782f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15782f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15782faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15782ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x157830380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1578307f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x157830c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1578310d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x157831540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1578319b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x157831e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x157832290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x157832700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x157832b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x157832fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x157833450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1578338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x157833d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1578341a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x157834610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x157834a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x157834ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x157835360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1578357d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x157835c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1578360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x157836520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x157836990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x157836e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x157837270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1578376e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x157837b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x157837fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x157838430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1578388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x157838d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x157839180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1578395f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x157839a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x157839ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15783a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15783a7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15783ac20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15783b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15783b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15783b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15783bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15783c250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15783c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15783cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15783cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15783d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15783d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15783dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15783e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15783e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15783ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15783eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15783f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15783f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15783fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x157840070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1578404e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x157840950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x157840dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x157841230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x157841db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x157842070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x157842330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1578427a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x157842c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x157843080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1578434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x157843960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x157843dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x157844240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1578446b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x157844b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x157844f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x157845400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x157845870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x157845ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x157846150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1578465c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x157846a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x157846ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x157847310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x157847780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x157847bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x157848060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1578484d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x157848940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x157848db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x157849220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x157849690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x157849b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x157849f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15784a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15784a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15784acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15784b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15784b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15784ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15784be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15784c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15784c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15784cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15784d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15784d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15784d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15784dd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15784e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15784e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15784eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15784ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15784f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15784f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15784fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x157850110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x157850580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1578509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x157850e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1578512d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x157851740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x157851bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x157852020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x157852490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x157852900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x157852d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1578531e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x157853650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x157853ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x157853f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1578543a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x157854810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x157854c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1578550f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x157855560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1578559d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x157856440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x157856b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x157857280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1578579a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x157857c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1578580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1578586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x157858ce0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.917s
user	0m0.243s
sys	0m0.131s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
