Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.561s
user	0m0.869s
sys	0m1.244s
++ nproc
+ make -j10
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  2%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  5%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Built target sha256
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 14%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama
[ 25%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 32%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 32%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target test-c
[ 35%] Built target llama-quantize-stats
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-simple-chat
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Built target llama-simple
[ 36%] Built target common
[ 36%] Built target llava_static
[ 38%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Built target llava_shared
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-sampling
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-grammar-integration
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-0
[ 50%] Linking CXX executable ../bin/test-arg-parser
[ 50%] Built target test-sampling
[ 50%] Built target test-grammar-parser
[ 50%] Built target test-llama-grammar
[ 50%] Built target test-json-schema-to-grammar
[ 51%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Built target test-log
[ 53%] Built target test-grammar-integration
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Linking CXX executable ../bin/test-gguf
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 56%] Built target test-arg-parser
[ 56%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 58%] Linking CXX executable ../bin/test-backend-ops
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 59%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-autorelease
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 59%] Linking CXX executable ../bin/test-barrier
[ 60%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 60%] Built target test-gguf
[ 60%] Built target test-chat-template
[ 60%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Built target test-model-load-cancel
[ 61%] Built target test-backend-ops
[ 61%] Built target test-autorelease
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-fns
[ 62%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 62%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 63%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Built target test-barrier
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 63%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 64%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 65%] Linking CXX executable ../../bin/llama-batched
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 68%] Built target test-quantize-fns
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Built target llama-batched-bench
[ 69%] Built target test-quantize-perf
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Built target test-rope
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Built target llama-batched
[ 70%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 70%] Built target llama-embedding
[ 70%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 70%] Built target llama-gbnf-validator
[ 70%] Built target llama-gguf-split
[ 70%] Built target llama-eval-callback
[ 70%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Built target llama-gritlm
[ 71%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 73%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 74%] Linking CXX executable ../../bin/llama-lookahead
[ 75%] Linking CXX executable ../../bin/llama-lookup
[ 75%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 76%] Linking CXX executable ../../bin/llama-lookup-stats
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 79%] Linking CXX executable ../../bin/llama-cli
[ 79%] Built target llama-infill
[ 79%] Built target llama-bench
[ 79%] Built target llama-imatrix
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Built target llama-lookahead
[ 80%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 81%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 82%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 82%] Built target llama-lookup-stats
[ 82%] Built target llama-lookup
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-cli
[ 82%] Built target llama-lookup-create
[ 82%] Generating loading.html.hpp
[ 83%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 83%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-passkey
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-parallel
[ 85%] Generating index.html.gz.hpp
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 88%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-save-load-state
[ 89%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 89%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-speculative
[ 89%] Linking CXX executable ../../bin/llama-run
[ 89%] Built target llama-quantize
[ 89%] Built target llama-passkey
[ 89%] Built target llama-perplexity
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Built target llama-retrieval
[ 90%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 90%] Built target llama-save-load-state
[ 90%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 90%] Built target llama-speculative-simple
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 90%] Built target llama-speculative
[ 90%] Built target llama-run
[ 90%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 91%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 91%] Built target llama-tokenize
[ 91%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 98%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 98%] Built target llama-tts
[ 98%] Built target llama-gen-docs
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-cvector-generator
[ 98%] Built target llama-qwen2vl-cli
[ 98%] Built target llama-export-lora
[ 98%] Built target llama-llava-cli
[ 99%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-vdot
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.957s
user	0m6.197s
sys	0m10.068s

main: quantize time =  3552.84 ms
main:    total time =  3552.84 ms

main: quantize time =  1992.87 ms
main:    total time =  1992.87 ms

main: quantize time =  1925.84 ms
main:    total time =  1925.84 ms

main: quantize time =  2617.11 ms
main:    total time =  2617.11 ms

main: quantize time =  2673.98 ms
main:    total time =  2673.98 ms

main: quantize time =  4802.56 ms
main:    total time =  4802.56 ms

main: quantize time =  5556.87 ms
main:    total time =  5556.87 ms

main: quantize time =  6900.37 ms
main:    total time =  6900.37 ms

main: quantize time =  5862.25 ms
main:    total time =  5862.25 ms

main: quantize time =  4685.86 ms
main:    total time =  4685.86 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.144 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.258 I main: llama backend init
0.00.000.263 I main: load the model and apply lora adapter, if any
0.00.031.066 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.041.897 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.915 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.919 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.931 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.932 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.932 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.933 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.935 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.935 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.936 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.937 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.937 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.938 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.939 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.945 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.945 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.946 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.050.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.052.595 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.060.401 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.060.404 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.060.405 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.060.405 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.060.406 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.060.407 I llama_model_loader: - type  f32:  194 tensors
0.00.060.407 I llama_model_loader: - type  f16:   98 tensors
0.00.092.056 I llm_load_vocab: special tokens cache size = 25
0.00.098.845 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.098.848 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.098.848 I llm_load_print_meta: arch             = gptneox
0.00.098.848 I llm_load_print_meta: vocab type       = BPE
0.00.098.849 I llm_load_print_meta: n_vocab          = 50304
0.00.098.849 I llm_load_print_meta: n_merges         = 50009
0.00.098.849 I llm_load_print_meta: vocab_only       = 0
0.00.098.849 I llm_load_print_meta: n_ctx_train      = 2048
0.00.098.849 I llm_load_print_meta: n_embd           = 2048
0.00.098.849 I llm_load_print_meta: n_layer          = 24
0.00.098.853 I llm_load_print_meta: n_head           = 16
0.00.098.853 I llm_load_print_meta: n_head_kv        = 16
0.00.098.853 I llm_load_print_meta: n_rot            = 32
0.00.098.854 I llm_load_print_meta: n_swa            = 0
0.00.098.854 I llm_load_print_meta: n_embd_head_k    = 128
0.00.098.854 I llm_load_print_meta: n_embd_head_v    = 128
0.00.098.855 I llm_load_print_meta: n_gqa            = 1
0.00.098.855 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.098.856 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.098.857 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.098.857 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.098.858 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.098.859 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.098.860 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.098.861 I llm_load_print_meta: n_ff             = 8192
0.00.098.861 I llm_load_print_meta: n_expert         = 0
0.00.098.861 I llm_load_print_meta: n_expert_used    = 0
0.00.098.861 I llm_load_print_meta: causal attn      = 1
0.00.098.861 I llm_load_print_meta: pooling type     = 0
0.00.098.874 I llm_load_print_meta: rope type        = 2
0.00.098.874 I llm_load_print_meta: rope scaling     = linear
0.00.098.875 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.098.875 I llm_load_print_meta: freq_scale_train = 1
0.00.098.875 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.098.876 I llm_load_print_meta: rope_finetuned   = unknown
0.00.098.876 I llm_load_print_meta: ssm_d_conv       = 0
0.00.098.876 I llm_load_print_meta: ssm_d_inner      = 0
0.00.098.876 I llm_load_print_meta: ssm_d_state      = 0
0.00.098.877 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.098.877 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.098.878 I llm_load_print_meta: model type       = 1.4B
0.00.098.878 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.098.879 I llm_load_print_meta: model params     = 1.41 B
0.00.098.879 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.098.879 I llm_load_print_meta: general.name     = 1.4B
0.00.098.880 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.098.880 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.098.880 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.098.880 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.098.880 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.098.881 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.098.881 I llm_load_print_meta: max token length = 1024
0.00.101.503 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.101.503 I llm_load_tensors: offloading output layer to GPU
0.00.101.503 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.101.522 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.101.523 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.102.498 I llama_new_context_with_model: n_seq_max     = 1
0.00.102.499 I llama_new_context_with_model: n_ctx         = 2048
0.00.102.499 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.102.499 I llama_new_context_with_model: n_batch       = 2048
0.00.102.499 I llama_new_context_with_model: n_ubatch      = 512
0.00.102.499 I llama_new_context_with_model: flash_attn    = 0
0.00.102.500 I llama_new_context_with_model: freq_base     = 10000.0
0.00.102.500 I llama_new_context_with_model: freq_scale    = 1
0.00.102.501 I ggml_metal_init: allocating
0.00.102.510 I ggml_metal_init: found device: Apple M4
0.00.102.512 I ggml_metal_init: picking default device: Apple M4
0.00.103.204 I ggml_metal_init: using embedded metal library
0.00.120.378 I ggml_metal_init: GPU name:   Apple M4
0.00.120.380 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.120.380 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.120.381 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.120.381 I ggml_metal_init: simdgroup reduction   = true
0.00.120.381 I ggml_metal_init: simdgroup matrix mul. = true
0.00.120.381 I ggml_metal_init: has bfloat            = true
0.00.120.381 I ggml_metal_init: use bfloat            = true
0.00.120.382 I ggml_metal_init: hasUnifiedMemory      = true
0.00.120.382 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.143.969 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.165.598 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.165.606 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.165.628 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.166.618 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.166.620 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.166.621 I llama_new_context_with_model: graph nodes  = 967
0.00.166.621 I llama_new_context_with_model: graph splits = 2
0.00.166.624 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.166.758 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.166.758 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.249.940 I main: llama threadpool init, n_threads = 4
0.00.249.977 I 
0.00.249.998 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.249.998 I 
0.00.250.077 I sampler seed: 1234
0.00.250.083 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.250.106 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.250.108 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.250.108 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.090.364 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58006.54 tokens per second)
0.02.090.365 I llama_perf_context_print:        load time =     218.86 ms
0.02.090.366 I llama_perf_context_print: prompt eval time =      43.74 ms /     7 tokens (    6.25 ms per token,   160.03 tokens per second)
0.02.090.367 I llama_perf_context_print:        eval time =    1793.58 ms /    63 runs   (   28.47 ms per token,    35.13 tokens per second)
0.02.090.368 I llama_perf_context_print:       total time =    1840.43 ms /    70 tokens
0.02.090.646 I ggml_metal_free: deallocating

real	0m2.374s
user	0m0.145s
sys	0m0.105s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.831 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.207 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.030.214 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.216 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.217 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.223 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.226 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.229 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.015 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.083 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.021 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.023 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.023 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.023 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.023 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.024 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.039.024 I llama_model_loader: - type  f32:  194 tensors
0.00.039.025 I llama_model_loader: - type q8_0:   98 tensors
0.00.061.573 I llm_load_vocab: special tokens cache size = 25
0.00.067.459 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.067.463 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.067.463 I llm_load_print_meta: arch             = gptneox
0.00.067.463 I llm_load_print_meta: vocab type       = BPE
0.00.067.464 I llm_load_print_meta: n_vocab          = 50304
0.00.067.464 I llm_load_print_meta: n_merges         = 50009
0.00.067.464 I llm_load_print_meta: vocab_only       = 0
0.00.067.464 I llm_load_print_meta: n_ctx_train      = 2048
0.00.067.467 I llm_load_print_meta: n_embd           = 2048
0.00.067.468 I llm_load_print_meta: n_layer          = 24
0.00.067.472 I llm_load_print_meta: n_head           = 16
0.00.067.473 I llm_load_print_meta: n_head_kv        = 16
0.00.067.474 I llm_load_print_meta: n_rot            = 32
0.00.067.474 I llm_load_print_meta: n_swa            = 0
0.00.067.474 I llm_load_print_meta: n_embd_head_k    = 128
0.00.067.474 I llm_load_print_meta: n_embd_head_v    = 128
0.00.067.476 I llm_load_print_meta: n_gqa            = 1
0.00.067.477 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.067.477 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.067.478 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.067.479 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.067.479 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.067.479 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.067.479 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.067.480 I llm_load_print_meta: n_ff             = 8192
0.00.067.480 I llm_load_print_meta: n_expert         = 0
0.00.067.480 I llm_load_print_meta: n_expert_used    = 0
0.00.067.480 I llm_load_print_meta: causal attn      = 1
0.00.067.481 I llm_load_print_meta: pooling type     = 0
0.00.067.497 I llm_load_print_meta: rope type        = 2
0.00.067.497 I llm_load_print_meta: rope scaling     = linear
0.00.067.498 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.067.499 I llm_load_print_meta: freq_scale_train = 1
0.00.067.499 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.067.499 I llm_load_print_meta: rope_finetuned   = unknown
0.00.067.500 I llm_load_print_meta: ssm_d_conv       = 0
0.00.067.500 I llm_load_print_meta: ssm_d_inner      = 0
0.00.067.500 I llm_load_print_meta: ssm_d_state      = 0
0.00.067.500 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.067.500 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.067.501 I llm_load_print_meta: model type       = 1.4B
0.00.067.501 I llm_load_print_meta: model ftype      = Q8_0
0.00.067.501 I llm_load_print_meta: model params     = 1.41 B
0.00.067.502 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.067.502 I llm_load_print_meta: general.name     = 1.4B
0.00.067.502 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.067.502 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.067.504 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.067.504 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.067.504 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.067.505 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.067.505 I llm_load_print_meta: max token length = 1024
0.00.070.004 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.004 I llm_load_tensors: offloading output layer to GPU
0.00.070.005 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.016 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.070.018 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.070.976 I llama_new_context_with_model: n_seq_max     = 1
0.00.070.977 I llama_new_context_with_model: n_ctx         = 2048
0.00.070.977 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.977 I llama_new_context_with_model: n_batch       = 2048
0.00.070.978 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.978 I llama_new_context_with_model: flash_attn    = 0
0.00.070.978 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.978 I llama_new_context_with_model: freq_scale    = 1
0.00.070.979 I ggml_metal_init: allocating
0.00.070.985 I ggml_metal_init: found device: Apple M4
0.00.070.987 I ggml_metal_init: picking default device: Apple M4
0.00.071.713 I ggml_metal_init: using embedded metal library
0.00.074.305 I ggml_metal_init: GPU name:   Apple M4
0.00.074.307 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.308 I ggml_metal_init: simdgroup reduction   = true
0.00.074.308 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.308 I ggml_metal_init: has bfloat            = true
0.00.074.308 I ggml_metal_init: use bfloat            = true
0.00.074.309 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.819 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.110.432 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.110.442 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.110.466 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.577 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.111.580 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.111.580 I llama_new_context_with_model: graph nodes  = 967
0.00.111.581 I llama_new_context_with_model: graph splits = 2
0.00.111.584 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.111.713 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.304.758 I main: llama threadpool init, n_threads = 4
0.01.304.810 I 
0.01.304.840 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.304.841 I 
0.01.305.094 I sampler seed: 1234
0.01.305.100 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.305.135 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.305.138 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.305.139 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.405.690 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47972.97 tokens per second)
0.02.405.690 I llama_perf_context_print:        load time =    1294.92 ms
0.02.405.692 I llama_perf_context_print: prompt eval time =      46.02 ms /     7 tokens (    6.57 ms per token,   152.12 tokens per second)
0.02.405.693 I llama_perf_context_print:        eval time =    1051.67 ms /    63 runs   (   16.69 ms per token,    59.90 tokens per second)
0.02.405.693 I llama_perf_context_print:       total time =    1100.94 ms /    70 tokens
0.02.405.929 I ggml_metal_free: deallocating

real	0m2.425s
user	0m0.118s
sys	0m0.234s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.016.254 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.029.808 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.029.813 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.029.816 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.029.817 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.029.817 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.029.817 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.029.818 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.029.819 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.029.819 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.029.819 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.029.820 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.029.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.029.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.029.820 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.029.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.029.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.029.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.156 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.386 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.779 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.780 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.781 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.781 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.782 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.782 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.039.782 I llama_model_loader: - type  f32:  194 tensors
0.00.039.783 I llama_model_loader: - type q4_0:   97 tensors
0.00.039.783 I llama_model_loader: - type q6_K:    1 tensors
0.00.067.103 I llm_load_vocab: special tokens cache size = 25
0.00.074.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.074.617 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.074.617 I llm_load_print_meta: arch             = gptneox
0.00.074.617 I llm_load_print_meta: vocab type       = BPE
0.00.074.618 I llm_load_print_meta: n_vocab          = 50304
0.00.074.618 I llm_load_print_meta: n_merges         = 50009
0.00.074.618 I llm_load_print_meta: vocab_only       = 0
0.00.074.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.074.618 I llm_load_print_meta: n_embd           = 2048
0.00.074.618 I llm_load_print_meta: n_layer          = 24
0.00.074.623 I llm_load_print_meta: n_head           = 16
0.00.074.624 I llm_load_print_meta: n_head_kv        = 16
0.00.074.624 I llm_load_print_meta: n_rot            = 32
0.00.074.624 I llm_load_print_meta: n_swa            = 0
0.00.074.624 I llm_load_print_meta: n_embd_head_k    = 128
0.00.074.624 I llm_load_print_meta: n_embd_head_v    = 128
0.00.074.625 I llm_load_print_meta: n_gqa            = 1
0.00.074.626 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.074.627 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.074.627 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.074.628 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.074.628 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.074.628 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.074.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.074.629 I llm_load_print_meta: n_ff             = 8192
0.00.074.629 I llm_load_print_meta: n_expert         = 0
0.00.074.629 I llm_load_print_meta: n_expert_used    = 0
0.00.074.629 I llm_load_print_meta: causal attn      = 1
0.00.074.629 I llm_load_print_meta: pooling type     = 0
0.00.074.645 I llm_load_print_meta: rope type        = 2
0.00.074.645 I llm_load_print_meta: rope scaling     = linear
0.00.074.646 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.074.646 I llm_load_print_meta: freq_scale_train = 1
0.00.074.646 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.074.647 I llm_load_print_meta: rope_finetuned   = unknown
0.00.074.648 I llm_load_print_meta: ssm_d_conv       = 0
0.00.074.648 I llm_load_print_meta: ssm_d_inner      = 0
0.00.074.648 I llm_load_print_meta: ssm_d_state      = 0
0.00.074.648 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.074.648 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.074.649 I llm_load_print_meta: model type       = 1.4B
0.00.074.649 I llm_load_print_meta: model ftype      = Q4_0
0.00.074.650 I llm_load_print_meta: model params     = 1.41 B
0.00.074.650 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.074.652 I llm_load_print_meta: general.name     = 1.4B
0.00.074.652 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.074.652 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.074.652 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.074.653 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.074.653 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.074.653 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.074.653 I llm_load_print_meta: max token length = 1024
0.00.077.127 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.077.127 I llm_load_tensors: offloading output layer to GPU
0.00.077.127 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.077.139 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.077.140 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.078.267 I llama_new_context_with_model: n_seq_max     = 1
0.00.078.268 I llama_new_context_with_model: n_ctx         = 2048
0.00.078.269 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.078.269 I llama_new_context_with_model: n_batch       = 2048
0.00.078.269 I llama_new_context_with_model: n_ubatch      = 512
0.00.078.269 I llama_new_context_with_model: flash_attn    = 0
0.00.078.270 I llama_new_context_with_model: freq_base     = 10000.0
0.00.078.270 I llama_new_context_with_model: freq_scale    = 1
0.00.078.271 I ggml_metal_init: allocating
0.00.078.274 I ggml_metal_init: found device: Apple M4
0.00.078.276 I ggml_metal_init: picking default device: Apple M4
0.00.079.138 I ggml_metal_init: using embedded metal library
0.00.083.271 I ggml_metal_init: GPU name:   Apple M4
0.00.083.273 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.083.274 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.083.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.083.275 I ggml_metal_init: simdgroup reduction   = true
0.00.083.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.083.275 I ggml_metal_init: has bfloat            = true
0.00.083.275 I ggml_metal_init: use bfloat            = true
0.00.083.276 I ggml_metal_init: hasUnifiedMemory      = true
0.00.083.277 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.680 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.123.984 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.123.993 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.124.017 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.125.119 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.125.122 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.125.123 I llama_new_context_with_model: graph nodes  = 967
0.00.125.123 I llama_new_context_with_model: graph splits = 2
0.00.125.130 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.125.257 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.125.258 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.667.717 I main: llama threadpool init, n_threads = 4
0.00.667.756 I 
0.00.667.785 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.667.785 I 
0.00.668.019 I sampler seed: 1234
0.00.668.023 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.668.064 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.668.064 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.668.064 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.351.505 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59019.12 tokens per second)
0.01.351.506 I llama_perf_context_print:        load time =     651.46 ms
0.01.351.506 I llama_perf_context_print: prompt eval time =      39.77 ms /     7 tokens (    5.68 ms per token,   176.01 tokens per second)
0.01.351.508 I llama_perf_context_print:        eval time =     640.69 ms /    63 runs   (   10.17 ms per token,    98.33 tokens per second)
0.01.351.508 I llama_perf_context_print:       total time =     683.79 ms /    70 tokens
0.01.351.709 I ggml_metal_free: deallocating

real	0m1.373s
user	0m0.127s
sys	0m0.152s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.751 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.051 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.055 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.056 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.057 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.057 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.058 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.058 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.059 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.059 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.059 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.060 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.060 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.060 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.061 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.065 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.069 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.070 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.814 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.846 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.644 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.645 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.645 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.646 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.646 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.646 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.031.647 I llama_model_loader: - type  f32:  194 tensors
0.00.031.647 I llama_model_loader: - type q4_1:   97 tensors
0.00.031.647 I llama_model_loader: - type q6_K:    1 tensors
0.00.052.550 I llm_load_vocab: special tokens cache size = 25
0.00.058.407 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.410 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.410 I llm_load_print_meta: arch             = gptneox
0.00.058.410 I llm_load_print_meta: vocab type       = BPE
0.00.058.410 I llm_load_print_meta: n_vocab          = 50304
0.00.058.411 I llm_load_print_meta: n_merges         = 50009
0.00.058.411 I llm_load_print_meta: vocab_only       = 0
0.00.058.411 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.411 I llm_load_print_meta: n_embd           = 2048
0.00.058.411 I llm_load_print_meta: n_layer          = 24
0.00.058.414 I llm_load_print_meta: n_head           = 16
0.00.058.415 I llm_load_print_meta: n_head_kv        = 16
0.00.058.415 I llm_load_print_meta: n_rot            = 32
0.00.058.415 I llm_load_print_meta: n_swa            = 0
0.00.058.416 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.416 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.419 I llm_load_print_meta: n_gqa            = 1
0.00.058.420 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.420 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.421 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.421 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.422 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.422 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.422 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.423 I llm_load_print_meta: n_ff             = 8192
0.00.058.423 I llm_load_print_meta: n_expert         = 0
0.00.058.423 I llm_load_print_meta: n_expert_used    = 0
0.00.058.425 I llm_load_print_meta: causal attn      = 1
0.00.058.425 I llm_load_print_meta: pooling type     = 0
0.00.058.437 I llm_load_print_meta: rope type        = 2
0.00.058.437 I llm_load_print_meta: rope scaling     = linear
0.00.058.438 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.438 I llm_load_print_meta: freq_scale_train = 1
0.00.058.439 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.439 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.439 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.439 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.439 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.439 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.440 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.441 I llm_load_print_meta: model type       = 1.4B
0.00.058.441 I llm_load_print_meta: model ftype      = Q4_1
0.00.058.441 I llm_load_print_meta: model params     = 1.41 B
0.00.058.442 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.058.442 I llm_load_print_meta: general.name     = 1.4B
0.00.058.442 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.442 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.443 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.443 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.443 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.058.443 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.443 I llm_load_print_meta: max token length = 1024
0.00.060.424 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.424 I llm_load_tensors: offloading output layer to GPU
0.00.060.425 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.435 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.060.436 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.061.355 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.356 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.357 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.357 I llama_new_context_with_model: n_batch       = 2048
0.00.061.357 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.357 I llama_new_context_with_model: flash_attn    = 0
0.00.061.358 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.358 I llama_new_context_with_model: freq_scale    = 1
0.00.061.358 I ggml_metal_init: allocating
0.00.061.366 I ggml_metal_init: found device: Apple M4
0.00.061.369 I ggml_metal_init: picking default device: Apple M4
0.00.061.983 I ggml_metal_init: using embedded metal library
0.00.064.355 I ggml_metal_init: GPU name:   Apple M4
0.00.064.356 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.064.357 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.064.357 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.064.357 I ggml_metal_init: simdgroup reduction   = true
0.00.064.358 I ggml_metal_init: simdgroup matrix mul. = true
0.00.064.358 I ggml_metal_init: has bfloat            = true
0.00.064.359 I ggml_metal_init: use bfloat            = true
0.00.064.360 I ggml_metal_init: hasUnifiedMemory      = true
0.00.064.360 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.074.218 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.094.152 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.094.161 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.094.188 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.095.192 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.095.194 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.095.194 I llama_new_context_with_model: graph nodes  = 967
0.00.095.194 I llama_new_context_with_model: graph splits = 2
0.00.095.197 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.095.326 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.095.326 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.732.993 I main: llama threadpool init, n_threads = 4
0.00.733.037 I 
0.00.733.061 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.733.061 I 
0.00.733.235 I sampler seed: 1234
0.00.733.239 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.733.275 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.733.276 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.733.276 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.465.839 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61418.69 tokens per second)
0.01.465.841 I llama_perf_context_print:        load time =     724.24 ms
0.01.465.841 I llama_perf_context_print: prompt eval time =      43.75 ms /     7 tokens (    6.25 ms per token,   160.00 tokens per second)
0.01.465.842 I llama_perf_context_print:        eval time =     685.85 ms /    63 runs   (   10.89 ms per token,    91.86 tokens per second)
0.01.465.843 I llama_perf_context_print:       total time =     732.85 ms /    70 tokens
0.01.466.042 I ggml_metal_free: deallocating

real	0m1.484s
user	0m0.110s
sys	0m0.140s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.016.102 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.023.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.603 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.604 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.606 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.606 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.608 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.608 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.609 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.610 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.610 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.611 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.805 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.932 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.053 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.054 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.055 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.055 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.055 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.056 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.033.056 I llama_model_loader: - type  f32:  194 tensors
0.00.033.056 I llama_model_loader: - type q5_0:   97 tensors
0.00.033.056 I llama_model_loader: - type q6_K:    1 tensors
0.00.058.897 I llm_load_vocab: special tokens cache size = 25
0.00.066.614 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.066.617 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.066.617 I llm_load_print_meta: arch             = gptneox
0.00.066.618 I llm_load_print_meta: vocab type       = BPE
0.00.066.618 I llm_load_print_meta: n_vocab          = 50304
0.00.066.618 I llm_load_print_meta: n_merges         = 50009
0.00.066.618 I llm_load_print_meta: vocab_only       = 0
0.00.066.618 I llm_load_print_meta: n_ctx_train      = 2048
0.00.066.618 I llm_load_print_meta: n_embd           = 2048
0.00.066.619 I llm_load_print_meta: n_layer          = 24
0.00.066.621 I llm_load_print_meta: n_head           = 16
0.00.066.622 I llm_load_print_meta: n_head_kv        = 16
0.00.066.622 I llm_load_print_meta: n_rot            = 32
0.00.066.622 I llm_load_print_meta: n_swa            = 0
0.00.066.623 I llm_load_print_meta: n_embd_head_k    = 128
0.00.066.623 I llm_load_print_meta: n_embd_head_v    = 128
0.00.066.623 I llm_load_print_meta: n_gqa            = 1
0.00.066.624 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.066.626 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.066.626 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.066.627 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.066.627 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.066.628 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.066.628 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.066.629 I llm_load_print_meta: n_ff             = 8192
0.00.066.629 I llm_load_print_meta: n_expert         = 0
0.00.066.629 I llm_load_print_meta: n_expert_used    = 0
0.00.066.630 I llm_load_print_meta: causal attn      = 1
0.00.066.631 I llm_load_print_meta: pooling type     = 0
0.00.066.643 I llm_load_print_meta: rope type        = 2
0.00.066.643 I llm_load_print_meta: rope scaling     = linear
0.00.066.643 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.066.644 I llm_load_print_meta: freq_scale_train = 1
0.00.066.644 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.066.644 I llm_load_print_meta: rope_finetuned   = unknown
0.00.066.644 I llm_load_print_meta: ssm_d_conv       = 0
0.00.066.645 I llm_load_print_meta: ssm_d_inner      = 0
0.00.066.645 I llm_load_print_meta: ssm_d_state      = 0
0.00.066.645 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.066.645 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.066.646 I llm_load_print_meta: model type       = 1.4B
0.00.066.646 I llm_load_print_meta: model ftype      = Q5_0
0.00.066.646 I llm_load_print_meta: model params     = 1.41 B
0.00.066.647 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.066.647 I llm_load_print_meta: general.name     = 1.4B
0.00.066.647 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.066.647 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.066.648 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.066.648 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.066.649 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.066.649 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.066.649 I llm_load_print_meta: max token length = 1024
0.00.068.921 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.068.921 I llm_load_tensors: offloading output layer to GPU
0.00.068.922 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.068.932 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.068.933 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.069.998 I llama_new_context_with_model: n_seq_max     = 1
0.00.069.999 I llama_new_context_with_model: n_ctx         = 2048
0.00.069.999 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.070.000 I llama_new_context_with_model: n_batch       = 2048
0.00.070.000 I llama_new_context_with_model: n_ubatch      = 512
0.00.070.000 I llama_new_context_with_model: flash_attn    = 0
0.00.070.001 I llama_new_context_with_model: freq_base     = 10000.0
0.00.070.001 I llama_new_context_with_model: freq_scale    = 1
0.00.070.001 I ggml_metal_init: allocating
0.00.070.008 I ggml_metal_init: found device: Apple M4
0.00.070.011 I ggml_metal_init: picking default device: Apple M4
0.00.070.699 I ggml_metal_init: using embedded metal library
0.00.073.714 I ggml_metal_init: GPU name:   Apple M4
0.00.073.716 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.073.717 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.073.717 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.073.717 I ggml_metal_init: simdgroup reduction   = true
0.00.073.718 I ggml_metal_init: simdgroup matrix mul. = true
0.00.073.718 I ggml_metal_init: has bfloat            = true
0.00.073.718 I ggml_metal_init: use bfloat            = true
0.00.073.718 I ggml_metal_init: hasUnifiedMemory      = true
0.00.073.719 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.085.728 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.497 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.107.503 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.107.522 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.489 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.108.491 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.108.491 I llama_new_context_with_model: graph nodes  = 967
0.00.108.491 I llama_new_context_with_model: graph splits = 2
0.00.108.494 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.637 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.638 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.796.114 I main: llama threadpool init, n_threads = 4
0.00.796.161 I 
0.00.796.185 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.796.185 I 
0.00.796.428 I sampler seed: 1234
0.00.796.432 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.796.473 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.796.473 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.796.473 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.581.117 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.581.118 I llama_perf_context_print:        load time =     780.00 ms
0.01.581.119 I llama_perf_context_print: prompt eval time =      43.20 ms /     7 tokens (    6.17 ms per token,   162.02 tokens per second)
0.01.581.120 I llama_perf_context_print:        eval time =     738.32 ms /    63 runs   (   11.72 ms per token,    85.33 tokens per second)
0.01.581.120 I llama_perf_context_print:       total time =     785.01 ms /    70 tokens
0.01.581.309 I ggml_metal_free: deallocating

real	0m1.605s
user	0m0.122s
sys	0m0.170s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.971 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.021.495 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.021.499 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.021.500 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.021.501 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.021.501 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.021.502 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.021.502 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.021.503 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.021.503 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.021.503 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.021.504 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.021.504 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.021.504 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.021.505 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.021.508 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.021.508 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.021.508 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.253 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.300 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.042 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.030.043 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.030.043 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.030.044 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.030.044 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.030.044 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.030.045 I llama_model_loader: - type  f32:  194 tensors
0.00.030.045 I llama_model_loader: - type q5_1:   97 tensors
0.00.030.045 I llama_model_loader: - type q6_K:    1 tensors
0.00.050.743 I llm_load_vocab: special tokens cache size = 25
0.00.056.667 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.056.670 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.056.670 I llm_load_print_meta: arch             = gptneox
0.00.056.671 I llm_load_print_meta: vocab type       = BPE
0.00.056.671 I llm_load_print_meta: n_vocab          = 50304
0.00.056.671 I llm_load_print_meta: n_merges         = 50009
0.00.056.671 I llm_load_print_meta: vocab_only       = 0
0.00.056.671 I llm_load_print_meta: n_ctx_train      = 2048
0.00.056.671 I llm_load_print_meta: n_embd           = 2048
0.00.056.672 I llm_load_print_meta: n_layer          = 24
0.00.056.675 I llm_load_print_meta: n_head           = 16
0.00.056.678 I llm_load_print_meta: n_head_kv        = 16
0.00.056.678 I llm_load_print_meta: n_rot            = 32
0.00.056.678 I llm_load_print_meta: n_swa            = 0
0.00.056.679 I llm_load_print_meta: n_embd_head_k    = 128
0.00.056.679 I llm_load_print_meta: n_embd_head_v    = 128
0.00.056.679 I llm_load_print_meta: n_gqa            = 1
0.00.056.680 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.056.681 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.056.681 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.056.682 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.056.682 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.056.682 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.056.682 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.056.683 I llm_load_print_meta: n_ff             = 8192
0.00.056.683 I llm_load_print_meta: n_expert         = 0
0.00.056.684 I llm_load_print_meta: n_expert_used    = 0
0.00.056.686 I llm_load_print_meta: causal attn      = 1
0.00.056.687 I llm_load_print_meta: pooling type     = 0
0.00.056.699 I llm_load_print_meta: rope type        = 2
0.00.056.699 I llm_load_print_meta: rope scaling     = linear
0.00.056.699 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.056.700 I llm_load_print_meta: freq_scale_train = 1
0.00.056.700 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.056.700 I llm_load_print_meta: rope_finetuned   = unknown
0.00.056.700 I llm_load_print_meta: ssm_d_conv       = 0
0.00.056.700 I llm_load_print_meta: ssm_d_inner      = 0
0.00.056.700 I llm_load_print_meta: ssm_d_state      = 0
0.00.056.701 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.056.701 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.056.702 I llm_load_print_meta: model type       = 1.4B
0.00.056.702 I llm_load_print_meta: model ftype      = Q5_1
0.00.056.702 I llm_load_print_meta: model params     = 1.41 B
0.00.056.705 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.056.705 I llm_load_print_meta: general.name     = 1.4B
0.00.056.705 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.056.706 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.056.706 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.056.706 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.056.706 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.056.706 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.056.707 I llm_load_print_meta: max token length = 1024
0.00.058.738 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.738 I llm_load_tensors: offloading output layer to GPU
0.00.058.738 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.749 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.058.750 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.059.700 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.701 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.701 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.701 I llama_new_context_with_model: n_batch       = 2048
0.00.059.701 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.701 I llama_new_context_with_model: flash_attn    = 0
0.00.059.702 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.702 I llama_new_context_with_model: freq_scale    = 1
0.00.059.703 I ggml_metal_init: allocating
0.00.059.709 I ggml_metal_init: found device: Apple M4
0.00.059.712 I ggml_metal_init: picking default device: Apple M4
0.00.060.322 I ggml_metal_init: using embedded metal library
0.00.062.679 I ggml_metal_init: GPU name:   Apple M4
0.00.062.681 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.682 I ggml_metal_init: simdgroup reduction   = true
0.00.062.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.683 I ggml_metal_init: has bfloat            = true
0.00.062.684 I ggml_metal_init: use bfloat            = true
0.00.062.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.610 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.091.859 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.091.874 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.091.898 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.092.970 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.092.972 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.092.972 I llama_new_context_with_model: graph nodes  = 967
0.00.092.972 I llama_new_context_with_model: graph splits = 2
0.00.092.974 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.093.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.093.123 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.920.920 I main: llama threadpool init, n_threads = 4
0.00.920.961 I 
0.00.920.982 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.920.983 I 
0.00.921.210 I sampler seed: 1234
0.00.921.217 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.921.228 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.921.229 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.921.229 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.765.453 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.765.454 I llama_perf_context_print:        load time =     911.95 ms
0.01.765.455 I llama_perf_context_print: prompt eval time =      42.26 ms /     7 tokens (    6.04 ms per token,   165.63 tokens per second)
0.01.765.456 I llama_perf_context_print:        eval time =     798.99 ms /    63 runs   (   12.68 ms per token,    78.85 tokens per second)
0.01.765.457 I llama_perf_context_print:       total time =     844.53 ms /    70 tokens
0.01.765.651 I ggml_metal_free: deallocating

real	0m1.782s
user	0m0.110s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.920 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.215 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.221 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.222 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.222 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.222 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.223 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.224 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.224 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.225 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.225 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.226 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.228 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.228 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.228 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.903 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.904 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.904 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.905 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.905 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.905 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.906 I llama_model_loader: - type  f32:  194 tensors
0.00.024.906 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.906 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.907 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.158 I llm_load_vocab: special tokens cache size = 25
0.00.052.055 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.058 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.059 I llm_load_print_meta: arch             = gptneox
0.00.052.059 I llm_load_print_meta: vocab type       = BPE
0.00.052.059 I llm_load_print_meta: n_vocab          = 50304
0.00.052.059 I llm_load_print_meta: n_merges         = 50009
0.00.052.059 I llm_load_print_meta: vocab_only       = 0
0.00.052.059 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.060 I llm_load_print_meta: n_embd           = 2048
0.00.052.060 I llm_load_print_meta: n_layer          = 24
0.00.052.062 I llm_load_print_meta: n_head           = 16
0.00.052.063 I llm_load_print_meta: n_head_kv        = 16
0.00.052.063 I llm_load_print_meta: n_rot            = 32
0.00.052.063 I llm_load_print_meta: n_swa            = 0
0.00.052.064 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.064 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.065 I llm_load_print_meta: n_gqa            = 1
0.00.052.066 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.068 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.069 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.071 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.072 I llm_load_print_meta: n_ff             = 8192
0.00.052.072 I llm_load_print_meta: n_expert         = 0
0.00.052.073 I llm_load_print_meta: n_expert_used    = 0
0.00.052.073 I llm_load_print_meta: causal attn      = 1
0.00.052.073 I llm_load_print_meta: pooling type     = 0
0.00.052.085 I llm_load_print_meta: rope type        = 2
0.00.052.085 I llm_load_print_meta: rope scaling     = linear
0.00.052.087 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.087 I llm_load_print_meta: freq_scale_train = 1
0.00.052.087 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.088 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.088 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.088 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.088 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.088 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.088 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.089 I llm_load_print_meta: model type       = 1.4B
0.00.052.089 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.052.089 I llm_load_print_meta: model params     = 1.41 B
0.00.052.090 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.052.090 I llm_load_print_meta: general.name     = 1.4B
0.00.052.090 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.090 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.095 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.096 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.097 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.098 I llm_load_print_meta: max token length = 1024
0.00.053.943 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.943 I llm_load_tensors: offloading output layer to GPU
0.00.053.943 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.953 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.955 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.851 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.852 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.852 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.852 I llama_new_context_with_model: n_batch       = 2048
0.00.054.852 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.853 I llama_new_context_with_model: flash_attn    = 0
0.00.054.853 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.853 I llama_new_context_with_model: freq_scale    = 1
0.00.054.854 I ggml_metal_init: allocating
0.00.054.859 I ggml_metal_init: found device: Apple M4
0.00.054.862 I ggml_metal_init: picking default device: Apple M4
0.00.055.456 I ggml_metal_init: using embedded metal library
0.00.057.802 I ggml_metal_init: GPU name:   Apple M4
0.00.057.804 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.804 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.804 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.805 I ggml_metal_init: simdgroup reduction   = true
0.00.057.805 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.805 I ggml_metal_init: has bfloat            = true
0.00.057.805 I ggml_metal_init: use bfloat            = true
0.00.057.805 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.760 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.087.087 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.096 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.120 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.195 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.197 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.197 I llama_new_context_with_model: graph nodes  = 967
0.00.088.197 I llama_new_context_with_model: graph splits = 2
0.00.088.200 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.334 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.335 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.528.844 I main: llama threadpool init, n_threads = 4
0.00.528.888 I 
0.00.528.915 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.528.916 I 
0.00.529.161 I sampler seed: 1234
0.00.529.167 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.529.190 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.529.191 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.529.191 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.202.858 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56800.00 tokens per second)
0.01.202.859 I llama_perf_context_print:        load time =     518.92 ms
0.01.202.860 I llama_perf_context_print: prompt eval time =      35.87 ms /     7 tokens (    5.12 ms per token,   195.17 tokens per second)
0.01.202.860 I llama_perf_context_print:        eval time =     635.33 ms /    63 runs   (   10.08 ms per token,    99.16 tokens per second)
0.01.202.860 I llama_perf_context_print:       total time =     674.02 ms /    70 tokens
0.01.203.156 I ggml_metal_free: deallocating

real	0m1.220s
user	0m0.111s
sys	0m0.112s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.123 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.759 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.765 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.766 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.767 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.767 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.767 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.768 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.769 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.769 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.771 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.773 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.773 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.774 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.776 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.776 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.776 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.588 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.642 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.462 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.464 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.464 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.464 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.465 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.465 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.466 I llama_model_loader: - type  f32:  194 tensors
0.00.024.466 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.466 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.466 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.467 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.437 I llm_load_vocab: special tokens cache size = 25
0.00.052.400 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.404 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.405 I llm_load_print_meta: arch             = gptneox
0.00.052.405 I llm_load_print_meta: vocab type       = BPE
0.00.052.405 I llm_load_print_meta: n_vocab          = 50304
0.00.052.405 I llm_load_print_meta: n_merges         = 50009
0.00.052.405 I llm_load_print_meta: vocab_only       = 0
0.00.052.406 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.406 I llm_load_print_meta: n_embd           = 2048
0.00.052.408 I llm_load_print_meta: n_layer          = 24
0.00.052.413 I llm_load_print_meta: n_head           = 16
0.00.052.413 I llm_load_print_meta: n_head_kv        = 16
0.00.052.416 I llm_load_print_meta: n_rot            = 32
0.00.052.416 I llm_load_print_meta: n_swa            = 0
0.00.052.416 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.416 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.417 I llm_load_print_meta: n_gqa            = 1
0.00.052.418 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.418 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.420 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.420 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.420 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.420 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.421 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.421 I llm_load_print_meta: n_ff             = 8192
0.00.052.421 I llm_load_print_meta: n_expert         = 0
0.00.052.422 I llm_load_print_meta: n_expert_used    = 0
0.00.052.422 I llm_load_print_meta: causal attn      = 1
0.00.052.424 I llm_load_print_meta: pooling type     = 0
0.00.052.438 I llm_load_print_meta: rope type        = 2
0.00.052.438 I llm_load_print_meta: rope scaling     = linear
0.00.052.438 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.440 I llm_load_print_meta: freq_scale_train = 1
0.00.052.440 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.440 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.441 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.441 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.441 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.441 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.441 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.441 I llm_load_print_meta: model type       = 1.4B
0.00.052.442 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.052.442 I llm_load_print_meta: model params     = 1.41 B
0.00.052.443 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.052.443 I llm_load_print_meta: general.name     = 1.4B
0.00.052.443 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.443 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.443 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.443 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.445 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.445 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.445 I llm_load_print_meta: max token length = 1024
0.00.054.486 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.486 I llm_load_tensors: offloading output layer to GPU
0.00.054.486 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.497 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.054.498 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.055.420 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.421 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.421 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.421 I llama_new_context_with_model: n_batch       = 2048
0.00.055.421 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.422 I llama_new_context_with_model: flash_attn    = 0
0.00.055.422 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.422 I llama_new_context_with_model: freq_scale    = 1
0.00.055.423 I ggml_metal_init: allocating
0.00.055.427 I ggml_metal_init: found device: Apple M4
0.00.055.429 I ggml_metal_init: picking default device: Apple M4
0.00.056.060 I ggml_metal_init: using embedded metal library
0.00.059.206 I ggml_metal_init: GPU name:   Apple M4
0.00.059.208 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.208 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.209 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.209 I ggml_metal_init: simdgroup reduction   = true
0.00.059.209 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.209 I ggml_metal_init: has bfloat            = true
0.00.059.210 I ggml_metal_init: use bfloat            = true
0.00.059.210 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.211 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.133 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.580 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.592 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.614 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.650 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.652 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.652 I llama_new_context_with_model: graph nodes  = 967
0.00.089.652 I llama_new_context_with_model: graph splits = 2
0.00.089.657 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.798 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.798 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.914.088 I main: llama threadpool init, n_threads = 4
0.00.914.143 I 
0.00.914.170 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.914.171 I 
0.00.914.430 I sampler seed: 1234
0.00.914.437 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.914.481 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.914.485 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.914.486 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.674.853 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48830.81 tokens per second)
0.01.674.853 I llama_perf_context_print:        load time =     904.95 ms
0.01.674.854 I llama_perf_context_print: prompt eval time =      50.23 ms /     7 tokens (    7.18 ms per token,   139.36 tokens per second)
0.01.674.855 I llama_perf_context_print:        eval time =     707.07 ms /    63 runs   (   11.22 ms per token,    89.10 tokens per second)
0.01.674.855 I llama_perf_context_print:       total time =     760.78 ms /    70 tokens
0.01.675.070 I ggml_metal_free: deallocating

real	0m1.694s
user	0m0.122s
sys	0m0.160s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.868 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.821 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.822 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.823 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.823 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.823 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.824 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.824 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.825 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.825 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.825 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.826 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.826 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.827 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.832 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.833 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.834 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.607 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.704 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.480 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.482 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.482 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.482 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.483 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.483 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.483 I llama_model_loader: - type  f32:  194 tensors
0.00.025.484 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.484 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.484 I llama_model_loader: - type q6_K:   13 tensors
0.00.045.918 I llm_load_vocab: special tokens cache size = 25
0.00.051.768 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.771 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.771 I llm_load_print_meta: arch             = gptneox
0.00.051.772 I llm_load_print_meta: vocab type       = BPE
0.00.051.772 I llm_load_print_meta: n_vocab          = 50304
0.00.051.772 I llm_load_print_meta: n_merges         = 50009
0.00.051.772 I llm_load_print_meta: vocab_only       = 0
0.00.051.773 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.773 I llm_load_print_meta: n_embd           = 2048
0.00.051.773 I llm_load_print_meta: n_layer          = 24
0.00.051.776 I llm_load_print_meta: n_head           = 16
0.00.051.776 I llm_load_print_meta: n_head_kv        = 16
0.00.051.777 I llm_load_print_meta: n_rot            = 32
0.00.051.777 I llm_load_print_meta: n_swa            = 0
0.00.051.777 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.778 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.779 I llm_load_print_meta: n_gqa            = 1
0.00.051.780 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.780 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.781 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.783 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.783 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.783 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.783 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.784 I llm_load_print_meta: n_ff             = 8192
0.00.051.784 I llm_load_print_meta: n_expert         = 0
0.00.051.786 I llm_load_print_meta: n_expert_used    = 0
0.00.051.787 I llm_load_print_meta: causal attn      = 1
0.00.051.787 I llm_load_print_meta: pooling type     = 0
0.00.051.799 I llm_load_print_meta: rope type        = 2
0.00.051.799 I llm_load_print_meta: rope scaling     = linear
0.00.051.799 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.799 I llm_load_print_meta: freq_scale_train = 1
0.00.051.800 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.800 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.800 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.801 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.801 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.801 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.801 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.801 I llm_load_print_meta: model type       = 1.4B
0.00.051.802 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.051.802 I llm_load_print_meta: model params     = 1.41 B
0.00.051.802 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.051.802 I llm_load_print_meta: general.name     = 1.4B
0.00.051.803 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.803 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.803 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.804 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.804 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.805 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.805 I llm_load_print_meta: max token length = 1024
0.00.053.869 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.870 I llm_load_tensors: offloading output layer to GPU
0.00.053.870 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.881 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.053.882 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.054.878 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.879 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.879 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.880 I llama_new_context_with_model: n_batch       = 2048
0.00.054.880 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.880 I llama_new_context_with_model: flash_attn    = 0
0.00.054.880 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.881 I llama_new_context_with_model: freq_scale    = 1
0.00.054.881 I ggml_metal_init: allocating
0.00.054.885 I ggml_metal_init: found device: Apple M4
0.00.054.887 I ggml_metal_init: picking default device: Apple M4
0.00.055.495 I ggml_metal_init: using embedded metal library
0.00.057.832 I ggml_metal_init: GPU name:   Apple M4
0.00.057.834 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.834 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.835 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.836 I ggml_metal_init: simdgroup reduction   = true
0.00.057.836 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.837 I ggml_metal_init: has bfloat            = true
0.00.057.837 I ggml_metal_init: use bfloat            = true
0.00.057.837 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.838 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.692 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.051 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.062 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.089 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.098 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.099 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.100 I llama_new_context_with_model: graph nodes  = 967
0.00.089.100 I llama_new_context_with_model: graph splits = 2
0.00.089.102 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.231 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.231 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.367 I main: llama threadpool init, n_threads = 4
0.00.668.410 I 
0.00.668.434 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.434 I 
0.00.668.660 I sampler seed: 1234
0.00.668.664 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.668.705 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.668.705 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.668.705 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.432.121 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 56126.48 tokens per second)
0.01.432.122 I llama_perf_context_print:        load time =     659.50 ms
0.01.432.122 I llama_perf_context_print: prompt eval time =      47.00 ms /     7 tokens (    6.71 ms per token,   148.95 tokens per second)
0.01.432.123 I llama_perf_context_print:        eval time =     713.27 ms /    63 runs   (   11.32 ms per token,    88.33 tokens per second)
0.01.432.123 I llama_perf_context_print:       total time =     763.76 ms /    70 tokens
0.01.432.310 I ggml_metal_free: deallocating

real	0m1.448s
user	0m0.110s
sys	0m0.141s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.018.398 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.069 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.027.073 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.074 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.075 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.075 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.075 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.076 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.077 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.077 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.077 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.077 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.078 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.078 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.078 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.080 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.080 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.080 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.030.995 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.031.996 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.806 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.808 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.808 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.808 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.808 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.809 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.035.809 I llama_model_loader: - type  f32:  194 tensors
0.00.035.810 I llama_model_loader: - type q5_K:   61 tensors
0.00.035.810 I llama_model_loader: - type q6_K:   37 tensors
0.00.060.557 I llm_load_vocab: special tokens cache size = 25
0.00.068.154 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.068.157 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.068.157 I llm_load_print_meta: arch             = gptneox
0.00.068.157 I llm_load_print_meta: vocab type       = BPE
0.00.068.157 I llm_load_print_meta: n_vocab          = 50304
0.00.068.158 I llm_load_print_meta: n_merges         = 50009
0.00.068.158 I llm_load_print_meta: vocab_only       = 0
0.00.068.158 I llm_load_print_meta: n_ctx_train      = 2048
0.00.068.158 I llm_load_print_meta: n_embd           = 2048
0.00.068.158 I llm_load_print_meta: n_layer          = 24
0.00.068.161 I llm_load_print_meta: n_head           = 16
0.00.068.162 I llm_load_print_meta: n_head_kv        = 16
0.00.068.162 I llm_load_print_meta: n_rot            = 32
0.00.068.162 I llm_load_print_meta: n_swa            = 0
0.00.068.162 I llm_load_print_meta: n_embd_head_k    = 128
0.00.068.163 I llm_load_print_meta: n_embd_head_v    = 128
0.00.068.163 I llm_load_print_meta: n_gqa            = 1
0.00.068.164 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.068.165 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.068.165 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.068.165 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.068.165 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.068.165 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.068.166 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.068.166 I llm_load_print_meta: n_ff             = 8192
0.00.068.166 I llm_load_print_meta: n_expert         = 0
0.00.068.166 I llm_load_print_meta: n_expert_used    = 0
0.00.068.167 I llm_load_print_meta: causal attn      = 1
0.00.068.167 I llm_load_print_meta: pooling type     = 0
0.00.068.180 I llm_load_print_meta: rope type        = 2
0.00.068.182 I llm_load_print_meta: rope scaling     = linear
0.00.068.182 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.068.182 I llm_load_print_meta: freq_scale_train = 1
0.00.068.182 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.068.183 I llm_load_print_meta: rope_finetuned   = unknown
0.00.068.183 I llm_load_print_meta: ssm_d_conv       = 0
0.00.068.183 I llm_load_print_meta: ssm_d_inner      = 0
0.00.068.183 I llm_load_print_meta: ssm_d_state      = 0
0.00.068.183 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.068.183 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.068.184 I llm_load_print_meta: model type       = 1.4B
0.00.068.185 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.068.186 I llm_load_print_meta: model params     = 1.41 B
0.00.068.186 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.068.186 I llm_load_print_meta: general.name     = 1.4B
0.00.068.187 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.068.187 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.068.187 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.068.187 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.068.187 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.068.189 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.068.189 I llm_load_print_meta: max token length = 1024
0.00.070.362 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.070.362 I llm_load_tensors: offloading output layer to GPU
0.00.070.362 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.070.373 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.070.374 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.071.368 I llama_new_context_with_model: n_seq_max     = 1
0.00.071.369 I llama_new_context_with_model: n_ctx         = 2048
0.00.071.370 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.071.370 I llama_new_context_with_model: n_batch       = 2048
0.00.071.370 I llama_new_context_with_model: n_ubatch      = 512
0.00.071.370 I llama_new_context_with_model: flash_attn    = 0
0.00.071.371 I llama_new_context_with_model: freq_base     = 10000.0
0.00.071.371 I llama_new_context_with_model: freq_scale    = 1
0.00.071.372 I ggml_metal_init: allocating
0.00.071.377 I ggml_metal_init: found device: Apple M4
0.00.071.379 I ggml_metal_init: picking default device: Apple M4
0.00.071.995 I ggml_metal_init: using embedded metal library
0.00.074.750 I ggml_metal_init: GPU name:   Apple M4
0.00.074.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.074.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.074.752 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.074.752 I ggml_metal_init: simdgroup reduction   = true
0.00.074.752 I ggml_metal_init: simdgroup matrix mul. = true
0.00.074.753 I ggml_metal_init: has bfloat            = true
0.00.074.753 I ggml_metal_init: use bfloat            = true
0.00.074.753 I ggml_metal_init: hasUnifiedMemory      = true
0.00.074.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.084.466 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.111 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.123 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.153 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.215 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.216 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.217 I llama_new_context_with_model: graph nodes  = 967
0.00.107.217 I llama_new_context_with_model: graph splits = 2
0.00.107.219 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.107.367 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.107.367 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.891.474 I main: llama threadpool init, n_threads = 4
0.00.891.519 I 
0.00.891.540 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.891.541 I 
0.00.891.791 I sampler seed: 1234
0.00.891.794 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.891.806 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.891.806 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.891.806 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.739.731 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58149.06 tokens per second)
0.01.739.732 I llama_perf_context_print:        load time =     873.07 ms
0.01.739.733 I llama_perf_context_print: prompt eval time =      51.66 ms /     7 tokens (    7.38 ms per token,   135.50 tokens per second)
0.01.739.733 I llama_perf_context_print:        eval time =     793.20 ms /    63 runs   (   12.59 ms per token,    79.42 tokens per second)
0.01.739.734 I llama_perf_context_print:       total time =     848.26 ms /    70 tokens
0.01.739.979 I ggml_metal_free: deallocating

real	0m1.757s
user	0m0.116s
sys	0m0.161s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.685 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.802 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.806 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.807 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.808 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.812 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.817 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.818 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.818 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.818 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.821 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.822 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.822 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.167 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.169 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.169 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.169 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.169 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.170 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.170 I llama_model_loader: - type  f32:  194 tensors
0.00.024.171 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.629 I llm_load_vocab: special tokens cache size = 25
0.00.050.649 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.652 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.653 I llm_load_print_meta: arch             = gptneox
0.00.050.653 I llm_load_print_meta: vocab type       = BPE
0.00.050.653 I llm_load_print_meta: n_vocab          = 50304
0.00.050.653 I llm_load_print_meta: n_merges         = 50009
0.00.050.654 I llm_load_print_meta: vocab_only       = 0
0.00.050.654 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.654 I llm_load_print_meta: n_embd           = 2048
0.00.050.654 I llm_load_print_meta: n_layer          = 24
0.00.050.656 I llm_load_print_meta: n_head           = 16
0.00.050.657 I llm_load_print_meta: n_head_kv        = 16
0.00.050.657 I llm_load_print_meta: n_rot            = 32
0.00.050.657 I llm_load_print_meta: n_swa            = 0
0.00.050.658 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.658 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.658 I llm_load_print_meta: n_gqa            = 1
0.00.050.659 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.662 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.662 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.663 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.663 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.663 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.663 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.664 I llm_load_print_meta: n_ff             = 8192
0.00.050.664 I llm_load_print_meta: n_expert         = 0
0.00.050.665 I llm_load_print_meta: n_expert_used    = 0
0.00.050.665 I llm_load_print_meta: causal attn      = 1
0.00.050.666 I llm_load_print_meta: pooling type     = 0
0.00.050.678 I llm_load_print_meta: rope type        = 2
0.00.050.678 I llm_load_print_meta: rope scaling     = linear
0.00.050.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.679 I llm_load_print_meta: freq_scale_train = 1
0.00.050.679 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.679 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.679 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.679 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.680 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.680 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.680 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.680 I llm_load_print_meta: model type       = 1.4B
0.00.050.680 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.681 I llm_load_print_meta: model params     = 1.41 B
0.00.050.681 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.681 I llm_load_print_meta: general.name     = 1.4B
0.00.050.681 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.682 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.682 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.682 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.682 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.682 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.683 I llm_load_print_meta: max token length = 1024
0.00.052.277 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.277 I llm_load_tensors: offloading output layer to GPU
0.00.052.278 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.288 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.289 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.117 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.118 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.118 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.118 I llama_new_context_with_model: n_batch       = 2048
0.00.053.118 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.119 I llama_new_context_with_model: flash_attn    = 0
0.00.053.119 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.119 I llama_new_context_with_model: freq_scale    = 1
0.00.053.120 I ggml_metal_init: allocating
0.00.053.123 I ggml_metal_init: found device: Apple M4
0.00.053.125 I ggml_metal_init: picking default device: Apple M4
0.00.053.705 I ggml_metal_init: using embedded metal library
0.00.056.020 I ggml_metal_init: GPU name:   Apple M4
0.00.056.021 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.022 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.022 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.022 I ggml_metal_init: simdgroup reduction   = true
0.00.056.023 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.023 I ggml_metal_init: has bfloat            = true
0.00.056.023 I ggml_metal_init: use bfloat            = true
0.00.056.024 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.024 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.916 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.977 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.982 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.000 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.014 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.015 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.015 I llama_new_context_with_model: graph nodes  = 967
0.00.087.016 I llama_new_context_with_model: graph splits = 2
0.00.087.018 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.161 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.161 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.750.954 I main: llama threadpool init, n_threads = 4
0.00.750.989 I 
0.00.751.011 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.011 I 
0.00.751.251 I sampler seed: 1234
0.00.751.256 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.290 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.310 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.311 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.633.063 I llama_perf_sampler_print:    sampling time =       1.24 ms /    71 runs   (    0.02 ms per token, 57396.93 tokens per second)
0.01.633.064 I llama_perf_context_print:        load time =     742.26 ms
0.01.633.065 I llama_perf_context_print: prompt eval time =      54.47 ms /     7 tokens (    7.78 ms per token,   128.50 tokens per second)
0.01.633.066 I llama_perf_context_print:        eval time =     824.25 ms /    63 runs   (   13.08 ms per token,    76.43 tokens per second)
0.01.633.066 I llama_perf_context_print:       total time =     882.11 ms /    70 tokens
0.01.633.325 I ggml_metal_free: deallocating

real	0m1.650s
user	0m0.110s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.561 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.963 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.107 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.121 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.125 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.126 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.127 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.128 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.128 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.130 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.132 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.133 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.133 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.135 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.141 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.141 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.142 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.046.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.053.472 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.053.475 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.053.476 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.053.476 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.053.476 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.053.477 I llama_model_loader: - type  f32:  194 tensors
0.00.053.478 I llama_model_loader: - type  f16:   98 tensors
0.00.083.780 I llm_load_vocab: special tokens cache size = 25
0.00.090.540 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.090.543 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.090.543 I llm_load_print_meta: arch             = gptneox
0.00.090.544 I llm_load_print_meta: vocab type       = BPE
0.00.090.544 I llm_load_print_meta: n_vocab          = 50304
0.00.090.544 I llm_load_print_meta: n_merges         = 50009
0.00.090.544 I llm_load_print_meta: vocab_only       = 0
0.00.090.544 I llm_load_print_meta: n_ctx_train      = 2048
0.00.090.544 I llm_load_print_meta: n_embd           = 2048
0.00.090.544 I llm_load_print_meta: n_layer          = 24
0.00.090.547 I llm_load_print_meta: n_head           = 16
0.00.090.548 I llm_load_print_meta: n_head_kv        = 16
0.00.090.548 I llm_load_print_meta: n_rot            = 32
0.00.090.548 I llm_load_print_meta: n_swa            = 0
0.00.090.548 I llm_load_print_meta: n_embd_head_k    = 128
0.00.090.549 I llm_load_print_meta: n_embd_head_v    = 128
0.00.090.549 I llm_load_print_meta: n_gqa            = 1
0.00.090.550 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.090.553 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.090.554 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.090.554 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.090.554 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.090.554 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.090.554 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.090.562 I llm_load_print_meta: n_ff             = 8192
0.00.090.563 I llm_load_print_meta: n_expert         = 0
0.00.090.564 I llm_load_print_meta: n_expert_used    = 0
0.00.090.564 I llm_load_print_meta: causal attn      = 1
0.00.090.564 I llm_load_print_meta: pooling type     = 0
0.00.090.577 I llm_load_print_meta: rope type        = 2
0.00.090.577 I llm_load_print_meta: rope scaling     = linear
0.00.090.578 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.090.578 I llm_load_print_meta: freq_scale_train = 1
0.00.090.578 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.090.578 I llm_load_print_meta: rope_finetuned   = unknown
0.00.090.579 I llm_load_print_meta: ssm_d_conv       = 0
0.00.090.579 I llm_load_print_meta: ssm_d_inner      = 0
0.00.090.579 I llm_load_print_meta: ssm_d_state      = 0
0.00.090.579 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.090.579 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.090.580 I llm_load_print_meta: model type       = 1.4B
0.00.090.580 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.090.582 I llm_load_print_meta: model params     = 1.41 B
0.00.090.583 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.090.584 I llm_load_print_meta: general.name     = 1.4B
0.00.090.584 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.090.584 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.090.585 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.090.585 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.090.585 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.090.585 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.090.586 I llm_load_print_meta: max token length = 1024
0.00.093.203 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.203 I llm_load_tensors: offloading output layer to GPU
0.00.093.203 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.213 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.215 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.094.150 I llama_new_context_with_model: n_seq_max     = 1
0.00.094.151 I llama_new_context_with_model: n_ctx         = 128
0.00.094.151 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.094.151 I llama_new_context_with_model: n_batch       = 128
0.00.094.152 I llama_new_context_with_model: n_ubatch      = 128
0.00.094.152 I llama_new_context_with_model: flash_attn    = 0
0.00.094.152 I llama_new_context_with_model: freq_base     = 10000.0
0.00.094.153 I llama_new_context_with_model: freq_scale    = 1
0.00.094.153 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.094.153 I ggml_metal_init: allocating
0.00.094.160 I ggml_metal_init: found device: Apple M4
0.00.094.162 I ggml_metal_init: picking default device: Apple M4
0.00.094.781 I ggml_metal_init: using embedded metal library
0.00.097.461 I ggml_metal_init: GPU name:   Apple M4
0.00.097.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.464 I ggml_metal_init: simdgroup reduction   = true
0.00.097.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.465 I ggml_metal_init: has bfloat            = true
0.00.097.465 I ggml_metal_init: use bfloat            = true
0.00.097.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.466 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.107.224 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.108.511 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.108.514 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.108.530 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.109.478 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.109.479 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.109.480 I llama_new_context_with_model: graph nodes  = 967
0.00.109.480 I llama_new_context_with_model: graph splits = 2
0.00.109.481 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.109.481 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.146.788 I 
0.01.146.848 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.146.899 I perplexity: tokenizing the input ..
0.01.160.696 I perplexity: tokenization took 13.796 ms
0.01.160.702 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.283.592 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.285.719 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.285.739 I llama_perf_context_print:        load time =    1122.81 ms
0.01.285.740 I llama_perf_context_print: prompt eval time =     121.95 ms /   128 tokens (    0.95 ms per token,  1049.61 tokens per second)
0.01.285.742 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.285.742 I llama_perf_context_print:       total time =     138.95 ms /   129 tokens
0.01.286.570 I ggml_metal_free: deallocating

real	0m1.479s
user	0m0.127s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.109 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.546 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.314 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.319 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.327 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.328 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.328 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.328 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.329 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.330 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.330 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.330 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.331 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.331 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.332 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.332 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.334 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.334 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.335 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.650 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.025.088 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.995 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.029.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.029.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.029.997 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.029.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.029.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.029.999 I llama_model_loader: - type  f32:  194 tensors
0.00.029.999 I llama_model_loader: - type q8_0:   98 tensors
0.00.054.173 I llm_load_vocab: special tokens cache size = 25
0.00.060.257 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.261 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.262 I llm_load_print_meta: arch             = gptneox
0.00.060.262 I llm_load_print_meta: vocab type       = BPE
0.00.060.262 I llm_load_print_meta: n_vocab          = 50304
0.00.060.262 I llm_load_print_meta: n_merges         = 50009
0.00.060.263 I llm_load_print_meta: vocab_only       = 0
0.00.060.263 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.263 I llm_load_print_meta: n_embd           = 2048
0.00.060.263 I llm_load_print_meta: n_layer          = 24
0.00.060.267 I llm_load_print_meta: n_head           = 16
0.00.060.267 I llm_load_print_meta: n_head_kv        = 16
0.00.060.268 I llm_load_print_meta: n_rot            = 32
0.00.060.268 I llm_load_print_meta: n_swa            = 0
0.00.060.268 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.268 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.269 I llm_load_print_meta: n_gqa            = 1
0.00.060.269 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.270 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.270 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.271 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.271 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.272 I llm_load_print_meta: n_ff             = 8192
0.00.060.272 I llm_load_print_meta: n_expert         = 0
0.00.060.272 I llm_load_print_meta: n_expert_used    = 0
0.00.060.272 I llm_load_print_meta: causal attn      = 1
0.00.060.273 I llm_load_print_meta: pooling type     = 0
0.00.060.286 I llm_load_print_meta: rope type        = 2
0.00.060.294 I llm_load_print_meta: rope scaling     = linear
0.00.060.296 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.297 I llm_load_print_meta: freq_scale_train = 1
0.00.060.297 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.297 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.298 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.298 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.298 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.298 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.298 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.299 I llm_load_print_meta: model type       = 1.4B
0.00.060.299 I llm_load_print_meta: model ftype      = Q8_0
0.00.060.302 I llm_load_print_meta: model params     = 1.41 B
0.00.060.303 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.060.303 I llm_load_print_meta: general.name     = 1.4B
0.00.060.303 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.304 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.305 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.305 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.305 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.305 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.305 I llm_load_print_meta: max token length = 1024
0.00.062.396 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.397 I llm_load_tensors: offloading output layer to GPU
0.00.062.397 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.408 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.062.409 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.063.350 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.351 I llama_new_context_with_model: n_ctx         = 128
0.00.063.351 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.063.351 I llama_new_context_with_model: n_batch       = 128
0.00.063.352 I llama_new_context_with_model: n_ubatch      = 128
0.00.063.352 I llama_new_context_with_model: flash_attn    = 0
0.00.063.352 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.352 I llama_new_context_with_model: freq_scale    = 1
0.00.063.354 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.063.355 I ggml_metal_init: allocating
0.00.063.359 I ggml_metal_init: found device: Apple M4
0.00.063.360 I ggml_metal_init: picking default device: Apple M4
0.00.063.950 I ggml_metal_init: using embedded metal library
0.00.066.613 I ggml_metal_init: GPU name:   Apple M4
0.00.066.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.615 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.615 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.615 I ggml_metal_init: simdgroup reduction   = true
0.00.066.615 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.616 I ggml_metal_init: has bfloat            = true
0.00.066.616 I ggml_metal_init: use bfloat            = true
0.00.066.616 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.617 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.759 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.077.079 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.077.083 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.077.099 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.078.049 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.078.050 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.078.051 I llama_new_context_with_model: graph nodes  = 967
0.00.078.051 I llama_new_context_with_model: graph splits = 2
0.00.078.052 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.078.052 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.852.995 I 
0.00.853.020 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.853.033 I perplexity: tokenizing the input ..
0.00.861.266 I perplexity: tokenization took 8.231 ms
0.00.861.269 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.985.782 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.987.066 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.987.080 I llama_perf_context_print:        load time =     842.45 ms
0.00.987.081 I llama_perf_context_print: prompt eval time =     124.29 ms /   128 tokens (    0.97 ms per token,  1029.87 tokens per second)
0.00.987.081 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.987.082 I llama_perf_context_print:       total time =     134.08 ms /   129 tokens
0.00.987.537 I ggml_metal_free: deallocating

real	0m1.006s
user	0m0.089s
sys	0m0.144s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.013 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.967 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.971 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.973 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.973 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.974 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.974 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.975 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.975 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.976 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.976 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.976 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.977 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.979 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.981 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.982 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.983 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.825 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.895 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.727 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.728 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.728 I llama_model_loader: - type  f32:  194 tensors
0.00.024.728 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.729 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.203 I llm_load_vocab: special tokens cache size = 25
0.00.052.059 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.062 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.062 I llm_load_print_meta: arch             = gptneox
0.00.052.062 I llm_load_print_meta: vocab type       = BPE
0.00.052.062 I llm_load_print_meta: n_vocab          = 50304
0.00.052.063 I llm_load_print_meta: n_merges         = 50009
0.00.052.063 I llm_load_print_meta: vocab_only       = 0
0.00.052.063 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.063 I llm_load_print_meta: n_embd           = 2048
0.00.052.063 I llm_load_print_meta: n_layer          = 24
0.00.052.066 I llm_load_print_meta: n_head           = 16
0.00.052.067 I llm_load_print_meta: n_head_kv        = 16
0.00.052.067 I llm_load_print_meta: n_rot            = 32
0.00.052.067 I llm_load_print_meta: n_swa            = 0
0.00.052.068 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.068 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.069 I llm_load_print_meta: n_gqa            = 1
0.00.052.070 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.070 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.071 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.071 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.071 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.072 I llm_load_print_meta: n_ff             = 8192
0.00.052.072 I llm_load_print_meta: n_expert         = 0
0.00.052.072 I llm_load_print_meta: n_expert_used    = 0
0.00.052.072 I llm_load_print_meta: causal attn      = 1
0.00.052.073 I llm_load_print_meta: pooling type     = 0
0.00.052.088 I llm_load_print_meta: rope type        = 2
0.00.052.088 I llm_load_print_meta: rope scaling     = linear
0.00.052.088 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.088 I llm_load_print_meta: freq_scale_train = 1
0.00.052.089 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.089 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.089 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.089 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.089 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.089 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.090 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.092 I llm_load_print_meta: model type       = 1.4B
0.00.052.092 I llm_load_print_meta: model ftype      = Q4_0
0.00.052.092 I llm_load_print_meta: model params     = 1.41 B
0.00.052.093 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.052.093 I llm_load_print_meta: general.name     = 1.4B
0.00.052.093 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.093 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.094 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.094 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.094 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.052.094 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.094 I llm_load_print_meta: max token length = 1024
0.00.054.144 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.144 I llm_load_tensors: offloading output layer to GPU
0.00.054.145 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.156 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.054.157 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.055.081 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.082 I llama_new_context_with_model: n_ctx         = 128
0.00.055.082 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.083 I llama_new_context_with_model: n_batch       = 128
0.00.055.083 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.083 I llama_new_context_with_model: flash_attn    = 0
0.00.055.083 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.084 I llama_new_context_with_model: freq_scale    = 1
0.00.055.084 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.084 I ggml_metal_init: allocating
0.00.055.088 I ggml_metal_init: found device: Apple M4
0.00.055.090 I ggml_metal_init: picking default device: Apple M4
0.00.055.659 I ggml_metal_init: using embedded metal library
0.00.058.076 I ggml_metal_init: GPU name:   Apple M4
0.00.058.078 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.078 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.079 I ggml_metal_init: simdgroup reduction   = true
0.00.058.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.079 I ggml_metal_init: has bfloat            = true
0.00.058.079 I ggml_metal_init: use bfloat            = true
0.00.058.080 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.080 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.388 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.069.684 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.686 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.701 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.639 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.640 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.640 I llama_new_context_with_model: graph nodes  = 967
0.00.070.640 I llama_new_context_with_model: graph splits = 2
0.00.070.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.581.118 I 
0.00.581.148 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.581.164 I perplexity: tokenizing the input ..
0.00.589.337 I perplexity: tokenization took 8.171 ms
0.00.589.341 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.711.848 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.713.116 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.713.141 I llama_perf_context_print:        load time =     571.10 ms
0.00.713.142 I llama_perf_context_print: prompt eval time =     122.28 ms /   128 tokens (    0.96 ms per token,  1046.78 tokens per second)
0.00.713.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.713.143 I llama_perf_context_print:       total time =     132.02 ms /   129 tokens
0.00.713.667 I ggml_metal_free: deallocating

real	0m0.729s
user	0m0.080s
sys	0m0.090s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.808 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.667 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.674 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.675 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.681 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.681 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.681 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.437 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.465 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.229 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.231 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.231 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.232 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.232 I llama_model_loader: - type  f32:  194 tensors
0.00.023.233 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.233 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.696 I llm_load_vocab: special tokens cache size = 25
0.00.049.547 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.550 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.550 I llm_load_print_meta: arch             = gptneox
0.00.049.550 I llm_load_print_meta: vocab type       = BPE
0.00.049.551 I llm_load_print_meta: n_vocab          = 50304
0.00.049.551 I llm_load_print_meta: n_merges         = 50009
0.00.049.551 I llm_load_print_meta: vocab_only       = 0
0.00.049.551 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.551 I llm_load_print_meta: n_embd           = 2048
0.00.049.551 I llm_load_print_meta: n_layer          = 24
0.00.049.554 I llm_load_print_meta: n_head           = 16
0.00.049.555 I llm_load_print_meta: n_head_kv        = 16
0.00.049.555 I llm_load_print_meta: n_rot            = 32
0.00.049.555 I llm_load_print_meta: n_swa            = 0
0.00.049.556 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.556 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.557 I llm_load_print_meta: n_gqa            = 1
0.00.049.557 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.558 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.560 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.560 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.561 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.561 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.561 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.563 I llm_load_print_meta: n_ff             = 8192
0.00.049.564 I llm_load_print_meta: n_expert         = 0
0.00.049.564 I llm_load_print_meta: n_expert_used    = 0
0.00.049.564 I llm_load_print_meta: causal attn      = 1
0.00.049.564 I llm_load_print_meta: pooling type     = 0
0.00.049.576 I llm_load_print_meta: rope type        = 2
0.00.049.576 I llm_load_print_meta: rope scaling     = linear
0.00.049.576 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.577 I llm_load_print_meta: freq_scale_train = 1
0.00.049.577 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.578 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.578 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.578 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.578 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.578 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.578 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.579 I llm_load_print_meta: model type       = 1.4B
0.00.049.579 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.579 I llm_load_print_meta: model params     = 1.41 B
0.00.049.580 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.581 I llm_load_print_meta: general.name     = 1.4B
0.00.049.581 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.581 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.582 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.582 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.582 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.582 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.582 I llm_load_print_meta: max token length = 1024
0.00.051.553 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.553 I llm_load_tensors: offloading output layer to GPU
0.00.051.554 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.564 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.565 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.461 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.461 I llama_new_context_with_model: n_ctx         = 128
0.00.052.462 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.462 I llama_new_context_with_model: n_batch       = 128
0.00.052.462 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.462 I llama_new_context_with_model: flash_attn    = 0
0.00.052.463 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.463 I llama_new_context_with_model: freq_scale    = 1
0.00.052.463 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.464 I ggml_metal_init: allocating
0.00.052.469 I ggml_metal_init: found device: Apple M4
0.00.052.472 I ggml_metal_init: picking default device: Apple M4
0.00.053.068 I ggml_metal_init: using embedded metal library
0.00.055.415 I ggml_metal_init: GPU name:   Apple M4
0.00.055.416 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.417 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.417 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.417 I ggml_metal_init: simdgroup reduction   = true
0.00.055.418 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.418 I ggml_metal_init: has bfloat            = true
0.00.055.418 I ggml_metal_init: use bfloat            = true
0.00.055.418 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.419 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.251 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.481 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.487 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.500 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.408 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.409 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.410 I llama_new_context_with_model: graph nodes  = 967
0.00.067.410 I llama_new_context_with_model: graph splits = 2
0.00.067.411 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.423 I 
0.00.661.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.472 I perplexity: tokenizing the input ..
0.00.669.288 I perplexity: tokenization took 7.814 ms
0.00.669.295 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.122 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.793.284 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.793.299 I llama_perf_context_print:        load time =     652.61 ms
0.00.793.300 I llama_perf_context_print: prompt eval time =     122.60 ms /   128 tokens (    0.96 ms per token,  1044.04 tokens per second)
0.00.793.300 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.301 I llama_perf_context_print:       total time =     131.88 ms /   129 tokens
0.00.793.718 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.078s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.089 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.697 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.701 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.703 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.704 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.704 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.705 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.706 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.706 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.706 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.709 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.709 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.710 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.711 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.712 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.712 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.573 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.301 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.302 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.302 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.303 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.303 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.304 I llama_model_loader: - type  f32:  194 tensors
0.00.024.304 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.305 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.926 I llm_load_vocab: special tokens cache size = 25
0.00.050.727 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.729 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.730 I llm_load_print_meta: arch             = gptneox
0.00.050.730 I llm_load_print_meta: vocab type       = BPE
0.00.050.730 I llm_load_print_meta: n_vocab          = 50304
0.00.050.730 I llm_load_print_meta: n_merges         = 50009
0.00.050.730 I llm_load_print_meta: vocab_only       = 0
0.00.050.730 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.731 I llm_load_print_meta: n_embd           = 2048
0.00.050.731 I llm_load_print_meta: n_layer          = 24
0.00.050.734 I llm_load_print_meta: n_head           = 16
0.00.050.735 I llm_load_print_meta: n_head_kv        = 16
0.00.050.735 I llm_load_print_meta: n_rot            = 32
0.00.050.735 I llm_load_print_meta: n_swa            = 0
0.00.050.735 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.735 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.738 I llm_load_print_meta: n_gqa            = 1
0.00.050.739 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.740 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.740 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.741 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.741 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.741 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.741 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.742 I llm_load_print_meta: n_ff             = 8192
0.00.050.742 I llm_load_print_meta: n_expert         = 0
0.00.050.742 I llm_load_print_meta: n_expert_used    = 0
0.00.050.742 I llm_load_print_meta: causal attn      = 1
0.00.050.743 I llm_load_print_meta: pooling type     = 0
0.00.050.754 I llm_load_print_meta: rope type        = 2
0.00.050.755 I llm_load_print_meta: rope scaling     = linear
0.00.050.755 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.755 I llm_load_print_meta: freq_scale_train = 1
0.00.050.755 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.756 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.756 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.756 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.756 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.756 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.756 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.757 I llm_load_print_meta: model type       = 1.4B
0.00.050.757 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.757 I llm_load_print_meta: model params     = 1.41 B
0.00.050.758 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.758 I llm_load_print_meta: general.name     = 1.4B
0.00.050.758 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.758 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.758 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.759 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.760 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.760 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.760 I llm_load_print_meta: max token length = 1024
0.00.052.748 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.748 I llm_load_tensors: offloading output layer to GPU
0.00.052.748 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.759 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.760 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.649 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.650 I llama_new_context_with_model: n_ctx         = 128
0.00.053.650 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.650 I llama_new_context_with_model: n_batch       = 128
0.00.053.650 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.650 I llama_new_context_with_model: flash_attn    = 0
0.00.053.651 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.651 I llama_new_context_with_model: freq_scale    = 1
0.00.053.651 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.652 I ggml_metal_init: allocating
0.00.053.655 I ggml_metal_init: found device: Apple M4
0.00.053.657 I ggml_metal_init: picking default device: Apple M4
0.00.054.239 I ggml_metal_init: using embedded metal library
0.00.056.570 I ggml_metal_init: GPU name:   Apple M4
0.00.056.572 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.572 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.572 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.573 I ggml_metal_init: simdgroup reduction   = true
0.00.056.573 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.573 I ggml_metal_init: has bfloat            = true
0.00.056.573 I ggml_metal_init: use bfloat            = true
0.00.056.574 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.292 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.518 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.520 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.534 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.456 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.457 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.458 I llama_new_context_with_model: graph nodes  = 967
0.00.068.458 I llama_new_context_with_model: graph splits = 2
0.00.068.459 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.459 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.702.530 I 
0.00.702.568 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.702.589 I perplexity: tokenizing the input ..
0.00.710.882 I perplexity: tokenization took 8.291 ms
0.00.710.888 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.845.798 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.846.964 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.846.978 I llama_perf_context_print:        load time =     692.43 ms
0.00.846.979 I llama_perf_context_print: prompt eval time =     134.68 ms /   128 tokens (    1.05 ms per token,   950.39 tokens per second)
0.00.846.980 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.846.980 I llama_perf_context_print:       total time =     144.45 ms /   129 tokens
0.00.847.509 I ggml_metal_free: deallocating

real	0m0.861s
user	0m0.079s
sys	0m0.104s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.809 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.703 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.707 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.710 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.710 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.711 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.711 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.712 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.712 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.713 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.713 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.713 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.714 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.716 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.718 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.497 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.529 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.296 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.297 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.298 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.298 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.298 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.299 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.299 I llama_model_loader: - type  f32:  194 tensors
0.00.023.299 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.300 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.429 I llm_load_vocab: special tokens cache size = 25
0.00.050.330 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.332 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.333 I llm_load_print_meta: arch             = gptneox
0.00.050.333 I llm_load_print_meta: vocab type       = BPE
0.00.050.333 I llm_load_print_meta: n_vocab          = 50304
0.00.050.333 I llm_load_print_meta: n_merges         = 50009
0.00.050.334 I llm_load_print_meta: vocab_only       = 0
0.00.050.334 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.334 I llm_load_print_meta: n_embd           = 2048
0.00.050.334 I llm_load_print_meta: n_layer          = 24
0.00.050.336 I llm_load_print_meta: n_head           = 16
0.00.050.337 I llm_load_print_meta: n_head_kv        = 16
0.00.050.338 I llm_load_print_meta: n_rot            = 32
0.00.050.339 I llm_load_print_meta: n_swa            = 0
0.00.050.341 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.341 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.342 I llm_load_print_meta: n_gqa            = 1
0.00.050.342 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.343 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.344 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.345 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.345 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.346 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.346 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.347 I llm_load_print_meta: n_ff             = 8192
0.00.050.347 I llm_load_print_meta: n_expert         = 0
0.00.050.347 I llm_load_print_meta: n_expert_used    = 0
0.00.050.347 I llm_load_print_meta: causal attn      = 1
0.00.050.347 I llm_load_print_meta: pooling type     = 0
0.00.050.359 I llm_load_print_meta: rope type        = 2
0.00.050.359 I llm_load_print_meta: rope scaling     = linear
0.00.050.359 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.360 I llm_load_print_meta: freq_scale_train = 1
0.00.050.361 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.361 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.361 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.362 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.362 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.362 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.362 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.362 I llm_load_print_meta: model type       = 1.4B
0.00.050.363 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.363 I llm_load_print_meta: model params     = 1.41 B
0.00.050.364 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.364 I llm_load_print_meta: general.name     = 1.4B
0.00.050.364 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.364 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.364 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.365 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.366 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.366 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.366 I llm_load_print_meta: max token length = 1024
0.00.052.463 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.463 I llm_load_tensors: offloading output layer to GPU
0.00.052.463 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.474 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.475 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.438 I llama_new_context_with_model: n_ctx         = 128
0.00.053.438 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.438 I llama_new_context_with_model: n_batch       = 128
0.00.053.438 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.438 I llama_new_context_with_model: flash_attn    = 0
0.00.053.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.439 I llama_new_context_with_model: freq_scale    = 1
0.00.053.439 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.440 I ggml_metal_init: allocating
0.00.053.443 I ggml_metal_init: found device: Apple M4
0.00.053.445 I ggml_metal_init: picking default device: Apple M4
0.00.054.017 I ggml_metal_init: using embedded metal library
0.00.056.373 I ggml_metal_init: GPU name:   Apple M4
0.00.056.375 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.375 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.375 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.375 I ggml_metal_init: simdgroup reduction   = true
0.00.056.376 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.376 I ggml_metal_init: has bfloat            = true
0.00.056.376 I ggml_metal_init: use bfloat            = true
0.00.056.376 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.377 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.324 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.648 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.650 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.665 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.589 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.590 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.590 I llama_new_context_with_model: graph nodes  = 967
0.00.068.590 I llama_new_context_with_model: graph splits = 2
0.00.068.591 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.592 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.712.263 I 
0.00.712.307 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.712.333 I perplexity: tokenizing the input ..
0.00.720.788 I perplexity: tokenization took 8.454 ms
0.00.720.796 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.855.609 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.856.778 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.856.794 I llama_perf_context_print:        load time =     703.45 ms
0.00.856.795 I llama_perf_context_print: prompt eval time =     134.59 ms /   128 tokens (    1.05 ms per token,   951.05 tokens per second)
0.00.856.796 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.856.796 I llama_perf_context_print:       total time =     144.54 ms /   129 tokens
0.00.857.216 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.080s
sys	0m0.112s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.970 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.468 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.473 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.475 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.481 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.481 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.232 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.273 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.119 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.120 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.120 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.121 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.121 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.121 I llama_model_loader: - type  f32:  194 tensors
0.00.025.122 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.122 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.122 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.688 I llm_load_vocab: special tokens cache size = 25
0.00.051.549 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.551 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.552 I llm_load_print_meta: arch             = gptneox
0.00.051.552 I llm_load_print_meta: vocab type       = BPE
0.00.051.552 I llm_load_print_meta: n_vocab          = 50304
0.00.051.553 I llm_load_print_meta: n_merges         = 50009
0.00.051.553 I llm_load_print_meta: vocab_only       = 0
0.00.051.553 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.553 I llm_load_print_meta: n_embd           = 2048
0.00.051.553 I llm_load_print_meta: n_layer          = 24
0.00.051.556 I llm_load_print_meta: n_head           = 16
0.00.051.558 I llm_load_print_meta: n_head_kv        = 16
0.00.051.558 I llm_load_print_meta: n_rot            = 32
0.00.051.558 I llm_load_print_meta: n_swa            = 0
0.00.051.559 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.559 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.560 I llm_load_print_meta: n_gqa            = 1
0.00.051.560 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.561 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.561 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.562 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.562 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.563 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.563 I llm_load_print_meta: n_ff             = 8192
0.00.051.564 I llm_load_print_meta: n_expert         = 0
0.00.051.564 I llm_load_print_meta: n_expert_used    = 0
0.00.051.564 I llm_load_print_meta: causal attn      = 1
0.00.051.564 I llm_load_print_meta: pooling type     = 0
0.00.051.576 I llm_load_print_meta: rope type        = 2
0.00.051.576 I llm_load_print_meta: rope scaling     = linear
0.00.051.576 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.576 I llm_load_print_meta: freq_scale_train = 1
0.00.051.576 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.577 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.577 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.577 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.577 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.577 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.578 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.578 I llm_load_print_meta: model type       = 1.4B
0.00.051.578 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.579 I llm_load_print_meta: model params     = 1.41 B
0.00.051.581 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.581 I llm_load_print_meta: general.name     = 1.4B
0.00.051.581 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.581 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.582 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.583 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.051.583 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.583 I llm_load_print_meta: max token length = 1024
0.00.053.456 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.457 I llm_load_tensors: offloading output layer to GPU
0.00.053.457 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.467 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.468 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.365 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.366 I llama_new_context_with_model: n_ctx         = 128
0.00.054.366 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.366 I llama_new_context_with_model: n_batch       = 128
0.00.054.367 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.367 I llama_new_context_with_model: flash_attn    = 0
0.00.054.367 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.367 I llama_new_context_with_model: freq_scale    = 1
0.00.054.368 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.368 I ggml_metal_init: allocating
0.00.054.374 I ggml_metal_init: found device: Apple M4
0.00.054.376 I ggml_metal_init: picking default device: Apple M4
0.00.054.941 I ggml_metal_init: using embedded metal library
0.00.057.252 I ggml_metal_init: GPU name:   Apple M4
0.00.057.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.253 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.254 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.254 I ggml_metal_init: simdgroup reduction   = true
0.00.057.254 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.254 I ggml_metal_init: has bfloat            = true
0.00.057.254 I ggml_metal_init: use bfloat            = true
0.00.057.255 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.255 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.898 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.190 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.193 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.208 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.126 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.127 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.127 I llama_new_context_with_model: graph nodes  = 967
0.00.069.128 I llama_new_context_with_model: graph splits = 2
0.00.069.129 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.129 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.714 I 
0.00.434.743 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.756 I perplexity: tokenizing the input ..
0.00.442.182 I perplexity: tokenization took 7.424 ms
0.00.442.185 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.575.104 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.576.465 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.576.481 I llama_perf_context_print:        load time =     423.74 ms
0.00.576.482 I llama_perf_context_print: prompt eval time =     132.68 ms /   128 tokens (    1.04 ms per token,   964.73 tokens per second)
0.00.576.483 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.576.484 I llama_perf_context_print:       total time =     141.77 ms /   129 tokens
0.00.577.071 I ggml_metal_free: deallocating

real	0m0.592s
user	0m0.077s
sys	0m0.066s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.707 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.372 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.376 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.378 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.378 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.379 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.379 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.379 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.380 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.381 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.381 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.381 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.384 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.384 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.384 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.387 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.387 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.388 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.220 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.200 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.899 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.900 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.900 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.900 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.900 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.901 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.022.901 I llama_model_loader: - type  f32:  194 tensors
0.00.022.901 I llama_model_loader: - type q3_K:   25 tensors
0.00.022.901 I llama_model_loader: - type q4_K:   71 tensors
0.00.022.902 I llama_model_loader: - type q5_K:    1 tensors
0.00.022.902 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.444 I llm_load_vocab: special tokens cache size = 25
0.00.049.329 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.332 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.332 I llm_load_print_meta: arch             = gptneox
0.00.049.332 I llm_load_print_meta: vocab type       = BPE
0.00.049.333 I llm_load_print_meta: n_vocab          = 50304
0.00.049.333 I llm_load_print_meta: n_merges         = 50009
0.00.049.333 I llm_load_print_meta: vocab_only       = 0
0.00.049.333 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.333 I llm_load_print_meta: n_embd           = 2048
0.00.049.333 I llm_load_print_meta: n_layer          = 24
0.00.049.336 I llm_load_print_meta: n_head           = 16
0.00.049.337 I llm_load_print_meta: n_head_kv        = 16
0.00.049.337 I llm_load_print_meta: n_rot            = 32
0.00.049.337 I llm_load_print_meta: n_swa            = 0
0.00.049.337 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.339 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.340 I llm_load_print_meta: n_gqa            = 1
0.00.049.341 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.341 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.342 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.342 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.342 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.343 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.343 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.344 I llm_load_print_meta: n_ff             = 8192
0.00.049.344 I llm_load_print_meta: n_expert         = 0
0.00.049.344 I llm_load_print_meta: n_expert_used    = 0
0.00.049.344 I llm_load_print_meta: causal attn      = 1
0.00.049.344 I llm_load_print_meta: pooling type     = 0
0.00.049.356 I llm_load_print_meta: rope type        = 2
0.00.049.356 I llm_load_print_meta: rope scaling     = linear
0.00.049.357 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.357 I llm_load_print_meta: freq_scale_train = 1
0.00.049.357 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.357 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.357 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.359 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.359 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.359 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.359 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.359 I llm_load_print_meta: model type       = 1.4B
0.00.049.360 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.360 I llm_load_print_meta: model params     = 1.41 B
0.00.049.361 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.361 I llm_load_print_meta: general.name     = 1.4B
0.00.049.361 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.361 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.361 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.362 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.362 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.362 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.362 I llm_load_print_meta: max token length = 1024
0.00.051.333 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.333 I llm_load_tensors: offloading output layer to GPU
0.00.051.334 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.344 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.345 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.340 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.341 I llama_new_context_with_model: n_ctx         = 128
0.00.052.342 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.342 I llama_new_context_with_model: n_batch       = 128
0.00.052.342 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.342 I llama_new_context_with_model: flash_attn    = 0
0.00.052.342 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.343 I llama_new_context_with_model: freq_scale    = 1
0.00.052.343 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.344 I ggml_metal_init: allocating
0.00.052.347 I ggml_metal_init: found device: Apple M4
0.00.052.349 I ggml_metal_init: picking default device: Apple M4
0.00.052.911 I ggml_metal_init: using embedded metal library
0.00.055.231 I ggml_metal_init: GPU name:   Apple M4
0.00.055.233 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.233 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.233 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.234 I ggml_metal_init: simdgroup reduction   = true
0.00.055.234 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.234 I ggml_metal_init: has bfloat            = true
0.00.055.234 I ggml_metal_init: use bfloat            = true
0.00.055.235 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.235 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.997 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.247 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.249 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.212 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.213 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.213 I llama_new_context_with_model: graph nodes  = 967
0.00.067.213 I llama_new_context_with_model: graph splits = 2
0.00.067.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.477.116 I 
0.00.477.153 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.477.167 I perplexity: tokenizing the input ..
0.00.485.337 I perplexity: tokenization took 8.168 ms
0.00.485.340 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.617.590 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.618.755 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.618.775 I llama_perf_context_print:        load time =     468.41 ms
0.00.618.776 I llama_perf_context_print: prompt eval time =     132.02 ms /   128 tokens (    1.03 ms per token,   969.59 tokens per second)
0.00.618.779 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.618.780 I llama_perf_context_print:       total time =     141.66 ms /   129 tokens
0.00.619.337 I ggml_metal_free: deallocating

real	0m0.633s
user	0m0.078s
sys	0m0.088s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.778 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.324 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.330 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.331 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.331 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.331 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.332 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.333 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.333 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.333 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.334 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.334 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.334 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.335 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.338 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.341 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.341 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.101 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.093 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.022.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.022.910 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.022.910 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.022.910 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.022.911 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.022.911 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.022.911 I llama_model_loader: - type  f32:  194 tensors
0.00.022.912 I llama_model_loader: - type q4_K:   61 tensors
0.00.022.912 I llama_model_loader: - type q5_K:   24 tensors
0.00.022.912 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.308 I llm_load_vocab: special tokens cache size = 25
0.00.049.115 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.117 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.118 I llm_load_print_meta: arch             = gptneox
0.00.049.118 I llm_load_print_meta: vocab type       = BPE
0.00.049.118 I llm_load_print_meta: n_vocab          = 50304
0.00.049.118 I llm_load_print_meta: n_merges         = 50009
0.00.049.118 I llm_load_print_meta: vocab_only       = 0
0.00.049.119 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.119 I llm_load_print_meta: n_embd           = 2048
0.00.049.119 I llm_load_print_meta: n_layer          = 24
0.00.049.121 I llm_load_print_meta: n_head           = 16
0.00.049.122 I llm_load_print_meta: n_head_kv        = 16
0.00.049.122 I llm_load_print_meta: n_rot            = 32
0.00.049.124 I llm_load_print_meta: n_swa            = 0
0.00.049.125 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.125 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.126 I llm_load_print_meta: n_gqa            = 1
0.00.049.126 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.132 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.132 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.133 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.133 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.133 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.133 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.134 I llm_load_print_meta: n_ff             = 8192
0.00.049.134 I llm_load_print_meta: n_expert         = 0
0.00.049.134 I llm_load_print_meta: n_expert_used    = 0
0.00.049.135 I llm_load_print_meta: causal attn      = 1
0.00.049.135 I llm_load_print_meta: pooling type     = 0
0.00.049.147 I llm_load_print_meta: rope type        = 2
0.00.049.147 I llm_load_print_meta: rope scaling     = linear
0.00.049.147 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.148 I llm_load_print_meta: freq_scale_train = 1
0.00.049.148 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.148 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.148 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.148 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.148 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.148 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.148 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.149 I llm_load_print_meta: model type       = 1.4B
0.00.049.149 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.150 I llm_load_print_meta: model params     = 1.41 B
0.00.049.150 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.150 I llm_load_print_meta: general.name     = 1.4B
0.00.049.150 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.151 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.151 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.151 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.151 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.151 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.152 I llm_load_print_meta: max token length = 1024
0.00.051.105 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.106 I llm_load_tensors: offloading output layer to GPU
0.00.051.106 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.116 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.117 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.017 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.018 I llama_new_context_with_model: n_ctx         = 128
0.00.052.018 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.018 I llama_new_context_with_model: n_batch       = 128
0.00.052.018 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.019 I llama_new_context_with_model: flash_attn    = 0
0.00.052.019 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.019 I llama_new_context_with_model: freq_scale    = 1
0.00.052.020 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.020 I ggml_metal_init: allocating
0.00.052.026 I ggml_metal_init: found device: Apple M4
0.00.052.029 I ggml_metal_init: picking default device: Apple M4
0.00.052.605 I ggml_metal_init: using embedded metal library
0.00.054.999 I ggml_metal_init: GPU name:   Apple M4
0.00.055.001 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.001 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.001 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.001 I ggml_metal_init: simdgroup reduction   = true
0.00.055.002 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.002 I ggml_metal_init: has bfloat            = true
0.00.055.002 I ggml_metal_init: use bfloat            = true
0.00.055.002 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.003 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.674 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.939 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.943 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.959 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.943 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.944 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.944 I llama_new_context_with_model: graph nodes  = 967
0.00.066.944 I llama_new_context_with_model: graph splits = 2
0.00.066.946 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.946 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.553.140 I 
0.00.553.175 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.553.187 I perplexity: tokenizing the input ..
0.00.561.019 I perplexity: tokenization took 7.831 ms
0.00.561.022 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.694.980 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.696.257 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.696.268 I llama_perf_context_print:        load time =     544.35 ms
0.00.696.269 I llama_perf_context_print: prompt eval time =     133.72 ms /   128 tokens (    1.04 ms per token,   957.20 tokens per second)
0.00.696.270 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.696.270 I llama_perf_context_print:       total time =     143.13 ms /   129 tokens
0.00.696.652 I ggml_metal_free: deallocating

real	0m0.711s
user	0m0.077s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.663 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.113 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.117 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.123 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.123 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.124 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.125 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.125 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.126 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.126 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.127 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.127 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.127 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.128 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.128 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.130 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.130 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.130 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.872 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.899 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.698 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.699 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.700 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.700 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.700 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.701 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.701 I llama_model_loader: - type  f32:  194 tensors
0.00.023.705 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.705 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.079 I llm_load_vocab: special tokens cache size = 25
0.00.049.955 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.957 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.958 I llm_load_print_meta: arch             = gptneox
0.00.049.958 I llm_load_print_meta: vocab type       = BPE
0.00.049.958 I llm_load_print_meta: n_vocab          = 50304
0.00.049.958 I llm_load_print_meta: n_merges         = 50009
0.00.049.958 I llm_load_print_meta: vocab_only       = 0
0.00.049.958 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.959 I llm_load_print_meta: n_embd           = 2048
0.00.049.959 I llm_load_print_meta: n_layer          = 24
0.00.049.962 I llm_load_print_meta: n_head           = 16
0.00.049.963 I llm_load_print_meta: n_head_kv        = 16
0.00.049.963 I llm_load_print_meta: n_rot            = 32
0.00.049.963 I llm_load_print_meta: n_swa            = 0
0.00.049.963 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.963 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.964 I llm_load_print_meta: n_gqa            = 1
0.00.049.965 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.966 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.966 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.966 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.967 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.967 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.967 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.968 I llm_load_print_meta: n_ff             = 8192
0.00.049.968 I llm_load_print_meta: n_expert         = 0
0.00.049.968 I llm_load_print_meta: n_expert_used    = 0
0.00.049.968 I llm_load_print_meta: causal attn      = 1
0.00.049.968 I llm_load_print_meta: pooling type     = 0
0.00.049.980 I llm_load_print_meta: rope type        = 2
0.00.049.980 I llm_load_print_meta: rope scaling     = linear
0.00.049.980 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.981 I llm_load_print_meta: freq_scale_train = 1
0.00.049.981 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.981 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.981 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.981 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.981 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.982 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.982 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.983 I llm_load_print_meta: model type       = 1.4B
0.00.049.983 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.049.983 I llm_load_print_meta: model params     = 1.41 B
0.00.049.984 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.049.984 I llm_load_print_meta: general.name     = 1.4B
0.00.049.984 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.985 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.986 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.986 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.987 I llm_load_print_meta: max token length = 1024
0.00.052.045 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.045 I llm_load_tensors: offloading output layer to GPU
0.00.052.045 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.056 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.057 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.052.970 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.971 I llama_new_context_with_model: n_ctx         = 128
0.00.052.971 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.971 I llama_new_context_with_model: n_batch       = 128
0.00.052.971 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.971 I llama_new_context_with_model: flash_attn    = 0
0.00.052.972 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.972 I llama_new_context_with_model: freq_scale    = 1
0.00.052.973 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.973 I ggml_metal_init: allocating
0.00.052.978 I ggml_metal_init: found device: Apple M4
0.00.052.980 I ggml_metal_init: picking default device: Apple M4
0.00.053.545 I ggml_metal_init: using embedded metal library
0.00.055.857 I ggml_metal_init: GPU name:   Apple M4
0.00.055.858 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.859 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.859 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.859 I ggml_metal_init: simdgroup reduction   = true
0.00.055.859 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.860 I ggml_metal_init: has bfloat            = true
0.00.055.860 I ggml_metal_init: use bfloat            = true
0.00.055.860 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.861 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.492 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.070 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.073 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.087 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.964 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.965 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.965 I llama_new_context_with_model: graph nodes  = 967
0.00.067.965 I llama_new_context_with_model: graph splits = 2
0.00.067.967 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.967 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.624.367 I 
0.00.624.410 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.624.425 I perplexity: tokenizing the input ..
0.00.632.692 I perplexity: tokenization took 8.265 ms
0.00.632.695 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.773.460 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.774.636 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.774.653 I llama_perf_context_print:        load time =     614.70 ms
0.00.774.654 I llama_perf_context_print: prompt eval time =     140.54 ms /   128 tokens (    1.10 ms per token,   910.78 tokens per second)
0.00.774.657 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.774.657 I llama_perf_context_print:       total time =     150.29 ms /   129 tokens
0.00.775.176 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.079s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.932 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.635 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.639 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.640 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.645 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.645 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.645 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.647 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.648 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.648 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.649 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.649 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.652 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.653 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.653 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.655 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.656 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.656 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.476 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.543 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.433 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.434 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.435 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.435 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.435 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.436 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.436 I llama_model_loader: - type  f32:  194 tensors
0.00.023.437 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.951 I llm_load_vocab: special tokens cache size = 25
0.00.049.758 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.761 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.761 I llm_load_print_meta: arch             = gptneox
0.00.049.761 I llm_load_print_meta: vocab type       = BPE
0.00.049.761 I llm_load_print_meta: n_vocab          = 50304
0.00.049.762 I llm_load_print_meta: n_merges         = 50009
0.00.049.762 I llm_load_print_meta: vocab_only       = 0
0.00.049.762 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.762 I llm_load_print_meta: n_embd           = 2048
0.00.049.762 I llm_load_print_meta: n_layer          = 24
0.00.049.765 I llm_load_print_meta: n_head           = 16
0.00.049.766 I llm_load_print_meta: n_head_kv        = 16
0.00.049.768 I llm_load_print_meta: n_rot            = 32
0.00.049.768 I llm_load_print_meta: n_swa            = 0
0.00.049.768 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.768 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.769 I llm_load_print_meta: n_gqa            = 1
0.00.049.770 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.770 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.777 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.777 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.777 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.779 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.779 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.780 I llm_load_print_meta: n_ff             = 8192
0.00.049.780 I llm_load_print_meta: n_expert         = 0
0.00.049.780 I llm_load_print_meta: n_expert_used    = 0
0.00.049.780 I llm_load_print_meta: causal attn      = 1
0.00.049.781 I llm_load_print_meta: pooling type     = 0
0.00.049.793 I llm_load_print_meta: rope type        = 2
0.00.049.793 I llm_load_print_meta: rope scaling     = linear
0.00.049.793 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.794 I llm_load_print_meta: freq_scale_train = 1
0.00.049.794 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.794 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.794 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.794 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.795 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.795 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.795 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.795 I llm_load_print_meta: model type       = 1.4B
0.00.049.796 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.796 I llm_load_print_meta: model params     = 1.41 B
0.00.049.796 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.797 I llm_load_print_meta: general.name     = 1.4B
0.00.049.797 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.797 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.797 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.797 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.799 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.799 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.799 I llm_load_print_meta: max token length = 1024
0.00.051.825 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.825 I llm_load_tensors: offloading output layer to GPU
0.00.051.826 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.836 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.837 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.780 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.781 I llama_new_context_with_model: n_ctx         = 128
0.00.052.781 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.781 I llama_new_context_with_model: n_batch       = 128
0.00.052.781 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.781 I llama_new_context_with_model: flash_attn    = 0
0.00.052.782 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.782 I llama_new_context_with_model: freq_scale    = 1
0.00.052.782 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.783 I ggml_metal_init: allocating
0.00.052.786 I ggml_metal_init: found device: Apple M4
0.00.052.788 I ggml_metal_init: picking default device: Apple M4
0.00.053.367 I ggml_metal_init: using embedded metal library
0.00.055.707 I ggml_metal_init: GPU name:   Apple M4
0.00.055.708 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.708 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.709 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.709 I ggml_metal_init: simdgroup reduction   = true
0.00.055.709 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.709 I ggml_metal_init: has bfloat            = true
0.00.055.709 I ggml_metal_init: use bfloat            = true
0.00.055.710 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.710 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.397 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.681 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.683 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.696 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.599 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.599 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.600 I llama_new_context_with_model: graph nodes  = 967
0.00.067.600 I llama_new_context_with_model: graph splits = 2
0.00.067.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.601 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.436.765 I 
0.00.436.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.436.817 I perplexity: tokenizing the input ..
0.00.444.905 I perplexity: tokenization took 8.087 ms
0.00.444.916 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.585.191 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.586.358 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.586.368 I llama_perf_context_print:        load time =     427.83 ms
0.00.586.369 I llama_perf_context_print: prompt eval time =     140.02 ms /   128 tokens (    1.09 ms per token,   914.13 tokens per second)
0.00.586.370 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.586.370 I llama_perf_context_print:       total time =     149.60 ms /   129 tokens
0.00.586.826 I ggml_metal_free: deallocating

real	0m0.600s
user	0m0.078s
sys	0m0.089s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.254 I build: 4425 (e39a9c10) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.411 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.010 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.015 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.017 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.034.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.034.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.034.023 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.034.025 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.034.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.034.025 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.034.026 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.034.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.034.027 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.034.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.034.030 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.034.031 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.031 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.041.691 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.043.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.951 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.953 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.954 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.954 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.955 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.955 I llama_model_loader: - type  f32:  194 tensors
0.00.049.956 I llama_model_loader: - type  f16:   98 tensors
0.00.077.857 I llm_load_vocab: special tokens cache size = 25
0.00.084.166 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.084.168 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.084.169 I llm_load_print_meta: arch             = gptneox
0.00.084.169 I llm_load_print_meta: vocab type       = BPE
0.00.084.169 I llm_load_print_meta: n_vocab          = 50304
0.00.084.169 I llm_load_print_meta: n_merges         = 50009
0.00.084.169 I llm_load_print_meta: vocab_only       = 0
0.00.084.170 I llm_load_print_meta: n_ctx_train      = 2048
0.00.084.170 I llm_load_print_meta: n_embd           = 2048
0.00.084.170 I llm_load_print_meta: n_layer          = 24
0.00.084.173 I llm_load_print_meta: n_head           = 16
0.00.084.174 I llm_load_print_meta: n_head_kv        = 16
0.00.084.174 I llm_load_print_meta: n_rot            = 32
0.00.084.174 I llm_load_print_meta: n_swa            = 0
0.00.084.174 I llm_load_print_meta: n_embd_head_k    = 128
0.00.084.174 I llm_load_print_meta: n_embd_head_v    = 128
0.00.084.175 I llm_load_print_meta: n_gqa            = 1
0.00.084.176 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.084.176 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.084.176 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.084.177 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.084.177 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.084.177 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.084.177 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.084.178 I llm_load_print_meta: n_ff             = 8192
0.00.084.178 I llm_load_print_meta: n_expert         = 0
0.00.084.178 I llm_load_print_meta: n_expert_used    = 0
0.00.084.178 I llm_load_print_meta: causal attn      = 1
0.00.084.178 I llm_load_print_meta: pooling type     = 0
0.00.084.190 I llm_load_print_meta: rope type        = 2
0.00.084.190 I llm_load_print_meta: rope scaling     = linear
0.00.084.191 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.084.192 I llm_load_print_meta: freq_scale_train = 1
0.00.084.192 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.084.192 I llm_load_print_meta: rope_finetuned   = unknown
0.00.084.192 I llm_load_print_meta: ssm_d_conv       = 0
0.00.084.192 I llm_load_print_meta: ssm_d_inner      = 0
0.00.084.192 I llm_load_print_meta: ssm_d_state      = 0
0.00.084.194 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.084.194 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.084.194 I llm_load_print_meta: model type       = 1.4B
0.00.084.195 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.084.195 I llm_load_print_meta: model params     = 1.41 B
0.00.084.196 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.084.196 I llm_load_print_meta: general.name     = 1.4B
0.00.084.197 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.084.198 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.084.198 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.084.198 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.084.198 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.084.198 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.084.199 I llm_load_print_meta: max token length = 1024
0.00.086.682 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.086.682 I llm_load_tensors: offloading output layer to GPU
0.00.086.682 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.086.693 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.086.694 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.087.643 I llama_new_context_with_model: n_seq_max     = 1
0.00.087.644 I llama_new_context_with_model: n_ctx         = 128
0.00.087.644 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.087.644 I llama_new_context_with_model: n_batch       = 128
0.00.087.645 I llama_new_context_with_model: n_ubatch      = 128
0.00.087.645 I llama_new_context_with_model: flash_attn    = 0
0.00.087.645 I llama_new_context_with_model: freq_base     = 10000.0
0.00.087.646 I llama_new_context_with_model: freq_scale    = 1
0.00.087.646 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.087.647 I ggml_metal_init: allocating
0.00.087.652 I ggml_metal_init: found device: Apple M4
0.00.087.655 I ggml_metal_init: picking default device: Apple M4
0.00.088.223 I ggml_metal_init: using embedded metal library
0.00.090.696 I ggml_metal_init: GPU name:   Apple M4
0.00.090.697 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.090.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.090.698 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.090.698 I ggml_metal_init: simdgroup reduction   = true
0.00.090.699 I ggml_metal_init: simdgroup matrix mul. = true
0.00.090.699 I ggml_metal_init: has bfloat            = true
0.00.090.699 I ggml_metal_init: use bfloat            = true
0.00.090.699 I ggml_metal_init: hasUnifiedMemory      = true
0.00.090.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.715 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.021 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.101.023 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.101.037 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.101.838 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.101.839 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.101.840 I llama_new_context_with_model: graph nodes  = 967
0.00.101.840 I llama_new_context_with_model: graph splits = 2
0.00.101.841 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.101.842 I 
0.00.101.866 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.101.867 I compute_imatrix: tokenizing the input ..
0.00.108.738 I compute_imatrix: tokenization took 6.87 ms
0.00.108.740 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.553.679 I compute_imatrix: 1.44 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.556.421 I llama_perf_context_print:        load time =    1532.27 ms
0.01.556.422 I llama_perf_context_print: prompt eval time =    1444.32 ms /   128 tokens (   11.28 ms per token,    88.62 tokens per second)
0.01.556.423 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.556.424 I llama_perf_context_print:       total time =    1535.00 ms /   129 tokens
0.01.556.969 I ggml_metal_free: deallocating

real	0m1.742s
user	0m0.168s
sys	0m0.254s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4425 (e39a9c10)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13c70a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13c70a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13c70aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13c70b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13c70ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13c70bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13c70c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13c70cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13c70d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13c70d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13c70daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13c70dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13c70eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13c70f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13c70fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13c7101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13c710910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13c711030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13c711750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13c711f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13c712640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13c712d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13c713480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13c713d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13c714440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13c714700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13c714d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13c715980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13c715ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13c716180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13c716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13c7168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13c717170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13c7176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13c717970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13c717e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13c7182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13c718750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13c718bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13c719090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13c719530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13c7199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13c719e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13c71a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13c71a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13c71abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13c71b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13c71bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13c71c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13c71c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13c71cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13c71d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13c71d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13c71df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13c71e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13c71ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13c71f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13c71f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13c71f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13c720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13c720420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13c7208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13c720d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13c721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13c7216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13c721b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13c721fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13c722480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13c722920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13c722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13c723260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13c723700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13c723ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13c7240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13c724640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13c724b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13c7250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13c725630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13c725b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13c7260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13c726620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13c726b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13c7270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13c727610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13c727b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13c7280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13c728600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13c728b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13c7290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13c7295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13c729b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13c72a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13c72a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13c72ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13c72b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13c72b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13c72bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13c71b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13c72bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13c72c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13c72cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13c72d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13c72d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13c72dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13c72e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13c72e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13c72ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13c72f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13c72f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13c72fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13c7301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13c730700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13c730c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13c7310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13c731590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13c731a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13c731ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13c732370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13c732810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13c732cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13c733150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13c7335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13c733a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13c733f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13c7343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13c734870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13c734d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13c7351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13c735650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13c735af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13c735f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13c736430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13c7368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13c736d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13c737210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13c7376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13c737b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13c737ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13c738490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13c738930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13c738dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13c739270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13c739710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13c739bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13c73a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13c73a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13c73a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13c73ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13c73b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13c73b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13c73bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13c73c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13c73c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13c73c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13c73ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13c73d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13c73d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13c73dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13c73e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13c73e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13c73ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13c73eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13c73f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13c73f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13c73fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13c740170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13c740610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13c740ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13c740f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13c7413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13c741890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13c741d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13c7421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13c742670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13c742b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13c742fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13c743450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13c7438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13c743d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13c744230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13c7446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13c744b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13c745010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13c7454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13c745950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13c745df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13c746290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13c746730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13c746bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13c747070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13c747510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13c7479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13c747e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13c7483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13c7488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13c748e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13c749390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13c749650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13c749c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13c74a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13c74a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13c74b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13c74b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13c74b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13c74bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13c74c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13c74cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13c74d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13c74d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13c74d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13c74e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13c74e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13c74ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13c74f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13c74f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13c74fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13c750150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13c7506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13c750bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13c751140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13c751690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13c751be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13c752130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13c752680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13c752bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13c753120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13c753670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13c753bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13c754110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13c754660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13c754bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13c755100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13c755650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13c755ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13c7560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13c756640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13c756b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13c7570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13c757630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13c757b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13c7580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13c758620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13c758b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13c7590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13c759610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13c759b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13c75a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13c75a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13c75ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13c75b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13c75b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13c75bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13c75c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13c75c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13c75cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13c75d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13c75d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13c75db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13c75e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13c75e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13c75eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13c75f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13c75f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13c75fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13c760050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13c7605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13c760af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13c760f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13c761430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13c7618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13c761d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13c762210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13c7626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13c762b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13c762ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13c763490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13c763930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13c763dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13c764270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13c764710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13c764bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13c765050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13c7655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13c765cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13c7663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13c766b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13c767220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13c7674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13c767cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13c767f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13c7685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.934 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.938 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x127404d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1274051c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127405630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127405aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x127405f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127406380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1274067f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x127406c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1274070d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127407540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1274079b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1274080a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x127408bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127409370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127409b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12740a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12740a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12740b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12740b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12740bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12740c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12740cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12740d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12740dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12740e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12740e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12740e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12740ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12740f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12740f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12740fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12740ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1274103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x127410670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127410ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x127410f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1274113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x127411830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127411ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x127412110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127412580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1274129f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127412e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1274132d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127413740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127413bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x127414020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127414490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x127414900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127414d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1274151e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127415650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127415ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x127415f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x1274163a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x127416810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127416d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127417280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1274176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127417b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127417fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127418440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1274188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x127418d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127419190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127419600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127419a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127419ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12741a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12741a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12741ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12741b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12741b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12741b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12741bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12741c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12741c6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12741cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12741cfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12741d420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12741d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12741dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12741e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12741e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12741ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12741eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12741f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12741f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12741fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x127420080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1274204f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127420960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127420dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127421240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1274216b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x127421b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127421f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127422400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127422870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127422ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x127423150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1274235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x127423a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127423ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127424310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127424780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127424bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127425060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1274254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127425940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x127425db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127426220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x127426690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127426b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127426f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1274273e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127427850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x127427cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127428130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1274285a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127428a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127428e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1274292f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127429760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x127429bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12742a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12742a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12742a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12742ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12742b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12742b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12742bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12742bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12742c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12742c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12742cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12742d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12742d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12742d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12742de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12742e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12742e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12742ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12742f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12742f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12742f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12742fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1274301e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127430650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x127430ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127430f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x1274313a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127431810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127431c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1274320f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127432560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1274329d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127432e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1274332b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127433720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x127433b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127434000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127434470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1274348e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127434d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1274351c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127435df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1274360b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127436370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1274367e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127436c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1274370c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127437530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1274379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x127437e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127438280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1274386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127438b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127438fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127439440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1274398b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x127439d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12743a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12743a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12743aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12743aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12743b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12743b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12743bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12743c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12743c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12743c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12743cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12743d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12743d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12743db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12743dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12743e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12743e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12743ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12743f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12743f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12743fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x127440050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1274404c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x127440930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x127440da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127441210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127441730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127441c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1274427b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127442a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127443030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1274435f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x127443bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127444170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x127444730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127444cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1274452b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127445870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127445e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1274463f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1274469b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x127446f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127447530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127447af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1274480b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x127448670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127448c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1274491f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1274497b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127449d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12744a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12744a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12744aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12744b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12744ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12744bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12744c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12744cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12744d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12744d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12744dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12744e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12744e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12744edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12744f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12744f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12744ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1274504f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x127450ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x127451070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x127451630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x127451bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1274521b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x127452770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x127452d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1274532f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1274538b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x127453e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x127454430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1274549f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x127454fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127455570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x127455b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1274560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1274566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127456c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127457170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127457670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127457b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x127458070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127458570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127458a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127458f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x127459470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127459970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127459e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12745a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12745a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12745ad70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12745b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12745b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12745c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12745c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12745cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12745d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12745d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12745e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12745e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12745ea60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1277044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x127704950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x127704dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x127705230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1277056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x127705b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x127705f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1277063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x127706860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x127706db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x127707220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1277078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x1277083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x127708b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x127709380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x127709aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12770a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12770a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12770b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12770b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12770bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12770c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12770cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12770d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12770db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12770de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12770e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12770e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12770e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12770ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12770f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12770f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12770fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12770ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x127710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1277107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x127710c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1277110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x127711540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1277119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x127711e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x127712290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x127712700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x127712b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x127712fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x127713450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1277138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x127713d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1277141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x127714610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x127714a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x127714ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x127715360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x1277157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x127715c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1277160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x127716620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x127716b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x127716f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x127717400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x127717870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x127717ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x127718150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1277185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x127718a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x127718ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x127719310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x127719780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x127719bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12771a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12771a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12771a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12771adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12771b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12771b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12771bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12771bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12771c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12771c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12771ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12771d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12771d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12771da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12771de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12771e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12771e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12771ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12771f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12771f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12771f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12771fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x127720200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x127720670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x127720ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x127720f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1277213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x127721830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x127721ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x127722110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x127722580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1277229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x127722e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x1277232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x127723b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x127723e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x127724290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x127724700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x127724b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x127724fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x127725450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x1277258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x127725d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1277261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x127726610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x127726a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x127726ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x127727360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1277277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x127727c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1277280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x127728520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x127728990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x127728e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x127729270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1277296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x127729b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x127729fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12772a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12772a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12772ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12772b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12772b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12772ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12772bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12772c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12772c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12772cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12772d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12772d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12772d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12772dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12772e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12772e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12772eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12772efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12772f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12772f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12772fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x127730160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1277305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x127730a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x127730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x127731320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x127731790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x127731c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x127732070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1277324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x127732950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x127732dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x127733230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1277336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x127733b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x127733f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1277343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x127734860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x127734cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x127735140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1277355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x127735a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x127735e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x127736300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x127736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x127736be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x127737050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1277374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x127737930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x127737da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x127738210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x127738680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x127738af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x127738f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1277393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x127739840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x127739cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12773a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12773a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12773aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12773ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12773b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12773b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12773bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12773c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12773c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12773c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12773cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12773d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12773d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12773dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12773df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12773e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12773e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12773ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12773f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12773f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12773f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12773fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1277402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x127740730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x127740ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x127741010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x127741b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x127741e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x127742110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x127742580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1277429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x127742e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1277432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x127743740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x127743bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x127744020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x127744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x127744900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x127744d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1277451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x127745650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x127745ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x127745f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1277463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x127746810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x127746c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1277470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x127747560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1277479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x127747e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1277482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x127748720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x127748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x127749000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x127749470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1277498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x127749d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12774a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12774a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12774aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12774af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12774b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12774b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12774bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12774c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12774c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12774c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12774ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12774d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12774d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12774db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12774dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12774e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12774e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12774ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12774f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12774f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12774fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12774fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x127750360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1277507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x127750c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1277510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x127751520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x127751990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x127751e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x127752270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1277526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x127752b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x127752fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x127753430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1277538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x127753d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x127754180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1277545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x127754a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x127754ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x127755340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1277557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x127756220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x127756940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x127757060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x127757780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x127757a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x127757eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1277584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x127758ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.771s
user	0m0.294s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4425 (e39a9c10)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14180a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14180a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14180ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14180b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14180b9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14180bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14180c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14180cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14180d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14180d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14180dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14180dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14180ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14180f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14180fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1418101b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1418108d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x141810ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x141811710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x141811ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x141812600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x141812d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x141813440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x141813ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x141814400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1418146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x141814cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x141815940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x141815e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x141816140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1418165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1418168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x141817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x141817670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x141817930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x141817dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x141818270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x141818710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x141818bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x141819050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1418194f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x141819990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x141819e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14181a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14181a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14181aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14181b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14181bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14181c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14181c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14181cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14181d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14181d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14181df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14181e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14181ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14181f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14181f320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14181f930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x141820120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1418203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x141820880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x141820d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1418211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x141821660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x141821b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x141821fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x141822440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1418228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x141822d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x141823220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1418236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x141823b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1418240b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x141824600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x141824b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1418250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1418255f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x141825b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x141826090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1418265e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x141826b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x141827080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1418275d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x141827b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x141828070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1418285c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x141828b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x141829060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1418295b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x141829b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14182a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14182a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14182aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14182b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14182b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14182bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14181b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14182bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14182c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14182cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14182d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14182d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14182dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14182e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14182e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14182ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14182f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14182f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14182fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x141830170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1418306c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x141830c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1418310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x141831550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1418319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x141831e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x141832330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1418327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x141832c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x141833110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1418335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x141833a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x141833ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x141834390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x141834830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x141834cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x141835170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x141835610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x141835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x141835f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1418363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x141836890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x141836d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1418371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x141837670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x141837b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x141837fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x141838450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1418388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x141838d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x141839230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1418396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x141839b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14183a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14183a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14183a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14183adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14183b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14183b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14183bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14183c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14183c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14183c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14183ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14183d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14183d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14183dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14183e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14183e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14183ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14183eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14183f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14183f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14183fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x141840130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1418405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x141840a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x141840f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1418413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x141841850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x141841cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x141842190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x141842630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x141842ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x141842f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x141843410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1418438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x141843d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1418441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x141844690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x141844b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x141844fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x141845470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x141845910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x141845db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x141846250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1418466f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x141846b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x141847030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1418474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x141847970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x141847e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x141848360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1418488b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x141848e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x141849350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x141849610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x141849c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14184a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14184a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14184b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14184b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14184b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14184bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14184c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14184cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14184d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14184d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14184d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14184e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14184e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14184ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14184f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14184f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14184fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x141850110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x141850660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x141850bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x141851100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x141851650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x141851ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1418520f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x141852640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x141852b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1418530e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x141853630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x141853b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1418540d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x141854620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x141854b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1418550c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x141855610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x141855b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1418560b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x141856600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x141856b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1418570a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1418575f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x141857b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x141858090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1418585e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x141858b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x141859080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1418595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x141859b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14185a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14185a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14185ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14185b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14185b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14185bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14185c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14185c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14185caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14185d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14185d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14185dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14185e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14185e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14185ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14185f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14185f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14185fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x141860010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x141860560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x141860ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x141860f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1418613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x141861890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x141861d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1418621d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x141862670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x141862b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x141862fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x141863450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1418638f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x141863d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x141864230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1418646d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x141864b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x141865010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x141865560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x141865c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1418663a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x141866ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1418671e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1418674a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x141867c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x141867f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x141868560 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.532 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.537 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140707180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1407075f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x140707a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140707ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x140708340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1407087b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x140708c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x140709090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x140709500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x140709970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x140709de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14070a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14070af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14070b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14070bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14070c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14070cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14070d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14070dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14070e390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14070eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14070f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14070f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140710010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x140710730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1407109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140710cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140711120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140711590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140711a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140711e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1407123a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140712810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x140712ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140712f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1407133b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140713820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140713c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140714100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140714570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1407149e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140714e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1407152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140715730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140715ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140716010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x140716480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1407168f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x140716d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1407171d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x140717640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140717ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x140717f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x140718390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x140718800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x140718c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x1407191e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x1407196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x140719b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x140719fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14071a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14071a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14071ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14071b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14071b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14071ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14071bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14071c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14071c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14071cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14071d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14071d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14071d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14071dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14071e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14071e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14071eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14071efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14071f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14071f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14071fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140720160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1407205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140720a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140720eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x140721320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140721790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140721c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x140722070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1407224e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140722950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x140722dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140723230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1407236a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140723b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140723f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1407243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140724860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140724cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140725140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1407255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140725a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x140725e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x140726300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x140726770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x140726be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x140727050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1407274c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x140727930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x140727da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x140728210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x140728680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x140728af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x140728f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1407293d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x140729840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x140729cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14072a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14072a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14072aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14072ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14072b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14072b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14072bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14072c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14072c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14072c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14072cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14072d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14072d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14072dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14072df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14072e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14072e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14072ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14072f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14072f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14072f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14072fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1407302c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140730730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140730ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x140731010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140731480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1407318f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x140731d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1407321d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140732640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140732ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140732f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x140733390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140733800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140733c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1407340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140734550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1407349c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x140734e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1407352a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x140735710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x140735b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x140735ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x140736460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1407368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x140736d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1407371b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x140737620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x140738250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x140738510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1407387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x140738c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1407390b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x140739520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x140739990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x140739e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14073a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14073a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14073ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x14073afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14073b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14073b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14073bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14073c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14073c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14073ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14073ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14073d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14073d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14073dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14073e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14073e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14073e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14073ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14073f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14073f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14073fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14073ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140740410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140740880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140740cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140741160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1407415d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140741a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x140741fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1407424b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140742920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x140742d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x140743200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140743670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x140743b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1407440a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x140744c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x140744ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x140745490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x140745a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x140746010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1407465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x140746b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x140747150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x140747710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x140747cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x140748290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x140748850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x140748e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1407493d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x140749990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x140749f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x14074a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x14074aad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x14074b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x14074b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x14074bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x14074c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14074c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14074cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14074d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14074d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14074de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14074e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14074ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14074efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14074f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14074fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140750110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1407506d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x140750c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140751250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140751810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x140751dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140752390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140752950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x140752f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1407534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140753a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x140754050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140754610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140754bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x140755190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x140755750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x140755d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1407562d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x140756890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x140756e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x140757410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1407579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x140757f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x140758550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x140758b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1407590d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1407595d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x140759ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x140759fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14075a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14075a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14075aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14075b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x14075b8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x14075bdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x14075c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14075c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14075ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14075d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14075d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14075dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14075e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14075ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14075f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14075fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14075fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1407605f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1407608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140760ec0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x140608ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x140609360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1406097d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x140609c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14060a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14060a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14060a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14060ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14060b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14060b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14060bb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14060c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14060cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14060d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14060dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14060e480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14060eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14060f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14060f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x140610110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x140610830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x140610f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x140611670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x140611d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1406124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x140612770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x140612a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x140612ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x140613310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x140613780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x140613c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x140614190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x140614600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x1406148c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x140614d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1406151a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x140615700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x140615c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x140616100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x140616600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x140616b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x140617000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x140617500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x140617a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x140617f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x140618370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1406187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x140618c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1406190c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x140619530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1406199a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x140619e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14061a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14061a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14061ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14061b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14061b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14061ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14061c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14061c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14061cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14061d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14061d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14061db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14061dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14061e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14061e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14061ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14061f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14061f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14061fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x140620010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1406204b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x140620a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x140620f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1406214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1406219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x140621f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x140622490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1406229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x140622f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x140623480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1406239d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x140623f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x140624470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1406249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x140624f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x140625460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1406259b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x140625f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x140626450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1406269a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x140626ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x140627440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x140627990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x140627ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x140628430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x140628980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x140628ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x140629420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x140629970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x140629ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14062a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14062a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14062aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14062b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14062b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14062bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14062c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14062c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14062ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14062d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14062d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14062ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14062e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14062e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14062ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14062f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14062f4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14062f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14062fe30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1406302d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x140630770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x140630c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1406310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x140631550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1406319f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x140631e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x140632330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1406327d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x140632c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x140633110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1406335b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x140633a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x140633ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x140634390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x140634830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x140634cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x140635170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x140635610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x140635ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x140635f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1406363f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x140636890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x140636d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1406371d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x140637670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x140637b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x140637fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x140638450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1406388f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x140638d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x140639230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1406396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x140639b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14063a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14063a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14063a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14063adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14063b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14063b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14063bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14063c070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14063c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14063c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14063ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14063d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14063d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14063dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14063e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14063e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14063ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14063eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14063f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14063f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14063fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x140640130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1406405d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x140640a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x140640f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1406413b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x140641850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x140641cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x140642190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x140642630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x140642ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x140642f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x140643410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1406438b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x140643d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1406441f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x140644690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x140644b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x140645080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1406455d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x140645b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x140646070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x140646330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x140646940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x140646f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x140647560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x140647d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1406481f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1406484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x140648ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1406490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1406498c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x140649d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14064a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14064a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14064ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14064b3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x14064b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x14064be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x14064c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x14064c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x14064ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x14064d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x14064d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x14064de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x14064e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x14064e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x14064ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x14064f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x14064f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x14064fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x140650350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1406508a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x140650df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x140651340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x140651890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x140651de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x140652330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x140652880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x140652dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x140653320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x140653870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x140653dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x140654310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x140654860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x140654db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x140655300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x140655850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x140655da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1406562f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x140656840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x140656d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1406572e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x140657830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x140657d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1406582d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x140658820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x140658d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1406592c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x140659810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x140659d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14065a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14065a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14065ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14065b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14065b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x14065bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x14065c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x14065c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x14065cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x14065d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x14065d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x14065dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x14065e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x14065e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x14065ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x14065eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x14065f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x14065f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x14065fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x140660170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x140660610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x140660ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x140660f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1406613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x140661890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x140661d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x140662280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1406629a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1406630c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1406637e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x140663f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1406641c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1406649b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x140664c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x140665280 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.923s
user	0m0.242s
sys	0m0.136s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
