### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.32 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.16 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.16 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.46 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.28 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.21 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.66 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.22 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.08 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.60 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.21 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.21 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.15 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.19 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   17.54 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.25 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.06 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.22 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.29 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    2.90 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.24 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  103.67 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.84 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   25.77 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.33 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 164.74 sec*proc (29 tests)

Total Test time (real) = 164.75 sec

real	2m44.774s
user	4m36.945s
sys	0m5.715s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.15 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.14 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.11 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.90 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.21 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.22 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.86 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.18 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.21 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.44 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.45 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   24.47 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.30 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.07 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.20 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  48.44 sec*proc (29 tests)

Total Test time (real) =  48.45 sec

real	0m48.465s
user	0m54.921s
sys	0m5.194s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.120 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.130 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.022.877 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.022.885 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.888 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.022.889 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.890 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.022.891 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.022.892 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.022.894 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.022.894 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.022.898 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.022.898 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.022.899 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.022.902 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.022.906 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.022.908 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.022.908 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.022.909 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.022.910 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.022.911 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.028.077 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.029.437 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.029.439 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.029.440 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.029.440 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.029.441 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.029.441 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.029.442 I llama_model_loader: - type  f32:  124 tensors
0.00.029.442 I llama_model_loader: - type  f16:   73 tensors
0.00.029.443 I print_info: file format = GGUF V3 (latest)
0.00.029.444 I print_info: file type   = F16
0.00.029.445 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.034.149 I load: special tokens cache size = 5
0.00.036.393 I load: token to piece cache size = 0.2032 MB
0.00.036.398 I print_info: arch             = bert
0.00.036.399 I print_info: vocab_only       = 0
0.00.036.399 I print_info: n_ctx_train      = 512
0.00.036.399 I print_info: n_embd           = 384
0.00.036.400 I print_info: n_layer          = 12
0.00.036.403 I print_info: n_head           = 12
0.00.036.404 I print_info: n_head_kv        = 12
0.00.036.405 I print_info: n_rot            = 32
0.00.036.405 I print_info: n_swa            = 0
0.00.036.405 I print_info: n_embd_head_k    = 32
0.00.036.405 I print_info: n_embd_head_v    = 32
0.00.036.406 I print_info: n_gqa            = 1
0.00.036.407 I print_info: n_embd_k_gqa     = 384
0.00.036.410 I print_info: n_embd_v_gqa     = 384
0.00.036.411 I print_info: f_norm_eps       = 1.0e-12
0.00.036.412 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.036.412 I print_info: f_clamp_kqv      = 0.0e+00
0.00.036.412 I print_info: f_max_alibi_bias = 0.0e+00
0.00.036.413 I print_info: f_logit_scale    = 0.0e+00
0.00.036.413 I print_info: n_ff             = 1536
0.00.036.414 I print_info: n_expert         = 0
0.00.036.414 I print_info: n_expert_used    = 0
0.00.036.414 I print_info: causal attn      = 0
0.00.036.415 I print_info: pooling type     = 2
0.00.036.415 I print_info: rope type        = 2
0.00.036.415 I print_info: rope scaling     = linear
0.00.036.416 I print_info: freq_base_train  = 10000.0
0.00.036.418 I print_info: freq_scale_train = 1
0.00.036.419 I print_info: n_ctx_orig_yarn  = 512
0.00.036.419 I print_info: rope_finetuned   = unknown
0.00.036.419 I print_info: ssm_d_conv       = 0
0.00.036.419 I print_info: ssm_d_inner      = 0
0.00.036.419 I print_info: ssm_d_state      = 0
0.00.036.420 I print_info: ssm_dt_rank      = 0
0.00.036.420 I print_info: ssm_dt_b_c_rms   = 0
0.00.036.420 I print_info: model type       = 33M
0.00.036.421 I print_info: model params     = 33.21 M
0.00.036.421 I print_info: general.name     = Bge Small
0.00.036.422 I print_info: vocab type       = WPM
0.00.036.422 I print_info: n_vocab          = 30522
0.00.036.422 I print_info: n_merges         = 0
0.00.036.430 I print_info: BOS token        = 101 '[CLS]'
0.00.036.431 I print_info: UNK token        = 100 '[UNK]'
0.00.036.431 I print_info: SEP token        = 102 '[SEP]'
0.00.036.431 I print_info: PAD token        = 0 '[PAD]'
0.00.036.431 I print_info: MASK token       = 103 '[MASK]'
0.00.036.432 I print_info: LF token         = 0 '[PAD]'
0.00.036.432 I print_info: max token length = 21
0.00.036.435 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.039.817 I load_tensors: offloading 12 repeating layers to GPU
0.00.039.819 I load_tensors: offloading output layer to GPU
0.00.039.819 I load_tensors: offloaded 13/13 layers to GPU
0.00.039.845 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.847 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.133 I llama_init_from_model: n_seq_max     = 1
0.00.040.134 I llama_init_from_model: n_ctx         = 512
0.00.040.135 I llama_init_from_model: n_ctx_per_seq = 512
0.00.040.135 I llama_init_from_model: n_batch       = 2048
0.00.040.135 I llama_init_from_model: n_ubatch      = 2048
0.00.040.136 I llama_init_from_model: flash_attn    = 0
0.00.040.136 I llama_init_from_model: freq_base     = 10000.0
0.00.040.137 I llama_init_from_model: freq_scale    = 1
0.00.040.137 I ggml_metal_init: allocating
0.00.040.142 I ggml_metal_init: found device: Apple M4
0.00.040.147 I ggml_metal_init: picking default device: Apple M4
0.00.040.948 I ggml_metal_init: using embedded metal library
0.00.045.223 I ggml_metal_init: GPU name:   Apple M4
0.00.045.225 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.226 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.226 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.227 I ggml_metal_init: simdgroup reduction   = true
0.00.045.227 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.227 I ggml_metal_init: has residency sets    = true
0.00.045.227 I ggml_metal_init: has bfloat            = true
0.00.045.228 I ggml_metal_init: use bfloat            = true
0.00.045.228 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.229 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.058.487 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.059.177 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.059.180 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.059.201 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.060.421 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.060.422 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.060.423 I llama_init_from_model: graph nodes  = 429
0.00.060.423 I llama_init_from_model: graph splits = 2
0.00.060.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.060.424 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.070.253 I 
0.00.070.279 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.070.917 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.075.508 I llama_perf_context_print:        load time =      54.12 ms
0.00.075.509 I llama_perf_context_print: prompt eval time =       4.46 ms /     9 tokens (    0.50 ms per token,  2019.30 tokens per second)
0.00.075.510 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.075.510 I llama_perf_context_print:       total time =       5.26 ms /    10 tokens
0.00.075.680 I ggml_metal_free: deallocating

real	0m0.255s
user	0m0.065s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.046 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.069 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.699 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.702 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.704 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.704 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.705 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.705 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.705 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.706 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.707 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.707 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.707 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.709 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.712 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.712 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.712 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.713 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.713 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.713 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.985 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.628 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.629 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.629 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.629 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.630 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.630 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.630 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.631 I llama_model_loader: - type  f32:  124 tensors
0.00.014.631 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.632 I print_info: file format = GGUF V3 (latest)
0.00.014.632 I print_info: file type   = Q8_0
0.00.014.633 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.017.117 I load: special tokens cache size = 5
0.00.018.307 I load: token to piece cache size = 0.2032 MB
0.00.018.310 I print_info: arch             = bert
0.00.018.310 I print_info: vocab_only       = 0
0.00.018.311 I print_info: n_ctx_train      = 512
0.00.018.311 I print_info: n_embd           = 384
0.00.018.311 I print_info: n_layer          = 12
0.00.018.315 I print_info: n_head           = 12
0.00.018.316 I print_info: n_head_kv        = 12
0.00.018.316 I print_info: n_rot            = 32
0.00.018.316 I print_info: n_swa            = 0
0.00.018.316 I print_info: n_embd_head_k    = 32
0.00.018.316 I print_info: n_embd_head_v    = 32
0.00.018.317 I print_info: n_gqa            = 1
0.00.018.318 I print_info: n_embd_k_gqa     = 384
0.00.018.318 I print_info: n_embd_v_gqa     = 384
0.00.018.319 I print_info: f_norm_eps       = 1.0e-12
0.00.018.319 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.018.319 I print_info: f_clamp_kqv      = 0.0e+00
0.00.018.320 I print_info: f_max_alibi_bias = 0.0e+00
0.00.018.320 I print_info: f_logit_scale    = 0.0e+00
0.00.018.322 I print_info: n_ff             = 1536
0.00.018.322 I print_info: n_expert         = 0
0.00.018.322 I print_info: n_expert_used    = 0
0.00.018.322 I print_info: causal attn      = 0
0.00.018.323 I print_info: pooling type     = 2
0.00.018.323 I print_info: rope type        = 2
0.00.018.323 I print_info: rope scaling     = linear
0.00.018.323 I print_info: freq_base_train  = 10000.0
0.00.018.323 I print_info: freq_scale_train = 1
0.00.018.324 I print_info: n_ctx_orig_yarn  = 512
0.00.018.324 I print_info: rope_finetuned   = unknown
0.00.018.324 I print_info: ssm_d_conv       = 0
0.00.018.324 I print_info: ssm_d_inner      = 0
0.00.018.324 I print_info: ssm_d_state      = 0
0.00.018.326 I print_info: ssm_dt_rank      = 0
0.00.018.326 I print_info: ssm_dt_b_c_rms   = 0
0.00.018.326 I print_info: model type       = 33M
0.00.018.327 I print_info: model params     = 33.21 M
0.00.018.327 I print_info: general.name     = Bge Small
0.00.018.327 I print_info: vocab type       = WPM
0.00.018.327 I print_info: n_vocab          = 30522
0.00.018.327 I print_info: n_merges         = 0
0.00.018.328 I print_info: BOS token        = 101 '[CLS]'
0.00.018.328 I print_info: UNK token        = 100 '[UNK]'
0.00.018.328 I print_info: SEP token        = 102 '[SEP]'
0.00.018.328 I print_info: PAD token        = 0 '[PAD]'
0.00.018.328 I print_info: MASK token       = 103 '[MASK]'
0.00.018.328 I print_info: LF token         = 0 '[PAD]'
0.00.018.329 I print_info: max token length = 21
0.00.018.329 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.020.109 I load_tensors: offloading 12 repeating layers to GPU
0.00.020.110 I load_tensors: offloading output layer to GPU
0.00.020.110 I load_tensors: offloaded 13/13 layers to GPU
0.00.020.116 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.117 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.020.284 I llama_init_from_model: n_seq_max     = 1
0.00.020.285 I llama_init_from_model: n_ctx         = 512
0.00.020.285 I llama_init_from_model: n_ctx_per_seq = 512
0.00.020.286 I llama_init_from_model: n_batch       = 2048
0.00.020.286 I llama_init_from_model: n_ubatch      = 2048
0.00.020.286 I llama_init_from_model: flash_attn    = 0
0.00.020.286 I llama_init_from_model: freq_base     = 10000.0
0.00.020.286 I llama_init_from_model: freq_scale    = 1
0.00.020.287 I ggml_metal_init: allocating
0.00.020.290 I ggml_metal_init: found device: Apple M4
0.00.020.293 I ggml_metal_init: picking default device: Apple M4
0.00.020.814 I ggml_metal_init: using embedded metal library
0.00.023.219 I ggml_metal_init: GPU name:   Apple M4
0.00.023.221 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.222 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.222 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.222 I ggml_metal_init: simdgroup reduction   = true
0.00.023.222 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.222 I ggml_metal_init: has residency sets    = true
0.00.023.223 I ggml_metal_init: has bfloat            = true
0.00.023.223 I ggml_metal_init: use bfloat            = true
0.00.023.223 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.224 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.034.039 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.034.655 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.034.657 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.034.671 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.035.705 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.035.707 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.035.707 I llama_init_from_model: graph nodes  = 429
0.00.035.707 I llama_init_from_model: graph splits = 2
0.00.035.709 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.035.709 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.039.835 I 
0.00.039.861 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.040.415 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.044.858 I llama_perf_context_print:        load time =      30.76 ms
0.00.044.859 I llama_perf_context_print: prompt eval time =       4.33 ms /     9 tokens (    0.48 ms per token,  2080.44 tokens per second)
0.00.044.860 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.044.861 I llama_perf_context_print:       total time =       5.02 ms /    10 tokens
0.00.045.036 I ggml_metal_free: deallocating

real	0m0.057s
user	0m0.031s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.230 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.639 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.036.509 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.514 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.516 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.036.517 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.518 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.036.519 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.036.519 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.036.521 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.036.522 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.036.522 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.036.526 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.036.527 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.036.530 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.036.531 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.036.532 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.036.532 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.533 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.043.412 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.045.801 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.209 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.050.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.211 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.050.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.050.212 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.050.212 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.050.212 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.050.213 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.050.213 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.050.214 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.050.214 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.050.214 I llama_model_loader: - type  f32:   40 tensors
0.00.050.215 I llama_model_loader: - type  f16:   30 tensors
0.00.050.215 I print_info: file format = GGUF V3 (latest)
0.00.050.216 I print_info: file type   = F16
0.00.050.217 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.054.423 W load: empty token at index 5
0.00.059.656 W load: model vocab missing newline token, using special_pad_id instead
0.00.061.253 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.061.290 I load: special tokens cache size = 5
0.00.318.808 I load: token to piece cache size = 1.5060 MB
0.00.318.815 I print_info: arch             = jina-bert-v2
0.00.318.815 I print_info: vocab_only       = 0
0.00.318.815 I print_info: n_ctx_train      = 8192
0.00.318.816 I print_info: n_embd           = 384
0.00.318.816 I print_info: n_layer          = 4
0.00.318.822 I print_info: n_head           = 12
0.00.318.823 I print_info: n_head_kv        = 12
0.00.318.824 I print_info: n_rot            = 32
0.00.318.824 I print_info: n_swa            = 0
0.00.318.825 I print_info: n_embd_head_k    = 32
0.00.318.825 I print_info: n_embd_head_v    = 32
0.00.318.826 I print_info: n_gqa            = 1
0.00.318.827 I print_info: n_embd_k_gqa     = 384
0.00.318.827 I print_info: n_embd_v_gqa     = 384
0.00.318.828 I print_info: f_norm_eps       = 1.0e-12
0.00.318.828 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.318.831 I print_info: f_clamp_kqv      = 0.0e+00
0.00.318.833 I print_info: f_max_alibi_bias = 8.0e+00
0.00.318.833 I print_info: f_logit_scale    = 0.0e+00
0.00.318.834 I print_info: n_ff             = 1536
0.00.318.834 I print_info: n_expert         = 0
0.00.318.834 I print_info: n_expert_used    = 0
0.00.318.834 I print_info: causal attn      = 0
0.00.318.834 I print_info: pooling type     = -1
0.00.318.834 I print_info: rope type        = -1
0.00.318.834 I print_info: rope scaling     = linear
0.00.318.836 I print_info: freq_base_train  = 10000.0
0.00.318.836 I print_info: freq_scale_train = 1
0.00.318.836 I print_info: n_ctx_orig_yarn  = 8192
0.00.318.837 I print_info: rope_finetuned   = unknown
0.00.318.837 I print_info: ssm_d_conv       = 0
0.00.318.837 I print_info: ssm_d_inner      = 0
0.00.318.837 I print_info: ssm_d_state      = 0
0.00.318.837 I print_info: ssm_dt_rank      = 0
0.00.318.837 I print_info: ssm_dt_b_c_rms   = 0
0.00.318.838 I print_info: model type       = 33M
0.00.318.838 I print_info: model params     = 32.90 M
0.00.318.839 I print_info: general.name     = Jina Bert Implementation
0.00.318.839 I print_info: vocab type       = BPE
0.00.318.839 I print_info: n_vocab          = 61056
0.00.318.839 I print_info: n_merges         = 39382
0.00.318.841 I print_info: BOS token        = 0 '<s>'
0.00.318.841 I print_info: EOS token        = 2 '</s>'
0.00.318.841 I print_info: UNK token        = 3 '<unk>'
0.00.318.842 I print_info: SEP token        = 2 '</s>'
0.00.318.843 I print_info: PAD token        = 1 '<pad>'
0.00.318.843 I print_info: MASK token       = 4 '<mask>'
0.00.318.843 I print_info: EOG token        = 2 '</s>'
0.00.318.843 I print_info: max token length = 45
0.00.318.844 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.321.069 I load_tensors: offloading 4 repeating layers to GPU
0.00.321.070 I load_tensors: offloading output layer to GPU
0.00.321.070 I load_tensors: offloaded 5/5 layers to GPU
0.00.321.094 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.321.095 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.321.409 I llama_init_from_model: n_seq_max     = 1
0.00.321.409 I llama_init_from_model: n_ctx         = 8192
0.00.321.410 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.321.410 I llama_init_from_model: n_batch       = 2048
0.00.321.410 I llama_init_from_model: n_ubatch      = 2048
0.00.321.410 I llama_init_from_model: flash_attn    = 0
0.00.321.410 I llama_init_from_model: freq_base     = 10000.0
0.00.321.411 I llama_init_from_model: freq_scale    = 1
0.00.321.411 I ggml_metal_init: allocating
0.00.321.415 I ggml_metal_init: found device: Apple M4
0.00.321.418 I ggml_metal_init: picking default device: Apple M4
0.00.321.979 I ggml_metal_init: using embedded metal library
0.00.339.289 I ggml_metal_init: GPU name:   Apple M4
0.00.339.291 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.339.291 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.339.292 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.339.292 I ggml_metal_init: simdgroup reduction   = true
0.00.339.292 I ggml_metal_init: simdgroup matrix mul. = true
0.00.339.292 I ggml_metal_init: has residency sets    = true
0.00.339.292 I ggml_metal_init: has bfloat            = true
0.00.339.292 I ggml_metal_init: use bfloat            = true
0.00.339.293 I ggml_metal_init: hasUnifiedMemory      = true
0.00.339.294 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.351.459 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.354.686 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.354.688 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.354.708 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.361.046 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.361.047 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.361.048 I llama_init_from_model: graph nodes  = 154
0.00.361.048 I llama_init_from_model: graph splits = 2
0.00.361.049 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.361.049 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.368.591 I 
0.00.368.629 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.369.038 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.369.038 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.369.053 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.369.053 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.369.059 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.369.059 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.369.572 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.373.092 I llama_perf_context_print:        load time =     344.94 ms
0.00.373.094 I llama_perf_context_print: prompt eval time =       3.51 ms /    62 tokens (    0.06 ms per token, 17658.79 tokens per second)
0.00.373.094 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.373.095 I llama_perf_context_print:       total time =       4.50 ms /    63 tokens
0.00.373.370 I ggml_metal_free: deallocating

real	0m1.075s
user	0m0.324s
sys	0m0.047s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.190 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.388 I main: llama backend init
0.00.000.394 I main: load the model and apply lora adapter, if any
0.00.047.998 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.060.785 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.806 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.807 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.808 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.808 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.809 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.811 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.812 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.812 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.818 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.822 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.823 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.824 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.828 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.829 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.842 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.070.026 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.078.693 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.078.702 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.078.703 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.078.703 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.078.704 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.078.705 I llama_model_loader: - type  f32:  194 tensors
0.00.078.705 I llama_model_loader: - type  f16:   98 tensors
0.00.078.707 I print_info: file format = GGUF V3 (latest)
0.00.078.715 I print_info: file type   = all F32 (guessed)
0.00.078.717 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.783 I load: special tokens cache size = 25
0.00.104.302 I load: token to piece cache size = 0.2984 MB
0.00.104.306 I print_info: arch             = gptneox
0.00.104.307 I print_info: vocab_only       = 0
0.00.104.307 I print_info: n_ctx_train      = 2048
0.00.104.307 I print_info: n_embd           = 2048
0.00.104.307 I print_info: n_layer          = 24
0.00.104.311 I print_info: n_head           = 16
0.00.104.312 I print_info: n_head_kv        = 16
0.00.104.312 I print_info: n_rot            = 32
0.00.104.313 I print_info: n_swa            = 0
0.00.104.313 I print_info: n_embd_head_k    = 128
0.00.104.313 I print_info: n_embd_head_v    = 128
0.00.104.314 I print_info: n_gqa            = 1
0.00.104.315 I print_info: n_embd_k_gqa     = 2048
0.00.104.316 I print_info: n_embd_v_gqa     = 2048
0.00.104.316 I print_info: f_norm_eps       = 1.0e-05
0.00.104.317 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.104.317 I print_info: f_clamp_kqv      = 0.0e+00
0.00.104.317 I print_info: f_max_alibi_bias = 0.0e+00
0.00.104.318 I print_info: f_logit_scale    = 0.0e+00
0.00.104.319 I print_info: n_ff             = 8192
0.00.104.319 I print_info: n_expert         = 0
0.00.104.319 I print_info: n_expert_used    = 0
0.00.104.319 I print_info: causal attn      = 1
0.00.104.319 I print_info: pooling type     = 0
0.00.104.319 I print_info: rope type        = 2
0.00.104.320 I print_info: rope scaling     = linear
0.00.104.320 I print_info: freq_base_train  = 10000.0
0.00.104.321 I print_info: freq_scale_train = 1
0.00.104.321 I print_info: n_ctx_orig_yarn  = 2048
0.00.104.321 I print_info: rope_finetuned   = unknown
0.00.104.321 I print_info: ssm_d_conv       = 0
0.00.104.322 I print_info: ssm_d_inner      = 0
0.00.104.322 I print_info: ssm_d_state      = 0
0.00.104.322 I print_info: ssm_dt_rank      = 0
0.00.104.322 I print_info: ssm_dt_b_c_rms   = 0
0.00.104.322 I print_info: model type       = 1.4B
0.00.104.323 I print_info: model params     = 1.41 B
0.00.104.323 I print_info: general.name     = 1.4B
0.00.104.323 I print_info: vocab type       = BPE
0.00.104.324 I print_info: n_vocab          = 50304
0.00.104.324 I print_info: n_merges         = 50009
0.00.104.324 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.104.325 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.104.326 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.104.326 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.104.326 I print_info: LF token         = 187 ''
0.00.104.326 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.104.327 I print_info: max token length = 1024
0.00.104.327 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.148.514 I load_tensors: offloading 24 repeating layers to GPU
0.00.148.516 I load_tensors: offloading output layer to GPU
0.00.148.516 I load_tensors: offloaded 25/25 layers to GPU
0.00.148.539 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.148.540 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.149.107 I llama_init_from_model: n_seq_max     = 1
0.00.149.108 I llama_init_from_model: n_ctx         = 2048
0.00.149.108 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.149.108 I llama_init_from_model: n_batch       = 2048
0.00.149.108 I llama_init_from_model: n_ubatch      = 512
0.00.149.108 I llama_init_from_model: flash_attn    = 0
0.00.149.109 I llama_init_from_model: freq_base     = 10000.0
0.00.149.109 I llama_init_from_model: freq_scale    = 1
0.00.149.110 I ggml_metal_init: allocating
0.00.149.149 I ggml_metal_init: found device: Apple M4
0.00.149.156 I ggml_metal_init: picking default device: Apple M4
0.00.149.802 I ggml_metal_init: using embedded metal library
0.00.173.531 I ggml_metal_init: GPU name:   Apple M4
0.00.173.533 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.173.534 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.173.534 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.173.535 I ggml_metal_init: simdgroup reduction   = true
0.00.173.535 I ggml_metal_init: simdgroup matrix mul. = true
0.00.173.535 I ggml_metal_init: has residency sets    = true
0.00.173.535 I ggml_metal_init: has bfloat            = true
0.00.173.535 I ggml_metal_init: use bfloat            = true
0.00.173.536 I ggml_metal_init: hasUnifiedMemory      = true
0.00.173.537 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.333.774 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.363.168 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.363.175 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.363.218 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.367.406 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.367.408 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.367.408 I llama_init_from_model: graph nodes  = 967
0.00.367.409 I llama_init_from_model: graph splits = 2
0.00.367.415 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.367.545 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.367.545 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.434.874 I main: llama threadpool init, n_threads = 4
0.00.434.918 I 
0.00.434.949 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.434.950 I 
0.00.435.131 I sampler seed: 1234
0.00.435.135 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.435.160 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.435.161 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.435.162 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.262.713 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59865.09 tokens per second)
0.02.262.714 I llama_perf_context_print:        load time =     385.97 ms
0.02.262.714 I llama_perf_context_print: prompt eval time =      43.68 ms /     7 tokens (    6.24 ms per token,   160.25 tokens per second)
0.02.262.715 I llama_perf_context_print:        eval time =    1781.00 ms /    63 runs   (   28.27 ms per token,    35.37 tokens per second)
0.02.262.716 I llama_perf_context_print:       total time =    1828.74 ms /    70 tokens
0.02.262.989 I ggml_metal_free: deallocating

real	0m2.635s
user	0m0.135s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.546 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.650 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.039.087 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.039.093 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.039.095 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.039.096 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.039.096 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.039.099 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.039.099 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.039.100 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.039.100 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.039.101 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.039.101 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.039.102 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.039.102 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.039.103 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.039.105 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.039.105 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.039.106 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.933 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.985 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.811 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.055.813 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.813 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.814 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.814 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.815 I llama_model_loader: - type  f32:  194 tensors
0.00.055.815 I llama_model_loader: - type  f16:   98 tensors
0.00.055.816 I print_info: file format = GGUF V3 (latest)
0.00.055.817 I print_info: file type   = all F32 (guessed)
0.00.055.818 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.067.726 I load: special tokens cache size = 25
0.00.075.538 I load: token to piece cache size = 0.2984 MB
0.00.075.541 I print_info: arch             = gptneox
0.00.075.542 I print_info: vocab_only       = 0
0.00.075.542 I print_info: n_ctx_train      = 2048
0.00.075.542 I print_info: n_embd           = 2048
0.00.075.542 I print_info: n_layer          = 24
0.00.075.546 I print_info: n_head           = 16
0.00.075.547 I print_info: n_head_kv        = 16
0.00.075.547 I print_info: n_rot            = 32
0.00.075.547 I print_info: n_swa            = 0
0.00.075.547 I print_info: n_embd_head_k    = 128
0.00.075.548 I print_info: n_embd_head_v    = 128
0.00.075.549 I print_info: n_gqa            = 1
0.00.075.550 I print_info: n_embd_k_gqa     = 2048
0.00.075.551 I print_info: n_embd_v_gqa     = 2048
0.00.075.551 I print_info: f_norm_eps       = 1.0e-05
0.00.075.552 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.075.552 I print_info: f_clamp_kqv      = 0.0e+00
0.00.075.552 I print_info: f_max_alibi_bias = 0.0e+00
0.00.075.552 I print_info: f_logit_scale    = 0.0e+00
0.00.075.553 I print_info: n_ff             = 8192
0.00.075.553 I print_info: n_expert         = 0
0.00.075.553 I print_info: n_expert_used    = 0
0.00.075.553 I print_info: causal attn      = 1
0.00.075.554 I print_info: pooling type     = 0
0.00.075.554 I print_info: rope type        = 2
0.00.075.556 I print_info: rope scaling     = linear
0.00.075.556 I print_info: freq_base_train  = 10000.0
0.00.075.556 I print_info: freq_scale_train = 1
0.00.075.557 I print_info: n_ctx_orig_yarn  = 2048
0.00.075.557 I print_info: rope_finetuned   = unknown
0.00.075.557 I print_info: ssm_d_conv       = 0
0.00.075.557 I print_info: ssm_d_inner      = 0
0.00.075.557 I print_info: ssm_d_state      = 0
0.00.075.557 I print_info: ssm_dt_rank      = 0
0.00.075.558 I print_info: ssm_dt_b_c_rms   = 0
0.00.075.558 I print_info: model type       = 1.4B
0.00.075.558 I print_info: model params     = 1.41 B
0.00.075.559 I print_info: general.name     = 1.4B
0.00.075.560 I print_info: vocab type       = BPE
0.00.075.560 I print_info: n_vocab          = 50304
0.00.075.560 I print_info: n_merges         = 50009
0.00.075.560 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.075.561 I print_info: LF token         = 187 ''
0.00.075.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.075.562 I print_info: max token length = 1024
0.00.075.562 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.260.388 I load_tensors: offloading 24 repeating layers to GPU
0.01.260.391 I load_tensors: offloading output layer to GPU
0.01.260.392 I load_tensors: offloaded 25/25 layers to GPU
0.01.260.416 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.260.418 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.261.433 I llama_init_from_model: n_seq_max     = 1
0.01.261.435 I llama_init_from_model: n_ctx         = 128
0.01.261.435 I llama_init_from_model: n_ctx_per_seq = 128
0.01.261.435 I llama_init_from_model: n_batch       = 128
0.01.261.436 I llama_init_from_model: n_ubatch      = 128
0.01.261.436 I llama_init_from_model: flash_attn    = 0
0.01.261.437 I llama_init_from_model: freq_base     = 10000.0
0.01.261.437 I llama_init_from_model: freq_scale    = 1
0.01.261.437 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.261.438 I ggml_metal_init: allocating
0.01.261.492 I ggml_metal_init: found device: Apple M4
0.01.261.500 I ggml_metal_init: picking default device: Apple M4
0.01.262.638 I ggml_metal_init: using embedded metal library
0.01.266.646 I ggml_metal_init: GPU name:   Apple M4
0.01.266.648 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.266.648 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.266.649 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.266.649 I ggml_metal_init: simdgroup reduction   = true
0.01.266.649 I ggml_metal_init: simdgroup matrix mul. = true
0.01.266.649 I ggml_metal_init: has residency sets    = true
0.01.266.649 I ggml_metal_init: has bfloat            = true
0.01.266.649 I ggml_metal_init: use bfloat            = true
0.01.266.650 I ggml_metal_init: hasUnifiedMemory      = true
0.01.266.651 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.278.108 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.279.892 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.279.896 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.279.923 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.281.594 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.281.595 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.281.596 I llama_init_from_model: graph nodes  = 967
0.01.281.596 I llama_init_from_model: graph splits = 2
0.01.281.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.281.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.318.199 I 
0.01.318.235 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.318.239 I perplexity: tokenizing the input ..
0.01.323.601 I perplexity: tokenization took 5.36 ms
0.01.323.605 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.442.619 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.443.931 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.443.966 I llama_perf_context_print:        load time =    1293.54 ms
0.01.443.967 I llama_perf_context_print: prompt eval time =     118.66 ms /   128 tokens (    0.93 ms per token,  1078.69 tokens per second)
0.01.443.967 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.443.968 I llama_perf_context_print:       total time =     125.77 ms /   129 tokens
0.01.444.337 I ggml_metal_free: deallocating

real	0m1.632s
user	0m0.097s
sys	0m0.252s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.052 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.094 I main: llama backend init
0.00.000.096 I main: load the model and apply lora adapter, if any
0.00.009.846 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.238 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.239 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.239 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.241 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.241 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.241 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.242 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.242 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.242 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.243 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.245 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.245 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.245 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.022 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.160 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.875 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.876 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.877 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.877 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.877 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.878 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.878 I llama_model_loader: - type  f32:  194 tensors
0.00.026.879 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.880 I print_info: file format = GGUF V3 (latest)
0.00.026.880 I print_info: file type   = Q8_0
0.00.026.881 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.034.997 I load: special tokens cache size = 25
0.00.041.121 I load: token to piece cache size = 0.2984 MB
0.00.041.127 I print_info: arch             = gptneox
0.00.041.127 I print_info: vocab_only       = 0
0.00.041.128 I print_info: n_ctx_train      = 2048
0.00.041.128 I print_info: n_embd           = 2048
0.00.041.128 I print_info: n_layer          = 24
0.00.041.134 I print_info: n_head           = 16
0.00.041.135 I print_info: n_head_kv        = 16
0.00.041.135 I print_info: n_rot            = 32
0.00.041.135 I print_info: n_swa            = 0
0.00.041.135 I print_info: n_embd_head_k    = 128
0.00.041.135 I print_info: n_embd_head_v    = 128
0.00.041.136 I print_info: n_gqa            = 1
0.00.041.136 I print_info: n_embd_k_gqa     = 2048
0.00.041.137 I print_info: n_embd_v_gqa     = 2048
0.00.041.137 I print_info: f_norm_eps       = 1.0e-05
0.00.041.138 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.138 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.138 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.138 I print_info: f_logit_scale    = 0.0e+00
0.00.041.139 I print_info: n_ff             = 8192
0.00.041.139 I print_info: n_expert         = 0
0.00.041.139 I print_info: n_expert_used    = 0
0.00.041.143 I print_info: causal attn      = 1
0.00.041.143 I print_info: pooling type     = 0
0.00.041.143 I print_info: rope type        = 2
0.00.041.143 I print_info: rope scaling     = linear
0.00.041.143 I print_info: freq_base_train  = 10000.0
0.00.041.144 I print_info: freq_scale_train = 1
0.00.041.144 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.144 I print_info: rope_finetuned   = unknown
0.00.041.144 I print_info: ssm_d_conv       = 0
0.00.041.145 I print_info: ssm_d_inner      = 0
0.00.041.145 I print_info: ssm_d_state      = 0
0.00.041.145 I print_info: ssm_dt_rank      = 0
0.00.041.146 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.147 I print_info: model type       = 1.4B
0.00.041.147 I print_info: model params     = 1.41 B
0.00.041.149 I print_info: general.name     = 1.4B
0.00.041.149 I print_info: vocab type       = BPE
0.00.041.150 I print_info: n_vocab          = 50304
0.00.041.150 I print_info: n_merges         = 50009
0.00.041.150 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.150 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.150 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.150 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.151 I print_info: LF token         = 187 ''
0.00.041.151 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.151 I print_info: max token length = 1024
0.00.041.151 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.917.846 I load_tensors: offloading 24 repeating layers to GPU
0.00.917.854 I load_tensors: offloading output layer to GPU
0.00.917.855 I load_tensors: offloaded 25/25 layers to GPU
0.00.917.879 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.917.881 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.918.739 I llama_init_from_model: n_seq_max     = 1
0.00.918.741 I llama_init_from_model: n_ctx         = 2048
0.00.918.741 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.918.742 I llama_init_from_model: n_batch       = 2048
0.00.918.742 I llama_init_from_model: n_ubatch      = 512
0.00.918.743 I llama_init_from_model: flash_attn    = 0
0.00.918.743 I llama_init_from_model: freq_base     = 10000.0
0.00.918.744 I llama_init_from_model: freq_scale    = 1
0.00.918.745 I ggml_metal_init: allocating
0.00.918.770 I ggml_metal_init: found device: Apple M4
0.00.918.778 I ggml_metal_init: picking default device: Apple M4
0.00.920.085 I ggml_metal_init: using embedded metal library
0.00.925.764 I ggml_metal_init: GPU name:   Apple M4
0.00.925.767 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.925.767 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.925.768 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.925.768 I ggml_metal_init: simdgroup reduction   = true
0.00.925.768 I ggml_metal_init: simdgroup matrix mul. = true
0.00.925.769 I ggml_metal_init: has residency sets    = true
0.00.925.769 I ggml_metal_init: has bfloat            = true
0.00.925.769 I ggml_metal_init: use bfloat            = true
0.00.925.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.925.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.941.705 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.998.890 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.998.896 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.998.939 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.004.177 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.004.179 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.004.179 I llama_init_from_model: graph nodes  = 967
0.01.004.179 I llama_init_from_model: graph splits = 2
0.01.004.186 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.004.316 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.004.317 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.061.630 I main: llama threadpool init, n_threads = 4
0.01.061.673 I 
0.01.061.697 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.061.697 I 
0.01.061.852 I sampler seed: 1234
0.01.061.857 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.061.893 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.061.896 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.061.896 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.154.307 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55339.05 tokens per second)
0.02.154.308 I llama_perf_context_print:        load time =    1051.05 ms
0.02.154.309 I llama_perf_context_print: prompt eval time =      49.22 ms /     7 tokens (    7.03 ms per token,   142.21 tokens per second)
0.02.154.309 I llama_perf_context_print:        eval time =    1040.35 ms /    63 runs   (   16.51 ms per token,    60.56 tokens per second)
0.02.154.310 I llama_perf_context_print:       total time =    1093.41 ms /    70 tokens
0.02.154.542 I ggml_metal_free: deallocating

real	0m2.173s
user	0m0.108s
sys	0m0.267s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.213 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.312 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.320 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.325 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.326 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.326 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.326 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.327 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.328 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.328 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.329 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.329 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.329 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.330 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.332 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.332 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.332 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.185 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.290 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.175 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.177 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.178 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.179 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.179 I llama_model_loader: - type  f32:  194 tensors
0.00.025.179 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.180 I print_info: file format = GGUF V3 (latest)
0.00.025.181 I print_info: file type   = Q8_0
0.00.025.182 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.033.544 I load: special tokens cache size = 25
0.00.039.321 I load: token to piece cache size = 0.2984 MB
0.00.039.325 I print_info: arch             = gptneox
0.00.039.326 I print_info: vocab_only       = 0
0.00.039.326 I print_info: n_ctx_train      = 2048
0.00.039.326 I print_info: n_embd           = 2048
0.00.039.326 I print_info: n_layer          = 24
0.00.039.330 I print_info: n_head           = 16
0.00.039.331 I print_info: n_head_kv        = 16
0.00.039.331 I print_info: n_rot            = 32
0.00.039.331 I print_info: n_swa            = 0
0.00.039.332 I print_info: n_embd_head_k    = 128
0.00.039.332 I print_info: n_embd_head_v    = 128
0.00.039.333 I print_info: n_gqa            = 1
0.00.039.333 I print_info: n_embd_k_gqa     = 2048
0.00.039.334 I print_info: n_embd_v_gqa     = 2048
0.00.039.335 I print_info: f_norm_eps       = 1.0e-05
0.00.039.335 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.335 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.335 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.335 I print_info: f_logit_scale    = 0.0e+00
0.00.039.336 I print_info: n_ff             = 8192
0.00.039.336 I print_info: n_expert         = 0
0.00.039.337 I print_info: n_expert_used    = 0
0.00.039.338 I print_info: causal attn      = 1
0.00.039.338 I print_info: pooling type     = 0
0.00.039.338 I print_info: rope type        = 2
0.00.039.338 I print_info: rope scaling     = linear
0.00.039.338 I print_info: freq_base_train  = 10000.0
0.00.039.339 I print_info: freq_scale_train = 1
0.00.039.339 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.339 I print_info: rope_finetuned   = unknown
0.00.039.339 I print_info: ssm_d_conv       = 0
0.00.039.339 I print_info: ssm_d_inner      = 0
0.00.039.340 I print_info: ssm_d_state      = 0
0.00.039.340 I print_info: ssm_dt_rank      = 0
0.00.039.341 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.342 I print_info: model type       = 1.4B
0.00.039.342 I print_info: model params     = 1.41 B
0.00.039.342 I print_info: general.name     = 1.4B
0.00.039.343 I print_info: vocab type       = BPE
0.00.039.343 I print_info: n_vocab          = 50304
0.00.039.343 I print_info: n_merges         = 50009
0.00.039.343 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.343 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: LF token         = 187 ''
0.00.039.344 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.344 I print_info: max token length = 1024
0.00.039.345 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.928.015 I load_tensors: offloading 24 repeating layers to GPU
0.00.928.023 I load_tensors: offloading output layer to GPU
0.00.928.024 I load_tensors: offloaded 25/25 layers to GPU
0.00.928.046 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.928.047 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.928.786 I llama_init_from_model: n_seq_max     = 1
0.00.928.789 I llama_init_from_model: n_ctx         = 128
0.00.928.789 I llama_init_from_model: n_ctx_per_seq = 128
0.00.928.789 I llama_init_from_model: n_batch       = 128
0.00.928.790 I llama_init_from_model: n_ubatch      = 128
0.00.928.790 I llama_init_from_model: flash_attn    = 0
0.00.928.791 I llama_init_from_model: freq_base     = 10000.0
0.00.928.792 I llama_init_from_model: freq_scale    = 1
0.00.928.792 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.928.793 I ggml_metal_init: allocating
0.00.928.846 I ggml_metal_init: found device: Apple M4
0.00.928.857 I ggml_metal_init: picking default device: Apple M4
0.00.929.968 I ggml_metal_init: using embedded metal library
0.00.934.269 I ggml_metal_init: GPU name:   Apple M4
0.00.934.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.934.279 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.934.279 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.934.280 I ggml_metal_init: simdgroup reduction   = true
0.00.934.280 I ggml_metal_init: simdgroup matrix mul. = true
0.00.934.280 I ggml_metal_init: has residency sets    = true
0.00.934.281 I ggml_metal_init: has bfloat            = true
0.00.934.281 I ggml_metal_init: use bfloat            = true
0.00.934.282 I ggml_metal_init: hasUnifiedMemory      = true
0.00.934.285 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.945.705 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.947.350 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.947.352 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.947.406 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.948.923 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.948.925 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.948.925 I llama_init_from_model: graph nodes  = 967
0.00.948.925 I llama_init_from_model: graph splits = 2
0.00.948.926 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.948.927 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.971.171 I 
0.00.971.208 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.971.211 I perplexity: tokenizing the input ..
0.00.975.152 I perplexity: tokenization took 3.938 ms
0.00.975.155 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.099.309 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.103.979 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.104.018 I llama_perf_context_print:        load time =     961.95 ms
0.01.104.021 I llama_perf_context_print: prompt eval time =     123.92 ms /   128 tokens (    0.97 ms per token,  1032.93 tokens per second)
0.01.104.023 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.104.024 I llama_perf_context_print:       total time =     132.85 ms /   129 tokens
0.01.104.713 I ggml_metal_free: deallocating

real	0m1.124s
user	0m0.088s
sys	0m0.138s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.093 I main: llama backend init
0.00.000.095 I main: load the model and apply lora adapter, if any
0.00.011.272 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.045 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.050 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.053 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.053 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.054 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.054 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.055 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.056 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.056 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.057 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.058 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.058 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.058 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.059 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.062 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.062 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.063 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.877 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.008 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.725 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.726 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.726 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.727 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.727 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.727 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.728 I llama_model_loader: - type  f32:  194 tensors
0.00.027.728 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.728 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.729 I print_info: file format = GGUF V3 (latest)
0.00.027.730 I print_info: file type   = Q4_0
0.00.027.733 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.035.801 I load: special tokens cache size = 25
0.00.041.877 I load: token to piece cache size = 0.2984 MB
0.00.041.880 I print_info: arch             = gptneox
0.00.041.880 I print_info: vocab_only       = 0
0.00.041.880 I print_info: n_ctx_train      = 2048
0.00.041.880 I print_info: n_embd           = 2048
0.00.041.881 I print_info: n_layer          = 24
0.00.041.885 I print_info: n_head           = 16
0.00.041.886 I print_info: n_head_kv        = 16
0.00.041.886 I print_info: n_rot            = 32
0.00.041.886 I print_info: n_swa            = 0
0.00.041.886 I print_info: n_embd_head_k    = 128
0.00.041.886 I print_info: n_embd_head_v    = 128
0.00.041.887 I print_info: n_gqa            = 1
0.00.041.893 I print_info: n_embd_k_gqa     = 2048
0.00.041.895 I print_info: n_embd_v_gqa     = 2048
0.00.041.896 I print_info: f_norm_eps       = 1.0e-05
0.00.041.896 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.896 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.897 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.897 I print_info: f_logit_scale    = 0.0e+00
0.00.041.898 I print_info: n_ff             = 8192
0.00.041.898 I print_info: n_expert         = 0
0.00.041.898 I print_info: n_expert_used    = 0
0.00.041.898 I print_info: causal attn      = 1
0.00.041.898 I print_info: pooling type     = 0
0.00.041.898 I print_info: rope type        = 2
0.00.041.898 I print_info: rope scaling     = linear
0.00.041.899 I print_info: freq_base_train  = 10000.0
0.00.041.899 I print_info: freq_scale_train = 1
0.00.041.902 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.903 I print_info: rope_finetuned   = unknown
0.00.041.903 I print_info: ssm_d_conv       = 0
0.00.041.903 I print_info: ssm_d_inner      = 0
0.00.041.903 I print_info: ssm_d_state      = 0
0.00.041.903 I print_info: ssm_dt_rank      = 0
0.00.041.903 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.904 I print_info: model type       = 1.4B
0.00.041.904 I print_info: model params     = 1.41 B
0.00.041.904 I print_info: general.name     = 1.4B
0.00.041.905 I print_info: vocab type       = BPE
0.00.041.905 I print_info: n_vocab          = 50304
0.00.041.905 I print_info: n_merges         = 50009
0.00.041.905 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.905 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.906 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.906 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.906 I print_info: LF token         = 187 ''
0.00.041.906 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.906 I print_info: max token length = 1024
0.00.041.907 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.076 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.094 I load_tensors: offloading output layer to GPU
0.00.585.095 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.129 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.585.131 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.812 I llama_init_from_model: n_seq_max     = 1
0.00.586.814 I llama_init_from_model: n_ctx         = 2048
0.00.586.814 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.586.815 I llama_init_from_model: n_batch       = 2048
0.00.586.815 I llama_init_from_model: n_ubatch      = 512
0.00.586.816 I llama_init_from_model: flash_attn    = 0
0.00.586.819 I llama_init_from_model: freq_base     = 10000.0
0.00.586.819 I llama_init_from_model: freq_scale    = 1
0.00.586.821 I ggml_metal_init: allocating
0.00.586.892 I ggml_metal_init: found device: Apple M4
0.00.586.904 I ggml_metal_init: picking default device: Apple M4
0.00.588.758 I ggml_metal_init: using embedded metal library
0.00.594.486 I ggml_metal_init: GPU name:   Apple M4
0.00.594.506 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.594.507 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.594.507 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.594.508 I ggml_metal_init: simdgroup reduction   = true
0.00.594.509 I ggml_metal_init: simdgroup matrix mul. = true
0.00.594.509 I ggml_metal_init: has residency sets    = true
0.00.594.509 I ggml_metal_init: has bfloat            = true
0.00.594.510 I ggml_metal_init: use bfloat            = true
0.00.594.514 I ggml_metal_init: hasUnifiedMemory      = true
0.00.594.518 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.813 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.672.735 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.672.746 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.672.780 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.828 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.677.830 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.677.830 I llama_init_from_model: graph nodes  = 967
0.00.677.830 I llama_init_from_model: graph splits = 2
0.00.677.835 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.677.968 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.677.969 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.746 I main: llama threadpool init, n_threads = 4
0.00.734.789 I 
0.00.734.812 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.813 I 
0.00.734.980 I sampler seed: 1234
0.00.734.985 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.735.023 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.735.027 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.735.027 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.420.712 I llama_perf_sampler_print:    sampling time =       1.49 ms /    71 runs   (    0.02 ms per token, 47683.01 tokens per second)
0.01.420.712 I llama_perf_context_print:        load time =     722.76 ms
0.01.420.714 I llama_perf_context_print: prompt eval time =      49.45 ms /     7 tokens (    7.06 ms per token,   141.57 tokens per second)
0.01.420.715 I llama_perf_context_print:        eval time =     633.25 ms /    63 runs   (   10.05 ms per token,    99.49 tokens per second)
0.01.420.716 I llama_perf_context_print:       total time =     686.68 ms /    70 tokens
0.01.420.918 I ggml_metal_free: deallocating

real	0m1.440s
user	0m0.111s
sys	0m0.206s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.214 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.667 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.820 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.826 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.828 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.829 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.829 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.829 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.829 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.830 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.831 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.831 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.831 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.832 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.835 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.835 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.837 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.837 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.837 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.694 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.901 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.810 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.028.811 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.812 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.812 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.812 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.813 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.813 I llama_model_loader: - type  f32:  194 tensors
0.00.028.814 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.814 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.815 I print_info: file format = GGUF V3 (latest)
0.00.028.815 I print_info: file type   = Q4_0
0.00.028.816 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.036.902 I load: special tokens cache size = 25
0.00.043.004 I load: token to piece cache size = 0.2984 MB
0.00.043.008 I print_info: arch             = gptneox
0.00.043.009 I print_info: vocab_only       = 0
0.00.043.009 I print_info: n_ctx_train      = 2048
0.00.043.009 I print_info: n_embd           = 2048
0.00.043.009 I print_info: n_layer          = 24
0.00.043.014 I print_info: n_head           = 16
0.00.043.014 I print_info: n_head_kv        = 16
0.00.043.017 I print_info: n_rot            = 32
0.00.043.017 I print_info: n_swa            = 0
0.00.043.017 I print_info: n_embd_head_k    = 128
0.00.043.017 I print_info: n_embd_head_v    = 128
0.00.043.018 I print_info: n_gqa            = 1
0.00.043.019 I print_info: n_embd_k_gqa     = 2048
0.00.043.019 I print_info: n_embd_v_gqa     = 2048
0.00.043.020 I print_info: f_norm_eps       = 1.0e-05
0.00.043.020 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.048 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.050 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.050 I print_info: f_logit_scale    = 0.0e+00
0.00.043.056 I print_info: n_ff             = 8192
0.00.043.058 I print_info: n_expert         = 0
0.00.043.058 I print_info: n_expert_used    = 0
0.00.043.058 I print_info: causal attn      = 1
0.00.043.059 I print_info: pooling type     = 0
0.00.043.059 I print_info: rope type        = 2
0.00.043.059 I print_info: rope scaling     = linear
0.00.043.059 I print_info: freq_base_train  = 10000.0
0.00.043.060 I print_info: freq_scale_train = 1
0.00.043.060 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.060 I print_info: rope_finetuned   = unknown
0.00.043.060 I print_info: ssm_d_conv       = 0
0.00.043.060 I print_info: ssm_d_inner      = 0
0.00.043.061 I print_info: ssm_d_state      = 0
0.00.043.061 I print_info: ssm_dt_rank      = 0
0.00.043.061 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.062 I print_info: model type       = 1.4B
0.00.043.062 I print_info: model params     = 1.41 B
0.00.043.062 I print_info: general.name     = 1.4B
0.00.043.063 I print_info: vocab type       = BPE
0.00.043.063 I print_info: n_vocab          = 50304
0.00.043.063 I print_info: n_merges         = 50009
0.00.043.063 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.064 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.064 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.064 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.065 I print_info: LF token         = 187 ''
0.00.043.065 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.065 I print_info: max token length = 1024
0.00.043.065 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.606.510 I load_tensors: offloading 24 repeating layers to GPU
0.00.606.521 I load_tensors: offloading output layer to GPU
0.00.606.522 I load_tensors: offloaded 25/25 layers to GPU
0.00.606.561 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.606.562 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.608.135 I llama_init_from_model: n_seq_max     = 1
0.00.608.138 I llama_init_from_model: n_ctx         = 128
0.00.608.139 I llama_init_from_model: n_ctx_per_seq = 128
0.00.608.139 I llama_init_from_model: n_batch       = 128
0.00.608.140 I llama_init_from_model: n_ubatch      = 128
0.00.608.140 I llama_init_from_model: flash_attn    = 0
0.00.608.142 I llama_init_from_model: freq_base     = 10000.0
0.00.608.143 I llama_init_from_model: freq_scale    = 1
0.00.608.144 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.608.149 I ggml_metal_init: allocating
0.00.608.225 I ggml_metal_init: found device: Apple M4
0.00.608.238 I ggml_metal_init: picking default device: Apple M4
0.00.610.059 I ggml_metal_init: using embedded metal library
0.00.615.840 I ggml_metal_init: GPU name:   Apple M4
0.00.615.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.615.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.615.850 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.615.851 I ggml_metal_init: simdgroup reduction   = true
0.00.615.851 I ggml_metal_init: simdgroup matrix mul. = true
0.00.615.851 I ggml_metal_init: has residency sets    = true
0.00.615.852 I ggml_metal_init: has bfloat            = true
0.00.615.852 I ggml_metal_init: use bfloat            = true
0.00.615.853 I ggml_metal_init: hasUnifiedMemory      = true
0.00.615.865 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.635.941 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.592 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.639.598 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.639.642 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.643.075 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.643.077 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.643.078 I llama_init_from_model: graph nodes  = 967
0.00.643.078 I llama_init_from_model: graph splits = 2
0.00.643.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.081 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.672.698 I 
0.00.672.771 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.672.778 I perplexity: tokenizing the input ..
0.00.679.809 I perplexity: tokenization took 7.028 ms
0.00.679.819 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.717 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.815.046 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.815.074 I llama_perf_context_print:        load time =     660.02 ms
0.00.815.075 I llama_perf_context_print: prompt eval time =     132.92 ms /   128 tokens (    1.04 ms per token,   962.96 tokens per second)
0.00.815.077 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.078 I llama_perf_context_print:       total time =     142.38 ms /   129 tokens
0.00.815.457 I ggml_metal_free: deallocating

real	0m0.846s
user	0m0.085s
sys	0m0.142s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.725 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.730 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.736 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.736 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.737 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.737 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.737 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.740 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.741 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.741 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.741 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.743 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.743 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.744 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.745 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.746 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.746 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.480 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.557 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.305 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.306 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.307 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.026.307 I llama_model_loader: - type  f32:  194 tensors
0.00.026.307 I llama_model_loader: - type q4_1:   97 tensors
0.00.026.308 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.308 I print_info: file format = GGUF V3 (latest)
0.00.026.309 I print_info: file type   = Q4_1
0.00.026.309 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.491 I load: special tokens cache size = 25
0.00.040.518 I load: token to piece cache size = 0.2984 MB
0.00.040.521 I print_info: arch             = gptneox
0.00.040.521 I print_info: vocab_only       = 0
0.00.040.521 I print_info: n_ctx_train      = 2048
0.00.040.521 I print_info: n_embd           = 2048
0.00.040.522 I print_info: n_layer          = 24
0.00.040.524 I print_info: n_head           = 16
0.00.040.525 I print_info: n_head_kv        = 16
0.00.040.527 I print_info: n_rot            = 32
0.00.040.527 I print_info: n_swa            = 0
0.00.040.527 I print_info: n_embd_head_k    = 128
0.00.040.527 I print_info: n_embd_head_v    = 128
0.00.040.528 I print_info: n_gqa            = 1
0.00.040.529 I print_info: n_embd_k_gqa     = 2048
0.00.040.529 I print_info: n_embd_v_gqa     = 2048
0.00.040.532 I print_info: f_norm_eps       = 1.0e-05
0.00.040.532 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.532 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.532 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.532 I print_info: f_logit_scale    = 0.0e+00
0.00.040.533 I print_info: n_ff             = 8192
0.00.040.533 I print_info: n_expert         = 0
0.00.040.533 I print_info: n_expert_used    = 0
0.00.040.533 I print_info: causal attn      = 1
0.00.040.533 I print_info: pooling type     = 0
0.00.040.534 I print_info: rope type        = 2
0.00.040.534 I print_info: rope scaling     = linear
0.00.040.534 I print_info: freq_base_train  = 10000.0
0.00.040.535 I print_info: freq_scale_train = 1
0.00.040.535 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.535 I print_info: rope_finetuned   = unknown
0.00.040.535 I print_info: ssm_d_conv       = 0
0.00.040.535 I print_info: ssm_d_inner      = 0
0.00.040.535 I print_info: ssm_d_state      = 0
0.00.040.536 I print_info: ssm_dt_rank      = 0
0.00.040.536 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.536 I print_info: model type       = 1.4B
0.00.040.536 I print_info: model params     = 1.41 B
0.00.040.537 I print_info: general.name     = 1.4B
0.00.040.537 I print_info: vocab type       = BPE
0.00.040.537 I print_info: n_vocab          = 50304
0.00.040.537 I print_info: n_merges         = 50009
0.00.040.538 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.538 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.538 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.538 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.539 I print_info: LF token         = 187 ''
0.00.040.539 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.539 I print_info: max token length = 1024
0.00.040.540 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.068 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.084 I load_tensors: offloading output layer to GPU
0.00.589.085 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.122 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.589.123 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.590.603 I llama_init_from_model: n_seq_max     = 1
0.00.590.606 I llama_init_from_model: n_ctx         = 2048
0.00.590.606 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.590.607 I llama_init_from_model: n_batch       = 2048
0.00.590.608 I llama_init_from_model: n_ubatch      = 512
0.00.590.608 I llama_init_from_model: flash_attn    = 0
0.00.590.610 I llama_init_from_model: freq_base     = 10000.0
0.00.590.611 I llama_init_from_model: freq_scale    = 1
0.00.590.613 I ggml_metal_init: allocating
0.00.590.692 I ggml_metal_init: found device: Apple M4
0.00.590.706 I ggml_metal_init: picking default device: Apple M4
0.00.592.567 I ggml_metal_init: using embedded metal library
0.00.599.256 I ggml_metal_init: GPU name:   Apple M4
0.00.599.260 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.260 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.261 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.262 I ggml_metal_init: simdgroup reduction   = true
0.00.599.262 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.262 I ggml_metal_init: has residency sets    = true
0.00.599.263 I ggml_metal_init: has bfloat            = true
0.00.599.263 I ggml_metal_init: use bfloat            = true
0.00.599.264 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.265 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.053 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.675.880 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.675.887 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.675.923 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.680.197 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.680.199 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.680.200 I llama_init_from_model: graph nodes  = 967
0.00.680.200 I llama_init_from_model: graph splits = 2
0.00.680.206 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.680.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.680.342 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.734.642 I main: llama threadpool init, n_threads = 4
0.00.734.688 I 
0.00.734.712 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.734.714 I 
0.00.734.868 I sampler seed: 1234
0.00.734.873 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.734.885 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.734.885 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.734.885 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.459.526 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.01.459.526 I llama_perf_context_print:        load time =     724.99 ms
0.01.459.527 I llama_perf_context_print: prompt eval time =      48.77 ms /     7 tokens (    6.97 ms per token,   143.53 tokens per second)
0.01.459.527 I llama_perf_context_print:        eval time =     673.17 ms /    63 runs   (   10.69 ms per token,    93.59 tokens per second)
0.01.459.528 I llama_perf_context_print:       total time =     725.61 ms /    70 tokens
0.01.459.790 I ggml_metal_free: deallocating

real	0m1.476s
user	0m0.110s
sys	0m0.200s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.036 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.190 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.204 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.204 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.205 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.205 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.206 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.207 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.207 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.207 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.208 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.208 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.209 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.212 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.213 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.905 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.000 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.804 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.805 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.805 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.806 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.806 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.806 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.807 I llama_model_loader: - type  f32:  194 tensors
0.00.024.807 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.808 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.808 I print_info: file format = GGUF V3 (latest)
0.00.024.809 I print_info: file type   = Q4_1
0.00.024.810 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.830 I load: special tokens cache size = 25
0.00.039.239 I load: token to piece cache size = 0.2984 MB
0.00.039.245 I print_info: arch             = gptneox
0.00.039.245 I print_info: vocab_only       = 0
0.00.039.245 I print_info: n_ctx_train      = 2048
0.00.039.246 I print_info: n_embd           = 2048
0.00.039.247 I print_info: n_layer          = 24
0.00.039.252 I print_info: n_head           = 16
0.00.039.252 I print_info: n_head_kv        = 16
0.00.039.252 I print_info: n_rot            = 32
0.00.039.253 I print_info: n_swa            = 0
0.00.039.253 I print_info: n_embd_head_k    = 128
0.00.039.253 I print_info: n_embd_head_v    = 128
0.00.039.254 I print_info: n_gqa            = 1
0.00.039.257 I print_info: n_embd_k_gqa     = 2048
0.00.039.258 I print_info: n_embd_v_gqa     = 2048
0.00.039.259 I print_info: f_norm_eps       = 1.0e-05
0.00.039.259 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.259 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.259 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.259 I print_info: f_logit_scale    = 0.0e+00
0.00.039.260 I print_info: n_ff             = 8192
0.00.039.260 I print_info: n_expert         = 0
0.00.039.260 I print_info: n_expert_used    = 0
0.00.039.261 I print_info: causal attn      = 1
0.00.039.261 I print_info: pooling type     = 0
0.00.039.261 I print_info: rope type        = 2
0.00.039.262 I print_info: rope scaling     = linear
0.00.039.264 I print_info: freq_base_train  = 10000.0
0.00.039.265 I print_info: freq_scale_train = 1
0.00.039.265 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.265 I print_info: rope_finetuned   = unknown
0.00.039.266 I print_info: ssm_d_conv       = 0
0.00.039.266 I print_info: ssm_d_inner      = 0
0.00.039.266 I print_info: ssm_d_state      = 0
0.00.039.266 I print_info: ssm_dt_rank      = 0
0.00.039.266 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.266 I print_info: model type       = 1.4B
0.00.039.270 I print_info: model params     = 1.41 B
0.00.039.270 I print_info: general.name     = 1.4B
0.00.039.271 I print_info: vocab type       = BPE
0.00.039.271 I print_info: n_vocab          = 50304
0.00.039.271 I print_info: n_merges         = 50009
0.00.039.271 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.271 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.272 I print_info: LF token         = 187 ''
0.00.039.273 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.273 I print_info: max token length = 1024
0.00.039.273 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.586.870 I load_tensors: offloading 24 repeating layers to GPU
0.00.586.877 I load_tensors: offloading output layer to GPU
0.00.586.878 I load_tensors: offloaded 25/25 layers to GPU
0.00.586.907 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.586.910 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.587.938 I llama_init_from_model: n_seq_max     = 1
0.00.587.945 I llama_init_from_model: n_ctx         = 128
0.00.587.946 I llama_init_from_model: n_ctx_per_seq = 128
0.00.587.947 I llama_init_from_model: n_batch       = 128
0.00.587.947 I llama_init_from_model: n_ubatch      = 128
0.00.587.947 I llama_init_from_model: flash_attn    = 0
0.00.587.949 I llama_init_from_model: freq_base     = 10000.0
0.00.587.950 I llama_init_from_model: freq_scale    = 1
0.00.587.950 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.587.952 I ggml_metal_init: allocating
0.00.588.023 I ggml_metal_init: found device: Apple M4
0.00.588.037 I ggml_metal_init: picking default device: Apple M4
0.00.589.784 I ggml_metal_init: using embedded metal library
0.00.595.294 I ggml_metal_init: GPU name:   Apple M4
0.00.595.301 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.302 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.302 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.303 I ggml_metal_init: simdgroup reduction   = true
0.00.595.303 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.303 I ggml_metal_init: has residency sets    = true
0.00.595.303 I ggml_metal_init: has bfloat            = true
0.00.595.304 I ggml_metal_init: use bfloat            = true
0.00.595.305 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.314 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.608.236 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.610.045 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.610.048 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.610.074 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.611.818 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.611.819 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.611.820 I llama_init_from_model: graph nodes  = 967
0.00.611.820 I llama_init_from_model: graph splits = 2
0.00.611.821 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.611.822 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.449 I 
0.00.638.489 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.492 I perplexity: tokenizing the input ..
0.00.642.581 I perplexity: tokenization took 4.087 ms
0.00.642.584 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.775.494 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.776.836 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.776.861 I llama_perf_context_print:        load time =     629.41 ms
0.00.776.861 I llama_perf_context_print: prompt eval time =     132.68 ms /   128 tokens (    1.04 ms per token,   964.73 tokens per second)
0.00.776.862 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.862 I llama_perf_context_print:       total time =     138.41 ms /   129 tokens
0.00.777.215 I ggml_metal_free: deallocating

real	0m0.791s
user	0m0.069s
sys	0m0.105s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.850 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.752 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.756 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.758 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.759 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.759 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.759 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.762 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.763 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.763 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.764 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.764 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.764 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.767 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.767 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.768 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.477 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.561 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.268 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.270 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.270 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.271 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.271 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.271 I llama_model_loader: - type  f32:  194 tensors
0.00.025.272 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.272 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.273 I print_info: file format = GGUF V3 (latest)
0.00.025.273 I print_info: file type   = Q5_0
0.00.025.274 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.093 I load: special tokens cache size = 25
0.00.038.877 I load: token to piece cache size = 0.2984 MB
0.00.038.880 I print_info: arch             = gptneox
0.00.038.880 I print_info: vocab_only       = 0
0.00.038.881 I print_info: n_ctx_train      = 2048
0.00.038.881 I print_info: n_embd           = 2048
0.00.038.881 I print_info: n_layer          = 24
0.00.038.883 I print_info: n_head           = 16
0.00.038.884 I print_info: n_head_kv        = 16
0.00.038.884 I print_info: n_rot            = 32
0.00.038.884 I print_info: n_swa            = 0
0.00.038.885 I print_info: n_embd_head_k    = 128
0.00.038.885 I print_info: n_embd_head_v    = 128
0.00.038.885 I print_info: n_gqa            = 1
0.00.038.886 I print_info: n_embd_k_gqa     = 2048
0.00.038.887 I print_info: n_embd_v_gqa     = 2048
0.00.038.887 I print_info: f_norm_eps       = 1.0e-05
0.00.038.888 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.888 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.888 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.888 I print_info: f_logit_scale    = 0.0e+00
0.00.038.889 I print_info: n_ff             = 8192
0.00.038.889 I print_info: n_expert         = 0
0.00.038.889 I print_info: n_expert_used    = 0
0.00.038.889 I print_info: causal attn      = 1
0.00.038.890 I print_info: pooling type     = 0
0.00.038.891 I print_info: rope type        = 2
0.00.038.893 I print_info: rope scaling     = linear
0.00.038.894 I print_info: freq_base_train  = 10000.0
0.00.038.894 I print_info: freq_scale_train = 1
0.00.038.894 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.894 I print_info: rope_finetuned   = unknown
0.00.038.894 I print_info: ssm_d_conv       = 0
0.00.038.895 I print_info: ssm_d_inner      = 0
0.00.038.895 I print_info: ssm_d_state      = 0
0.00.038.895 I print_info: ssm_dt_rank      = 0
0.00.038.895 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.895 I print_info: model type       = 1.4B
0.00.038.895 I print_info: model params     = 1.41 B
0.00.038.896 I print_info: general.name     = 1.4B
0.00.038.896 I print_info: vocab type       = BPE
0.00.038.896 I print_info: n_vocab          = 50304
0.00.038.897 I print_info: n_merges         = 50009
0.00.038.897 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.897 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.898 I print_info: LF token         = 187 ''
0.00.038.898 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.898 I print_info: max token length = 1024
0.00.038.898 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.644.121 I load_tensors: offloading 24 repeating layers to GPU
0.00.644.136 I load_tensors: offloading output layer to GPU
0.00.644.137 I load_tensors: offloaded 25/25 layers to GPU
0.00.644.168 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.644.170 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.645.834 I llama_init_from_model: n_seq_max     = 1
0.00.645.844 I llama_init_from_model: n_ctx         = 2048
0.00.645.844 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.645.845 I llama_init_from_model: n_batch       = 2048
0.00.645.845 I llama_init_from_model: n_ubatch      = 512
0.00.645.845 I llama_init_from_model: flash_attn    = 0
0.00.645.848 I llama_init_from_model: freq_base     = 10000.0
0.00.645.848 I llama_init_from_model: freq_scale    = 1
0.00.645.851 I ggml_metal_init: allocating
0.00.645.909 I ggml_metal_init: found device: Apple M4
0.00.645.927 I ggml_metal_init: picking default device: Apple M4
0.00.648.072 I ggml_metal_init: using embedded metal library
0.00.654.722 I ggml_metal_init: GPU name:   Apple M4
0.00.654.727 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.654.728 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.654.728 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.654.729 I ggml_metal_init: simdgroup reduction   = true
0.00.654.729 I ggml_metal_init: simdgroup matrix mul. = true
0.00.654.729 I ggml_metal_init: has residency sets    = true
0.00.654.730 I ggml_metal_init: has bfloat            = true
0.00.654.730 I ggml_metal_init: use bfloat            = true
0.00.654.731 I ggml_metal_init: hasUnifiedMemory      = true
0.00.654.733 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.672.510 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.382 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.727.390 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.430 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.732.348 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.732.350 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.732.350 I llama_init_from_model: graph nodes  = 967
0.00.732.351 I llama_init_from_model: graph splits = 2
0.00.732.356 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.732.477 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.732.478 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.790.631 I main: llama threadpool init, n_threads = 4
0.00.790.675 I 
0.00.790.699 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.790.700 I 
0.00.790.872 I sampler seed: 1234
0.00.790.877 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.790.888 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.790.888 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.790.888 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.579.852 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52945.56 tokens per second)
0.01.579.852 I llama_perf_context_print:        load time =     781.05 ms
0.01.579.853 I llama_perf_context_print: prompt eval time =      53.13 ms /     7 tokens (    7.59 ms per token,   131.76 tokens per second)
0.01.579.854 I llama_perf_context_print:        eval time =     732.96 ms /    63 runs   (   11.63 ms per token,    85.95 tokens per second)
0.01.579.854 I llama_perf_context_print:       total time =     789.95 ms /    70 tokens
0.01.580.095 I ggml_metal_free: deallocating

real	0m1.598s
user	0m0.110s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.102 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.797 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.936 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.942 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.944 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.945 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.945 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.945 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.946 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.947 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.947 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.948 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.948 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.948 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.949 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.951 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.951 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.951 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.679 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.831 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.602 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.603 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.604 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.605 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.605 I llama_model_loader: - type  f32:  194 tensors
0.00.024.606 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.606 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.607 I print_info: file format = GGUF V3 (latest)
0.00.024.607 I print_info: file type   = Q5_0
0.00.024.609 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.005 I load: special tokens cache size = 25
0.00.039.111 I load: token to piece cache size = 0.2984 MB
0.00.039.115 I print_info: arch             = gptneox
0.00.039.115 I print_info: vocab_only       = 0
0.00.039.115 I print_info: n_ctx_train      = 2048
0.00.039.116 I print_info: n_embd           = 2048
0.00.039.116 I print_info: n_layer          = 24
0.00.039.120 I print_info: n_head           = 16
0.00.039.121 I print_info: n_head_kv        = 16
0.00.039.121 I print_info: n_rot            = 32
0.00.039.121 I print_info: n_swa            = 0
0.00.039.121 I print_info: n_embd_head_k    = 128
0.00.039.125 I print_info: n_embd_head_v    = 128
0.00.039.126 I print_info: n_gqa            = 1
0.00.039.127 I print_info: n_embd_k_gqa     = 2048
0.00.039.127 I print_info: n_embd_v_gqa     = 2048
0.00.039.128 I print_info: f_norm_eps       = 1.0e-05
0.00.039.128 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.128 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.128 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.128 I print_info: f_logit_scale    = 0.0e+00
0.00.039.130 I print_info: n_ff             = 8192
0.00.039.130 I print_info: n_expert         = 0
0.00.039.130 I print_info: n_expert_used    = 0
0.00.039.130 I print_info: causal attn      = 1
0.00.039.131 I print_info: pooling type     = 0
0.00.039.131 I print_info: rope type        = 2
0.00.039.131 I print_info: rope scaling     = linear
0.00.039.131 I print_info: freq_base_train  = 10000.0
0.00.039.131 I print_info: freq_scale_train = 1
0.00.039.132 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.132 I print_info: rope_finetuned   = unknown
0.00.039.132 I print_info: ssm_d_conv       = 0
0.00.039.132 I print_info: ssm_d_inner      = 0
0.00.039.132 I print_info: ssm_d_state      = 0
0.00.039.132 I print_info: ssm_dt_rank      = 0
0.00.039.133 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.133 I print_info: model type       = 1.4B
0.00.039.133 I print_info: model params     = 1.41 B
0.00.039.133 I print_info: general.name     = 1.4B
0.00.039.134 I print_info: vocab type       = BPE
0.00.039.134 I print_info: n_vocab          = 50304
0.00.039.134 I print_info: n_merges         = 50009
0.00.039.134 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.135 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.135 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.135 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.135 I print_info: LF token         = 187 ''
0.00.039.136 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.137 I print_info: max token length = 1024
0.00.039.137 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.617.269 I load_tensors: offloading 24 repeating layers to GPU
0.00.617.288 I load_tensors: offloading output layer to GPU
0.00.617.289 I load_tensors: offloaded 25/25 layers to GPU
0.00.617.324 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.617.325 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.619.155 I llama_init_from_model: n_seq_max     = 1
0.00.619.157 I llama_init_from_model: n_ctx         = 128
0.00.619.158 I llama_init_from_model: n_ctx_per_seq = 128
0.00.619.158 I llama_init_from_model: n_batch       = 128
0.00.619.159 I llama_init_from_model: n_ubatch      = 128
0.00.619.159 I llama_init_from_model: flash_attn    = 0
0.00.619.161 I llama_init_from_model: freq_base     = 10000.0
0.00.619.162 I llama_init_from_model: freq_scale    = 1
0.00.619.162 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.619.165 I ggml_metal_init: allocating
0.00.619.249 I ggml_metal_init: found device: Apple M4
0.00.619.263 I ggml_metal_init: picking default device: Apple M4
0.00.621.054 I ggml_metal_init: using embedded metal library
0.00.627.789 I ggml_metal_init: GPU name:   Apple M4
0.00.627.797 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.627.798 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.627.799 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.627.799 I ggml_metal_init: simdgroup reduction   = true
0.00.627.800 I ggml_metal_init: simdgroup matrix mul. = true
0.00.627.800 I ggml_metal_init: has residency sets    = true
0.00.627.800 I ggml_metal_init: has bfloat            = true
0.00.627.801 I ggml_metal_init: use bfloat            = true
0.00.627.802 I ggml_metal_init: hasUnifiedMemory      = true
0.00.627.805 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.485 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.650.027 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.650.034 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.650.085 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.653.234 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.653.236 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.653.236 I llama_init_from_model: graph nodes  = 967
0.00.653.236 I llama_init_from_model: graph splits = 2
0.00.653.240 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.653.240 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.685.438 I 
0.00.685.524 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.685.532 I perplexity: tokenizing the input ..
0.00.692.280 I perplexity: tokenization took 6.746 ms
0.00.692.289 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.839.852 I perplexity: 0.15 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.841.274 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.841.296 I llama_perf_context_print:        load time =     676.63 ms
0.00.841.297 I llama_perf_context_print: prompt eval time =     147.16 ms /   128 tokens (    1.15 ms per token,   869.81 tokens per second)
0.00.841.297 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.841.298 I llama_perf_context_print:       total time =     155.86 ms /   129 tokens
0.00.841.672 I ggml_metal_free: deallocating

real	0m0.855s
user	0m0.080s
sys	0m0.125s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.010.731 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.579 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.018.584 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.591 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.595 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.596 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.596 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.597 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.598 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.599 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.599 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.432 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.171 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.172 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.027.172 I llama_model_loader: - type  f32:  194 tensors
0.00.027.172 I llama_model_loader: - type q5_1:   97 tensors
0.00.027.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.027.173 I print_info: file format = GGUF V3 (latest)
0.00.027.174 I print_info: file type   = Q5_1
0.00.027.175 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.035.372 I load: special tokens cache size = 25
0.00.041.211 I load: token to piece cache size = 0.2984 MB
0.00.041.214 I print_info: arch             = gptneox
0.00.041.214 I print_info: vocab_only       = 0
0.00.041.215 I print_info: n_ctx_train      = 2048
0.00.041.215 I print_info: n_embd           = 2048
0.00.041.215 I print_info: n_layer          = 24
0.00.041.217 I print_info: n_head           = 16
0.00.041.218 I print_info: n_head_kv        = 16
0.00.041.218 I print_info: n_rot            = 32
0.00.041.219 I print_info: n_swa            = 0
0.00.041.219 I print_info: n_embd_head_k    = 128
0.00.041.219 I print_info: n_embd_head_v    = 128
0.00.041.221 I print_info: n_gqa            = 1
0.00.041.222 I print_info: n_embd_k_gqa     = 2048
0.00.041.223 I print_info: n_embd_v_gqa     = 2048
0.00.041.223 I print_info: f_norm_eps       = 1.0e-05
0.00.041.224 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.224 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.224 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.224 I print_info: f_logit_scale    = 0.0e+00
0.00.041.225 I print_info: n_ff             = 8192
0.00.041.225 I print_info: n_expert         = 0
0.00.041.225 I print_info: n_expert_used    = 0
0.00.041.226 I print_info: causal attn      = 1
0.00.041.226 I print_info: pooling type     = 0
0.00.041.226 I print_info: rope type        = 2
0.00.041.226 I print_info: rope scaling     = linear
0.00.041.226 I print_info: freq_base_train  = 10000.0
0.00.041.227 I print_info: freq_scale_train = 1
0.00.041.227 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.227 I print_info: rope_finetuned   = unknown
0.00.041.227 I print_info: ssm_d_conv       = 0
0.00.041.227 I print_info: ssm_d_inner      = 0
0.00.041.228 I print_info: ssm_d_state      = 0
0.00.041.228 I print_info: ssm_dt_rank      = 0
0.00.041.228 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.228 I print_info: model type       = 1.4B
0.00.041.228 I print_info: model params     = 1.41 B
0.00.041.229 I print_info: general.name     = 1.4B
0.00.041.229 I print_info: vocab type       = BPE
0.00.041.229 I print_info: n_vocab          = 50304
0.00.041.229 I print_info: n_merges         = 50009
0.00.041.230 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.230 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.230 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.230 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.231 I print_info: LF token         = 187 ''
0.00.041.231 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.231 I print_info: max token length = 1024
0.00.041.232 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.716.350 I load_tensors: offloading 24 repeating layers to GPU
0.00.716.368 I load_tensors: offloading output layer to GPU
0.00.716.369 I load_tensors: offloaded 25/25 layers to GPU
0.00.716.403 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.716.424 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.717.851 I llama_init_from_model: n_seq_max     = 1
0.00.717.854 I llama_init_from_model: n_ctx         = 2048
0.00.717.854 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.717.855 I llama_init_from_model: n_batch       = 2048
0.00.717.855 I llama_init_from_model: n_ubatch      = 512
0.00.717.856 I llama_init_from_model: flash_attn    = 0
0.00.717.857 I llama_init_from_model: freq_base     = 10000.0
0.00.717.857 I llama_init_from_model: freq_scale    = 1
0.00.717.858 I ggml_metal_init: allocating
0.00.717.868 I ggml_metal_init: found device: Apple M4
0.00.717.879 I ggml_metal_init: picking default device: Apple M4
0.00.719.402 I ggml_metal_init: using embedded metal library
0.00.725.720 I ggml_metal_init: GPU name:   Apple M4
0.00.725.723 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.725.724 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.725.725 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.725.725 I ggml_metal_init: simdgroup reduction   = true
0.00.725.726 I ggml_metal_init: simdgroup matrix mul. = true
0.00.725.726 I ggml_metal_init: has residency sets    = true
0.00.725.726 I ggml_metal_init: has bfloat            = true
0.00.725.726 I ggml_metal_init: use bfloat            = true
0.00.725.727 I ggml_metal_init: hasUnifiedMemory      = true
0.00.725.728 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.743.585 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.800.607 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.800.613 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.800.644 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.805.041 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.805.043 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.805.044 I llama_init_from_model: graph nodes  = 967
0.00.805.044 I llama_init_from_model: graph splits = 2
0.00.805.050 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.805.178 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.805.179 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.851.932 I main: llama threadpool init, n_threads = 4
0.00.851.973 I 
0.00.851.993 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.851.994 I 
0.00.852.112 I sampler seed: 1234
0.00.852.116 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.852.149 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.852.153 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.852.153 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.691.807 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53383.46 tokens per second)
0.01.691.808 I llama_perf_context_print:        load time =     840.49 ms
0.01.691.808 I llama_perf_context_print: prompt eval time =      42.28 ms /     7 tokens (    6.04 ms per token,   165.55 tokens per second)
0.01.691.809 I llama_perf_context_print:        eval time =     794.54 ms /    63 runs   (   12.61 ms per token,    79.29 tokens per second)
0.01.691.810 I llama_perf_context_print:       total time =     840.59 ms /    70 tokens
0.01.692.067 I ggml_metal_free: deallocating

real	0m1.712s
user	0m0.109s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.103 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.498 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.504 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.505 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.510 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.510 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.510 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.511 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.512 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.512 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.513 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.513 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.513 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.513 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.515 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.517 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.517 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.268 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.199 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.200 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.200 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.201 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.201 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.202 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.203 I llama_model_loader: - type  f32:  194 tensors
0.00.026.203 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.203 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.204 I print_info: file format = GGUF V3 (latest)
0.00.026.204 I print_info: file type   = Q5_1
0.00.026.206 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.300 I load: special tokens cache size = 25
0.00.040.223 I load: token to piece cache size = 0.2984 MB
0.00.040.227 I print_info: arch             = gptneox
0.00.040.228 I print_info: vocab_only       = 0
0.00.040.228 I print_info: n_ctx_train      = 2048
0.00.040.228 I print_info: n_embd           = 2048
0.00.040.228 I print_info: n_layer          = 24
0.00.040.232 I print_info: n_head           = 16
0.00.040.233 I print_info: n_head_kv        = 16
0.00.040.237 I print_info: n_rot            = 32
0.00.040.237 I print_info: n_swa            = 0
0.00.040.237 I print_info: n_embd_head_k    = 128
0.00.040.237 I print_info: n_embd_head_v    = 128
0.00.040.237 I print_info: n_gqa            = 1
0.00.040.238 I print_info: n_embd_k_gqa     = 2048
0.00.040.239 I print_info: n_embd_v_gqa     = 2048
0.00.040.239 I print_info: f_norm_eps       = 1.0e-05
0.00.040.240 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.240 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.241 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.242 I print_info: f_logit_scale    = 0.0e+00
0.00.040.242 I print_info: n_ff             = 8192
0.00.040.242 I print_info: n_expert         = 0
0.00.040.243 I print_info: n_expert_used    = 0
0.00.040.243 I print_info: causal attn      = 1
0.00.040.243 I print_info: pooling type     = 0
0.00.040.243 I print_info: rope type        = 2
0.00.040.243 I print_info: rope scaling     = linear
0.00.040.243 I print_info: freq_base_train  = 10000.0
0.00.040.244 I print_info: freq_scale_train = 1
0.00.040.244 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.244 I print_info: rope_finetuned   = unknown
0.00.040.244 I print_info: ssm_d_conv       = 0
0.00.040.244 I print_info: ssm_d_inner      = 0
0.00.040.244 I print_info: ssm_d_state      = 0
0.00.040.244 I print_info: ssm_dt_rank      = 0
0.00.040.245 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.245 I print_info: model type       = 1.4B
0.00.040.245 I print_info: model params     = 1.41 B
0.00.040.245 I print_info: general.name     = 1.4B
0.00.040.246 I print_info: vocab type       = BPE
0.00.040.246 I print_info: n_vocab          = 50304
0.00.040.247 I print_info: n_merges         = 50009
0.00.040.247 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.247 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.248 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.248 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.248 I print_info: LF token         = 187 ''
0.00.040.248 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.248 I print_info: max token length = 1024
0.00.040.249 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.727.041 I load_tensors: offloading 24 repeating layers to GPU
0.00.727.044 I load_tensors: offloading output layer to GPU
0.00.727.045 I load_tensors: offloaded 25/25 layers to GPU
0.00.727.069 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.727.070 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.728.325 I llama_init_from_model: n_seq_max     = 1
0.00.728.327 I llama_init_from_model: n_ctx         = 128
0.00.728.328 I llama_init_from_model: n_ctx_per_seq = 128
0.00.728.328 I llama_init_from_model: n_batch       = 128
0.00.728.329 I llama_init_from_model: n_ubatch      = 128
0.00.728.329 I llama_init_from_model: flash_attn    = 0
0.00.728.331 I llama_init_from_model: freq_base     = 10000.0
0.00.728.331 I llama_init_from_model: freq_scale    = 1
0.00.728.332 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.728.333 I ggml_metal_init: allocating
0.00.728.391 I ggml_metal_init: found device: Apple M4
0.00.728.404 I ggml_metal_init: picking default device: Apple M4
0.00.729.921 I ggml_metal_init: using embedded metal library
0.00.736.061 I ggml_metal_init: GPU name:   Apple M4
0.00.736.065 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.736.066 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.736.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.736.067 I ggml_metal_init: simdgroup reduction   = true
0.00.736.067 I ggml_metal_init: simdgroup matrix mul. = true
0.00.736.067 I ggml_metal_init: has residency sets    = true
0.00.736.067 I ggml_metal_init: has bfloat            = true
0.00.736.067 I ggml_metal_init: use bfloat            = true
0.00.736.068 I ggml_metal_init: hasUnifiedMemory      = true
0.00.736.070 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.752.907 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.756.275 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.756.281 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.756.324 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.759.533 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.759.534 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.759.535 I llama_init_from_model: graph nodes  = 967
0.00.759.535 I llama_init_from_model: graph splits = 2
0.00.759.537 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.759.538 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.793.361 I 
0.00.793.446 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.793.453 I perplexity: tokenizing the input ..
0.00.800.258 I perplexity: tokenization took 6.802 ms
0.00.800.268 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.940.893 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.942.229 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.942.253 I llama_perf_context_print:        load time =     783.25 ms
0.00.942.254 I llama_perf_context_print: prompt eval time =     139.69 ms /   128 tokens (    1.09 ms per token,   916.34 tokens per second)
0.00.942.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.942.255 I llama_perf_context_print:       total time =     148.90 ms /   129 tokens
0.00.942.650 I ggml_metal_free: deallocating

real	0m0.959s
user	0m0.078s
sys	0m0.153s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.009.177 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.017.347 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.349 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.349 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.350 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.350 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.352 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.352 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.353 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.355 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.356 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.356 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.096 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.217 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.926 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.927 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.928 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.928 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.928 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.929 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.929 I llama_model_loader: - type  f32:  194 tensors
0.00.025.929 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.930 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.930 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.931 I print_info: file format = GGUF V3 (latest)
0.00.025.931 I print_info: file type   = Q2_K - Medium
0.00.025.932 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.737 I load: special tokens cache size = 25
0.00.039.671 I load: token to piece cache size = 0.2984 MB
0.00.039.674 I print_info: arch             = gptneox
0.00.039.674 I print_info: vocab_only       = 0
0.00.039.675 I print_info: n_ctx_train      = 2048
0.00.039.675 I print_info: n_embd           = 2048
0.00.039.675 I print_info: n_layer          = 24
0.00.039.678 I print_info: n_head           = 16
0.00.039.678 I print_info: n_head_kv        = 16
0.00.039.681 I print_info: n_rot            = 32
0.00.039.681 I print_info: n_swa            = 0
0.00.039.681 I print_info: n_embd_head_k    = 128
0.00.039.681 I print_info: n_embd_head_v    = 128
0.00.039.682 I print_info: n_gqa            = 1
0.00.039.683 I print_info: n_embd_k_gqa     = 2048
0.00.039.683 I print_info: n_embd_v_gqa     = 2048
0.00.039.684 I print_info: f_norm_eps       = 1.0e-05
0.00.039.684 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.684 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.685 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.685 I print_info: f_logit_scale    = 0.0e+00
0.00.039.686 I print_info: n_ff             = 8192
0.00.039.686 I print_info: n_expert         = 0
0.00.039.691 I print_info: n_expert_used    = 0
0.00.039.692 I print_info: causal attn      = 1
0.00.039.692 I print_info: pooling type     = 0
0.00.039.693 I print_info: rope type        = 2
0.00.039.693 I print_info: rope scaling     = linear
0.00.039.694 I print_info: freq_base_train  = 10000.0
0.00.039.694 I print_info: freq_scale_train = 1
0.00.039.694 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.695 I print_info: rope_finetuned   = unknown
0.00.039.695 I print_info: ssm_d_conv       = 0
0.00.039.697 I print_info: ssm_d_inner      = 0
0.00.039.697 I print_info: ssm_d_state      = 0
0.00.039.697 I print_info: ssm_dt_rank      = 0
0.00.039.697 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.697 I print_info: model type       = 1.4B
0.00.039.697 I print_info: model params     = 1.41 B
0.00.039.698 I print_info: general.name     = 1.4B
0.00.039.698 I print_info: vocab type       = BPE
0.00.039.698 I print_info: n_vocab          = 50304
0.00.039.699 I print_info: n_merges         = 50009
0.00.039.699 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.699 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.700 I print_info: LF token         = 187 ''
0.00.039.700 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.705 I print_info: max token length = 1024
0.00.039.706 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.407.446 I load_tensors: offloading 24 repeating layers to GPU
0.00.407.467 I load_tensors: offloading output layer to GPU
0.00.407.468 I load_tensors: offloaded 25/25 layers to GPU
0.00.407.503 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.407.504 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.409.006 I llama_init_from_model: n_seq_max     = 1
0.00.409.010 I llama_init_from_model: n_ctx         = 2048
0.00.409.010 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.409.011 I llama_init_from_model: n_batch       = 2048
0.00.409.011 I llama_init_from_model: n_ubatch      = 512
0.00.409.011 I llama_init_from_model: flash_attn    = 0
0.00.409.014 I llama_init_from_model: freq_base     = 10000.0
0.00.409.015 I llama_init_from_model: freq_scale    = 1
0.00.409.017 I ggml_metal_init: allocating
0.00.409.096 I ggml_metal_init: found device: Apple M4
0.00.409.108 I ggml_metal_init: picking default device: Apple M4
0.00.411.012 I ggml_metal_init: using embedded metal library
0.00.416.789 I ggml_metal_init: GPU name:   Apple M4
0.00.416.808 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.416.809 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.416.809 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.416.810 I ggml_metal_init: simdgroup reduction   = true
0.00.416.810 I ggml_metal_init: simdgroup matrix mul. = true
0.00.416.811 I ggml_metal_init: has residency sets    = true
0.00.416.811 I ggml_metal_init: has bfloat            = true
0.00.416.812 I ggml_metal_init: use bfloat            = true
0.00.416.814 I ggml_metal_init: hasUnifiedMemory      = true
0.00.416.819 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.439.327 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.502.073 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.502.079 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.502.120 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.506.510 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.506.512 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.506.512 I llama_init_from_model: graph nodes  = 967
0.00.506.513 I llama_init_from_model: graph splits = 2
0.00.506.518 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.506.651 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.506.652 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.565.002 I main: llama threadpool init, n_threads = 4
0.00.565.059 I 
0.00.565.081 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.565.084 I 
0.00.565.245 I sampler seed: 1234
0.00.565.250 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.565.305 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.565.308 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.565.308 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.239.829 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.239.829 I llama_perf_context_print:        load time =     555.11 ms
0.01.239.831 I llama_perf_context_print: prompt eval time =      35.81 ms /     7 tokens (    5.12 ms per token,   195.45 tokens per second)
0.01.239.831 I llama_perf_context_print:        eval time =     635.84 ms /    63 runs   (   10.09 ms per token,    99.08 tokens per second)
0.01.239.832 I llama_perf_context_print:       total time =     675.54 ms /    70 tokens
0.01.240.082 I ggml_metal_free: deallocating

real	0m1.257s
user	0m0.114s
sys	0m0.171s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.869 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.121 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.127 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.129 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.129 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.129 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.130 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.131 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.131 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.132 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.132 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.134 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.135 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.136 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.136 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.137 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.940 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.820 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.820 I llama_model_loader: - type  f32:  194 tensors
0.00.024.821 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.821 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.821 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.822 I print_info: file format = GGUF V3 (latest)
0.00.024.822 I print_info: file type   = Q2_K - Medium
0.00.024.823 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.759 I load: special tokens cache size = 25
0.00.038.755 I load: token to piece cache size = 0.2984 MB
0.00.038.760 I print_info: arch             = gptneox
0.00.038.760 I print_info: vocab_only       = 0
0.00.038.760 I print_info: n_ctx_train      = 2048
0.00.038.760 I print_info: n_embd           = 2048
0.00.038.760 I print_info: n_layer          = 24
0.00.038.765 I print_info: n_head           = 16
0.00.038.766 I print_info: n_head_kv        = 16
0.00.038.767 I print_info: n_rot            = 32
0.00.038.767 I print_info: n_swa            = 0
0.00.038.768 I print_info: n_embd_head_k    = 128
0.00.038.768 I print_info: n_embd_head_v    = 128
0.00.038.768 I print_info: n_gqa            = 1
0.00.038.769 I print_info: n_embd_k_gqa     = 2048
0.00.038.770 I print_info: n_embd_v_gqa     = 2048
0.00.038.771 I print_info: f_norm_eps       = 1.0e-05
0.00.038.771 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.771 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.773 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.774 I print_info: f_logit_scale    = 0.0e+00
0.00.038.774 I print_info: n_ff             = 8192
0.00.038.774 I print_info: n_expert         = 0
0.00.038.774 I print_info: n_expert_used    = 0
0.00.038.775 I print_info: causal attn      = 1
0.00.038.775 I print_info: pooling type     = 0
0.00.038.776 I print_info: rope type        = 2
0.00.038.777 I print_info: rope scaling     = linear
0.00.038.777 I print_info: freq_base_train  = 10000.0
0.00.038.777 I print_info: freq_scale_train = 1
0.00.038.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.789 I print_info: rope_finetuned   = unknown
0.00.038.789 I print_info: ssm_d_conv       = 0
0.00.038.790 I print_info: ssm_d_inner      = 0
0.00.038.790 I print_info: ssm_d_state      = 0
0.00.038.790 I print_info: ssm_dt_rank      = 0
0.00.038.790 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.790 I print_info: model type       = 1.4B
0.00.038.791 I print_info: model params     = 1.41 B
0.00.038.791 I print_info: general.name     = 1.4B
0.00.038.792 I print_info: vocab type       = BPE
0.00.038.792 I print_info: n_vocab          = 50304
0.00.038.792 I print_info: n_merges         = 50009
0.00.038.794 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.794 I print_info: LF token         = 187 ''
0.00.038.795 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.795 I print_info: max token length = 1024
0.00.038.795 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.399.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.399.513 I load_tensors: offloading output layer to GPU
0.00.399.513 I load_tensors: offloaded 25/25 layers to GPU
0.00.399.551 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.399.553 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.401.345 I llama_init_from_model: n_seq_max     = 1
0.00.401.348 I llama_init_from_model: n_ctx         = 128
0.00.401.349 I llama_init_from_model: n_ctx_per_seq = 128
0.00.401.349 I llama_init_from_model: n_batch       = 128
0.00.401.349 I llama_init_from_model: n_ubatch      = 128
0.00.401.350 I llama_init_from_model: flash_attn    = 0
0.00.401.352 I llama_init_from_model: freq_base     = 10000.0
0.00.401.353 I llama_init_from_model: freq_scale    = 1
0.00.401.353 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.401.355 I ggml_metal_init: allocating
0.00.401.467 I ggml_metal_init: found device: Apple M4
0.00.401.482 I ggml_metal_init: picking default device: Apple M4
0.00.403.458 I ggml_metal_init: using embedded metal library
0.00.408.862 I ggml_metal_init: GPU name:   Apple M4
0.00.408.880 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.408.881 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.408.882 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.408.882 I ggml_metal_init: simdgroup reduction   = true
0.00.408.883 I ggml_metal_init: simdgroup matrix mul. = true
0.00.408.883 I ggml_metal_init: has residency sets    = true
0.00.408.883 I ggml_metal_init: has bfloat            = true
0.00.408.884 I ggml_metal_init: use bfloat            = true
0.00.408.886 I ggml_metal_init: hasUnifiedMemory      = true
0.00.408.890 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.431.094 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.434.638 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.434.645 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.434.690 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.438.111 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.438.113 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.438.113 I llama_init_from_model: graph nodes  = 967
0.00.438.114 I llama_init_from_model: graph splits = 2
0.00.438.117 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.438.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.470.708 I 
0.00.470.796 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.470.804 I perplexity: tokenizing the input ..
0.00.477.568 I perplexity: tokenization took 6.76 ms
0.00.477.575 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.622.439 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.623.784 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.623.805 I llama_perf_context_print:        load time =     461.83 ms
0.00.623.805 I llama_perf_context_print: prompt eval time =     143.92 ms /   128 tokens (    1.12 ms per token,   889.41 tokens per second)
0.00.623.806 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.623.807 I llama_perf_context_print:       total time =     153.10 ms /   129 tokens
0.00.624.177 I ggml_metal_free: deallocating

real	0m0.638s
user	0m0.082s
sys	0m0.089s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.882 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.298 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.303 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.305 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.306 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.306 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.306 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.307 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.307 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.308 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.310 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.310 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.311 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.316 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.316 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.316 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.046 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.202 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.922 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.923 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.923 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.924 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.924 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.925 I llama_model_loader: - type  f32:  194 tensors
0.00.024.925 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.925 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.926 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.926 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.926 I print_info: file format = GGUF V3 (latest)
0.00.024.927 I print_info: file type   = Q3_K - Medium
0.00.024.928 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.115 I load: special tokens cache size = 25
0.00.039.262 I load: token to piece cache size = 0.2984 MB
0.00.039.265 I print_info: arch             = gptneox
0.00.039.265 I print_info: vocab_only       = 0
0.00.039.266 I print_info: n_ctx_train      = 2048
0.00.039.266 I print_info: n_embd           = 2048
0.00.039.266 I print_info: n_layer          = 24
0.00.039.269 I print_info: n_head           = 16
0.00.039.270 I print_info: n_head_kv        = 16
0.00.039.270 I print_info: n_rot            = 32
0.00.039.270 I print_info: n_swa            = 0
0.00.039.270 I print_info: n_embd_head_k    = 128
0.00.039.271 I print_info: n_embd_head_v    = 128
0.00.039.271 I print_info: n_gqa            = 1
0.00.039.272 I print_info: n_embd_k_gqa     = 2048
0.00.039.273 I print_info: n_embd_v_gqa     = 2048
0.00.039.274 I print_info: f_norm_eps       = 1.0e-05
0.00.039.274 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.279 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.279 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.279 I print_info: f_logit_scale    = 0.0e+00
0.00.039.280 I print_info: n_ff             = 8192
0.00.039.280 I print_info: n_expert         = 0
0.00.039.280 I print_info: n_expert_used    = 0
0.00.039.282 I print_info: causal attn      = 1
0.00.039.284 I print_info: pooling type     = 0
0.00.039.284 I print_info: rope type        = 2
0.00.039.284 I print_info: rope scaling     = linear
0.00.039.285 I print_info: freq_base_train  = 10000.0
0.00.039.285 I print_info: freq_scale_train = 1
0.00.039.285 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.285 I print_info: rope_finetuned   = unknown
0.00.039.285 I print_info: ssm_d_conv       = 0
0.00.039.286 I print_info: ssm_d_inner      = 0
0.00.039.286 I print_info: ssm_d_state      = 0
0.00.039.286 I print_info: ssm_dt_rank      = 0
0.00.039.286 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.286 I print_info: model type       = 1.4B
0.00.039.287 I print_info: model params     = 1.41 B
0.00.039.287 I print_info: general.name     = 1.4B
0.00.039.287 I print_info: vocab type       = BPE
0.00.039.288 I print_info: n_vocab          = 50304
0.00.039.288 I print_info: n_merges         = 50009
0.00.039.288 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.288 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: LF token         = 187 ''
0.00.039.289 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.289 I print_info: max token length = 1024
0.00.039.289 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.490.928 I load_tensors: offloading 24 repeating layers to GPU
0.00.490.947 I load_tensors: offloading output layer to GPU
0.00.490.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.490.982 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.490.990 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.492.725 I llama_init_from_model: n_seq_max     = 1
0.00.492.727 I llama_init_from_model: n_ctx         = 2048
0.00.492.728 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.492.729 I llama_init_from_model: n_batch       = 2048
0.00.492.729 I llama_init_from_model: n_ubatch      = 512
0.00.492.730 I llama_init_from_model: flash_attn    = 0
0.00.492.732 I llama_init_from_model: freq_base     = 10000.0
0.00.492.733 I llama_init_from_model: freq_scale    = 1
0.00.492.734 I ggml_metal_init: allocating
0.00.492.809 I ggml_metal_init: found device: Apple M4
0.00.492.822 I ggml_metal_init: picking default device: Apple M4
0.00.494.737 I ggml_metal_init: using embedded metal library
0.00.500.885 I ggml_metal_init: GPU name:   Apple M4
0.00.500.890 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.500.891 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.500.892 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.500.892 I ggml_metal_init: simdgroup reduction   = true
0.00.500.893 I ggml_metal_init: simdgroup matrix mul. = true
0.00.500.893 I ggml_metal_init: has residency sets    = true
0.00.500.893 I ggml_metal_init: has bfloat            = true
0.00.500.894 I ggml_metal_init: use bfloat            = true
0.00.500.895 I ggml_metal_init: hasUnifiedMemory      = true
0.00.500.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.520.117 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.578.529 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.578.535 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.578.570 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.595.548 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.595.550 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.595.551 I llama_init_from_model: graph nodes  = 967
0.00.595.551 I llama_init_from_model: graph splits = 2
0.00.595.558 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.595.687 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.595.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.652.827 I main: llama threadpool init, n_threads = 4
0.00.652.872 I 
0.00.652.895 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.652.896 I 
0.00.653.048 I sampler seed: 1234
0.00.653.053 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.653.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.653.074 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.653.074 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.399.639 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.399.640 I llama_perf_context_print:        load time =     643.23 ms
0.01.399.641 I llama_perf_context_print: prompt eval time =      49.65 ms /     7 tokens (    7.09 ms per token,   140.99 tokens per second)
0.01.399.641 I llama_perf_context_print:        eval time =     694.07 ms /    63 runs   (   11.02 ms per token,    90.77 tokens per second)
0.01.399.642 I llama_perf_context_print:       total time =     747.53 ms /    70 tokens
0.01.399.928 I ggml_metal_free: deallocating

real	0m1.415s
user	0m0.110s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.727 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.884 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.891 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.893 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.895 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.895 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.895 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.896 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.897 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.897 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.898 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.898 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.898 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.899 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.899 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.902 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.902 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.903 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.765 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.751 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.752 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.753 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.753 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.754 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.754 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.754 I llama_model_loader: - type  f32:  194 tensors
0.00.024.755 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.755 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.755 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.755 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.756 I print_info: file format = GGUF V3 (latest)
0.00.024.757 I print_info: file type   = Q3_K - Medium
0.00.024.763 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.033.242 I load: special tokens cache size = 25
0.00.039.402 I load: token to piece cache size = 0.2984 MB
0.00.039.406 I print_info: arch             = gptneox
0.00.039.406 I print_info: vocab_only       = 0
0.00.039.406 I print_info: n_ctx_train      = 2048
0.00.039.407 I print_info: n_embd           = 2048
0.00.039.407 I print_info: n_layer          = 24
0.00.039.411 I print_info: n_head           = 16
0.00.039.412 I print_info: n_head_kv        = 16
0.00.039.412 I print_info: n_rot            = 32
0.00.039.412 I print_info: n_swa            = 0
0.00.039.412 I print_info: n_embd_head_k    = 128
0.00.039.412 I print_info: n_embd_head_v    = 128
0.00.039.413 I print_info: n_gqa            = 1
0.00.039.414 I print_info: n_embd_k_gqa     = 2048
0.00.039.415 I print_info: n_embd_v_gqa     = 2048
0.00.039.415 I print_info: f_norm_eps       = 1.0e-05
0.00.039.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.417 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.417 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.417 I print_info: f_logit_scale    = 0.0e+00
0.00.039.418 I print_info: n_ff             = 8192
0.00.039.418 I print_info: n_expert         = 0
0.00.039.418 I print_info: n_expert_used    = 0
0.00.039.418 I print_info: causal attn      = 1
0.00.039.418 I print_info: pooling type     = 0
0.00.039.422 I print_info: rope type        = 2
0.00.039.423 I print_info: rope scaling     = linear
0.00.039.424 I print_info: freq_base_train  = 10000.0
0.00.039.424 I print_info: freq_scale_train = 1
0.00.039.424 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.424 I print_info: rope_finetuned   = unknown
0.00.039.424 I print_info: ssm_d_conv       = 0
0.00.039.425 I print_info: ssm_d_inner      = 0
0.00.039.425 I print_info: ssm_d_state      = 0
0.00.039.425 I print_info: ssm_dt_rank      = 0
0.00.039.425 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.425 I print_info: model type       = 1.4B
0.00.039.427 I print_info: model params     = 1.41 B
0.00.039.427 I print_info: general.name     = 1.4B
0.00.039.427 I print_info: vocab type       = BPE
0.00.039.427 I print_info: n_vocab          = 50304
0.00.039.427 I print_info: n_merges         = 50009
0.00.039.428 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.428 I print_info: LF token         = 187 ''
0.00.039.429 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.429 I print_info: max token length = 1024
0.00.039.429 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.491.547 I load_tensors: offloading 24 repeating layers to GPU
0.00.491.562 I load_tensors: offloading output layer to GPU
0.00.491.563 I load_tensors: offloaded 25/25 layers to GPU
0.00.491.598 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.491.599 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.493.280 I llama_init_from_model: n_seq_max     = 1
0.00.493.282 I llama_init_from_model: n_ctx         = 128
0.00.493.283 I llama_init_from_model: n_ctx_per_seq = 128
0.00.493.283 I llama_init_from_model: n_batch       = 128
0.00.493.284 I llama_init_from_model: n_ubatch      = 128
0.00.493.284 I llama_init_from_model: flash_attn    = 0
0.00.493.287 I llama_init_from_model: freq_base     = 10000.0
0.00.493.287 I llama_init_from_model: freq_scale    = 1
0.00.493.288 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.493.290 I ggml_metal_init: allocating
0.00.493.360 I ggml_metal_init: found device: Apple M4
0.00.493.374 I ggml_metal_init: picking default device: Apple M4
0.00.495.257 I ggml_metal_init: using embedded metal library
0.00.500.806 I ggml_metal_init: GPU name:   Apple M4
0.00.500.825 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.500.826 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.500.826 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.500.827 I ggml_metal_init: simdgroup reduction   = true
0.00.500.827 I ggml_metal_init: simdgroup matrix mul. = true
0.00.500.828 I ggml_metal_init: has residency sets    = true
0.00.500.828 I ggml_metal_init: has bfloat            = true
0.00.500.829 I ggml_metal_init: use bfloat            = true
0.00.500.830 I ggml_metal_init: hasUnifiedMemory      = true
0.00.500.834 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.521.696 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.525.187 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.525.194 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.525.235 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.528.536 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.528.538 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.528.538 I llama_init_from_model: graph nodes  = 967
0.00.528.539 I llama_init_from_model: graph splits = 2
0.00.528.542 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.528.542 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.554.049 I 
0.00.554.115 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.554.122 I perplexity: tokenizing the input ..
0.00.561.093 I perplexity: tokenization took 6.968 ms
0.00.561.098 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.693.903 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.695.332 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.695.356 I llama_perf_context_print:        load time =     545.31 ms
0.00.695.357 I llama_perf_context_print: prompt eval time =     131.86 ms /   128 tokens (    1.03 ms per token,   970.74 tokens per second)
0.00.695.357 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.695.358 I llama_perf_context_print:       total time =     141.31 ms /   129 tokens
0.00.695.680 I ggml_metal_free: deallocating

real	0m0.709s
user	0m0.081s
sys	0m0.117s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.532 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.538 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.538 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.539 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.539 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.539 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.540 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.541 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.541 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.541 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.542 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.542 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.543 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.544 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.545 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.545 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.304 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.399 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.183 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.184 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.185 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.186 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.186 I llama_model_loader: - type  f32:  194 tensors
0.00.025.187 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.187 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.187 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.188 I print_info: file format = GGUF V3 (latest)
0.00.025.188 I print_info: file type   = Q4_K - Medium
0.00.025.189 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.334 I load: special tokens cache size = 25
0.00.039.282 I load: token to piece cache size = 0.2984 MB
0.00.039.285 I print_info: arch             = gptneox
0.00.039.285 I print_info: vocab_only       = 0
0.00.039.285 I print_info: n_ctx_train      = 2048
0.00.039.285 I print_info: n_embd           = 2048
0.00.039.285 I print_info: n_layer          = 24
0.00.039.288 I print_info: n_head           = 16
0.00.039.289 I print_info: n_head_kv        = 16
0.00.039.289 I print_info: n_rot            = 32
0.00.039.289 I print_info: n_swa            = 0
0.00.039.289 I print_info: n_embd_head_k    = 128
0.00.039.292 I print_info: n_embd_head_v    = 128
0.00.039.293 I print_info: n_gqa            = 1
0.00.039.293 I print_info: n_embd_k_gqa     = 2048
0.00.039.294 I print_info: n_embd_v_gqa     = 2048
0.00.039.294 I print_info: f_norm_eps       = 1.0e-05
0.00.039.295 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.296 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.297 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.297 I print_info: f_logit_scale    = 0.0e+00
0.00.039.298 I print_info: n_ff             = 8192
0.00.039.298 I print_info: n_expert         = 0
0.00.039.298 I print_info: n_expert_used    = 0
0.00.039.298 I print_info: causal attn      = 1
0.00.039.299 I print_info: pooling type     = 0
0.00.039.299 I print_info: rope type        = 2
0.00.039.299 I print_info: rope scaling     = linear
0.00.039.299 I print_info: freq_base_train  = 10000.0
0.00.039.301 I print_info: freq_scale_train = 1
0.00.039.301 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.301 I print_info: rope_finetuned   = unknown
0.00.039.301 I print_info: ssm_d_conv       = 0
0.00.039.301 I print_info: ssm_d_inner      = 0
0.00.039.302 I print_info: ssm_d_state      = 0
0.00.039.302 I print_info: ssm_dt_rank      = 0
0.00.039.302 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.302 I print_info: model type       = 1.4B
0.00.039.303 I print_info: model params     = 1.41 B
0.00.039.303 I print_info: general.name     = 1.4B
0.00.039.303 I print_info: vocab type       = BPE
0.00.039.303 I print_info: n_vocab          = 50304
0.00.039.304 I print_info: n_merges         = 50009
0.00.039.304 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.304 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.304 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.304 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.305 I print_info: LF token         = 187 ''
0.00.039.305 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.305 I print_info: max token length = 1024
0.00.039.306 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.527.592 I load_tensors: offloading 24 repeating layers to GPU
0.00.527.608 I load_tensors: offloading output layer to GPU
0.00.527.609 I load_tensors: offloaded 25/25 layers to GPU
0.00.527.645 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.527.646 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.529.333 I llama_init_from_model: n_seq_max     = 1
0.00.529.336 I llama_init_from_model: n_ctx         = 2048
0.00.529.336 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.529.337 I llama_init_from_model: n_batch       = 2048
0.00.529.337 I llama_init_from_model: n_ubatch      = 512
0.00.529.337 I llama_init_from_model: flash_attn    = 0
0.00.529.340 I llama_init_from_model: freq_base     = 10000.0
0.00.529.340 I llama_init_from_model: freq_scale    = 1
0.00.529.343 I ggml_metal_init: allocating
0.00.529.422 I ggml_metal_init: found device: Apple M4
0.00.529.435 I ggml_metal_init: picking default device: Apple M4
0.00.531.681 I ggml_metal_init: using embedded metal library
0.00.537.826 I ggml_metal_init: GPU name:   Apple M4
0.00.537.831 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.537.831 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.537.832 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.537.833 I ggml_metal_init: simdgroup reduction   = true
0.00.537.833 I ggml_metal_init: simdgroup matrix mul. = true
0.00.537.834 I ggml_metal_init: has residency sets    = true
0.00.537.834 I ggml_metal_init: has bfloat            = true
0.00.537.834 I ggml_metal_init: use bfloat            = true
0.00.537.835 I ggml_metal_init: hasUnifiedMemory      = true
0.00.537.837 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.971 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.425 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.620.433 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.620.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.625.791 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.625.793 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.625.793 I llama_init_from_model: graph nodes  = 967
0.00.625.794 I llama_init_from_model: graph splits = 2
0.00.625.799 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.625.929 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.625.930 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.649 I main: llama threadpool init, n_threads = 4
0.00.684.703 I 
0.00.684.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.731 I 
0.00.684.910 I sampler seed: 1234
0.00.684.916 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.684.949 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.684.954 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.684.954 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.445.256 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 47908.23 tokens per second)
0.01.445.257 I llama_perf_context_print:        load time =     673.97 ms
0.01.445.257 I llama_perf_context_print: prompt eval time =      57.50 ms /     7 tokens (    8.21 ms per token,   121.74 tokens per second)
0.01.445.258 I llama_perf_context_print:        eval time =     699.74 ms /    63 runs   (   11.11 ms per token,    90.03 tokens per second)
0.01.445.258 I llama_perf_context_print:       total time =     761.34 ms /    70 tokens
0.01.445.495 I ggml_metal_free: deallocating

real	0m1.466s
user	0m0.114s
sys	0m0.211s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.117 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.732 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.455 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.461 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.467 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.467 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.468 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.468 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.469 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.470 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.470 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.470 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.471 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.471 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.471 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.473 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.473 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.474 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.266 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.336 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.104 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.106 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.108 I llama_model_loader: - type  f32:  194 tensors
0.00.024.108 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.108 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.108 I llama_model_loader: - type q6_K:   13 tensors
0.00.024.109 I print_info: file format = GGUF V3 (latest)
0.00.024.113 I print_info: file type   = Q4_K - Medium
0.00.024.115 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.139 I load: special tokens cache size = 25
0.00.038.166 I load: token to piece cache size = 0.2984 MB
0.00.038.170 I print_info: arch             = gptneox
0.00.038.170 I print_info: vocab_only       = 0
0.00.038.171 I print_info: n_ctx_train      = 2048
0.00.038.171 I print_info: n_embd           = 2048
0.00.038.171 I print_info: n_layer          = 24
0.00.038.175 I print_info: n_head           = 16
0.00.038.176 I print_info: n_head_kv        = 16
0.00.038.176 I print_info: n_rot            = 32
0.00.038.176 I print_info: n_swa            = 0
0.00.038.176 I print_info: n_embd_head_k    = 128
0.00.038.177 I print_info: n_embd_head_v    = 128
0.00.038.179 I print_info: n_gqa            = 1
0.00.038.179 I print_info: n_embd_k_gqa     = 2048
0.00.038.180 I print_info: n_embd_v_gqa     = 2048
0.00.038.181 I print_info: f_norm_eps       = 1.0e-05
0.00.038.181 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.181 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.181 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.183 I print_info: f_logit_scale    = 0.0e+00
0.00.038.183 I print_info: n_ff             = 8192
0.00.038.184 I print_info: n_expert         = 0
0.00.038.184 I print_info: n_expert_used    = 0
0.00.038.184 I print_info: causal attn      = 1
0.00.038.184 I print_info: pooling type     = 0
0.00.038.184 I print_info: rope type        = 2
0.00.038.184 I print_info: rope scaling     = linear
0.00.038.185 I print_info: freq_base_train  = 10000.0
0.00.038.185 I print_info: freq_scale_train = 1
0.00.038.185 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.185 I print_info: rope_finetuned   = unknown
0.00.038.186 I print_info: ssm_d_conv       = 0
0.00.038.186 I print_info: ssm_d_inner      = 0
0.00.038.186 I print_info: ssm_d_state      = 0
0.00.038.186 I print_info: ssm_dt_rank      = 0
0.00.038.186 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.186 I print_info: model type       = 1.4B
0.00.038.187 I print_info: model params     = 1.41 B
0.00.038.187 I print_info: general.name     = 1.4B
0.00.038.187 I print_info: vocab type       = BPE
0.00.038.188 I print_info: n_vocab          = 50304
0.00.038.188 I print_info: n_merges         = 50009
0.00.038.188 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.188 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.188 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.189 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.190 I print_info: LF token         = 187 ''
0.00.038.190 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.190 I print_info: max token length = 1024
0.00.038.191 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.528.533 I load_tensors: offloading 24 repeating layers to GPU
0.00.528.550 I load_tensors: offloading output layer to GPU
0.00.528.551 I load_tensors: offloaded 25/25 layers to GPU
0.00.528.586 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.528.593 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.530.261 I llama_init_from_model: n_seq_max     = 1
0.00.530.264 I llama_init_from_model: n_ctx         = 128
0.00.530.264 I llama_init_from_model: n_ctx_per_seq = 128
0.00.530.265 I llama_init_from_model: n_batch       = 128
0.00.530.266 I llama_init_from_model: n_ubatch      = 128
0.00.530.266 I llama_init_from_model: flash_attn    = 0
0.00.530.268 I llama_init_from_model: freq_base     = 10000.0
0.00.530.269 I llama_init_from_model: freq_scale    = 1
0.00.530.269 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.530.272 I ggml_metal_init: allocating
0.00.530.351 I ggml_metal_init: found device: Apple M4
0.00.530.364 I ggml_metal_init: picking default device: Apple M4
0.00.532.191 I ggml_metal_init: using embedded metal library
0.00.539.049 I ggml_metal_init: GPU name:   Apple M4
0.00.539.059 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.539.060 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.539.061 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.539.061 I ggml_metal_init: simdgroup reduction   = true
0.00.539.061 I ggml_metal_init: simdgroup matrix mul. = true
0.00.539.062 I ggml_metal_init: has residency sets    = true
0.00.539.062 I ggml_metal_init: has bfloat            = true
0.00.539.062 I ggml_metal_init: use bfloat            = true
0.00.539.063 I ggml_metal_init: hasUnifiedMemory      = true
0.00.539.067 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.556.997 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.560.536 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.560.546 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.560.605 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.563.827 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.563.829 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.563.829 I llama_init_from_model: graph nodes  = 967
0.00.563.830 I llama_init_from_model: graph splits = 2
0.00.563.832 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.563.832 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.592.499 I 
0.00.592.579 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.592.588 I perplexity: tokenizing the input ..
0.00.599.911 I perplexity: tokenization took 7.32 ms
0.00.599.918 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.733.644 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.734.970 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.734.999 I llama_perf_context_print:        load time =     583.76 ms
0.00.735.000 I llama_perf_context_print: prompt eval time =     132.76 ms /   128 tokens (    1.04 ms per token,   964.16 tokens per second)
0.00.735.000 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.735.001 I llama_perf_context_print:       total time =     142.51 ms /   129 tokens
0.00.735.423 I ggml_metal_free: deallocating

real	0m0.749s
user	0m0.080s
sys	0m0.133s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.085 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.008.798 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.564 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.569 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.571 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.571 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.572 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.573 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.573 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.574 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.574 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.574 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.575 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.575 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.576 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.577 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.577 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.578 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.339 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.456 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.194 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.195 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.196 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.197 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.197 I llama_model_loader: - type  f32:  194 tensors
0.00.024.197 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.198 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.198 I print_info: file format = GGUF V3 (latest)
0.00.024.199 I print_info: file type   = Q5_K - Medium
0.00.024.204 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.386 I load: special tokens cache size = 25
0.00.038.418 I load: token to piece cache size = 0.2984 MB
0.00.038.421 I print_info: arch             = gptneox
0.00.038.421 I print_info: vocab_only       = 0
0.00.038.421 I print_info: n_ctx_train      = 2048
0.00.038.421 I print_info: n_embd           = 2048
0.00.038.422 I print_info: n_layer          = 24
0.00.038.424 I print_info: n_head           = 16
0.00.038.425 I print_info: n_head_kv        = 16
0.00.038.425 I print_info: n_rot            = 32
0.00.038.427 I print_info: n_swa            = 0
0.00.038.427 I print_info: n_embd_head_k    = 128
0.00.038.428 I print_info: n_embd_head_v    = 128
0.00.038.428 I print_info: n_gqa            = 1
0.00.038.429 I print_info: n_embd_k_gqa     = 2048
0.00.038.430 I print_info: n_embd_v_gqa     = 2048
0.00.038.430 I print_info: f_norm_eps       = 1.0e-05
0.00.038.431 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.431 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.431 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.431 I print_info: f_logit_scale    = 0.0e+00
0.00.038.432 I print_info: n_ff             = 8192
0.00.038.432 I print_info: n_expert         = 0
0.00.038.432 I print_info: n_expert_used    = 0
0.00.038.432 I print_info: causal attn      = 1
0.00.038.432 I print_info: pooling type     = 0
0.00.038.433 I print_info: rope type        = 2
0.00.038.433 I print_info: rope scaling     = linear
0.00.038.433 I print_info: freq_base_train  = 10000.0
0.00.038.434 I print_info: freq_scale_train = 1
0.00.038.434 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.434 I print_info: rope_finetuned   = unknown
0.00.038.434 I print_info: ssm_d_conv       = 0
0.00.038.434 I print_info: ssm_d_inner      = 0
0.00.038.434 I print_info: ssm_d_state      = 0
0.00.038.434 I print_info: ssm_dt_rank      = 0
0.00.038.435 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.435 I print_info: model type       = 1.4B
0.00.038.435 I print_info: model params     = 1.41 B
0.00.038.435 I print_info: general.name     = 1.4B
0.00.038.436 I print_info: vocab type       = BPE
0.00.038.436 I print_info: n_vocab          = 50304
0.00.038.436 I print_info: n_merges         = 50009
0.00.038.437 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.437 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.437 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.437 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.437 I print_info: LF token         = 187 ''
0.00.038.438 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.439 I print_info: max token length = 1024
0.00.038.439 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.597.499 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.509 I load_tensors: offloading output layer to GPU
0.00.597.510 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.545 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.597.546 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.599.225 I llama_init_from_model: n_seq_max     = 1
0.00.599.229 I llama_init_from_model: n_ctx         = 2048
0.00.599.230 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.599.231 I llama_init_from_model: n_batch       = 2048
0.00.599.231 I llama_init_from_model: n_ubatch      = 512
0.00.599.231 I llama_init_from_model: flash_attn    = 0
0.00.599.233 I llama_init_from_model: freq_base     = 10000.0
0.00.599.233 I llama_init_from_model: freq_scale    = 1
0.00.599.235 I ggml_metal_init: allocating
0.00.599.325 I ggml_metal_init: found device: Apple M4
0.00.599.338 I ggml_metal_init: picking default device: Apple M4
0.00.601.687 I ggml_metal_init: using embedded metal library
0.00.608.300 I ggml_metal_init: GPU name:   Apple M4
0.00.608.304 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.608.305 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.608.306 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.608.306 I ggml_metal_init: simdgroup reduction   = true
0.00.608.307 I ggml_metal_init: simdgroup matrix mul. = true
0.00.608.307 I ggml_metal_init: has residency sets    = true
0.00.608.307 I ggml_metal_init: has bfloat            = true
0.00.608.307 I ggml_metal_init: use bfloat            = true
0.00.608.308 I ggml_metal_init: hasUnifiedMemory      = true
0.00.608.310 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.126 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.685.513 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.685.518 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.685.562 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.689.876 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.689.877 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.689.877 I llama_init_from_model: graph nodes  = 967
0.00.689.878 I llama_init_from_model: graph splits = 2
0.00.689.883 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.690.012 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.690.013 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.904 I main: llama threadpool init, n_threads = 4
0.00.760.948 I 
0.00.760.969 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.970 I 
0.00.761.153 I sampler seed: 1234
0.00.761.156 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.167 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.167 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.167 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.615.637 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54573.41 tokens per second)
0.01.615.638 I llama_perf_context_print:        load time =     751.38 ms
0.01.615.639 I llama_perf_context_print: prompt eval time =      65.01 ms /     7 tokens (    9.29 ms per token,   107.68 tokens per second)
0.01.615.639 I llama_perf_context_print:        eval time =     786.70 ms /    63 runs   (   12.49 ms per token,    80.08 tokens per second)
0.01.615.640 I llama_perf_context_print:       total time =     855.46 ms /    70 tokens
0.01.615.904 I ggml_metal_free: deallocating

real	0m1.633s
user	0m0.110s
sys	0m0.224s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.115 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.899 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.775 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.782 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.783 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.784 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.784 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.784 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.785 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.786 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.786 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.789 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.789 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.790 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.790 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.793 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.579 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.653 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.506 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.507 I llama_model_loader: - type  f32:  194 tensors
0.00.025.507 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.508 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.508 I print_info: file format = GGUF V3 (latest)
0.00.025.509 I print_info: file type   = Q5_K - Medium
0.00.025.510 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.982 I load: special tokens cache size = 25
0.00.039.981 I load: token to piece cache size = 0.2984 MB
0.00.039.986 I print_info: arch             = gptneox
0.00.039.986 I print_info: vocab_only       = 0
0.00.039.986 I print_info: n_ctx_train      = 2048
0.00.039.986 I print_info: n_embd           = 2048
0.00.039.987 I print_info: n_layer          = 24
0.00.039.991 I print_info: n_head           = 16
0.00.039.992 I print_info: n_head_kv        = 16
0.00.039.992 I print_info: n_rot            = 32
0.00.039.992 I print_info: n_swa            = 0
0.00.039.992 I print_info: n_embd_head_k    = 128
0.00.039.993 I print_info: n_embd_head_v    = 128
0.00.039.996 I print_info: n_gqa            = 1
0.00.039.997 I print_info: n_embd_k_gqa     = 2048
0.00.039.997 I print_info: n_embd_v_gqa     = 2048
0.00.039.998 I print_info: f_norm_eps       = 1.0e-05
0.00.039.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.998 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.999 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.999 I print_info: f_logit_scale    = 0.0e+00
0.00.039.999 I print_info: n_ff             = 8192
0.00.040.000 I print_info: n_expert         = 0
0.00.040.000 I print_info: n_expert_used    = 0
0.00.040.000 I print_info: causal attn      = 1
0.00.040.000 I print_info: pooling type     = 0
0.00.040.000 I print_info: rope type        = 2
0.00.040.001 I print_info: rope scaling     = linear
0.00.040.002 I print_info: freq_base_train  = 10000.0
0.00.040.002 I print_info: freq_scale_train = 1
0.00.040.002 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.002 I print_info: rope_finetuned   = unknown
0.00.040.002 I print_info: ssm_d_conv       = 0
0.00.040.003 I print_info: ssm_d_inner      = 0
0.00.040.003 I print_info: ssm_d_state      = 0
0.00.040.003 I print_info: ssm_dt_rank      = 0
0.00.040.003 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.003 I print_info: model type       = 1.4B
0.00.040.004 I print_info: model params     = 1.41 B
0.00.040.004 I print_info: general.name     = 1.4B
0.00.040.004 I print_info: vocab type       = BPE
0.00.040.004 I print_info: n_vocab          = 50304
0.00.040.004 I print_info: n_merges         = 50009
0.00.040.005 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.005 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.005 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.005 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.006 I print_info: LF token         = 187 ''
0.00.040.006 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.006 I print_info: max token length = 1024
0.00.040.007 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.589.472 I load_tensors: offloading 24 repeating layers to GPU
0.00.589.480 I load_tensors: offloading output layer to GPU
0.00.589.481 I load_tensors: offloaded 25/25 layers to GPU
0.00.589.512 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.589.518 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.591.235 I llama_init_from_model: n_seq_max     = 1
0.00.591.238 I llama_init_from_model: n_ctx         = 128
0.00.591.238 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.239 I llama_init_from_model: n_batch       = 128
0.00.591.239 I llama_init_from_model: n_ubatch      = 128
0.00.591.239 I llama_init_from_model: flash_attn    = 0
0.00.591.240 I llama_init_from_model: freq_base     = 10000.0
0.00.591.241 I llama_init_from_model: freq_scale    = 1
0.00.591.241 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.244 I ggml_metal_init: allocating
0.00.591.318 I ggml_metal_init: found device: Apple M4
0.00.591.331 I ggml_metal_init: picking default device: Apple M4
0.00.592.977 I ggml_metal_init: using embedded metal library
0.00.599.458 I ggml_metal_init: GPU name:   Apple M4
0.00.599.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.463 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.465 I ggml_metal_init: simdgroup reduction   = true
0.00.599.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.465 I ggml_metal_init: has residency sets    = true
0.00.599.465 I ggml_metal_init: has bfloat            = true
0.00.599.466 I ggml_metal_init: use bfloat            = true
0.00.599.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.475 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.253 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.824 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.828 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.870 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.309 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.310 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.311 I llama_init_from_model: graph nodes  = 967
0.00.624.311 I llama_init_from_model: graph splits = 2
0.00.624.315 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.315 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.508 I 
0.00.658.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.608 I perplexity: tokenizing the input ..
0.00.665.686 I perplexity: tokenization took 7.076 ms
0.00.665.694 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.802.125 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.803.447 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.803.468 I llama_perf_context_print:        load time =     648.60 ms
0.00.803.469 I llama_perf_context_print: prompt eval time =     135.83 ms /   128 tokens (    1.06 ms per token,   942.35 tokens per second)
0.00.803.470 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.803.470 I llama_perf_context_print:       total time =     144.97 ms /   129 tokens
0.00.803.851 I ggml_metal_free: deallocating

real	0m0.820s
user	0m0.080s
sys	0m0.137s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.842 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.416 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.421 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.423 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.423 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.424 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.424 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.424 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.425 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.428 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.428 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.429 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.429 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.429 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.430 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.432 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.432 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.433 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.181 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.311 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.001 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.002 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.003 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.003 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.003 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.003 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.004 I llama_model_loader: - type  f32:  194 tensors
0.00.024.004 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.004 I print_info: file format = GGUF V3 (latest)
0.00.024.005 I print_info: file type   = Q6_K
0.00.024.005 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.031.854 I load: special tokens cache size = 25
0.00.037.845 I load: token to piece cache size = 0.2984 MB
0.00.037.848 I print_info: arch             = gptneox
0.00.037.848 I print_info: vocab_only       = 0
0.00.037.848 I print_info: n_ctx_train      = 2048
0.00.037.848 I print_info: n_embd           = 2048
0.00.037.848 I print_info: n_layer          = 24
0.00.037.851 I print_info: n_head           = 16
0.00.037.852 I print_info: n_head_kv        = 16
0.00.037.852 I print_info: n_rot            = 32
0.00.037.853 I print_info: n_swa            = 0
0.00.037.854 I print_info: n_embd_head_k    = 128
0.00.037.854 I print_info: n_embd_head_v    = 128
0.00.037.855 I print_info: n_gqa            = 1
0.00.037.856 I print_info: n_embd_k_gqa     = 2048
0.00.037.856 I print_info: n_embd_v_gqa     = 2048
0.00.037.857 I print_info: f_norm_eps       = 1.0e-05
0.00.037.857 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.857 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.858 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.859 I print_info: f_logit_scale    = 0.0e+00
0.00.037.860 I print_info: n_ff             = 8192
0.00.037.860 I print_info: n_expert         = 0
0.00.037.860 I print_info: n_expert_used    = 0
0.00.037.860 I print_info: causal attn      = 1
0.00.037.860 I print_info: pooling type     = 0
0.00.037.860 I print_info: rope type        = 2
0.00.037.861 I print_info: rope scaling     = linear
0.00.037.861 I print_info: freq_base_train  = 10000.0
0.00.037.861 I print_info: freq_scale_train = 1
0.00.037.862 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.862 I print_info: rope_finetuned   = unknown
0.00.037.862 I print_info: ssm_d_conv       = 0
0.00.037.862 I print_info: ssm_d_inner      = 0
0.00.037.862 I print_info: ssm_d_state      = 0
0.00.037.863 I print_info: ssm_dt_rank      = 0
0.00.037.863 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.863 I print_info: model type       = 1.4B
0.00.037.863 I print_info: model params     = 1.41 B
0.00.037.864 I print_info: general.name     = 1.4B
0.00.037.864 I print_info: vocab type       = BPE
0.00.037.864 I print_info: n_vocab          = 50304
0.00.037.865 I print_info: n_merges         = 50009
0.00.037.865 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.865 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.865 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.865 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.866 I print_info: LF token         = 187 ''
0.00.037.866 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.866 I print_info: max token length = 1024
0.00.037.867 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.629.985 I load_tensors: offloading 24 repeating layers to GPU
0.00.629.995 I load_tensors: offloading output layer to GPU
0.00.629.996 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.030 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.630.031 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.631.741 I llama_init_from_model: n_seq_max     = 1
0.00.631.743 I llama_init_from_model: n_ctx         = 2048
0.00.631.743 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.631.744 I llama_init_from_model: n_batch       = 2048
0.00.631.744 I llama_init_from_model: n_ubatch      = 512
0.00.631.744 I llama_init_from_model: flash_attn    = 0
0.00.631.745 I llama_init_from_model: freq_base     = 10000.0
0.00.631.746 I llama_init_from_model: freq_scale    = 1
0.00.631.747 I ggml_metal_init: allocating
0.00.631.790 I ggml_metal_init: found device: Apple M4
0.00.631.799 I ggml_metal_init: picking default device: Apple M4
0.00.633.314 I ggml_metal_init: using embedded metal library
0.00.639.832 I ggml_metal_init: GPU name:   Apple M4
0.00.639.836 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.836 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.838 I ggml_metal_init: simdgroup reduction   = true
0.00.639.838 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.838 I ggml_metal_init: has residency sets    = true
0.00.639.838 I ggml_metal_init: has bfloat            = true
0.00.639.839 I ggml_metal_init: use bfloat            = true
0.00.639.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.887 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.712.366 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.712.372 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.712.404 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.716.760 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.716.762 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.716.762 I llama_init_from_model: graph nodes  = 967
0.00.716.762 I llama_init_from_model: graph splits = 2
0.00.716.768 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.716.892 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.770.238 I main: llama threadpool init, n_threads = 4
0.00.770.276 I 
0.00.770.297 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.770.298 I 
0.00.770.413 I sampler seed: 1234
0.00.770.418 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, top_n_sigma = -1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.770.427 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.770.428 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.770.428 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.655.795 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 53143.71 tokens per second)
0.01.655.796 I llama_perf_context_print:        load time =     760.66 ms
0.01.655.797 I llama_perf_context_print: prompt eval time =      57.44 ms /     7 tokens (    8.21 ms per token,   121.86 tokens per second)
0.01.655.797 I llama_perf_context_print:        eval time =     825.04 ms /    63 runs   (   13.10 ms per token,    76.36 tokens per second)
0.01.655.798 I llama_perf_context_print:       total time =     886.29 ms /    70 tokens
0.01.656.053 I ggml_metal_free: deallocating

real	0m1.672s
user	0m0.109s
sys	0m0.198s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.114 I build: 4789 (9c42b171) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.988 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.686 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.692 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.694 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.694 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.695 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.695 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.695 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.696 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.697 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.697 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.698 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.698 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.699 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.700 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.701 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.701 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.458 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.378 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.380 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.380 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.380 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.381 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.381 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.382 I llama_model_loader: - type  f32:  194 tensors
0.00.024.382 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.383 I print_info: file format = GGUF V3 (latest)
0.00.024.383 I print_info: file type   = Q6_K
0.00.024.385 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.470 I load: special tokens cache size = 25
0.00.038.539 I load: token to piece cache size = 0.2984 MB
0.00.038.543 I print_info: arch             = gptneox
0.00.038.543 I print_info: vocab_only       = 0
0.00.038.543 I print_info: n_ctx_train      = 2048
0.00.038.543 I print_info: n_embd           = 2048
0.00.038.544 I print_info: n_layer          = 24
0.00.038.547 I print_info: n_head           = 16
0.00.038.547 I print_info: n_head_kv        = 16
0.00.038.548 I print_info: n_rot            = 32
0.00.038.548 I print_info: n_swa            = 0
0.00.038.548 I print_info: n_embd_head_k    = 128
0.00.038.548 I print_info: n_embd_head_v    = 128
0.00.038.552 I print_info: n_gqa            = 1
0.00.038.552 I print_info: n_embd_k_gqa     = 2048
0.00.038.553 I print_info: n_embd_v_gqa     = 2048
0.00.038.553 I print_info: f_norm_eps       = 1.0e-05
0.00.038.554 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.554 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.554 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.554 I print_info: f_logit_scale    = 0.0e+00
0.00.038.555 I print_info: n_ff             = 8192
0.00.038.555 I print_info: n_expert         = 0
0.00.038.555 I print_info: n_expert_used    = 0
0.00.038.555 I print_info: causal attn      = 1
0.00.038.555 I print_info: pooling type     = 0
0.00.038.555 I print_info: rope type        = 2
0.00.038.556 I print_info: rope scaling     = linear
0.00.038.556 I print_info: freq_base_train  = 10000.0
0.00.038.556 I print_info: freq_scale_train = 1
0.00.038.556 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.557 I print_info: rope_finetuned   = unknown
0.00.038.557 I print_info: ssm_d_conv       = 0
0.00.038.557 I print_info: ssm_d_inner      = 0
0.00.038.557 I print_info: ssm_d_state      = 0
0.00.038.557 I print_info: ssm_dt_rank      = 0
0.00.038.558 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.558 I print_info: model type       = 1.4B
0.00.038.558 I print_info: model params     = 1.41 B
0.00.038.558 I print_info: general.name     = 1.4B
0.00.038.559 I print_info: vocab type       = BPE
0.00.038.559 I print_info: n_vocab          = 50304
0.00.038.559 I print_info: n_merges         = 50009
0.00.038.560 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.560 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.560 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.560 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.560 I print_info: LF token         = 187 ''
0.00.038.562 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.562 I print_info: max token length = 1024
0.00.038.562 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.590.347 I load_tensors: offloading 24 repeating layers to GPU
0.00.590.356 I load_tensors: offloading output layer to GPU
0.00.590.357 I load_tensors: offloaded 25/25 layers to GPU
0.00.590.385 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.590.387 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.591.823 I llama_init_from_model: n_seq_max     = 1
0.00.591.825 I llama_init_from_model: n_ctx         = 128
0.00.591.826 I llama_init_from_model: n_ctx_per_seq = 128
0.00.591.826 I llama_init_from_model: n_batch       = 128
0.00.591.826 I llama_init_from_model: n_ubatch      = 128
0.00.591.827 I llama_init_from_model: flash_attn    = 0
0.00.591.828 I llama_init_from_model: freq_base     = 10000.0
0.00.591.828 I llama_init_from_model: freq_scale    = 1
0.00.591.829 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.591.830 I ggml_metal_init: allocating
0.00.591.893 I ggml_metal_init: found device: Apple M4
0.00.591.905 I ggml_metal_init: picking default device: Apple M4
0.00.593.411 I ggml_metal_init: using embedded metal library
0.00.599.559 I ggml_metal_init: GPU name:   Apple M4
0.00.599.563 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.599.564 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.599.565 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.599.565 I ggml_metal_init: simdgroup reduction   = true
0.00.599.565 I ggml_metal_init: simdgroup matrix mul. = true
0.00.599.566 I ggml_metal_init: has residency sets    = true
0.00.599.566 I ggml_metal_init: has bfloat            = true
0.00.599.566 I ggml_metal_init: use bfloat            = true
0.00.599.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.599.570 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.229 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.620.748 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.620.752 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.620.812 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.624.034 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.624.036 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.624.037 I llama_init_from_model: graph nodes  = 967
0.00.624.037 I llama_init_from_model: graph splits = 2
0.00.624.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.624.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.577 I 
0.00.658.669 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | SME = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.677 I perplexity: tokenizing the input ..
0.00.665.930 I perplexity: tokenization took 7.252 ms
0.00.665.936 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.728 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.799.142 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.799.166 I llama_perf_context_print:        load time =     649.58 ms
0.00.799.168 I llama_perf_context_print: prompt eval time =     131.39 ms /   128 tokens (    1.03 ms per token,   974.18 tokens per second)
0.00.799.168 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.169 I llama_perf_context_print:       total time =     140.59 ms /   129 tokens
0.00.799.555 I ggml_metal_free: deallocating

real	0m0.813s
user	0m0.078s
sys	0m0.131s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4789 (9c42b171)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ae07ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ae085f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ae08ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ae09150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ae09700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ae09cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ae0a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ae0a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ae0adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ae0b2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ae0b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ae0bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ae0c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ae0cf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ae0d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ae0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ae0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ae0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ae0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ae0fbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ae10310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ae10a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ae11150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ae119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ae12110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ae123d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ae129e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ae13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ae13b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ae13e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ae142f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ae145b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ae14e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ae15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ae15640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ae15ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ae15f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ae16420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ae168c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ae16d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ae17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ae176a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ae17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ae17fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ae182a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ae188b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ae18ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ae197e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ae19df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ae1a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ae1aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ae1b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ae1b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ae1bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ae1c430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ae1c8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ae1cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ae1d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ae1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ae1de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ae1e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ae1e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ae1ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ae1eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ae1f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ae1f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ae1fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ae20150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ae205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ae20a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ae20f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ae213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ae21870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ae21dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ae22310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ae22860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ae22db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ae23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ae23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ae23da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ae242f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ae24840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ae24d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ae252e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ae25830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ae25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ae262d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ae26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ae26d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ae272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ae27810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ae27d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ae282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ae28800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ae28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ae292a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ae297f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ae194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ae29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ae2a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ae2a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ae2aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ae2b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ae2b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ae2bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ae2c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ae2c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ae2ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ae2d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ae2d930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ae2de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ae2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ae2e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ae2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ae2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ae2f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ae2fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11ae04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11ae046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11ae04b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11ae04f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11ae053f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11ae05860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11ae064d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11ae06790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11ae06a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11ae06ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11ae07330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11ae077a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11ae07c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11ae08080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11ae084f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11ae08960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11ae08dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11ae09240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11ae096b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11ae09b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11ae09f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11ae0a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11ae0a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11ae0ace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11ae0b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11ae0b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11ae0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11ae0bea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11ae0c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11ae0c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11ae0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11ae0d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11ae0d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11ae0d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11ae0ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11ae0e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11ae0e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11ae0eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11ae0ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11ae0f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11ae0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11ae0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11ae10130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11ae105a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11ae10a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11ae10e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11ae112f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11ae11760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11ae11bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11ae12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11ae124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11ae12920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11ae12d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11ae13200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11ae13670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11ae13ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11ae13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11ae143c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11ae14830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11ae14ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11ae15110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11ae15580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11ae159f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11ae15e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11ae162d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11ae16740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11ae16bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11ae17020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11ae17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11ae17900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11ae17d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11ae181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11ae18650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11ae18ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11ae18f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11ae193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11ae19810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11ae19c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11ae1a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11ae1a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11ae1a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11ae1ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11ae1b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11ae1b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11ae1bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11ae1c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11ae1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11ae1c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11ae1ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11ae1d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11ae1d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11ae1dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11ae1e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11ae1eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11ae1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11ae1f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11ae1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11ae200a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11ae20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11ae20c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11ae211e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11ae217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11ae21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11ae22320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11ae228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11ae22ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11ae23460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11ae23a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11ae23fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11ae245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11ae24b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11ae25120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11ae256e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11ae25ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11ae26260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11ae26820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11ae26de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11ae273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11ae27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11ae27f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11ae284e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11ae28aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11ae29060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11ae29620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11ae29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11ae2a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11ae2a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11ae2ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11ae2b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11ae2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11ae2be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11ae2c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11ae2c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11ae2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11ae2d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11ae2db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11ae2e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11ae2e6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11ae2ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11ae2f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11ae2f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11ae2fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11ae30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11ae30920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11ae30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11ae314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11ae31a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11ae32020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11ae325e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11ae32ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11ae33160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11ae33660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11ae33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11ae34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11ae34560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11ae34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11ae34f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11ae35460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11ae35960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11ae35e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11ae36360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11ae36860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11ae36d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11ae37260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11ae37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x11ae37c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x11ae38160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x11ae38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x11ae38b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x11ae39060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x11ae39560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x11ae39a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x11ae39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x11ae3a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x11ae3a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11ae3ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11ae3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11ae3bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11ae3c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11ae3cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11ae3d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11ae3d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11ae3db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11ae3e150 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.717.793 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.717.796 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x110f04ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x110f04f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x110f053c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x110f05830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x110f05ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x110f06110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x110f06580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x110f069f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x110f06e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x110f073e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x110f07850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x110f07ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x110f089f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x110f091a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x110f099b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x110f0a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x110f0a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x110f0af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x110f0b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x110f0be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x110f0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x110f0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x110f0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x110f0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x110f0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x110f0e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x110f0e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x110f0eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x110f0f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x110f0f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x110f0f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x110f0fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x110f10280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x110f10540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x110f109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x110f10e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x110f11290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x110f11700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x110f11b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x110f11fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x110f12450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x110f128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x110f12d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x110f131a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x110f13610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x110f13a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x110f13ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x110f14360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x110f147d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x110f14c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x110f150b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x110f15520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x110f15990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x110f15e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x110f16270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x110f166e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x110f16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x110f17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x110f175c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x110f17a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x110f17ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x110f18310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x110f18780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x110f18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x110f19060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x110f194d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x110f19940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x110f19db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x110f1a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x110f1a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x110f1ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x110f1af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x110f1b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x110f1b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x110f1bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x110f1c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x110f1c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x110f1ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x110f1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x110f1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x110f1d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x110f1dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x110f1e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x110f1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x110f1e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x110f1ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x110f1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x110f1f670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x110f1fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x110f1ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x110f203c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x110f20830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x110f20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x110f21110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x110f21580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x110f219f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x110f21e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x110f222d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x110f22740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x110f22bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x110f23020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x110f23490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x110f23900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x110f23d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x110f241e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x110f24650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x110f24ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x110f24f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x110f253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x110f25810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x110f25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x110f260f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x110f26560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x110f269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x110f26e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x110f272b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x110f27720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x110f27b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x110f28000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x110f28470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x110f288e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x110f28d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x110f291c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x110f29630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x110f29aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x110f29f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x110f2a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x110f2a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x110f2ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x110f2b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x110f2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x110f2b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x110f2be20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x110f2c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x110f2c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x110f2cb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x110f2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x110f2d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x110f2d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x110f2dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x110f2e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x110f2e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x110f2ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x110f2eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x110f2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x110f2f7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x110f2fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x110f300b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x110f30520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x110f30990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x110f30e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x110f31270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x110f316e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x110f31b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x110f31fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x110f32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x110f328a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x110f32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x110f33180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x110f335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x110f33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x110f33ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x110f34340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x110f347b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x110f34c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x110f35090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x110f35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x110f35f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x110f36240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x110f366b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x110f36b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x110f36f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x110f37400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x110f37870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x110f37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x110f38150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x110f385c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x110f38a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x110f38ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x110f39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x110f39780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x110f39bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x110f3a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x110f3a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x110f3a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x110f3adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x110f3b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x110f3b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x110f3bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x110f3bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x110f3c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x110f3c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x110f3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x110f3d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x110f3d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x110f3da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x110f3de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x110f3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x110f3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x110f3ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x110f3f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x110f3f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x110f3fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x110f3ff20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x110f40390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x110f40800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x110f40c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x110f410e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x110f41600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x110f41b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x110f42680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x110f42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x110f42f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x110f434c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x110f43a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x110f44040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x110f44600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x110f44bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x110f45180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x110f45740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x110f45d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x110f462c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x110f46880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x110f46e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x110f47400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x110f479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x110f47f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x110f48540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x110f48b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x110f490c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x110f49680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x110f49c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x110f4a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x110f4a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x110f4ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x110f4b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x110f4b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x110f4bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x110f4c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x110f4ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x110f4d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x110f4d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x110f4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x110f4e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x110f4e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x110f4ecc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x110f4f280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x110f4f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x110f4fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x110f503c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x110f50980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x110f50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x110f51500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x110f51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x110f52080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x110f52640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x110f52c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x110f531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x110f53780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x110f53d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x110f54300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x110f548c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x110f54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x110f55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x110f55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x110f55fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x110f56580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x110f56b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x110f57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x110f57540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x110f57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x110f57f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x110f58440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x110f58940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x110f58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x110f59340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x110f59840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x110f59d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x110f5a240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x110f5a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x110f5ac40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x110f5b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x110f5b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x110f5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x110f5c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x110f5c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x110f5ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x110f5cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x110f5d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x110f5d940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x110f5de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x110f5e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x110f5e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x110f5f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x110f5f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x110f60090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x110f607b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x110f60a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x110f61260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x110f61520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x110f61b30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13ae088b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13ae08e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13ae1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13ae1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13ae1d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13ae12690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13ae19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13ae19aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13ae1a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13ae18b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13ae18560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13ae1b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13ae11690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13ae07510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13ae1d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13ae29f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13ae14870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13ae14b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13ae12ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13ae12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13ae13220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13ae2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13ae30120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13ae303e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13ae306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13ae30960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13ae30c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13ae30ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13ae311a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13ae31460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13ae31720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13ae319e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13ae31ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13ae31f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13ae32220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13ae324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13ae327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13ae32a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13ae32d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13ae32fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13ae332a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13ae33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13ae33820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13ae33ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13ae33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13ae34060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13ae34320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13ae345e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13ae348a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13ae34b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13ae34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13ae350e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13ae353a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13ae35660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13ae35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13ae35be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13ae35ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13ae36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13ae36420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13ae366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13ae369a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13ae36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13ae36f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13ae371e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13ae374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13ae37760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13ae37a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13ae37ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13ae37fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13ae38260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13ae38520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13ae387e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13ae38aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13ae38d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13ae39020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13ae392e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13ae395a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13ae39860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13ae39b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13ae39de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13ae3a330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13ae3a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13ae3add0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13ae3b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13ae3b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13ae3bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13ae3c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13ae3c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13ae3cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13ae3d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13ae3d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13ae3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13ae3e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13ae3e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13ae3ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13ae3f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13ae3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13ae3fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13ae402d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13ae40820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13ae40d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13ae412c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13ae41810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13ae41d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13ae422b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13ae42800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13ae42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13ae432a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13ae437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13ae43d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13ae44290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13ae447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13ae44d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13ae451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13ae45670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13ae45b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13ae45fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13ae46450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13ae468f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13ae46d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x13ae47230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x13ae476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x13ae47b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x13ae48010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x13ae484b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x13ae48950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x13ae48df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13ae49290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13ae49730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13ae49bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13ae4a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13ae4a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13ae4a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13ae4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13ae4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13ae4b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13ae4bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13ae4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13ae4c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13ae4ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13ae4ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13ae4d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13ae4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13ae4dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13ae4e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13ae4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13ae4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13ae4ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13ae4f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13ae4f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13ae4fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13ae50190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13ae50630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13ae50ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13ae50f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13ae51410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13ae518b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13ae51d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13ae521f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13ae52690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13ae52b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13ae52fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13ae53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13ae53910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13ae53db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13ae54250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13ae546f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13ae54b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13ae55030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13ae554d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13ae55970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13ae55e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13ae562b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13ae56750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13ae56bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13ae57090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13ae57530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13ae579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13ae57e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13ae58310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13ae587b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13ae58c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x13ae590f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x13ae59590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13ae59a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13ae59ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13ae5a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13ae5a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13ae5acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13ae5b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13ae5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13ae5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13ae5bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13ae5c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13ae5c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13ae5cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13ae5d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13ae5d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13ae5dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13ae5e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13ae5e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13ae5f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13ae5f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13ae5f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13ae5fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13ae604d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13ae60cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13ae61160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13ae61600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13ae61aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13ae62250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13ae627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13ae62cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13ae63240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13ae63790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13ae63ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13ae64230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13ae64780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13ae64cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13ae65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13ae65770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13ae65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13ae66210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13ae66760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13ae66cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x13ae67200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x13ae67750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x13ae67ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x13ae681f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x13ae68740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x13ae68c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x13ae691e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x13ae69730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x13ae69c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x13ae6a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x13ae6a720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x13ae6ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x13ae6b1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x13ae6b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x13ae6bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x13ae6c1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13ae6c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13ae6cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13ae6d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13ae6d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13ae6dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13ae6e190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13ae6e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13ae6ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13ae6f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13ae6f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13ae6fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13ae70170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13ae706c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13ae70c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13ae71160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13ae716b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13ae71c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13ae72150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13ae726a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13ae72bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13ae73140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13ae73690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13ae73be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13ae74130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13ae74680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13ae74bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13ae75070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13ae75510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13ae759b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13ae75e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13ae762f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13ae76790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13ae76c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13ae770d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x13ae77570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x13ae77a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x13ae77eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x13ae78350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x13ae787f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x13ae78c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x13ae79130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x13ae795d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x13ae79a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x13ae79f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x13ae7a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x13ae7a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x13ae7acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x13ae7b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x13ae7b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x13ae7bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x13ae7bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x13ae7c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x13ae7cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x13ae7d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x13ae7da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x13ae7e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x13ae7e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x13ae7ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x13ae7eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x13ae7f4c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.776s
user	0m0.280s
sys	0m0.324s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4789 (9c42b171)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 ''
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14270e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14270ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14270f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14270f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14270fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1427104a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142710a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142711000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1427115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x142711ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142711fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1427124b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x142712fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142713780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142713f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1427146b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142714dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1427154f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x142715c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1427163e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142716b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x142717220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142717940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1427181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142718900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x142718bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1427191d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142719e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14271a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14271a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14271aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14271ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14271b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14271bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14271be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14271c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14271c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14271cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14271d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14271d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14271d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14271de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14271e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14271e7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14271ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14271f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14271f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14271ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1427205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142720bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x142721200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142721810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142721e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142722430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142722c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1427230c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142723560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142723820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x142723e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142724620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1427248e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142724d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x142725220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1427256c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142725b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142726000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1427264a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142726940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142726de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142727280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x142727720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142727bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142728060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x1427285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x142728b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142729050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1427295a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142729af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14272a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14272a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14272aae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14272b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14272b580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14272bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14272c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14272c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14272cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14272d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14272d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14272dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14272e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14272e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14272eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14272eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14272f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14272fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14272ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14271fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x142730450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x142730c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x142731150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1427316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x142731bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x142732140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x142732690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x142732be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x142733130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x142733680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x142733bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x142734120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x142734670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x142734bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x142735110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1427355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x142735a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x142735ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x142736390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x142736830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x142736cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x142737170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142737610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142737ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x142737f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1427383f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142738890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142738d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1427391d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142739670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142739b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142739fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14273a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14273a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14273ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14273b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14273b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14273bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14273c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14273c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14273c950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14273cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14273d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14273d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14273dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14273e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14273e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14273e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14273ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14273f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14273f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14273fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1427400d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142740570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142740a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142740eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142741350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x1427417f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142741c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142742130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x1427425d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142742a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142742f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1427433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x142743850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142743cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142744190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142744630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x142744ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142744f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142745410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1427458b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142745d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1427461f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142746690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142746b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x142746fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142747470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142747910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142747db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x142748250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1427486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x142748b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x142749030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1427494d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x142749970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x142749e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14274a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14274a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14274abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14274b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14274b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14274b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14274be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14274c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14274c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14274cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14274d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14274d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14274db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14274e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14274e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14274ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14274f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14274f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14274fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1427502a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1427508b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1427510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x142751540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1427519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x142751e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x142752630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x142752b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1427530d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142753620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x142753b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1427540c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142754610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x142754b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1427550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142755600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x142755b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1427560a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1427565f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x142756b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142757090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1427575e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142757b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142758080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1427585d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142758b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142759070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1427595c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142759b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14275a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14275a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14275ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14275b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14275b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14275baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14275c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14275c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14275cae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14275d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14275d580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14275dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14275e020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14275e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14275eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14275f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14275f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14275fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x142760000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x142760550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x142760aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x142760ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x142761540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x142761a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x142761fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x142762530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x142762a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x142762fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x142763520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x142763a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x142763fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x142764510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x142764a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x142764fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x142765450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1427658f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x142765d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x142766230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1427666d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x142766b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x142767010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1427674b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x142767950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x142767df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x142768290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x142768730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x142768bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x142769070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x142769510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1427699b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x142769e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x14276a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x14276a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x14276ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x14276b0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x14276b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x14276ba10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x14276beb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x14276c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14276c8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14276cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14276d6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14276de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14276e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14276e7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14276efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14276f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14276f8a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.103.324 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.327 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14276f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14274e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14274ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14274e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x142721ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1427214c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x142723ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x142750560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x142718e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14271f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x142720290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x1427208a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14271ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x142720eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x142717e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x1427240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x142730710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14276eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14271b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14271b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x142750b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14274f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x142719490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x142719750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x142719a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14276fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14276ffc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x142770280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x142770540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x142770800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x142770ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x142770d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x142771040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x142771300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1427715c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x142771880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x142771b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x142771e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1427720c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x142772380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x142772640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x142772900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x142772bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x142772e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x142773140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x142773400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1427736c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x142773980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x142773c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x142773f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1427741c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x142774480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x142774740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x142774a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x142774cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x142774f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x142775240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x142775500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1427757c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x142775a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x142775d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x142776000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1427762c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x142776580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x142776840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x142776b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x142776dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x142777080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x142777340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x142777600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1427778c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x142777b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x142777e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x142778100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1427783c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x142778680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x142778940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x142778c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x142778ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x142779180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x142779440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x142779700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1427799c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x142779c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x142779f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14277a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14277a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14277a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14277aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14277ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14277afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14277b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14277b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14277b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14277bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14277bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14277c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14277c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14277c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14277c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14277cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14277ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14277d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14277d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14277d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14277d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14277dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14277de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14277e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14277e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14277e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14277e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14277ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14277ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14277f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14277f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14277f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14277fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14277fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14277ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x142780240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x142780500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1427807c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x142780a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x142780d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x142781000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x1427812c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x142781580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x142781840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x142781b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x142781dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x142782080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x142782340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x142782600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1427828c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x142782b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x142782e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x142783100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1427833c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x142783680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x142783940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x142783c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x142783ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x142784180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x142784440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x142784700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1427849c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x142784c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x142784f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x142785200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1427854c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x142785780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x142785a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x142785d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x142785fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x142786280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x142786540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x142786800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x142786ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x142786d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x142787040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x142787300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1427875c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x142787880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x142787b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x142787e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1427880c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x142788380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x142788640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x142788900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x142788bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x142788e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x142789140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x142789400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1427896c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x142789980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x142789c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x142789f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x14278a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x14278a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x14278a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x14278aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14278acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14278af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14278b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14278b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14278b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14278ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14278bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14278c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14278c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14278c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14278c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14278cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14278cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14278d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14278d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14278d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14278d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14278db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14278de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14278e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14278e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14278e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x14278e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x14278ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x14278eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x14278f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x14278f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x14278f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x14278fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x14278ff90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x142790250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x142790510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1427907d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x142790d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x142791270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1427917c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x142791d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x142792260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1427927b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x142792d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x142793250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1427937a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x142793cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x142794240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x142794790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x142794ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x142795230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x142795780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x142795cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x142796220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x142796770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x142796cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x142797210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x142797760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x142797cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x142798200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x142798750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x142798ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1427991f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x142799740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x142799c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14279a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14279a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14279ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14279b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14279b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14279bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x14279c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x14279c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x14279cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x14279d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x14279d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x14279dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x14279e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x14279e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x14279ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x14279f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x14279f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x14279fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1427a0180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1427a06d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1427a0c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1427a1170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1427a16c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1427a1c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1427a1ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1427a2190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1427a2450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1427a28c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1427a2d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1427a31a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1427a3610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1427a3a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1427a3ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1427a4360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1427a47d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1427a4c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1427a50b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1427a5520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1427a5990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1427a5e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1427a6270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x1427a66e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x1427a6b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x1427a6fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x1427a7430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x1427a78a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x1427a7d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x1427a8180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1427a85f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1427a8a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1427a94c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1427a9be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1427aa300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1427aaa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1427aace0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1427ab4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1427ab790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1427abda0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x175a04b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x175a04fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x175a05440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x175a058b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x175a05d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x175a06190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x175a06600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x175a06a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x175a06ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x175a07350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x175a077c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x175a07e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x175a089a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x175a09150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x175a09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x175a0a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x175a0a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x175a0aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x175a0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x175a0bdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x175a0c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x175a0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x175a0d310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x175a0da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x175a0e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x175a0e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x175a0e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x175a0eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x175a0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x175a0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x175a0f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x175a0fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x175a10230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x175a104f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x175a10960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x175a10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x175a11240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x175a116b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x175a11b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x175a11f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x175a12400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x175a12870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x175a12ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x175a13150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x175a135c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x175a13a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x175a13ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x175a14310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x175a14780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x175a14bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x175a15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x175a154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x175a15940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x175a15db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x175a16220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x175a16690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x175a16c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x175a17100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x175a17570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x175a179e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x175a17e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x175a182c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x175a18730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x175a18ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x175a19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x175a19480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x175a198f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x175a19d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x175a1a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x175a1a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x175a1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x175a1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x175a1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x175a1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x175a1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x175a1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x175a1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x175a1c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x175a1ce30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x175a1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x175a1d710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x175a1db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x175a1dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x175a1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x175a1e8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x175a1ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x175a1f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x175a1f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x175a1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x175a1ff00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x175a20370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x175a207e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x175a20c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x175a210c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x175a21530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x175a219a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x175a21e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x175a22280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x175a22ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x175a23000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x175a235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x175a23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x175a24110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x175a246c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x175a24c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x175a25220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x175a257d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x175a25d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x175a26330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x175a268e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x175a26e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x175a27440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x175a279f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x175a27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x175a284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x175a289a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x175a28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x175a293a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x175a298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x175a29da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x175a2a2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x175a2a7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x175a2aca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x175a2b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x175a2b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x175a2bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x175a2c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x175a2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x175a2caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x175a2cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x175a2d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x175a2d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x175a2dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x175a2e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x175a2e8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x175a2eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x175a2f2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x175a2f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x175a2fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x175a301a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x175a306a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x175a30ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x175a310a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x175a315a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x175a31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x175a31fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x175a324a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x175a329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x175a32ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x175a333a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x175a338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x175a33da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x175a342a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x175a347a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x175a34ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x175a351a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x175a356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x175a35ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x175a360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x175a365a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x175a36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x175a36fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x175a374a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x175a379a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x175a37ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x175a383a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x175a388a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x175a38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x175a392a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x175a397a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x175a39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x175a3a1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x175a3a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x175a3aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x175a3b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x175a3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x175a3baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x175a3bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x175a3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x175a3c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x175a3cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x175a3d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x175a3d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x175a3dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x175a3e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x175a3e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x175a3eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x175a3f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x175a3f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x175a3fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x175a400a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x175a405a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x175a40aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x175a40fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x175a41550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x175a41b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x175a420b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x175a42660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x175a42c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x175a43280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x175a43890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x175a44080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x175a44520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x175a447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x175a44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x175a45400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x175a45bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x175a46090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x175a46530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x175a469d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x175a47180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x175a476d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x175a47c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x175a48170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x175a486c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x175a48c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x175a49160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x175a496b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x175a49c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x175a4a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x175a4a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x175a4abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x175a4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x175a4b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x175a4bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x175a4c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x175a4c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x175a4cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x175a4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x175a4d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x175a4dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x175a4e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x175a4e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x175a4ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x175a4f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x175a4f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x175a4fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x175a500f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x175a50640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x175a50b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x175a510e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x175a51630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x175a51b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x175a520d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x175a52620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x175a52b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x175a530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x175a53610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x175a53b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x175a540b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x175a54600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x175a54b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x175a550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x175a555f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x175a55b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x175a56090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x175a565e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x175a56b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x175a57080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x175a575d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x175a57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x175a58070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x175a585c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x175a58b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x175a59060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x175a595b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x175a59b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x175a59fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x175a5a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x175a5a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x175a5ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x175a5b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x175a5b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x175a5bb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x175a5c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x175a5c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x175a5c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x175a5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x175a5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x175a5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x175a5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x175a5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x175a5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x175a5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x175a5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x175a5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x175a5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x175a5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x175a600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x175a60560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x175a60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x175a60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x175a613f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x175a61b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x175a62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x175a62950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x175a63070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x175a63330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x175a63b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x175a63de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x175a643f0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.957s
user	0m0.232s
sys	0m0.176s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.44 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.95 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.39 sec*proc (2 tests)

Total Test time (real) =   1.40 sec
        1.42 real         0.51 user         0.19 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.22 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.30 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.12 user         0.08 sys
```
