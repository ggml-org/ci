Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:299 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.3s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.531s
user	0m0.846s
sys	0m1.187s
++ nproc
+ make -j10
[  0%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  0%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Built target build_info
[  5%] Built target sha1
[  5%] Built target sha256
[  5%] Built target xxhash
[  5%] Linking CXX shared library libggml-base.dylib
[  5%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  9%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 11%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[ 12%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 12%] Linking CXX shared library libggml-blas.dylib
[ 12%] Linking CXX shared library libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library libggml.dylib
[ 14%] Built target ggml
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 23%] Linking CXX executable ../../bin/llama-gguf
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 25%] Linking CXX shared library libllama.dylib
[ 25%] Built target llama-gguf-hash
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 25%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 25%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 26%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 31%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 31%] Linking CXX executable ../../bin/llama-simple
[ 32%] Linking CXX executable ../../bin/llama-simple-chat
[ 32%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 33%] Built target llava
[ 34%] Linking CXX executable ../../bin/llama-quantize-stats
[ 36%] Linking CXX static library libcommon.a
[ 36%] Linking CXX static library libllava_static.a
[ 36%] Linking CXX shared library libllava_shared.dylib
[ 36%] Built target llama-simple
[ 36%] Built target llama-simple-chat
[ 36%] Built target test-c
[ 36%] Built target llama-quantize-stats
[ 36%] Built target llava_static
[ 36%] Built target common
[ 36%] Built target llava_shared
[ 39%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Linking CXX executable ../bin/test-tokenizer-0
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 45%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-log
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-grammar-parser
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-log
[ 49%] Built target test-grammar-integration
[ 50%] Built target test-sampling
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 50%] Built target test-json-schema-to-grammar
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 51%] Built target test-arg-parser
[ 51%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 54%] Linking CXX executable ../bin/test-chat-template
[ 55%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 55%] Linking CXX executable ../bin/test-backend-ops
[ 56%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-gguf
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-model-load-cancel
[ 58%] Linking CXX executable ../bin/test-autorelease
[ 58%] Linking CXX executable ../bin/test-barrier
[ 59%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 61%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Built target test-chat-template
[ 62%] Built target test-backend-ops
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-model-load-cancel
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Built target test-autorelease
[ 62%] Linking CXX executable ../../bin/llama-batched-bench
[ 63%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Linking CXX executable ../bin/test-rope
[ 65%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target test-barrier
[ 67%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 67%] Built target test-quantize-fns
[ 68%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 68%] Linking CXX executable ../../bin/llama-batched
[ 68%] Linking CXX executable ../../bin/llama-embedding
[ 68%] Linking CXX executable ../../bin/llama-eval-callback
[ 68%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 69%] Built target test-quantize-perf
[ 69%] Built target llama-batched-bench
[ 70%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-gguf-split
[ 70%] Built target test-rope
[ 70%] Linking CXX executable ../../bin/llama-gritlm
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 70%] Built target llama-batched
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-embedding
[ 73%] Built target llama-gguf-split
[ 73%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 75%] Built target llama-gritlm
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Built target llama-imatrix
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 78%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 78%] Linking CXX executable ../../bin/llama-lookup
[ 78%] Linking CXX executable ../../bin/llama-lookup-create
[ 78%] Linking CXX executable ../../bin/llama-lookup-merge
[ 78%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 78%] Built target llama-infill
[ 78%] Linking CXX executable ../../bin/llama-cli
[ 78%] Built target llama-lookahead
[ 78%] Built target llama-bench
[ 79%] Linking CXX executable ../../bin/llama-parallel
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-cli
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup
[ 81%] Generating loading.html.hpp
[ 82%] Linking CXX executable ../../bin/llama-perplexity
[ 83%] Linking CXX executable ../../bin/llama-quantize
[ 84%] Linking CXX executable ../../bin/llama-retrieval
[ 84%] Built target llama-parallel
[ 84%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Generating index.html.gz.hpp
[ 84%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 84%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 84%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Built target llama-passkey
[ 85%] Linking CXX executable ../../bin/llama-save-load-state
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 86%] Built target llama-retrieval
[ 87%] Linking CXX executable ../../bin/llama-run
[ 88%] Built target llama-quantize
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 89%] Built target llama-perplexity
[ 90%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 91%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 93%] Linking CXX executable ../../bin/llama-tts
[ 93%] Built target llama-save-load-state
[ 93%] Built target llama-speculative
[ 93%] Built target llama-run
[ 93%] Built target llama-speculative-simple
[ 93%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-gen-docs
[ 93%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-tts
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Built target llama-convert-llama2c-to-ggml
[ 96%] Built target llama-gen-docs
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Built target llama-cvector-generator
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m2.979s
user	0m6.106s
sys	0m9.922s

main: quantize time =  5678.22 ms
main:    total time =  5678.22 ms

main: quantize time =  1787.75 ms
main:    total time =  1787.75 ms

main: quantize time =  1671.35 ms
main:    total time =  1671.35 ms

main: quantize time =  2263.79 ms
main:    total time =  2263.79 ms

main: quantize time =  1850.72 ms
main:    total time =  1850.72 ms

main: quantize time =  4946.56 ms
main:    total time =  4946.56 ms

main: quantize time =  5279.47 ms
main:    total time =  5279.47 ms

main: quantize time =  6363.26 ms
main:    total time =  6363.26 ms

main: quantize time =  5498.24 ms
main:    total time =  5498.24 ms

main: quantize time =  4342.34 ms
main:    total time =  4342.34 ms
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.161 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.255 I main: llama backend init
0.00.000.260 I main: load the model and apply lora adapter, if any
0.00.035.530 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.049.212 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.049.223 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.049.237 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.049.238 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.049.239 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.049.239 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.049.240 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.049.242 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.049.243 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.049.243 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.049.244 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.049.249 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.049.250 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.049.251 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.049.255 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.049.255 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.049.259 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.058.133 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.060.479 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.067.503 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.067.505 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.067.505 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.067.506 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.067.506 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.067.507 I llama_model_loader: - type  f32:  194 tensors
0.00.067.507 I llama_model_loader: - type  f16:   98 tensors
0.00.096.441 I llm_load_vocab: special tokens cache size = 25
0.00.103.045 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.103.048 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.103.048 I llm_load_print_meta: arch             = gptneox
0.00.103.048 I llm_load_print_meta: vocab type       = BPE
0.00.103.048 I llm_load_print_meta: n_vocab          = 50304
0.00.103.048 I llm_load_print_meta: n_merges         = 50009
0.00.103.049 I llm_load_print_meta: vocab_only       = 0
0.00.103.049 I llm_load_print_meta: n_ctx_train      = 2048
0.00.103.049 I llm_load_print_meta: n_embd           = 2048
0.00.103.049 I llm_load_print_meta: n_layer          = 24
0.00.103.052 I llm_load_print_meta: n_head           = 16
0.00.103.053 I llm_load_print_meta: n_head_kv        = 16
0.00.103.053 I llm_load_print_meta: n_rot            = 32
0.00.103.053 I llm_load_print_meta: n_swa            = 0
0.00.103.053 I llm_load_print_meta: n_embd_head_k    = 128
0.00.103.053 I llm_load_print_meta: n_embd_head_v    = 128
0.00.103.054 I llm_load_print_meta: n_gqa            = 1
0.00.103.055 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.103.055 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.103.059 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.103.060 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.103.060 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.103.060 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.103.061 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.103.061 I llm_load_print_meta: n_ff             = 8192
0.00.103.061 I llm_load_print_meta: n_expert         = 0
0.00.103.062 I llm_load_print_meta: n_expert_used    = 0
0.00.103.062 I llm_load_print_meta: causal attn      = 1
0.00.103.062 I llm_load_print_meta: pooling type     = 0
0.00.103.062 I llm_load_print_meta: rope type        = 2
0.00.103.062 I llm_load_print_meta: rope scaling     = linear
0.00.103.063 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.103.063 I llm_load_print_meta: freq_scale_train = 1
0.00.103.063 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.103.063 I llm_load_print_meta: rope_finetuned   = unknown
0.00.103.064 I llm_load_print_meta: ssm_d_conv       = 0
0.00.103.064 I llm_load_print_meta: ssm_d_inner      = 0
0.00.103.064 I llm_load_print_meta: ssm_d_state      = 0
0.00.103.064 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.103.064 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.103.064 I llm_load_print_meta: model type       = 1.4B
0.00.103.065 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.103.065 I llm_load_print_meta: model params     = 1.41 B
0.00.103.066 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.103.066 I llm_load_print_meta: general.name     = 1.4B
0.00.103.066 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.103.066 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.103.067 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.103.067 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.103.067 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.103.067 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.103.067 I llm_load_print_meta: max token length = 1024
0.00.105.077 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.105.077 I llm_load_tensors: offloading output layer to GPU
0.00.105.077 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.105.095 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.105.096 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.105.986 I llama_new_context_with_model: n_seq_max     = 1
0.00.105.987 I llama_new_context_with_model: n_ctx         = 2048
0.00.105.987 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.105.987 I llama_new_context_with_model: n_batch       = 2048
0.00.105.988 I llama_new_context_with_model: n_ubatch      = 512
0.00.105.988 I llama_new_context_with_model: flash_attn    = 0
0.00.105.988 I llama_new_context_with_model: freq_base     = 10000.0
0.00.105.988 I llama_new_context_with_model: freq_scale    = 1
0.00.105.989 I ggml_metal_init: allocating
0.00.105.992 I ggml_metal_init: found device: Apple M4
0.00.105.994 I ggml_metal_init: picking default device: Apple M4
0.00.106.643 I ggml_metal_init: using embedded metal library
0.00.116.430 I ggml_metal_init: GPU name:   Apple M4
0.00.116.432 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.116.432 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.116.432 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.116.432 I ggml_metal_init: simdgroup reduction   = true
0.00.116.433 I ggml_metal_init: simdgroup matrix mul. = true
0.00.116.433 I ggml_metal_init: has bfloat            = true
0.00.116.433 I ggml_metal_init: use bfloat            = true
0.00.116.433 I ggml_metal_init: hasUnifiedMemory      = true
0.00.116.434 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.139.848 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.158.249 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.158.254 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.158.281 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.159.189 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.159.190 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.159.191 I llama_new_context_with_model: graph nodes  = 967
0.00.159.191 I llama_new_context_with_model: graph splits = 2
0.00.159.194 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.159.335 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.159.336 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.236.611 I main: llama threadpool init, n_threads = 4
0.00.236.666 I 
0.00.236.698 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.236.701 I 
0.00.236.890 I sampler seed: 1234
0.00.236.895 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.236.930 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.236.931 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.236.932 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.035.991 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59314.95 tokens per second)
0.02.035.992 I llama_perf_context_print:        load time =     201.06 ms
0.02.035.993 I llama_perf_context_print: prompt eval time =      43.51 ms /     7 tokens (    6.22 ms per token,   160.88 tokens per second)
0.02.035.993 I llama_perf_context_print:        eval time =    1752.72 ms /    63 runs   (   27.82 ms per token,    35.94 tokens per second)
0.02.035.994 I llama_perf_context_print:       total time =    1799.39 ms /    70 tokens
0.02.036.262 I ggml_metal_free: deallocating

real	0m2.385s
user	0m0.147s
sys	0m0.103s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.009.476 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.556 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.563 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.563 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.563 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.565 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.565 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.565 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.566 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.566 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.566 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.567 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.568 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.569 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.569 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.410 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.502 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.348 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.350 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.350 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.350 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.351 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.351 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.352 I llama_model_loader: - type  f32:  194 tensors
0.00.027.352 I llama_model_loader: - type q8_0:   98 tensors
0.00.049.214 I llm_load_vocab: special tokens cache size = 25
0.00.055.252 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.257 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.257 I llm_load_print_meta: arch             = gptneox
0.00.055.258 I llm_load_print_meta: vocab type       = BPE
0.00.055.258 I llm_load_print_meta: n_vocab          = 50304
0.00.055.258 I llm_load_print_meta: n_merges         = 50009
0.00.055.258 I llm_load_print_meta: vocab_only       = 0
0.00.055.258 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.258 I llm_load_print_meta: n_embd           = 2048
0.00.055.259 I llm_load_print_meta: n_layer          = 24
0.00.055.264 I llm_load_print_meta: n_head           = 16
0.00.055.265 I llm_load_print_meta: n_head_kv        = 16
0.00.055.265 I llm_load_print_meta: n_rot            = 32
0.00.055.265 I llm_load_print_meta: n_swa            = 0
0.00.055.265 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.265 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.268 I llm_load_print_meta: n_gqa            = 1
0.00.055.268 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.269 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.270 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.270 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.270 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.271 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.271 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.272 I llm_load_print_meta: n_ff             = 8192
0.00.055.272 I llm_load_print_meta: n_expert         = 0
0.00.055.272 I llm_load_print_meta: n_expert_used    = 0
0.00.055.272 I llm_load_print_meta: causal attn      = 1
0.00.055.273 I llm_load_print_meta: pooling type     = 0
0.00.055.273 I llm_load_print_meta: rope type        = 2
0.00.055.274 I llm_load_print_meta: rope scaling     = linear
0.00.055.274 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.274 I llm_load_print_meta: freq_scale_train = 1
0.00.055.274 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.275 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.275 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.275 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.275 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.275 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.276 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.278 I llm_load_print_meta: model type       = 1.4B
0.00.055.278 I llm_load_print_meta: model ftype      = Q8_0
0.00.055.279 I llm_load_print_meta: model params     = 1.41 B
0.00.055.279 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.055.279 I llm_load_print_meta: general.name     = 1.4B
0.00.055.280 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.280 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.280 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.280 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.280 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.055.280 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.281 I llm_load_print_meta: max token length = 1024
0.00.057.635 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.057.635 I llm_load_tensors: offloading output layer to GPU
0.00.057.635 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.057.646 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.057.647 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.058.561 I llama_new_context_with_model: n_seq_max     = 1
0.00.058.561 I llama_new_context_with_model: n_ctx         = 2048
0.00.058.562 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.058.562 I llama_new_context_with_model: n_batch       = 2048
0.00.058.562 I llama_new_context_with_model: n_ubatch      = 512
0.00.058.562 I llama_new_context_with_model: flash_attn    = 0
0.00.058.563 I llama_new_context_with_model: freq_base     = 10000.0
0.00.058.563 I llama_new_context_with_model: freq_scale    = 1
0.00.058.564 I ggml_metal_init: allocating
0.00.058.570 I ggml_metal_init: found device: Apple M4
0.00.058.572 I ggml_metal_init: picking default device: Apple M4
0.00.059.308 I ggml_metal_init: using embedded metal library
0.00.062.010 I ggml_metal_init: GPU name:   Apple M4
0.00.062.012 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.012 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.012 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.013 I ggml_metal_init: simdgroup reduction   = true
0.00.062.013 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.013 I ggml_metal_init: has bfloat            = true
0.00.062.013 I ggml_metal_init: use bfloat            = true
0.00.062.014 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.015 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.072.486 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.797 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.805 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.829 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.940 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.944 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.944 I llama_new_context_with_model: graph nodes  = 967
0.00.097.945 I llama_new_context_with_model: graph splits = 2
0.00.097.948 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.098.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.098.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.328.664 I main: llama threadpool init, n_threads = 4
0.01.328.702 I 
0.01.328.723 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.328.723 I 
0.01.328.886 I sampler seed: 1234
0.01.328.890 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.328.904 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.328.905 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.328.905 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.413.083 I llama_perf_sampler_print:    sampling time =       1.13 ms /    71 runs   (    0.02 ms per token, 62665.49 tokens per second)
0.02.413.084 I llama_perf_context_print:        load time =    1319.18 ms
0.02.413.084 I llama_perf_context_print: prompt eval time =      40.21 ms /     7 tokens (    5.74 ms per token,   174.07 tokens per second)
0.02.413.085 I llama_perf_context_print:        eval time =    1041.07 ms /    63 runs   (   16.52 ms per token,    60.51 tokens per second)
0.02.413.087 I llama_perf_context_print:       total time =    1084.42 ms /    70 tokens
0.02.413.313 I ggml_metal_free: deallocating

real	0m2.430s
user	0m0.113s
sys	0m0.280s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.018.097 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.038.866 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.038.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.038.876 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.038.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.038.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.038.879 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.038.879 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.038.880 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.038.880 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.038.881 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.038.881 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.038.881 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.038.881 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.038.882 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.038.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.038.885 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.038.885 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.391 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.048.656 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.048.658 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.048.658 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.048.658 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.048.659 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.048.659 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.048.660 I llama_model_loader: - type  f32:  194 tensors
0.00.048.660 I llama_model_loader: - type q4_0:   97 tensors
0.00.048.660 I llama_model_loader: - type q6_K:    1 tensors
0.00.077.267 I llm_load_vocab: special tokens cache size = 25
0.00.087.533 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.087.537 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.087.537 I llm_load_print_meta: arch             = gptneox
0.00.087.538 I llm_load_print_meta: vocab type       = BPE
0.00.087.538 I llm_load_print_meta: n_vocab          = 50304
0.00.087.538 I llm_load_print_meta: n_merges         = 50009
0.00.087.539 I llm_load_print_meta: vocab_only       = 0
0.00.087.539 I llm_load_print_meta: n_ctx_train      = 2048
0.00.087.539 I llm_load_print_meta: n_embd           = 2048
0.00.087.539 I llm_load_print_meta: n_layer          = 24
0.00.087.543 I llm_load_print_meta: n_head           = 16
0.00.087.544 I llm_load_print_meta: n_head_kv        = 16
0.00.087.544 I llm_load_print_meta: n_rot            = 32
0.00.087.544 I llm_load_print_meta: n_swa            = 0
0.00.087.545 I llm_load_print_meta: n_embd_head_k    = 128
0.00.087.545 I llm_load_print_meta: n_embd_head_v    = 128
0.00.087.546 I llm_load_print_meta: n_gqa            = 1
0.00.087.547 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.087.548 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.087.549 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.087.551 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.087.551 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.087.552 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.087.552 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.087.553 I llm_load_print_meta: n_ff             = 8192
0.00.087.553 I llm_load_print_meta: n_expert         = 0
0.00.087.553 I llm_load_print_meta: n_expert_used    = 0
0.00.087.554 I llm_load_print_meta: causal attn      = 1
0.00.087.554 I llm_load_print_meta: pooling type     = 0
0.00.087.554 I llm_load_print_meta: rope type        = 2
0.00.087.554 I llm_load_print_meta: rope scaling     = linear
0.00.087.555 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.087.556 I llm_load_print_meta: freq_scale_train = 1
0.00.087.556 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.087.556 I llm_load_print_meta: rope_finetuned   = unknown
0.00.087.557 I llm_load_print_meta: ssm_d_conv       = 0
0.00.087.557 I llm_load_print_meta: ssm_d_inner      = 0
0.00.087.558 I llm_load_print_meta: ssm_d_state      = 0
0.00.087.558 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.087.558 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.087.558 I llm_load_print_meta: model type       = 1.4B
0.00.087.559 I llm_load_print_meta: model ftype      = Q4_0
0.00.087.559 I llm_load_print_meta: model params     = 1.41 B
0.00.087.560 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.087.560 I llm_load_print_meta: general.name     = 1.4B
0.00.087.561 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.087.561 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.087.564 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.087.564 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.087.565 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.087.565 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.087.565 I llm_load_print_meta: max token length = 1024
0.00.090.516 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.090.516 I llm_load_tensors: offloading output layer to GPU
0.00.090.516 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.090.528 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.090.530 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.091.831 I llama_new_context_with_model: n_seq_max     = 1
0.00.091.832 I llama_new_context_with_model: n_ctx         = 2048
0.00.091.833 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.091.833 I llama_new_context_with_model: n_batch       = 2048
0.00.091.833 I llama_new_context_with_model: n_ubatch      = 512
0.00.091.834 I llama_new_context_with_model: flash_attn    = 0
0.00.091.834 I llama_new_context_with_model: freq_base     = 10000.0
0.00.091.834 I llama_new_context_with_model: freq_scale    = 1
0.00.091.835 I ggml_metal_init: allocating
0.00.091.843 I ggml_metal_init: found device: Apple M4
0.00.091.846 I ggml_metal_init: picking default device: Apple M4
0.00.092.831 I ggml_metal_init: using embedded metal library
0.00.096.564 I ggml_metal_init: GPU name:   Apple M4
0.00.096.566 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.096.566 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.096.567 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.096.567 I ggml_metal_init: simdgroup reduction   = true
0.00.096.567 I ggml_metal_init: simdgroup matrix mul. = true
0.00.096.567 I ggml_metal_init: has bfloat            = true
0.00.096.568 I ggml_metal_init: use bfloat            = true
0.00.096.568 I ggml_metal_init: hasUnifiedMemory      = true
0.00.096.569 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.109.493 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.134.737 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.134.746 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.134.770 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.135.910 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.135.913 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.135.913 I llama_new_context_with_model: graph nodes  = 967
0.00.135.914 I llama_new_context_with_model: graph splits = 2
0.00.135.918 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.136.046 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.136.047 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.715.908 I main: llama threadpool init, n_threads = 4
0.00.715.989 I 
0.00.716.034 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.716.034 I 
0.00.716.310 I sampler seed: 1234
0.00.716.317 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.716.339 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.716.341 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.716.341 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.401.821 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56891.03 tokens per second)
0.01.401.822 I llama_perf_context_print:        load time =     697.80 ms
0.01.401.823 I llama_perf_context_print: prompt eval time =      45.89 ms /     7 tokens (    6.56 ms per token,   152.55 tokens per second)
0.01.401.823 I llama_perf_context_print:        eval time =     636.47 ms /    63 runs   (   10.10 ms per token,    98.98 tokens per second)
0.01.401.825 I llama_perf_context_print:       total time =     685.92 ms /    70 tokens
0.01.402.058 I ggml_metal_free: deallocating

real	0m1.418s
user	0m0.132s
sys	0m0.176s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.008.632 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.027.238 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.027.242 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.244 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.244 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.244 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.245 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.245 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.246 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.246 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.246 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.247 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.247 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.247 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.248 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.250 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.250 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.251 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.319 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.192 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.193 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.193 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.194 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.194 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.194 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.036.195 I llama_model_loader: - type  f32:  194 tensors
0.00.036.195 I llama_model_loader: - type q4_1:   97 tensors
0.00.036.195 I llama_model_loader: - type q6_K:    1 tensors
0.00.059.441 I llm_load_vocab: special tokens cache size = 25
0.00.065.508 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.065.511 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.065.512 I llm_load_print_meta: arch             = gptneox
0.00.065.512 I llm_load_print_meta: vocab type       = BPE
0.00.065.512 I llm_load_print_meta: n_vocab          = 50304
0.00.065.512 I llm_load_print_meta: n_merges         = 50009
0.00.065.512 I llm_load_print_meta: vocab_only       = 0
0.00.065.512 I llm_load_print_meta: n_ctx_train      = 2048
0.00.065.513 I llm_load_print_meta: n_embd           = 2048
0.00.065.513 I llm_load_print_meta: n_layer          = 24
0.00.065.515 I llm_load_print_meta: n_head           = 16
0.00.065.516 I llm_load_print_meta: n_head_kv        = 16
0.00.065.516 I llm_load_print_meta: n_rot            = 32
0.00.065.516 I llm_load_print_meta: n_swa            = 0
0.00.065.516 I llm_load_print_meta: n_embd_head_k    = 128
0.00.065.517 I llm_load_print_meta: n_embd_head_v    = 128
0.00.065.517 I llm_load_print_meta: n_gqa            = 1
0.00.065.518 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.065.519 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.065.519 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.065.520 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.065.520 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.065.520 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.065.520 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.065.521 I llm_load_print_meta: n_ff             = 8192
0.00.065.521 I llm_load_print_meta: n_expert         = 0
0.00.065.521 I llm_load_print_meta: n_expert_used    = 0
0.00.065.523 I llm_load_print_meta: causal attn      = 1
0.00.065.525 I llm_load_print_meta: pooling type     = 0
0.00.065.525 I llm_load_print_meta: rope type        = 2
0.00.065.525 I llm_load_print_meta: rope scaling     = linear
0.00.065.525 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.065.526 I llm_load_print_meta: freq_scale_train = 1
0.00.065.526 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.065.526 I llm_load_print_meta: rope_finetuned   = unknown
0.00.065.527 I llm_load_print_meta: ssm_d_conv       = 0
0.00.065.527 I llm_load_print_meta: ssm_d_inner      = 0
0.00.065.527 I llm_load_print_meta: ssm_d_state      = 0
0.00.065.527 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.065.527 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.065.527 I llm_load_print_meta: model type       = 1.4B
0.00.065.528 I llm_load_print_meta: model ftype      = Q4_1
0.00.065.528 I llm_load_print_meta: model params     = 1.41 B
0.00.065.529 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.065.529 I llm_load_print_meta: general.name     = 1.4B
0.00.065.529 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.065.529 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.065.529 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.065.529 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.065.530 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.065.530 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.065.530 I llm_load_print_meta: max token length = 1024
0.00.067.398 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.067.398 I llm_load_tensors: offloading output layer to GPU
0.00.067.398 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.067.408 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.067.409 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.068.240 I llama_new_context_with_model: n_seq_max     = 1
0.00.068.241 I llama_new_context_with_model: n_ctx         = 2048
0.00.068.241 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.068.242 I llama_new_context_with_model: n_batch       = 2048
0.00.068.242 I llama_new_context_with_model: n_ubatch      = 512
0.00.068.242 I llama_new_context_with_model: flash_attn    = 0
0.00.068.242 I llama_new_context_with_model: freq_base     = 10000.0
0.00.068.242 I llama_new_context_with_model: freq_scale    = 1
0.00.068.243 I ggml_metal_init: allocating
0.00.068.245 I ggml_metal_init: found device: Apple M4
0.00.068.247 I ggml_metal_init: picking default device: Apple M4
0.00.068.865 I ggml_metal_init: using embedded metal library
0.00.071.180 I ggml_metal_init: GPU name:   Apple M4
0.00.071.182 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.182 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.182 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.183 I ggml_metal_init: simdgroup reduction   = true
0.00.071.183 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.183 I ggml_metal_init: has bfloat            = true
0.00.071.184 I ggml_metal_init: use bfloat            = true
0.00.071.184 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.185 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.101 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.776 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.783 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.803 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.959 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.960 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.961 I llama_new_context_with_model: graph nodes  = 967
0.00.103.961 I llama_new_context_with_model: graph splits = 2
0.00.103.964 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.104.117 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.104.117 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.226 I main: llama threadpool init, n_threads = 4
0.00.771.263 I 
0.00.771.287 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.287 I 
0.00.771.439 I sampler seed: 1234
0.00.771.443 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.771.456 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.771.458 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.771.458 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.497.405 I llama_perf_sampler_print:    sampling time =       1.10 ms /    71 runs   (    0.02 ms per token, 64428.31 tokens per second)
0.01.497.405 I llama_perf_context_print:        load time =     762.59 ms
0.01.497.409 I llama_perf_context_print: prompt eval time =      39.91 ms /     7 tokens (    5.70 ms per token,   175.41 tokens per second)
0.01.497.410 I llama_perf_context_print:        eval time =     683.12 ms /    63 runs   (   10.84 ms per token,    92.22 tokens per second)
0.01.497.410 I llama_perf_context_print:       total time =     726.18 ms /    70 tokens
0.01.497.576 I ggml_metal_free: deallocating

real	0m1.525s
user	0m0.112s
sys	0m0.169s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.064 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.017.634 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.452 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.031.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.459 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.459 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.459 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.464 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.466 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.466 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.467 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.467 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.468 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.468 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.471 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.471 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.472 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.122 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.871 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.304 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.045.306 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.045.307 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.045.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.045.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.045.309 I llama_model_loader: - type  f32:  194 tensors
0.00.045.309 I llama_model_loader: - type q5_0:   97 tensors
0.00.045.309 I llama_model_loader: - type q6_K:    1 tensors
0.00.085.326 I llm_load_vocab: special tokens cache size = 25
0.00.093.757 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.093.760 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.093.761 I llm_load_print_meta: arch             = gptneox
0.00.093.761 I llm_load_print_meta: vocab type       = BPE
0.00.093.761 I llm_load_print_meta: n_vocab          = 50304
0.00.093.762 I llm_load_print_meta: n_merges         = 50009
0.00.093.762 I llm_load_print_meta: vocab_only       = 0
0.00.093.762 I llm_load_print_meta: n_ctx_train      = 2048
0.00.093.762 I llm_load_print_meta: n_embd           = 2048
0.00.093.762 I llm_load_print_meta: n_layer          = 24
0.00.093.765 I llm_load_print_meta: n_head           = 16
0.00.093.766 I llm_load_print_meta: n_head_kv        = 16
0.00.093.766 I llm_load_print_meta: n_rot            = 32
0.00.093.767 I llm_load_print_meta: n_swa            = 0
0.00.093.767 I llm_load_print_meta: n_embd_head_k    = 128
0.00.093.767 I llm_load_print_meta: n_embd_head_v    = 128
0.00.093.768 I llm_load_print_meta: n_gqa            = 1
0.00.093.769 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.093.769 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.093.770 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.093.771 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.093.771 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.093.771 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.093.771 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.093.772 I llm_load_print_meta: n_ff             = 8192
0.00.093.772 I llm_load_print_meta: n_expert         = 0
0.00.093.772 I llm_load_print_meta: n_expert_used    = 0
0.00.093.772 I llm_load_print_meta: causal attn      = 1
0.00.093.773 I llm_load_print_meta: pooling type     = 0
0.00.093.773 I llm_load_print_meta: rope type        = 2
0.00.093.773 I llm_load_print_meta: rope scaling     = linear
0.00.093.773 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.093.774 I llm_load_print_meta: freq_scale_train = 1
0.00.093.774 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.093.774 I llm_load_print_meta: rope_finetuned   = unknown
0.00.093.774 I llm_load_print_meta: ssm_d_conv       = 0
0.00.093.774 I llm_load_print_meta: ssm_d_inner      = 0
0.00.093.785 I llm_load_print_meta: ssm_d_state      = 0
0.00.093.787 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.093.787 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.093.788 I llm_load_print_meta: model type       = 1.4B
0.00.093.789 I llm_load_print_meta: model ftype      = Q5_0
0.00.093.789 I llm_load_print_meta: model params     = 1.41 B
0.00.093.790 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.093.790 I llm_load_print_meta: general.name     = 1.4B
0.00.093.791 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.093.791 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.093.791 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.093.791 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.093.792 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.093.792 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.093.792 I llm_load_print_meta: max token length = 1024
0.00.096.212 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.096.213 I llm_load_tensors: offloading output layer to GPU
0.00.096.213 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.096.223 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.096.224 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.097.289 I llama_new_context_with_model: n_seq_max     = 1
0.00.097.290 I llama_new_context_with_model: n_ctx         = 2048
0.00.097.290 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.097.290 I llama_new_context_with_model: n_batch       = 2048
0.00.097.291 I llama_new_context_with_model: n_ubatch      = 512
0.00.097.291 I llama_new_context_with_model: flash_attn    = 0
0.00.097.291 I llama_new_context_with_model: freq_base     = 10000.0
0.00.097.292 I llama_new_context_with_model: freq_scale    = 1
0.00.097.292 I ggml_metal_init: allocating
0.00.097.300 I ggml_metal_init: found device: Apple M4
0.00.097.303 I ggml_metal_init: picking default device: Apple M4
0.00.098.086 I ggml_metal_init: using embedded metal library
0.00.101.274 I ggml_metal_init: GPU name:   Apple M4
0.00.101.276 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.101.277 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.101.277 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.101.277 I ggml_metal_init: simdgroup reduction   = true
0.00.101.277 I ggml_metal_init: simdgroup matrix mul. = true
0.00.101.278 I ggml_metal_init: has bfloat            = true
0.00.101.278 I ggml_metal_init: use bfloat            = true
0.00.101.278 I ggml_metal_init: hasUnifiedMemory      = true
0.00.101.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.111.531 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.132.635 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.132.643 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.132.664 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.133.639 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.133.640 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.133.641 I llama_new_context_with_model: graph nodes  = 967
0.00.133.641 I llama_new_context_with_model: graph splits = 2
0.00.133.644 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.133.768 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.133.768 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.814.245 I main: llama threadpool init, n_threads = 4
0.00.814.329 I 
0.00.814.376 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.814.380 I 
0.00.814.710 I sampler seed: 1234
0.00.814.720 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.814.746 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.814.748 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.814.749 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.605.949 I llama_perf_sampler_print:    sampling time =       1.26 ms /    71 runs   (    0.02 ms per token, 56215.36 tokens per second)
0.01.605.950 I llama_perf_context_print:        load time =     796.60 ms
0.01.605.951 I llama_perf_context_print: prompt eval time =      44.19 ms /     7 tokens (    6.31 ms per token,   158.42 tokens per second)
0.01.605.951 I llama_perf_context_print:        eval time =     743.95 ms /    63 runs   (   11.81 ms per token,    84.68 tokens per second)
0.01.605.952 I llama_perf_context_print:       total time =     791.71 ms /    70 tokens
0.01.606.182 I ggml_metal_free: deallocating

real	0m1.635s
user	0m0.157s
sys	0m0.205s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.034 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.535 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.025.538 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.540 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.540 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.540 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.540 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.541 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.541 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.542 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.542 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.542 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.542 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.543 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.543 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.544 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.545 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.546 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.354 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.398 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.204 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.205 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.206 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.206 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.206 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.207 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.034.207 I llama_model_loader: - type  f32:  194 tensors
0.00.034.207 I llama_model_loader: - type q5_1:   97 tensors
0.00.034.208 I llama_model_loader: - type q6_K:    1 tensors
0.00.057.550 I llm_load_vocab: special tokens cache size = 25
0.00.064.927 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.930 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.930 I llm_load_print_meta: arch             = gptneox
0.00.064.930 I llm_load_print_meta: vocab type       = BPE
0.00.064.931 I llm_load_print_meta: n_vocab          = 50304
0.00.064.931 I llm_load_print_meta: n_merges         = 50009
0.00.064.931 I llm_load_print_meta: vocab_only       = 0
0.00.064.931 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.931 I llm_load_print_meta: n_embd           = 2048
0.00.064.932 I llm_load_print_meta: n_layer          = 24
0.00.064.934 I llm_load_print_meta: n_head           = 16
0.00.064.935 I llm_load_print_meta: n_head_kv        = 16
0.00.064.936 I llm_load_print_meta: n_rot            = 32
0.00.064.936 I llm_load_print_meta: n_swa            = 0
0.00.064.936 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.936 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.937 I llm_load_print_meta: n_gqa            = 1
0.00.064.937 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.938 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.939 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.940 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.941 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.941 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.941 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.942 I llm_load_print_meta: n_ff             = 8192
0.00.064.942 I llm_load_print_meta: n_expert         = 0
0.00.064.942 I llm_load_print_meta: n_expert_used    = 0
0.00.064.942 I llm_load_print_meta: causal attn      = 1
0.00.064.942 I llm_load_print_meta: pooling type     = 0
0.00.064.942 I llm_load_print_meta: rope type        = 2
0.00.064.942 I llm_load_print_meta: rope scaling     = linear
0.00.064.945 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.945 I llm_load_print_meta: freq_scale_train = 1
0.00.064.945 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.945 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.945 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.946 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.946 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.946 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.946 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.946 I llm_load_print_meta: model type       = 1.4B
0.00.064.946 I llm_load_print_meta: model ftype      = Q5_1
0.00.064.947 I llm_load_print_meta: model params     = 1.41 B
0.00.064.947 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.064.947 I llm_load_print_meta: general.name     = 1.4B
0.00.064.948 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.948 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.952 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.952 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.953 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.953 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.953 I llm_load_print_meta: max token length = 1024
0.00.066.960 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.960 I llm_load_tensors: offloading output layer to GPU
0.00.066.960 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.970 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.066.971 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.067.832 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.833 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.833 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.833 I llama_new_context_with_model: n_batch       = 2048
0.00.067.834 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.834 I llama_new_context_with_model: flash_attn    = 0
0.00.067.834 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.834 I llama_new_context_with_model: freq_scale    = 1
0.00.067.835 I ggml_metal_init: allocating
0.00.067.839 I ggml_metal_init: found device: Apple M4
0.00.067.841 I ggml_metal_init: picking default device: Apple M4
0.00.068.451 I ggml_metal_init: using embedded metal library
0.00.070.983 I ggml_metal_init: GPU name:   Apple M4
0.00.070.985 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.070.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.070.986 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.070.986 I ggml_metal_init: simdgroup reduction   = true
0.00.070.986 I ggml_metal_init: simdgroup matrix mul. = true
0.00.070.986 I ggml_metal_init: has bfloat            = true
0.00.070.986 I ggml_metal_init: use bfloat            = true
0.00.070.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.070.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.080.026 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.102.397 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.102.405 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.102.424 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.103.483 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.103.486 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.103.486 I llama_new_context_with_model: graph nodes  = 967
0.00.103.486 I llama_new_context_with_model: graph splits = 2
0.00.103.489 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.635 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.635 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.026.485 I main: llama threadpool init, n_threads = 4
0.01.026.557 I 
0.01.026.601 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.026.601 I 
0.01.026.905 I sampler seed: 1234
0.01.026.914 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.026.939 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.026.942 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.026.942 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.865.279 I llama_perf_sampler_print:    sampling time =       1.57 ms /    71 runs   (    0.02 ms per token, 45194.14 tokens per second)
0.01.865.280 I llama_perf_context_print:        load time =    1017.44 ms
0.01.865.281 I llama_perf_context_print: prompt eval time =      42.94 ms /     7 tokens (    6.13 ms per token,   163.02 tokens per second)
0.01.865.281 I llama_perf_context_print:        eval time =     792.17 ms /    63 runs   (   12.57 ms per token,    79.53 tokens per second)
0.01.865.282 I llama_perf_context_print:       total time =     838.80 ms /    70 tokens
0.01.865.560 I ggml_metal_free: deallocating

real	0m1.881s
user	0m0.131s
sys	0m0.211s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.018.876 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.016 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.025.021 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.022 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.023 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.023 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.023 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.024 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.024 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.025 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.025 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.025 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.026 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.026 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.027 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.028 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.029 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.029 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.526 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.811 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.035.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.035.234 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.035.234 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.035.235 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.035.235 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.035.235 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.035.236 I llama_model_loader: - type  f32:  194 tensors
0.00.035.236 I llama_model_loader: - type q2_K:   49 tensors
0.00.035.236 I llama_model_loader: - type q3_K:   48 tensors
0.00.035.237 I llama_model_loader: - type q6_K:    1 tensors
0.00.064.254 I llm_load_vocab: special tokens cache size = 25
0.00.072.080 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.072.083 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.072.084 I llm_load_print_meta: arch             = gptneox
0.00.072.084 I llm_load_print_meta: vocab type       = BPE
0.00.072.084 I llm_load_print_meta: n_vocab          = 50304
0.00.072.084 I llm_load_print_meta: n_merges         = 50009
0.00.072.085 I llm_load_print_meta: vocab_only       = 0
0.00.072.085 I llm_load_print_meta: n_ctx_train      = 2048
0.00.072.085 I llm_load_print_meta: n_embd           = 2048
0.00.072.085 I llm_load_print_meta: n_layer          = 24
0.00.072.088 I llm_load_print_meta: n_head           = 16
0.00.072.088 I llm_load_print_meta: n_head_kv        = 16
0.00.072.089 I llm_load_print_meta: n_rot            = 32
0.00.072.089 I llm_load_print_meta: n_swa            = 0
0.00.072.089 I llm_load_print_meta: n_embd_head_k    = 128
0.00.072.091 I llm_load_print_meta: n_embd_head_v    = 128
0.00.072.092 I llm_load_print_meta: n_gqa            = 1
0.00.072.093 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.072.093 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.072.100 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.072.102 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.072.102 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.072.103 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.072.103 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.072.104 I llm_load_print_meta: n_ff             = 8192
0.00.072.104 I llm_load_print_meta: n_expert         = 0
0.00.072.104 I llm_load_print_meta: n_expert_used    = 0
0.00.072.104 I llm_load_print_meta: causal attn      = 1
0.00.072.104 I llm_load_print_meta: pooling type     = 0
0.00.072.104 I llm_load_print_meta: rope type        = 2
0.00.072.105 I llm_load_print_meta: rope scaling     = linear
0.00.072.105 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.072.109 I llm_load_print_meta: freq_scale_train = 1
0.00.072.109 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.072.109 I llm_load_print_meta: rope_finetuned   = unknown
0.00.072.109 I llm_load_print_meta: ssm_d_conv       = 0
0.00.072.109 I llm_load_print_meta: ssm_d_inner      = 0
0.00.072.110 I llm_load_print_meta: ssm_d_state      = 0
0.00.072.110 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.072.112 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.072.112 I llm_load_print_meta: model type       = 1.4B
0.00.072.113 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.072.113 I llm_load_print_meta: model params     = 1.41 B
0.00.072.114 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.072.114 I llm_load_print_meta: general.name     = 1.4B
0.00.072.114 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.072.114 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.072.115 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.072.115 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.072.117 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.072.117 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.072.117 I llm_load_print_meta: max token length = 1024
0.00.074.335 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.074.335 I llm_load_tensors: offloading output layer to GPU
0.00.074.336 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.074.346 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.074.348 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.075.618 I llama_new_context_with_model: n_seq_max     = 1
0.00.075.619 I llama_new_context_with_model: n_ctx         = 2048
0.00.075.619 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.075.620 I llama_new_context_with_model: n_batch       = 2048
0.00.075.620 I llama_new_context_with_model: n_ubatch      = 512
0.00.075.620 I llama_new_context_with_model: flash_attn    = 0
0.00.075.621 I llama_new_context_with_model: freq_base     = 10000.0
0.00.075.621 I llama_new_context_with_model: freq_scale    = 1
0.00.075.622 I ggml_metal_init: allocating
0.00.075.630 I ggml_metal_init: found device: Apple M4
0.00.075.634 I ggml_metal_init: picking default device: Apple M4
0.00.076.466 I ggml_metal_init: using embedded metal library
0.00.080.033 I ggml_metal_init: GPU name:   Apple M4
0.00.080.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.080.036 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.080.036 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.080.037 I ggml_metal_init: simdgroup reduction   = true
0.00.080.037 I ggml_metal_init: simdgroup matrix mul. = true
0.00.080.037 I ggml_metal_init: has bfloat            = true
0.00.080.037 I ggml_metal_init: use bfloat            = true
0.00.080.038 I ggml_metal_init: hasUnifiedMemory      = true
0.00.080.038 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.091.659 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.114.950 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.114.957 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.114.976 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.115.967 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.115.969 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.115.969 I llama_new_context_with_model: graph nodes  = 967
0.00.115.969 I llama_new_context_with_model: graph splits = 2
0.00.115.972 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.116.097 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.116.097 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.514.963 I main: llama threadpool init, n_threads = 4
0.00.515.023 I 
0.00.515.056 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.515.056 I 
0.00.515.256 I sampler seed: 1234
0.00.515.263 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.515.282 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.515.284 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.515.284 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.203.688 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.01.203.689 I llama_perf_context_print:        load time =     496.08 ms
0.01.203.690 I llama_perf_context_print: prompt eval time =      36.38 ms /     7 tokens (    5.20 ms per token,   192.43 tokens per second)
0.01.203.691 I llama_perf_context_print:        eval time =     649.03 ms /    63 runs   (   10.30 ms per token,    97.07 tokens per second)
0.01.203.691 I llama_perf_context_print:       total time =     688.73 ms /    70 tokens
0.01.203.864 I ggml_metal_free: deallocating

real	0m1.232s
user	0m0.128s
sys	0m0.122s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.008.461 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.072 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.023.076 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.078 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.078 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.082 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.082 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.082 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.083 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.084 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.084 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.084 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.085 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.085 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.085 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.087 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.087 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.089 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.966 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.999 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.862 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.031.863 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.863 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.864 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.864 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.864 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.031.865 I llama_model_loader: - type  f32:  194 tensors
0.00.031.865 I llama_model_loader: - type q3_K:   25 tensors
0.00.031.865 I llama_model_loader: - type q4_K:   71 tensors
0.00.031.866 I llama_model_loader: - type q5_K:    1 tensors
0.00.031.866 I llama_model_loader: - type q6_K:    1 tensors
0.00.054.101 I llm_load_vocab: special tokens cache size = 25
0.00.059.971 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.974 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.974 I llm_load_print_meta: arch             = gptneox
0.00.059.974 I llm_load_print_meta: vocab type       = BPE
0.00.059.975 I llm_load_print_meta: n_vocab          = 50304
0.00.059.975 I llm_load_print_meta: n_merges         = 50009
0.00.059.975 I llm_load_print_meta: vocab_only       = 0
0.00.059.975 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.975 I llm_load_print_meta: n_embd           = 2048
0.00.059.976 I llm_load_print_meta: n_layer          = 24
0.00.059.978 I llm_load_print_meta: n_head           = 16
0.00.059.979 I llm_load_print_meta: n_head_kv        = 16
0.00.059.979 I llm_load_print_meta: n_rot            = 32
0.00.059.979 I llm_load_print_meta: n_swa            = 0
0.00.059.979 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.979 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.980 I llm_load_print_meta: n_gqa            = 1
0.00.059.981 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.981 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.982 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.983 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.983 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.983 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.984 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.984 I llm_load_print_meta: n_ff             = 8192
0.00.059.985 I llm_load_print_meta: n_expert         = 0
0.00.059.985 I llm_load_print_meta: n_expert_used    = 0
0.00.059.985 I llm_load_print_meta: causal attn      = 1
0.00.059.985 I llm_load_print_meta: pooling type     = 0
0.00.059.985 I llm_load_print_meta: rope type        = 2
0.00.059.985 I llm_load_print_meta: rope scaling     = linear
0.00.059.987 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.988 I llm_load_print_meta: freq_scale_train = 1
0.00.059.988 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.988 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.988 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.988 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.988 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.989 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.989 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.989 I llm_load_print_meta: model type       = 1.4B
0.00.059.989 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.059.990 I llm_load_print_meta: model params     = 1.41 B
0.00.059.990 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.059.990 I llm_load_print_meta: general.name     = 1.4B
0.00.059.991 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.991 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.991 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.991 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.992 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.059.993 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.994 I llm_load_print_meta: max token length = 1024
0.00.061.733 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.734 I llm_load_tensors: offloading output layer to GPU
0.00.061.734 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.744 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.061.745 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.062.573 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.574 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.574 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.574 I llama_new_context_with_model: n_batch       = 2048
0.00.062.574 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.574 I llama_new_context_with_model: flash_attn    = 0
0.00.062.575 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.575 I llama_new_context_with_model: freq_scale    = 1
0.00.062.576 I ggml_metal_init: allocating
0.00.062.579 I ggml_metal_init: found device: Apple M4
0.00.062.581 I ggml_metal_init: picking default device: Apple M4
0.00.063.168 I ggml_metal_init: using embedded metal library
0.00.065.486 I ggml_metal_init: GPU name:   Apple M4
0.00.065.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.488 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.488 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.488 I ggml_metal_init: simdgroup reduction   = true
0.00.065.489 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.489 I ggml_metal_init: has bfloat            = true
0.00.065.489 I ggml_metal_init: use bfloat            = true
0.00.065.489 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.490 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.148 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.095.383 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.095.389 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.095.410 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.096.451 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.096.453 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.096.453 I llama_new_context_with_model: graph nodes  = 967
0.00.096.453 I llama_new_context_with_model: graph splits = 2
0.00.096.456 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.096.601 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.958 I main: llama threadpool init, n_threads = 4
0.00.594.993 I 
0.00.595.019 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.595.019 I 
0.00.595.178 I sampler seed: 1234
0.00.595.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.595.224 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.595.226 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.595.226 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.341.929 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57676.69 tokens per second)
0.01.341.930 I llama_perf_context_print:        load time =     586.49 ms
0.01.341.930 I llama_perf_context_print: prompt eval time =      40.81 ms /     7 tokens (    5.83 ms per token,   171.51 tokens per second)
0.01.341.932 I llama_perf_context_print:        eval time =     702.85 ms /    63 runs   (   11.16 ms per token,    89.64 tokens per second)
0.01.341.932 I llama_perf_context_print:       total time =     746.97 ms /    70 tokens
0.01.342.170 I ggml_metal_free: deallocating

real	0m1.358s
user	0m0.112s
sys	0m0.144s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.069 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.008.898 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.344 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.024.349 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.350 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.024.351 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.351 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.024.351 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.024.351 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.024.352 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.024.352 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.024.353 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.024.353 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.024.353 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.024.354 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.024.354 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.024.357 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.024.357 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.024.357 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.028.257 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.029.318 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.033.105 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.033.106 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.033.106 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.033.107 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.033.107 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.033.107 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.033.108 I llama_model_loader: - type  f32:  194 tensors
0.00.033.108 I llama_model_loader: - type q4_K:   61 tensors
0.00.033.108 I llama_model_loader: - type q5_K:   24 tensors
0.00.033.109 I llama_model_loader: - type q6_K:   13 tensors
0.00.054.932 I llm_load_vocab: special tokens cache size = 25
0.00.060.922 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.060.925 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.060.926 I llm_load_print_meta: arch             = gptneox
0.00.060.926 I llm_load_print_meta: vocab type       = BPE
0.00.060.926 I llm_load_print_meta: n_vocab          = 50304
0.00.060.927 I llm_load_print_meta: n_merges         = 50009
0.00.060.927 I llm_load_print_meta: vocab_only       = 0
0.00.060.927 I llm_load_print_meta: n_ctx_train      = 2048
0.00.060.927 I llm_load_print_meta: n_embd           = 2048
0.00.060.927 I llm_load_print_meta: n_layer          = 24
0.00.060.930 I llm_load_print_meta: n_head           = 16
0.00.060.930 I llm_load_print_meta: n_head_kv        = 16
0.00.060.931 I llm_load_print_meta: n_rot            = 32
0.00.060.931 I llm_load_print_meta: n_swa            = 0
0.00.060.931 I llm_load_print_meta: n_embd_head_k    = 128
0.00.060.931 I llm_load_print_meta: n_embd_head_v    = 128
0.00.060.934 I llm_load_print_meta: n_gqa            = 1
0.00.060.935 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.060.935 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.060.936 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.060.936 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.060.937 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.060.937 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.060.937 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.060.938 I llm_load_print_meta: n_ff             = 8192
0.00.060.938 I llm_load_print_meta: n_expert         = 0
0.00.060.940 I llm_load_print_meta: n_expert_used    = 0
0.00.060.941 I llm_load_print_meta: causal attn      = 1
0.00.060.941 I llm_load_print_meta: pooling type     = 0
0.00.060.941 I llm_load_print_meta: rope type        = 2
0.00.060.941 I llm_load_print_meta: rope scaling     = linear
0.00.060.941 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.060.942 I llm_load_print_meta: freq_scale_train = 1
0.00.060.947 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.060.947 I llm_load_print_meta: rope_finetuned   = unknown
0.00.060.948 I llm_load_print_meta: ssm_d_conv       = 0
0.00.060.948 I llm_load_print_meta: ssm_d_inner      = 0
0.00.060.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.060.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.060.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.060.948 I llm_load_print_meta: model type       = 1.4B
0.00.060.949 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.060.949 I llm_load_print_meta: model params     = 1.41 B
0.00.060.950 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.060.950 I llm_load_print_meta: general.name     = 1.4B
0.00.060.950 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.060.950 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.060.950 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.060.950 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.060.951 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.060.951 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.060.952 I llm_load_print_meta: max token length = 1024
0.00.062.703 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.062.704 I llm_load_tensors: offloading output layer to GPU
0.00.062.704 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.062.714 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.062.715 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.063.546 I llama_new_context_with_model: n_seq_max     = 1
0.00.063.546 I llama_new_context_with_model: n_ctx         = 2048
0.00.063.546 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.063.546 I llama_new_context_with_model: n_batch       = 2048
0.00.063.547 I llama_new_context_with_model: n_ubatch      = 512
0.00.063.547 I llama_new_context_with_model: flash_attn    = 0
0.00.063.547 I llama_new_context_with_model: freq_base     = 10000.0
0.00.063.548 I llama_new_context_with_model: freq_scale    = 1
0.00.063.548 I ggml_metal_init: allocating
0.00.063.554 I ggml_metal_init: found device: Apple M4
0.00.063.556 I ggml_metal_init: picking default device: Apple M4
0.00.064.164 I ggml_metal_init: using embedded metal library
0.00.066.551 I ggml_metal_init: GPU name:   Apple M4
0.00.066.553 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.066.553 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.066.553 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.066.555 I ggml_metal_init: simdgroup reduction   = true
0.00.066.555 I ggml_metal_init: simdgroup matrix mul. = true
0.00.066.555 I ggml_metal_init: has bfloat            = true
0.00.066.555 I ggml_metal_init: use bfloat            = true
0.00.066.556 I ggml_metal_init: hasUnifiedMemory      = true
0.00.066.556 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.838 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.200 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.207 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.226 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.274 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.276 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.276 I llama_new_context_with_model: graph nodes  = 967
0.00.097.277 I llama_new_context_with_model: graph splits = 2
0.00.097.279 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.432 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.433 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.736.403 I main: llama threadpool init, n_threads = 4
0.00.736.444 I 
0.00.736.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.736.464 I 
0.00.736.623 I sampler seed: 1234
0.00.736.627 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.736.646 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.736.646 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.736.646 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.493.755 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55993.69 tokens per second)
0.01.493.756 I llama_perf_context_print:        load time =     727.50 ms
0.01.493.756 I llama_perf_context_print: prompt eval time =      47.46 ms /     7 tokens (    6.78 ms per token,   147.50 tokens per second)
0.01.493.757 I llama_perf_context_print:        eval time =     706.53 ms /    63 runs   (   11.21 ms per token,    89.17 tokens per second)
0.01.493.757 I llama_perf_context_print:       total time =     757.35 ms /    70 tokens
0.01.493.939 I ggml_metal_free: deallocating

real	0m1.511s
user	0m0.111s
sys	0m0.177s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.071 I main: load the model and apply lora adapter, if any
0.00.017.672 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.028.306 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.028.311 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.313 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.313 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.313 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.314 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.314 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.315 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.315 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.315 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.316 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.316 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.316 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.317 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.319 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.319 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.320 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.826 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.034.156 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.039.079 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.039.081 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.039.082 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.039.082 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.039.082 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.039.083 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.039.083 I llama_model_loader: - type  f32:  194 tensors
0.00.039.084 I llama_model_loader: - type q5_K:   61 tensors
0.00.039.084 I llama_model_loader: - type q6_K:   37 tensors
0.00.069.922 I llm_load_vocab: special tokens cache size = 25
0.00.079.758 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.079.762 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.079.762 I llm_load_print_meta: arch             = gptneox
0.00.079.763 I llm_load_print_meta: vocab type       = BPE
0.00.079.763 I llm_load_print_meta: n_vocab          = 50304
0.00.079.763 I llm_load_print_meta: n_merges         = 50009
0.00.079.763 I llm_load_print_meta: vocab_only       = 0
0.00.079.764 I llm_load_print_meta: n_ctx_train      = 2048
0.00.079.764 I llm_load_print_meta: n_embd           = 2048
0.00.079.764 I llm_load_print_meta: n_layer          = 24
0.00.079.769 I llm_load_print_meta: n_head           = 16
0.00.079.770 I llm_load_print_meta: n_head_kv        = 16
0.00.079.770 I llm_load_print_meta: n_rot            = 32
0.00.079.770 I llm_load_print_meta: n_swa            = 0
0.00.079.770 I llm_load_print_meta: n_embd_head_k    = 128
0.00.079.771 I llm_load_print_meta: n_embd_head_v    = 128
0.00.079.774 I llm_load_print_meta: n_gqa            = 1
0.00.079.775 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.079.775 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.079.776 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.079.777 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.079.778 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.079.778 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.079.786 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.079.790 I llm_load_print_meta: n_ff             = 8192
0.00.079.790 I llm_load_print_meta: n_expert         = 0
0.00.079.790 I llm_load_print_meta: n_expert_used    = 0
0.00.079.792 I llm_load_print_meta: causal attn      = 1
0.00.079.794 I llm_load_print_meta: pooling type     = 0
0.00.079.794 I llm_load_print_meta: rope type        = 2
0.00.079.794 I llm_load_print_meta: rope scaling     = linear
0.00.079.795 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.079.795 I llm_load_print_meta: freq_scale_train = 1
0.00.079.796 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.079.798 I llm_load_print_meta: rope_finetuned   = unknown
0.00.079.798 I llm_load_print_meta: ssm_d_conv       = 0
0.00.079.798 I llm_load_print_meta: ssm_d_inner      = 0
0.00.079.798 I llm_load_print_meta: ssm_d_state      = 0
0.00.079.798 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.079.798 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.079.799 I llm_load_print_meta: model type       = 1.4B
0.00.079.800 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.079.800 I llm_load_print_meta: model params     = 1.41 B
0.00.079.801 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.079.801 I llm_load_print_meta: general.name     = 1.4B
0.00.079.801 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.079.802 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.079.802 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.079.804 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.079.804 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.079.805 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.079.806 I llm_load_print_meta: max token length = 1024
0.00.082.405 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.082.406 I llm_load_tensors: offloading output layer to GPU
0.00.082.406 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.082.417 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.082.418 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.083.589 I llama_new_context_with_model: n_seq_max     = 1
0.00.083.590 I llama_new_context_with_model: n_ctx         = 2048
0.00.083.590 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.083.590 I llama_new_context_with_model: n_batch       = 2048
0.00.083.591 I llama_new_context_with_model: n_ubatch      = 512
0.00.083.591 I llama_new_context_with_model: flash_attn    = 0
0.00.083.592 I llama_new_context_with_model: freq_base     = 10000.0
0.00.083.592 I llama_new_context_with_model: freq_scale    = 1
0.00.083.593 I ggml_metal_init: allocating
0.00.083.602 I ggml_metal_init: found device: Apple M4
0.00.083.604 I ggml_metal_init: picking default device: Apple M4
0.00.084.391 I ggml_metal_init: using embedded metal library
0.00.087.819 I ggml_metal_init: GPU name:   Apple M4
0.00.087.821 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.087.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.087.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.087.823 I ggml_metal_init: simdgroup reduction   = true
0.00.087.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.087.824 I ggml_metal_init: has bfloat            = true
0.00.087.824 I ggml_metal_init: use bfloat            = true
0.00.087.824 I ggml_metal_init: hasUnifiedMemory      = true
0.00.087.825 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.797 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.120.866 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.120.872 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.120.893 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.121.819 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.121.820 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.121.820 I llama_new_context_with_model: graph nodes  = 967
0.00.121.821 I llama_new_context_with_model: graph splits = 2
0.00.121.823 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.121.964 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.121.965 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.827.567 I main: llama threadpool init, n_threads = 4
0.00.827.641 I 
0.00.827.681 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.827.681 I 
0.00.827.980 I sampler seed: 1234
0.00.827.988 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.828.074 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.828.078 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.828.079 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.672.328 I llama_perf_sampler_print:    sampling time =       1.18 ms /    71 runs   (    0.02 ms per token, 60271.65 tokens per second)
0.01.672.329 I llama_perf_context_print:        load time =     809.89 ms
0.01.672.330 I llama_perf_context_print: prompt eval time =      52.48 ms /     7 tokens (    7.50 ms per token,   133.38 tokens per second)
0.01.672.330 I llama_perf_context_print:        eval time =     788.71 ms /    63 runs   (   12.52 ms per token,    79.88 tokens per second)
0.01.672.331 I llama_perf_context_print:       total time =     844.76 ms /    70 tokens
0.01.672.573 I ggml_metal_free: deallocating

real	0m1.703s
user	0m0.138s
sys	0m0.194s
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.009.418 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.597 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.601 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.602 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.607 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.607 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.608 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.608 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.609 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.609 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.609 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.610 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.610 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.611 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.613 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.613 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.345 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.373 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.087 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.089 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.089 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.089 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.089 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.090 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.090 I llama_model_loader: - type  f32:  194 tensors
0.00.024.090 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.952 I llm_load_vocab: special tokens cache size = 25
0.00.049.863 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.866 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.866 I llm_load_print_meta: arch             = gptneox
0.00.049.867 I llm_load_print_meta: vocab type       = BPE
0.00.049.867 I llm_load_print_meta: n_vocab          = 50304
0.00.049.867 I llm_load_print_meta: n_merges         = 50009
0.00.049.868 I llm_load_print_meta: vocab_only       = 0
0.00.049.868 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.868 I llm_load_print_meta: n_embd           = 2048
0.00.049.868 I llm_load_print_meta: n_layer          = 24
0.00.049.870 I llm_load_print_meta: n_head           = 16
0.00.049.871 I llm_load_print_meta: n_head_kv        = 16
0.00.049.871 I llm_load_print_meta: n_rot            = 32
0.00.049.871 I llm_load_print_meta: n_swa            = 0
0.00.049.872 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.872 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.872 I llm_load_print_meta: n_gqa            = 1
0.00.049.873 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.874 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.874 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.875 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.875 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.875 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.875 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.876 I llm_load_print_meta: n_ff             = 8192
0.00.049.876 I llm_load_print_meta: n_expert         = 0
0.00.049.879 I llm_load_print_meta: n_expert_used    = 0
0.00.049.879 I llm_load_print_meta: causal attn      = 1
0.00.049.879 I llm_load_print_meta: pooling type     = 0
0.00.049.879 I llm_load_print_meta: rope type        = 2
0.00.049.879 I llm_load_print_meta: rope scaling     = linear
0.00.049.880 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.880 I llm_load_print_meta: freq_scale_train = 1
0.00.049.880 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.881 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.881 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.881 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.881 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.881 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.881 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.882 I llm_load_print_meta: model type       = 1.4B
0.00.049.882 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.882 I llm_load_print_meta: model params     = 1.41 B
0.00.049.883 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.883 I llm_load_print_meta: general.name     = 1.4B
0.00.049.883 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.884 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.884 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.884 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.884 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.885 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.885 I llm_load_print_meta: max token length = 1024
0.00.051.692 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.692 I llm_load_tensors: offloading output layer to GPU
0.00.051.693 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.703 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.704 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.571 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.572 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.572 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.573 I llama_new_context_with_model: n_batch       = 2048
0.00.052.573 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.573 I llama_new_context_with_model: flash_attn    = 0
0.00.052.573 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.574 I llama_new_context_with_model: freq_scale    = 1
0.00.052.574 I ggml_metal_init: allocating
0.00.052.580 I ggml_metal_init: found device: Apple M4
0.00.052.584 I ggml_metal_init: picking default device: Apple M4
0.00.053.157 I ggml_metal_init: using embedded metal library
0.00.055.547 I ggml_metal_init: GPU name:   Apple M4
0.00.055.549 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.549 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.550 I ggml_metal_init: simdgroup reduction   = true
0.00.055.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.550 I ggml_metal_init: has bfloat            = true
0.00.055.550 I ggml_metal_init: use bfloat            = true
0.00.055.550 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.551 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.017 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.748 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.756 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.789 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.925 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.928 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.928 I llama_new_context_with_model: graph nodes  = 967
0.00.085.928 I llama_new_context_with_model: graph splits = 2
0.00.085.931 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.070 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.070 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.809.620 I main: llama threadpool init, n_threads = 4
0.00.809.673 I 
0.00.809.713 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.809.713 I 
0.00.809.878 I sampler seed: 1234
0.00.809.886 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.809.920 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.809.923 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.809.923 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.676.710 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57817.59 tokens per second)
0.01.676.711 I llama_perf_context_print:        load time =     800.20 ms
0.01.676.712 I llama_perf_context_print: prompt eval time =      55.02 ms /     7 tokens (    7.86 ms per token,   127.24 tokens per second)
0.01.676.713 I llama_perf_context_print:        eval time =     808.68 ms /    63 runs   (   12.84 ms per token,    77.91 tokens per second)
0.01.676.713 I llama_perf_context_print:       total time =     867.09 ms /    70 tokens
0.01.676.896 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.107s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.563 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.541 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.037.841 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.037.848 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.037.851 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.037.851 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.037.852 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.037.855 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.037.856 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.037.857 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.037.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.037.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.037.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.037.860 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.037.861 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.037.862 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.037.865 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.037.865 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.037.866 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.279 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.222 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.055.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.055.397 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.055.398 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.055.398 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.055.399 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.055.399 I llama_model_loader: - type  f32:  194 tensors
0.00.055.400 I llama_model_loader: - type  f16:   98 tensors
0.00.084.468 I llm_load_vocab: special tokens cache size = 25
0.00.091.129 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.091.134 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.091.134 I llm_load_print_meta: arch             = gptneox
0.00.091.135 I llm_load_print_meta: vocab type       = BPE
0.00.091.135 I llm_load_print_meta: n_vocab          = 50304
0.00.091.135 I llm_load_print_meta: n_merges         = 50009
0.00.091.135 I llm_load_print_meta: vocab_only       = 0
0.00.091.135 I llm_load_print_meta: n_ctx_train      = 2048
0.00.091.135 I llm_load_print_meta: n_embd           = 2048
0.00.091.136 I llm_load_print_meta: n_layer          = 24
0.00.091.138 I llm_load_print_meta: n_head           = 16
0.00.091.138 I llm_load_print_meta: n_head_kv        = 16
0.00.091.139 I llm_load_print_meta: n_rot            = 32
0.00.091.139 I llm_load_print_meta: n_swa            = 0
0.00.091.139 I llm_load_print_meta: n_embd_head_k    = 128
0.00.091.139 I llm_load_print_meta: n_embd_head_v    = 128
0.00.091.142 I llm_load_print_meta: n_gqa            = 1
0.00.091.142 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.091.143 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.091.143 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.091.144 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.091.144 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.091.145 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.091.148 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.091.149 I llm_load_print_meta: n_ff             = 8192
0.00.091.149 I llm_load_print_meta: n_expert         = 0
0.00.091.149 I llm_load_print_meta: n_expert_used    = 0
0.00.091.149 I llm_load_print_meta: causal attn      = 1
0.00.091.149 I llm_load_print_meta: pooling type     = 0
0.00.091.149 I llm_load_print_meta: rope type        = 2
0.00.091.150 I llm_load_print_meta: rope scaling     = linear
0.00.091.150 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.091.150 I llm_load_print_meta: freq_scale_train = 1
0.00.091.150 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.091.151 I llm_load_print_meta: rope_finetuned   = unknown
0.00.091.151 I llm_load_print_meta: ssm_d_conv       = 0
0.00.091.151 I llm_load_print_meta: ssm_d_inner      = 0
0.00.091.151 I llm_load_print_meta: ssm_d_state      = 0
0.00.091.151 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.091.151 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.091.152 I llm_load_print_meta: model type       = 1.4B
0.00.091.152 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.091.153 I llm_load_print_meta: model params     = 1.41 B
0.00.091.153 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.091.154 I llm_load_print_meta: general.name     = 1.4B
0.00.091.154 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.091.154 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.091.155 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.091.155 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.091.156 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.091.156 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.091.156 I llm_load_print_meta: max token length = 1024
0.00.093.087 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.093.087 I llm_load_tensors: offloading output layer to GPU
0.00.093.088 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.093.105 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.093.106 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.093.944 I llama_new_context_with_model: n_seq_max     = 1
0.00.093.945 I llama_new_context_with_model: n_ctx         = 128
0.00.093.945 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.093.946 I llama_new_context_with_model: n_batch       = 128
0.00.093.946 I llama_new_context_with_model: n_ubatch      = 128
0.00.093.946 I llama_new_context_with_model: flash_attn    = 0
0.00.093.946 I llama_new_context_with_model: freq_base     = 10000.0
0.00.093.947 I llama_new_context_with_model: freq_scale    = 1
0.00.093.947 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.093.947 I ggml_metal_init: allocating
0.00.093.950 I ggml_metal_init: found device: Apple M4
0.00.093.952 I ggml_metal_init: picking default device: Apple M4
0.00.094.529 I ggml_metal_init: using embedded metal library
0.00.097.055 I ggml_metal_init: GPU name:   Apple M4
0.00.097.056 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.097.057 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.097.057 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.097.057 I ggml_metal_init: simdgroup reduction   = true
0.00.097.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.097.058 I ggml_metal_init: has bfloat            = true
0.00.097.058 I ggml_metal_init: use bfloat            = true
0.00.097.058 I ggml_metal_init: hasUnifiedMemory      = true
0.00.097.059 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.106.191 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.107.496 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.107.499 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.107.512 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.108.413 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.108.414 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.108.414 I llama_new_context_with_model: graph nodes  = 967
0.00.108.414 I llama_new_context_with_model: graph splits = 2
0.00.108.415 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.108.415 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.585.113 I 
0.01.585.143 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.585.181 I perplexity: tokenizing the input ..
0.01.597.633 I perplexity: tokenization took 12.449 ms
0.01.597.639 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.717.512 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.718.825 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.718.857 I llama_perf_context_print:        load time =    1561.56 ms
0.01.718.857 I llama_perf_context_print: prompt eval time =     119.01 ms /   128 tokens (    0.93 ms per token,  1075.56 tokens per second)
0.01.718.858 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.718.859 I llama_perf_context_print:       total time =     133.75 ms /   129 tokens
0.01.719.722 I ggml_metal_free: deallocating

real	0m1.937s
user	0m0.125s
sys	0m0.323s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.300 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.179 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.157 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.164 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.166 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.167 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.168 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.169 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.171 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.176 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.177 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.177 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.250 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.308 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.863 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.866 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.866 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.867 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.867 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.868 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.028.869 I llama_model_loader: - type  f32:  194 tensors
0.00.028.870 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.659 I llm_load_vocab: special tokens cache size = 25
0.00.064.042 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.046 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.046 I llm_load_print_meta: arch             = gptneox
0.00.064.047 I llm_load_print_meta: vocab type       = BPE
0.00.064.047 I llm_load_print_meta: n_vocab          = 50304
0.00.064.047 I llm_load_print_meta: n_merges         = 50009
0.00.064.047 I llm_load_print_meta: vocab_only       = 0
0.00.064.047 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.048 I llm_load_print_meta: n_embd           = 2048
0.00.064.048 I llm_load_print_meta: n_layer          = 24
0.00.064.051 I llm_load_print_meta: n_head           = 16
0.00.064.052 I llm_load_print_meta: n_head_kv        = 16
0.00.064.052 I llm_load_print_meta: n_rot            = 32
0.00.064.052 I llm_load_print_meta: n_swa            = 0
0.00.064.052 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.052 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.053 I llm_load_print_meta: n_gqa            = 1
0.00.064.054 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.056 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.056 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.057 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.057 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.059 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.059 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.060 I llm_load_print_meta: n_ff             = 8192
0.00.064.060 I llm_load_print_meta: n_expert         = 0
0.00.064.060 I llm_load_print_meta: n_expert_used    = 0
0.00.064.060 I llm_load_print_meta: causal attn      = 1
0.00.064.060 I llm_load_print_meta: pooling type     = 0
0.00.064.060 I llm_load_print_meta: rope type        = 2
0.00.064.061 I llm_load_print_meta: rope scaling     = linear
0.00.064.061 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.061 I llm_load_print_meta: freq_scale_train = 1
0.00.064.062 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.062 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.062 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.062 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.062 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.062 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.063 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.063 I llm_load_print_meta: model type       = 1.4B
0.00.064.063 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.068 I llm_load_print_meta: model params     = 1.41 B
0.00.064.068 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.068 I llm_load_print_meta: general.name     = 1.4B
0.00.064.069 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.069 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.069 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.069 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.070 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.064.070 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.070 I llm_load_print_meta: max token length = 1024
0.00.066.299 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.299 I llm_load_tensors: offloading output layer to GPU
0.00.066.299 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.309 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.311 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.405 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.406 I llama_new_context_with_model: n_ctx         = 128
0.00.067.407 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.067.407 I llama_new_context_with_model: n_batch       = 128
0.00.067.407 I llama_new_context_with_model: n_ubatch      = 128
0.00.067.407 I llama_new_context_with_model: flash_attn    = 0
0.00.067.408 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.408 I llama_new_context_with_model: freq_scale    = 1
0.00.067.409 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.067.409 I ggml_metal_init: allocating
0.00.067.413 I ggml_metal_init: found device: Apple M4
0.00.067.415 I ggml_metal_init: picking default device: Apple M4
0.00.068.132 I ggml_metal_init: using embedded metal library
0.00.071.142 I ggml_metal_init: GPU name:   Apple M4
0.00.071.145 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.145 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.145 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.146 I ggml_metal_init: simdgroup reduction   = true
0.00.071.146 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.146 I ggml_metal_init: has bfloat            = true
0.00.071.146 I ggml_metal_init: use bfloat            = true
0.00.071.146 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.147 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.082.067 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.083.536 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.083.539 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.083.553 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.084.503 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.084.504 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.084.505 I llama_new_context_with_model: graph nodes  = 967
0.00.084.505 I llama_new_context_with_model: graph splits = 2
0.00.084.506 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.084.506 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.964.269 I 
0.00.964.298 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.964.310 I perplexity: tokenizing the input ..
0.00.972.753 I perplexity: tokenization took 8.44 ms
0.00.972.759 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.097.683 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.098.775 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.098.787 I llama_perf_context_print:        load time =     953.08 ms
0.01.098.789 I llama_perf_context_print: prompt eval time =     124.70 ms /   128 tokens (    0.97 ms per token,  1026.42 tokens per second)
0.01.098.790 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.098.790 I llama_perf_context_print:       total time =     134.52 ms /   129 tokens
0.01.099.132 I ggml_metal_free: deallocating

real	0m1.116s
user	0m0.093s
sys	0m0.207s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.266 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.611 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.693 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.697 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.699 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.699 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.699 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.700 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.700 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.701 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.701 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.702 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.702 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.702 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.703 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.440 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.455 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.196 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.197 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.197 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.198 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.198 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.199 I llama_model_loader: - type  f32:  194 tensors
0.00.024.199 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.199 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.771 I llm_load_vocab: special tokens cache size = 25
0.00.049.658 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.660 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.661 I llm_load_print_meta: arch             = gptneox
0.00.049.661 I llm_load_print_meta: vocab type       = BPE
0.00.049.661 I llm_load_print_meta: n_vocab          = 50304
0.00.049.662 I llm_load_print_meta: n_merges         = 50009
0.00.049.662 I llm_load_print_meta: vocab_only       = 0
0.00.049.662 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.662 I llm_load_print_meta: n_embd           = 2048
0.00.049.662 I llm_load_print_meta: n_layer          = 24
0.00.049.665 I llm_load_print_meta: n_head           = 16
0.00.049.666 I llm_load_print_meta: n_head_kv        = 16
0.00.049.666 I llm_load_print_meta: n_rot            = 32
0.00.049.666 I llm_load_print_meta: n_swa            = 0
0.00.049.666 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.669 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.669 I llm_load_print_meta: n_gqa            = 1
0.00.049.670 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.670 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.671 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.671 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.671 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.672 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.672 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.672 I llm_load_print_meta: n_ff             = 8192
0.00.049.672 I llm_load_print_meta: n_expert         = 0
0.00.049.673 I llm_load_print_meta: n_expert_used    = 0
0.00.049.673 I llm_load_print_meta: causal attn      = 1
0.00.049.673 I llm_load_print_meta: pooling type     = 0
0.00.049.673 I llm_load_print_meta: rope type        = 2
0.00.049.673 I llm_load_print_meta: rope scaling     = linear
0.00.049.674 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.674 I llm_load_print_meta: freq_scale_train = 1
0.00.049.679 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.680 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.680 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.680 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.681 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.681 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.681 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.681 I llm_load_print_meta: model type       = 1.4B
0.00.049.682 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.682 I llm_load_print_meta: model params     = 1.41 B
0.00.049.683 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.683 I llm_load_print_meta: general.name     = 1.4B
0.00.049.684 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.685 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.685 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.686 I llm_load_print_meta: max token length = 1024
0.00.051.420 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.420 I llm_load_tensors: offloading output layer to GPU
0.00.051.420 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.430 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.431 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.272 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.273 I llama_new_context_with_model: n_ctx         = 128
0.00.052.273 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.273 I llama_new_context_with_model: n_batch       = 128
0.00.052.273 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.273 I llama_new_context_with_model: flash_attn    = 0
0.00.052.274 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.274 I llama_new_context_with_model: freq_scale    = 1
0.00.052.274 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.275 I ggml_metal_init: allocating
0.00.052.281 I ggml_metal_init: found device: Apple M4
0.00.052.283 I ggml_metal_init: picking default device: Apple M4
0.00.052.862 I ggml_metal_init: using embedded metal library
0.00.055.198 I ggml_metal_init: GPU name:   Apple M4
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.200 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.200 I ggml_metal_init: simdgroup reduction   = true
0.00.055.201 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.201 I ggml_metal_init: has bfloat            = true
0.00.055.201 I ggml_metal_init: use bfloat            = true
0.00.055.201 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.202 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.679 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.967 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.969 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.983 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.803 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.804 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.804 I llama_new_context_with_model: graph nodes  = 967
0.00.066.805 I llama_new_context_with_model: graph splits = 2
0.00.066.806 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.806 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.613.205 I 
0.00.613.237 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.613.249 I perplexity: tokenizing the input ..
0.00.620.752 I perplexity: tokenization took 7.501 ms
0.00.620.757 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.743.336 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.744.433 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.744.444 I llama_perf_context_print:        load time =     603.59 ms
0.00.744.445 I llama_perf_context_print: prompt eval time =     122.35 ms /   128 tokens (    0.96 ms per token,  1046.18 tokens per second)
0.00.744.446 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.744.446 I llama_perf_context_print:       total time =     131.24 ms /   129 tokens
0.00.744.915 I ggml_metal_free: deallocating

real	0m0.759s
user	0m0.078s
sys	0m0.113s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.614 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.484 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.488 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.490 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.490 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.492 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.492 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.493 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.493 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.494 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.494 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.494 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.495 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.495 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.497 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.497 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.497 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.123 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.728 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.729 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.730 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.730 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.730 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.731 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.731 I llama_model_loader: - type  f32:  194 tensors
0.00.023.731 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.732 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.150 I llm_load_vocab: special tokens cache size = 25
0.00.050.226 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.228 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.228 I llm_load_print_meta: arch             = gptneox
0.00.050.229 I llm_load_print_meta: vocab type       = BPE
0.00.050.229 I llm_load_print_meta: n_vocab          = 50304
0.00.050.229 I llm_load_print_meta: n_merges         = 50009
0.00.050.229 I llm_load_print_meta: vocab_only       = 0
0.00.050.229 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.230 I llm_load_print_meta: n_embd           = 2048
0.00.050.230 I llm_load_print_meta: n_layer          = 24
0.00.050.232 I llm_load_print_meta: n_head           = 16
0.00.050.233 I llm_load_print_meta: n_head_kv        = 16
0.00.050.233 I llm_load_print_meta: n_rot            = 32
0.00.050.234 I llm_load_print_meta: n_swa            = 0
0.00.050.235 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.235 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.236 I llm_load_print_meta: n_gqa            = 1
0.00.050.236 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.237 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.238 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.238 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.238 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.238 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.238 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.239 I llm_load_print_meta: n_ff             = 8192
0.00.050.239 I llm_load_print_meta: n_expert         = 0
0.00.050.239 I llm_load_print_meta: n_expert_used    = 0
0.00.050.240 I llm_load_print_meta: causal attn      = 1
0.00.050.240 I llm_load_print_meta: pooling type     = 0
0.00.050.240 I llm_load_print_meta: rope type        = 2
0.00.050.240 I llm_load_print_meta: rope scaling     = linear
0.00.050.241 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.241 I llm_load_print_meta: freq_scale_train = 1
0.00.050.243 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.243 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.243 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.244 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.244 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.244 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.244 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.244 I llm_load_print_meta: model type       = 1.4B
0.00.050.245 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.245 I llm_load_print_meta: model params     = 1.41 B
0.00.050.246 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.246 I llm_load_print_meta: general.name     = 1.4B
0.00.050.246 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.250 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.251 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.251 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.251 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.251 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.251 I llm_load_print_meta: max token length = 1024
0.00.051.862 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.862 I llm_load_tensors: offloading output layer to GPU
0.00.051.862 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.872 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.873 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.682 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.682 I llama_new_context_with_model: n_ctx         = 128
0.00.052.683 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.683 I llama_new_context_with_model: n_batch       = 128
0.00.052.683 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.683 I llama_new_context_with_model: flash_attn    = 0
0.00.052.684 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.684 I llama_new_context_with_model: freq_scale    = 1
0.00.052.684 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.685 I ggml_metal_init: allocating
0.00.052.690 I ggml_metal_init: found device: Apple M4
0.00.052.692 I ggml_metal_init: picking default device: Apple M4
0.00.053.267 I ggml_metal_init: using embedded metal library
0.00.055.600 I ggml_metal_init: GPU name:   Apple M4
0.00.055.601 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.602 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.602 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.603 I ggml_metal_init: simdgroup reduction   = true
0.00.055.603 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.603 I ggml_metal_init: has bfloat            = true
0.00.055.603 I ggml_metal_init: use bfloat            = true
0.00.055.603 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.604 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.899 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.158 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.160 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.175 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.993 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.994 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.994 I llama_new_context_with_model: graph nodes  = 967
0.00.066.994 I llama_new_context_with_model: graph splits = 2
0.00.066.996 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.996 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.643.835 I 
0.00.643.868 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.643.881 I perplexity: tokenizing the input ..
0.00.651.728 I perplexity: tokenization took 7.846 ms
0.00.651.733 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.774.968 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.776.057 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.776.073 I llama_perf_context_print:        load time =     634.22 ms
0.00.776.075 I llama_perf_context_print: prompt eval time =     123.02 ms /   128 tokens (    0.96 ms per token,  1040.52 tokens per second)
0.00.776.076 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.776.077 I llama_perf_context_print:       total time =     132.24 ms /   129 tokens
0.00.776.489 I ggml_metal_free: deallocating

real	0m0.790s
user	0m0.078s
sys	0m0.134s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.170 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.971 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.974 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.975 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.976 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.977 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.981 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.981 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.982 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.982 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.983 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.983 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.983 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.984 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.984 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.986 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.986 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.986 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.739 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.828 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.582 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.583 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.583 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.584 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.584 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.585 I llama_model_loader: - type  f32:  194 tensors
0.00.024.585 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.585 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.910 I llm_load_vocab: special tokens cache size = 25
0.00.050.857 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.859 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.860 I llm_load_print_meta: arch             = gptneox
0.00.050.860 I llm_load_print_meta: vocab type       = BPE
0.00.050.860 I llm_load_print_meta: n_vocab          = 50304
0.00.050.860 I llm_load_print_meta: n_merges         = 50009
0.00.050.861 I llm_load_print_meta: vocab_only       = 0
0.00.050.861 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.861 I llm_load_print_meta: n_embd           = 2048
0.00.050.861 I llm_load_print_meta: n_layer          = 24
0.00.050.864 I llm_load_print_meta: n_head           = 16
0.00.050.864 I llm_load_print_meta: n_head_kv        = 16
0.00.050.864 I llm_load_print_meta: n_rot            = 32
0.00.050.865 I llm_load_print_meta: n_swa            = 0
0.00.050.865 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.865 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.866 I llm_load_print_meta: n_gqa            = 1
0.00.050.867 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.867 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.868 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.868 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.868 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.869 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.869 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.869 I llm_load_print_meta: n_ff             = 8192
0.00.050.870 I llm_load_print_meta: n_expert         = 0
0.00.050.870 I llm_load_print_meta: n_expert_used    = 0
0.00.050.870 I llm_load_print_meta: causal attn      = 1
0.00.050.870 I llm_load_print_meta: pooling type     = 0
0.00.050.870 I llm_load_print_meta: rope type        = 2
0.00.050.871 I llm_load_print_meta: rope scaling     = linear
0.00.050.871 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.871 I llm_load_print_meta: freq_scale_train = 1
0.00.050.872 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.872 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.872 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.872 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.872 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.873 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.873 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.873 I llm_load_print_meta: model type       = 1.4B
0.00.050.873 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.874 I llm_load_print_meta: model params     = 1.41 B
0.00.050.875 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.875 I llm_load_print_meta: general.name     = 1.4B
0.00.050.875 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.875 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.875 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.875 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.876 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.876 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.876 I llm_load_print_meta: max token length = 1024
0.00.052.505 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.505 I llm_load_tensors: offloading output layer to GPU
0.00.052.505 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.515 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.516 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.357 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.358 I llama_new_context_with_model: n_ctx         = 128
0.00.053.358 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.358 I llama_new_context_with_model: n_batch       = 128
0.00.053.358 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.358 I llama_new_context_with_model: flash_attn    = 0
0.00.053.359 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.359 I llama_new_context_with_model: freq_scale    = 1
0.00.053.359 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.360 I ggml_metal_init: allocating
0.00.053.363 I ggml_metal_init: found device: Apple M4
0.00.053.365 I ggml_metal_init: picking default device: Apple M4
0.00.053.929 I ggml_metal_init: using embedded metal library
0.00.056.211 I ggml_metal_init: GPU name:   Apple M4
0.00.056.213 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.213 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.213 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.214 I ggml_metal_init: simdgroup reduction   = true
0.00.056.214 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.214 I ggml_metal_init: has bfloat            = true
0.00.056.214 I ggml_metal_init: use bfloat            = true
0.00.056.214 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.215 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.809 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.247 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.250 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.264 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.141 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.142 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.142 I llama_new_context_with_model: graph nodes  = 967
0.00.068.143 I llama_new_context_with_model: graph splits = 2
0.00.068.144 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.144 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.849 I 
0.00.756.879 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.893 I perplexity: tokenizing the input ..
0.00.764.597 I perplexity: tokenization took 7.703 ms
0.00.764.601 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.995 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.901.190 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.901.207 I llama_perf_context_print:        load time =     746.68 ms
0.00.901.208 I llama_perf_context_print: prompt eval time =     135.17 ms /   128 tokens (    1.06 ms per token,   946.93 tokens per second)
0.00.901.209 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.901.209 I llama_perf_context_print:       total time =     144.36 ms /   129 tokens
0.00.901.668 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.079s
sys	0m0.157s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.351 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.798 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.803 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.804 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.805 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.805 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.805 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.806 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.806 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.807 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.808 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.809 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.809 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.809 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.810 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.811 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.812 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.812 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.544 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.584 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.320 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.321 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.321 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.321 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.322 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.322 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.323 I llama_model_loader: - type  f32:  194 tensors
0.00.023.323 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.323 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.854 I llm_load_vocab: special tokens cache size = 25
0.00.049.814 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.817 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.817 I llm_load_print_meta: arch             = gptneox
0.00.049.818 I llm_load_print_meta: vocab type       = BPE
0.00.049.818 I llm_load_print_meta: n_vocab          = 50304
0.00.049.818 I llm_load_print_meta: n_merges         = 50009
0.00.049.818 I llm_load_print_meta: vocab_only       = 0
0.00.049.819 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.819 I llm_load_print_meta: n_embd           = 2048
0.00.049.819 I llm_load_print_meta: n_layer          = 24
0.00.049.822 I llm_load_print_meta: n_head           = 16
0.00.049.822 I llm_load_print_meta: n_head_kv        = 16
0.00.049.822 I llm_load_print_meta: n_rot            = 32
0.00.049.823 I llm_load_print_meta: n_swa            = 0
0.00.049.823 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.823 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.824 I llm_load_print_meta: n_gqa            = 1
0.00.049.824 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.825 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.826 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.826 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.826 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.826 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.826 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.827 I llm_load_print_meta: n_ff             = 8192
0.00.049.827 I llm_load_print_meta: n_expert         = 0
0.00.049.827 I llm_load_print_meta: n_expert_used    = 0
0.00.049.827 I llm_load_print_meta: causal attn      = 1
0.00.049.828 I llm_load_print_meta: pooling type     = 0
0.00.049.828 I llm_load_print_meta: rope type        = 2
0.00.049.828 I llm_load_print_meta: rope scaling     = linear
0.00.049.829 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.829 I llm_load_print_meta: freq_scale_train = 1
0.00.049.829 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.829 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.830 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.830 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.830 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.830 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.830 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.830 I llm_load_print_meta: model type       = 1.4B
0.00.049.831 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.831 I llm_load_print_meta: model params     = 1.41 B
0.00.049.832 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.833 I llm_load_print_meta: general.name     = 1.4B
0.00.049.833 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.834 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.834 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.834 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.835 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.835 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.835 I llm_load_print_meta: max token length = 1024
0.00.051.478 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.478 I llm_load_tensors: offloading output layer to GPU
0.00.051.478 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.489 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.490 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.336 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.337 I llama_new_context_with_model: n_ctx         = 128
0.00.052.337 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.337 I llama_new_context_with_model: n_batch       = 128
0.00.052.338 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.338 I llama_new_context_with_model: flash_attn    = 0
0.00.052.338 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.339 I llama_new_context_with_model: freq_scale    = 1
0.00.052.339 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.340 I ggml_metal_init: allocating
0.00.052.346 I ggml_metal_init: found device: Apple M4
0.00.052.349 I ggml_metal_init: picking default device: Apple M4
0.00.052.912 I ggml_metal_init: using embedded metal library
0.00.055.242 I ggml_metal_init: GPU name:   Apple M4
0.00.055.244 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.244 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.244 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.245 I ggml_metal_init: simdgroup reduction   = true
0.00.055.245 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.245 I ggml_metal_init: has bfloat            = true
0.00.055.245 I ggml_metal_init: use bfloat            = true
0.00.055.245 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.246 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.726 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.990 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.994 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.017 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.929 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.930 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.930 I llama_new_context_with_model: graph nodes  = 967
0.00.066.931 I llama_new_context_with_model: graph splits = 2
0.00.066.932 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.932 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.789.519 I 
0.00.789.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.789.561 I perplexity: tokenizing the input ..
0.00.797.207 I perplexity: tokenization took 7.645 ms
0.00.797.210 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.932.549 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.933.761 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.933.774 I llama_perf_context_print:        load time =     780.16 ms
0.00.933.775 I llama_perf_context_print: prompt eval time =     135.12 ms /   128 tokens (    1.06 ms per token,   947.31 tokens per second)
0.00.933.776 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.933.776 I llama_perf_context_print:       total time =     144.26 ms /   129 tokens
0.00.934.202 I ggml_metal_free: deallocating

real	0m0.947s
user	0m0.078s
sys	0m0.158s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.086 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.736 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.192 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.197 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.199 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.199 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.199 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.200 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.203 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.203 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.203 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.204 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.204 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.204 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.209 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.210 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.211 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.211 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.947 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.770 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.770 I llama_model_loader: - type  f32:  194 tensors
0.00.023.771 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.771 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.771 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.296 I llm_load_vocab: special tokens cache size = 25
0.00.050.257 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.260 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.260 I llm_load_print_meta: arch             = gptneox
0.00.050.260 I llm_load_print_meta: vocab type       = BPE
0.00.050.261 I llm_load_print_meta: n_vocab          = 50304
0.00.050.261 I llm_load_print_meta: n_merges         = 50009
0.00.050.261 I llm_load_print_meta: vocab_only       = 0
0.00.050.261 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.261 I llm_load_print_meta: n_embd           = 2048
0.00.050.261 I llm_load_print_meta: n_layer          = 24
0.00.050.264 I llm_load_print_meta: n_head           = 16
0.00.050.265 I llm_load_print_meta: n_head_kv        = 16
0.00.050.265 I llm_load_print_meta: n_rot            = 32
0.00.050.265 I llm_load_print_meta: n_swa            = 0
0.00.050.266 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.266 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.266 I llm_load_print_meta: n_gqa            = 1
0.00.050.267 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.268 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.273 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.273 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.273 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.273 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.274 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.274 I llm_load_print_meta: n_ff             = 8192
0.00.050.275 I llm_load_print_meta: n_expert         = 0
0.00.050.277 I llm_load_print_meta: n_expert_used    = 0
0.00.050.277 I llm_load_print_meta: causal attn      = 1
0.00.050.277 I llm_load_print_meta: pooling type     = 0
0.00.050.277 I llm_load_print_meta: rope type        = 2
0.00.050.277 I llm_load_print_meta: rope scaling     = linear
0.00.050.278 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.278 I llm_load_print_meta: freq_scale_train = 1
0.00.050.278 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.278 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.279 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.279 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.279 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.279 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.279 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.279 I llm_load_print_meta: model type       = 1.4B
0.00.050.280 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.280 I llm_load_print_meta: model params     = 1.41 B
0.00.050.281 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.281 I llm_load_print_meta: general.name     = 1.4B
0.00.050.281 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.282 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.282 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.282 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.283 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.283 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.283 I llm_load_print_meta: max token length = 1024
0.00.051.852 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.852 I llm_load_tensors: offloading output layer to GPU
0.00.051.852 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.862 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.863 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.052.680 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.681 I llama_new_context_with_model: n_ctx         = 128
0.00.052.681 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.681 I llama_new_context_with_model: n_batch       = 128
0.00.052.681 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.681 I llama_new_context_with_model: flash_attn    = 0
0.00.052.682 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.682 I llama_new_context_with_model: freq_scale    = 1
0.00.052.682 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.683 I ggml_metal_init: allocating
0.00.052.688 I ggml_metal_init: found device: Apple M4
0.00.052.692 I ggml_metal_init: picking default device: Apple M4
0.00.053.224 I ggml_metal_init: using embedded metal library
0.00.055.540 I ggml_metal_init: GPU name:   Apple M4
0.00.055.542 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.542 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.542 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.543 I ggml_metal_init: simdgroup reduction   = true
0.00.055.543 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.543 I ggml_metal_init: has bfloat            = true
0.00.055.543 I ggml_metal_init: use bfloat            = true
0.00.055.543 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.544 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.833 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.083 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.085 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.098 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.942 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.943 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.944 I llama_new_context_with_model: graph nodes  = 967
0.00.066.944 I llama_new_context_with_model: graph splits = 2
0.00.066.945 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.945 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.428.183 I 
0.00.428.226 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.428.239 I perplexity: tokenizing the input ..
0.00.435.752 I perplexity: tokenization took 7.512 ms
0.00.435.756 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.568.384 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.569.512 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.569.530 I llama_perf_context_print:        load time =     418.44 ms
0.00.569.532 I llama_perf_context_print: prompt eval time =     132.41 ms /   128 tokens (    1.03 ms per token,   966.73 tokens per second)
0.00.569.533 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.569.533 I llama_perf_context_print:       total time =     141.35 ms /   129 tokens
0.00.570.098 I ggml_metal_free: deallocating

real	0m0.585s
user	0m0.078s
sys	0m0.092s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.428 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.162 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.169 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.171 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.171 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.172 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.172 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.172 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.174 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.174 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.176 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.176 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.939 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.966 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.675 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.678 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.678 I llama_model_loader: - type  f32:  194 tensors
0.00.024.678 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.679 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.679 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.679 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.124 I llm_load_vocab: special tokens cache size = 25
0.00.049.904 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.907 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.907 I llm_load_print_meta: arch             = gptneox
0.00.049.907 I llm_load_print_meta: vocab type       = BPE
0.00.049.908 I llm_load_print_meta: n_vocab          = 50304
0.00.049.908 I llm_load_print_meta: n_merges         = 50009
0.00.049.908 I llm_load_print_meta: vocab_only       = 0
0.00.049.908 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.908 I llm_load_print_meta: n_embd           = 2048
0.00.049.909 I llm_load_print_meta: n_layer          = 24
0.00.049.911 I llm_load_print_meta: n_head           = 16
0.00.049.912 I llm_load_print_meta: n_head_kv        = 16
0.00.049.912 I llm_load_print_meta: n_rot            = 32
0.00.049.912 I llm_load_print_meta: n_swa            = 0
0.00.049.912 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.912 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.913 I llm_load_print_meta: n_gqa            = 1
0.00.049.914 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.914 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.915 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.915 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.916 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.916 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.918 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.918 I llm_load_print_meta: n_ff             = 8192
0.00.049.919 I llm_load_print_meta: n_expert         = 0
0.00.049.919 I llm_load_print_meta: n_expert_used    = 0
0.00.049.919 I llm_load_print_meta: causal attn      = 1
0.00.049.919 I llm_load_print_meta: pooling type     = 0
0.00.049.919 I llm_load_print_meta: rope type        = 2
0.00.049.921 I llm_load_print_meta: rope scaling     = linear
0.00.049.921 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.921 I llm_load_print_meta: freq_scale_train = 1
0.00.049.922 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.922 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.922 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.923 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.924 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.924 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.924 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.924 I llm_load_print_meta: model type       = 1.4B
0.00.049.925 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.925 I llm_load_print_meta: model params     = 1.41 B
0.00.049.926 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.926 I llm_load_print_meta: general.name     = 1.4B
0.00.049.926 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.927 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.928 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.929 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.929 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.929 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.929 I llm_load_print_meta: max token length = 1024
0.00.051.363 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.363 I llm_load_tensors: offloading output layer to GPU
0.00.051.364 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.368 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.369 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.144 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.145 I llama_new_context_with_model: n_ctx         = 128
0.00.052.145 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.145 I llama_new_context_with_model: n_batch       = 128
0.00.052.146 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.146 I llama_new_context_with_model: flash_attn    = 0
0.00.052.146 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.146 I llama_new_context_with_model: freq_scale    = 1
0.00.052.147 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.147 I ggml_metal_init: allocating
0.00.052.150 I ggml_metal_init: found device: Apple M4
0.00.052.152 I ggml_metal_init: picking default device: Apple M4
0.00.052.670 I ggml_metal_init: using embedded metal library
0.00.054.973 I ggml_metal_init: GPU name:   Apple M4
0.00.054.975 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.975 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.975 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.976 I ggml_metal_init: simdgroup reduction   = true
0.00.054.976 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.976 I ggml_metal_init: has bfloat            = true
0.00.054.976 I ggml_metal_init: use bfloat            = true
0.00.054.976 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.977 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.063.943 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.273 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.276 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.292 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.202 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.203 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.204 I llama_new_context_with_model: graph nodes  = 967
0.00.066.204 I llama_new_context_with_model: graph splits = 2
0.00.066.205 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.205 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.495 I 
0.00.505.530 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.541 I perplexity: tokenizing the input ..
0.00.513.142 I perplexity: tokenization took 7.599 ms
0.00.513.145 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.645.252 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.646.442 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.646.457 I llama_perf_context_print:        load time =     495.06 ms
0.00.646.458 I llama_perf_context_print: prompt eval time =     131.89 ms /   128 tokens (    1.03 ms per token,   970.54 tokens per second)
0.00.646.459 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.646.460 I llama_perf_context_print:       total time =     140.96 ms /   129 tokens
0.00.647.000 I ggml_metal_free: deallocating

real	0m0.660s
user	0m0.076s
sys	0m0.103s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.695 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.351 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.355 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.357 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.357 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.357 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.358 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.359 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.359 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.360 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.361 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.362 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.070 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.114 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.778 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.779 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.780 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.780 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.780 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.781 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.781 I llama_model_loader: - type  f32:  194 tensors
0.00.023.781 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.782 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.782 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.363 I llm_load_vocab: special tokens cache size = 25
0.00.049.319 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.322 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.322 I llm_load_print_meta: arch             = gptneox
0.00.049.322 I llm_load_print_meta: vocab type       = BPE
0.00.049.323 I llm_load_print_meta: n_vocab          = 50304
0.00.049.323 I llm_load_print_meta: n_merges         = 50009
0.00.049.323 I llm_load_print_meta: vocab_only       = 0
0.00.049.323 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.323 I llm_load_print_meta: n_embd           = 2048
0.00.049.324 I llm_load_print_meta: n_layer          = 24
0.00.049.326 I llm_load_print_meta: n_head           = 16
0.00.049.326 I llm_load_print_meta: n_head_kv        = 16
0.00.049.327 I llm_load_print_meta: n_rot            = 32
0.00.049.327 I llm_load_print_meta: n_swa            = 0
0.00.049.327 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.327 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.328 I llm_load_print_meta: n_gqa            = 1
0.00.049.329 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.329 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.330 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.330 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.330 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.331 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.331 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.331 I llm_load_print_meta: n_ff             = 8192
0.00.049.332 I llm_load_print_meta: n_expert         = 0
0.00.049.332 I llm_load_print_meta: n_expert_used    = 0
0.00.049.332 I llm_load_print_meta: causal attn      = 1
0.00.049.332 I llm_load_print_meta: pooling type     = 0
0.00.049.332 I llm_load_print_meta: rope type        = 2
0.00.049.332 I llm_load_print_meta: rope scaling     = linear
0.00.049.336 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.336 I llm_load_print_meta: freq_scale_train = 1
0.00.049.336 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.337 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.337 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.337 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.338 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.338 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.338 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.339 I llm_load_print_meta: model type       = 1.4B
0.00.049.339 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.340 I llm_load_print_meta: model params     = 1.41 B
0.00.049.340 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.340 I llm_load_print_meta: general.name     = 1.4B
0.00.049.341 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.344 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.345 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.345 I llm_load_print_meta: max token length = 1024
0.00.051.084 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.085 I llm_load_tensors: offloading output layer to GPU
0.00.051.085 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.095 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.096 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.051.920 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.921 I llama_new_context_with_model: n_ctx         = 128
0.00.051.921 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.921 I llama_new_context_with_model: n_batch       = 128
0.00.051.921 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.922 I llama_new_context_with_model: flash_attn    = 0
0.00.051.922 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.922 I llama_new_context_with_model: freq_scale    = 1
0.00.051.923 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.923 I ggml_metal_init: allocating
0.00.051.929 I ggml_metal_init: found device: Apple M4
0.00.051.931 I ggml_metal_init: picking default device: Apple M4
0.00.052.491 I ggml_metal_init: using embedded metal library
0.00.054.839 I ggml_metal_init: GPU name:   Apple M4
0.00.054.841 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.841 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.842 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.842 I ggml_metal_init: simdgroup reduction   = true
0.00.054.842 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.842 I ggml_metal_init: has bfloat            = true
0.00.054.842 I ggml_metal_init: use bfloat            = true
0.00.054.843 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.843 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.286 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.616 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.618 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.631 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.491 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.491 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.492 I llama_new_context_with_model: graph nodes  = 967
0.00.066.492 I llama_new_context_with_model: graph splits = 2
0.00.066.493 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.493 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.610.893 I 
0.00.610.922 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.610.935 I perplexity: tokenizing the input ..
0.00.618.499 I perplexity: tokenization took 7.562 ms
0.00.618.505 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.753.215 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.754.331 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.754.343 I llama_perf_context_print:        load time =     601.19 ms
0.00.754.343 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.74 tokens per second)
0.00.754.344 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.754.345 I llama_perf_context_print:       total time =     143.45 ms /   129 tokens
0.00.754.642 I ggml_metal_free: deallocating

real	0m0.768s
user	0m0.077s
sys	0m0.135s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.104 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.741 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.746 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.747 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.753 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.753 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.754 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.755 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.756 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.756 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.756 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.757 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.757 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.759 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.459 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.469 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.189 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.190 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.191 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.191 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.192 I llama_model_loader: - type  f32:  194 tensors
0.00.025.192 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.193 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.734 I llm_load_vocab: special tokens cache size = 25
0.00.050.708 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.711 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.712 I llm_load_print_meta: arch             = gptneox
0.00.050.712 I llm_load_print_meta: vocab type       = BPE
0.00.050.712 I llm_load_print_meta: n_vocab          = 50304
0.00.050.713 I llm_load_print_meta: n_merges         = 50009
0.00.050.713 I llm_load_print_meta: vocab_only       = 0
0.00.050.713 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.713 I llm_load_print_meta: n_embd           = 2048
0.00.050.713 I llm_load_print_meta: n_layer          = 24
0.00.050.716 I llm_load_print_meta: n_head           = 16
0.00.050.716 I llm_load_print_meta: n_head_kv        = 16
0.00.050.717 I llm_load_print_meta: n_rot            = 32
0.00.050.717 I llm_load_print_meta: n_swa            = 0
0.00.050.719 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.719 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.720 I llm_load_print_meta: n_gqa            = 1
0.00.050.721 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.721 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.722 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.722 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.723 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.724 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.724 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.725 I llm_load_print_meta: n_ff             = 8192
0.00.050.725 I llm_load_print_meta: n_expert         = 0
0.00.050.725 I llm_load_print_meta: n_expert_used    = 0
0.00.050.725 I llm_load_print_meta: causal attn      = 1
0.00.050.725 I llm_load_print_meta: pooling type     = 0
0.00.050.725 I llm_load_print_meta: rope type        = 2
0.00.050.726 I llm_load_print_meta: rope scaling     = linear
0.00.050.726 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.726 I llm_load_print_meta: freq_scale_train = 1
0.00.050.726 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.727 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.727 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.728 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.735 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.737 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.737 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.737 I llm_load_print_meta: model type       = 1.4B
0.00.050.738 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.738 I llm_load_print_meta: model params     = 1.41 B
0.00.050.739 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.739 I llm_load_print_meta: general.name     = 1.4B
0.00.050.739 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.739 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.740 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.050.740 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.740 I llm_load_print_meta: max token length = 1024
0.00.052.478 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.479 I llm_load_tensors: offloading output layer to GPU
0.00.052.479 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.490 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.491 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.316 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.317 I llama_new_context_with_model: n_ctx         = 128
0.00.053.317 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.317 I llama_new_context_with_model: n_batch       = 128
0.00.053.317 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.317 I llama_new_context_with_model: flash_attn    = 0
0.00.053.318 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.318 I llama_new_context_with_model: freq_scale    = 1
0.00.053.318 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.319 I ggml_metal_init: allocating
0.00.053.323 I ggml_metal_init: found device: Apple M4
0.00.053.325 I ggml_metal_init: picking default device: Apple M4
0.00.053.888 I ggml_metal_init: using embedded metal library
0.00.056.197 I ggml_metal_init: GPU name:   Apple M4
0.00.056.198 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.199 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.199 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.199 I ggml_metal_init: simdgroup reduction   = true
0.00.056.199 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.199 I ggml_metal_init: has bfloat            = true
0.00.056.200 I ggml_metal_init: use bfloat            = true
0.00.056.200 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.201 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.441 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.694 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.696 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.711 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.590 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.591 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.591 I llama_new_context_with_model: graph nodes  = 967
0.00.067.592 I llama_new_context_with_model: graph splits = 2
0.00.067.593 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.593 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.694.621 I 
0.00.694.648 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.694.661 I perplexity: tokenizing the input ..
0.00.702.583 I perplexity: tokenization took 7.92 ms
0.00.702.587 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.843.461 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.844.582 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.844.597 I llama_perf_context_print:        load time =     683.51 ms
0.00.844.598 I llama_perf_context_print: prompt eval time =     140.65 ms /   128 tokens (    1.10 ms per token,   910.03 tokens per second)
0.00.844.599 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.844.599 I llama_perf_context_print:       total time =     149.98 ms /   129 tokens
0.00.845.029 I ggml_metal_free: deallocating

real	0m0.859s
user	0m0.078s
sys	0m0.152s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.748 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.453 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.014.457 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.458 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.463 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.463 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.464 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.464 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.465 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.465 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.465 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.466 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.466 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.466 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.467 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.468 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.469 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.469 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.286 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.328 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.125 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.127 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.127 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.127 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.128 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.128 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.128 I llama_model_loader: - type  f32:  194 tensors
0.00.023.129 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.456 I llm_load_vocab: special tokens cache size = 25
0.00.049.491 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.493 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.494 I llm_load_print_meta: arch             = gptneox
0.00.049.494 I llm_load_print_meta: vocab type       = BPE
0.00.049.494 I llm_load_print_meta: n_vocab          = 50304
0.00.049.494 I llm_load_print_meta: n_merges         = 50009
0.00.049.495 I llm_load_print_meta: vocab_only       = 0
0.00.049.495 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.495 I llm_load_print_meta: n_embd           = 2048
0.00.049.495 I llm_load_print_meta: n_layer          = 24
0.00.049.498 I llm_load_print_meta: n_head           = 16
0.00.049.498 I llm_load_print_meta: n_head_kv        = 16
0.00.049.499 I llm_load_print_meta: n_rot            = 32
0.00.049.499 I llm_load_print_meta: n_swa            = 0
0.00.049.499 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.499 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.500 I llm_load_print_meta: n_gqa            = 1
0.00.049.500 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.501 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.501 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.502 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.505 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.505 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.505 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.505 I llm_load_print_meta: n_ff             = 8192
0.00.049.506 I llm_load_print_meta: n_expert         = 0
0.00.049.506 I llm_load_print_meta: n_expert_used    = 0
0.00.049.506 I llm_load_print_meta: causal attn      = 1
0.00.049.506 I llm_load_print_meta: pooling type     = 0
0.00.049.506 I llm_load_print_meta: rope type        = 2
0.00.049.506 I llm_load_print_meta: rope scaling     = linear
0.00.049.507 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.507 I llm_load_print_meta: freq_scale_train = 1
0.00.049.507 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.508 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.508 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.508 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.508 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.510 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.510 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.510 I llm_load_print_meta: model type       = 1.4B
0.00.049.511 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.511 I llm_load_print_meta: model params     = 1.41 B
0.00.049.512 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.512 I llm_load_print_meta: general.name     = 1.4B
0.00.049.512 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.512 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.512 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.513 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.049.517 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.517 I llm_load_print_meta: max token length = 1024
0.00.051.283 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.283 I llm_load_tensors: offloading output layer to GPU
0.00.051.284 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.294 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.295 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.120 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.120 I llama_new_context_with_model: n_ctx         = 128
0.00.052.120 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.121 I llama_new_context_with_model: n_batch       = 128
0.00.052.121 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.121 I llama_new_context_with_model: flash_attn    = 0
0.00.052.121 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.122 I llama_new_context_with_model: freq_scale    = 1
0.00.052.122 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.122 I ggml_metal_init: allocating
0.00.052.128 I ggml_metal_init: found device: Apple M4
0.00.052.131 I ggml_metal_init: picking default device: Apple M4
0.00.052.714 I ggml_metal_init: using embedded metal library
0.00.055.067 I ggml_metal_init: GPU name:   Apple M4
0.00.055.069 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.069 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.069 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.070 I ggml_metal_init: simdgroup reduction   = true
0.00.055.070 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.070 I ggml_metal_init: has bfloat            = true
0.00.055.070 I ggml_metal_init: use bfloat            = true
0.00.055.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.071 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.561 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.785 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.790 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.805 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.642 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.643 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.644 I llama_new_context_with_model: graph nodes  = 967
0.00.066.644 I llama_new_context_with_model: graph splits = 2
0.00.066.645 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.645 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.602 I 
0.00.709.627 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.640 I perplexity: tokenizing the input ..
0.00.717.233 I perplexity: tokenization took 7.592 ms
0.00.717.236 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.660 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.858.770 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.858.781 I llama_perf_context_print:        load time =     700.85 ms
0.00.858.783 I llama_perf_context_print: prompt eval time =     140.21 ms /   128 tokens (    1.10 ms per token,   912.95 tokens per second)
0.00.858.783 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.784 I llama_perf_context_print:       total time =     149.18 ms /   129 tokens
0.00.859.231 I ggml_metal_free: deallocating

real	0m0.872s
user	0m0.078s
sys	0m0.155s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.242 I build: 4428 (e6e7c75d) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.756 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.033.202 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.033.206 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.033.208 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.033.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.033.213 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.033.214 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.033.214 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.033.215 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.033.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.033.216 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.033.218 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.033.219 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.033.219 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.033.219 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.033.223 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.033.223 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.033.224 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.040.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.042.660 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.068 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.049.070 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.071 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.049.071 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.049.071 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.049.072 I llama_model_loader: - type  f32:  194 tensors
0.00.049.072 I llama_model_loader: - type  f16:   98 tensors
0.00.076.418 I llm_load_vocab: special tokens cache size = 25
0.00.083.063 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.083.066 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.083.066 I llm_load_print_meta: arch             = gptneox
0.00.083.067 I llm_load_print_meta: vocab type       = BPE
0.00.083.067 I llm_load_print_meta: n_vocab          = 50304
0.00.083.067 I llm_load_print_meta: n_merges         = 50009
0.00.083.067 I llm_load_print_meta: vocab_only       = 0
0.00.083.067 I llm_load_print_meta: n_ctx_train      = 2048
0.00.083.068 I llm_load_print_meta: n_embd           = 2048
0.00.083.068 I llm_load_print_meta: n_layer          = 24
0.00.083.070 I llm_load_print_meta: n_head           = 16
0.00.083.071 I llm_load_print_meta: n_head_kv        = 16
0.00.083.075 I llm_load_print_meta: n_rot            = 32
0.00.083.075 I llm_load_print_meta: n_swa            = 0
0.00.083.075 I llm_load_print_meta: n_embd_head_k    = 128
0.00.083.075 I llm_load_print_meta: n_embd_head_v    = 128
0.00.083.077 I llm_load_print_meta: n_gqa            = 1
0.00.083.078 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.083.078 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.083.079 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.083.079 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.083.084 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.083.084 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.083.084 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.083.085 I llm_load_print_meta: n_ff             = 8192
0.00.083.085 I llm_load_print_meta: n_expert         = 0
0.00.083.085 I llm_load_print_meta: n_expert_used    = 0
0.00.083.085 I llm_load_print_meta: causal attn      = 1
0.00.083.087 I llm_load_print_meta: pooling type     = 0
0.00.083.087 I llm_load_print_meta: rope type        = 2
0.00.083.087 I llm_load_print_meta: rope scaling     = linear
0.00.083.087 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.083.088 I llm_load_print_meta: freq_scale_train = 1
0.00.083.088 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.083.088 I llm_load_print_meta: rope_finetuned   = unknown
0.00.083.088 I llm_load_print_meta: ssm_d_conv       = 0
0.00.083.088 I llm_load_print_meta: ssm_d_inner      = 0
0.00.083.088 I llm_load_print_meta: ssm_d_state      = 0
0.00.083.088 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.083.089 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.083.089 I llm_load_print_meta: model type       = 1.4B
0.00.083.091 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.083.091 I llm_load_print_meta: model params     = 1.41 B
0.00.083.092 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.083.092 I llm_load_print_meta: general.name     = 1.4B
0.00.083.092 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.083.092 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.083.093 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.083.093 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.083.093 I llm_load_print_meta: LF token         = 128 'Ä'
0.00.083.093 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.083.093 I llm_load_print_meta: max token length = 1024
0.00.085.103 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.085.103 I llm_load_tensors: offloading output layer to GPU
0.00.085.103 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.085.113 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.085.114 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.085.960 I llama_new_context_with_model: n_seq_max     = 1
0.00.085.961 I llama_new_context_with_model: n_ctx         = 128
0.00.085.961 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.085.961 I llama_new_context_with_model: n_batch       = 128
0.00.085.961 I llama_new_context_with_model: n_ubatch      = 128
0.00.085.961 I llama_new_context_with_model: flash_attn    = 0
0.00.085.962 I llama_new_context_with_model: freq_base     = 10000.0
0.00.085.962 I llama_new_context_with_model: freq_scale    = 1
0.00.085.962 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.085.963 I ggml_metal_init: allocating
0.00.085.971 I ggml_metal_init: found device: Apple M4
0.00.085.974 I ggml_metal_init: picking default device: Apple M4
0.00.086.581 I ggml_metal_init: using embedded metal library
0.00.089.091 I ggml_metal_init: GPU name:   Apple M4
0.00.089.093 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.089.093 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.089.094 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.089.094 I ggml_metal_init: simdgroup reduction   = true
0.00.089.094 I ggml_metal_init: simdgroup matrix mul. = true
0.00.089.094 I ggml_metal_init: has bfloat            = true
0.00.089.094 I ggml_metal_init: use bfloat            = true
0.00.089.095 I ggml_metal_init: hasUnifiedMemory      = true
0.00.089.095 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.098.204 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.099.572 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.099.576 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.099.591 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.485 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.100.486 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.100.486 I llama_new_context_with_model: graph nodes  = 967
0.00.100.487 I llama_new_context_with_model: graph splits = 2
0.00.100.488 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.100.488 I 
0.00.100.521 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.100.523 I compute_imatrix: tokenizing the input ..
0.00.107.091 I compute_imatrix: tokenization took 6.567 ms
0.00.107.092 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.791.890 I compute_imatrix: 1.68 seconds per pass - ETA 0.02 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.794.752 I llama_perf_context_print:        load time =    1771.13 ms
0.01.794.753 I llama_perf_context_print: prompt eval time =    1684.30 ms /   128 tokens (   13.16 ms per token,    76.00 tokens per second)
0.01.794.755 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.794.756 I llama_perf_context_print:       total time =    1773.99 ms /   129 tokens
0.01.795.844 I ggml_metal_free: deallocating

real	0m1.992s
user	0m0.159s
sys	0m0.332s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4428 (e6e7c75d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a0b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a0bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a0c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a0c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a0cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a0d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a0d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a0de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a0e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a0e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a0ee30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a0f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134a0fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134a10600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134a10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134a11530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134a11c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134a12370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134a12a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134a13260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134a13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134a140a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134a147c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134a15060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134a15780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134a15a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134a16050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134a16cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134a17200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134a174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134a17960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134a17c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134a184b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134a189f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134a18cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134a19150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134a195f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134a19a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134a19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134a1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134a1a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134a1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134a1b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134a1b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134a1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134a1bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134a1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134a1ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134a1d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134a1da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134a1e080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134a1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134a1eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134a1f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134a1faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134a1ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134a203e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134a206a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134a20cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134a214a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134a21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134a21c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134a220a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134a22540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134a229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134a22e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134a23320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134a237c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134a23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134a24100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134a245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134a24a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134a24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134a25430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134a25980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134a25ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134a26420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134a26970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134a26ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134a27410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134a27960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134a27eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134a28400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134a28950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134a28ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134a293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134a29940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134a29e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134a2a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134a2a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134a2ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134a2b3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134a2b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134a2be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134a2c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134a2c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134a2ce60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134a1cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134a2d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134a2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134a2dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134a2e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134a2ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134a2efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134a2f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134a2fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134a2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134a30500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134a30a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134a30fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134a314f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134a31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134a31f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134a32430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134a328d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134a32d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134a33210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134a336b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134a33b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134a33ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134a34490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134a34930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134a34dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134a35270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134a35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134a35bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134a36050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134a364f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134a36990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134a36e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134a372d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134a37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134a37c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134a380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134a38550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134a389f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134a38e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134a39330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134a397d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134a39c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134a3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134a3a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134a3aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134a3aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134a3b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134a3b830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134a3bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134a3c170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134a3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134a3cab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134a3cf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134a3d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134a3d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134a3dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134a3e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134a3e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134a3eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134a3efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134a3f450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134a3f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134a3fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134a40230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134a406d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134a40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134a41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134a414b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134a41950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134a41df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134a42290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134a42730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134a42bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134a43070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134a43510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134a439b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134a43e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134a442f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134a44790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134a44c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134a450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134a45570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134a45a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134a45eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134a46350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134a467f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134a46c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134a47130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134a475d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134a47a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134a47f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134a483b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134a48850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134a48cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134a49190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134a496e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134a49c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134a4a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134a4a6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134a4a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134a4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134a4b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134a4bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134a4c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134a4c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134a4cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134a4d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134a4d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134a4df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134a4e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134a4e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134a4ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134a4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134a4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134a4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134a504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134a509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134a50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134a51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134a519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134a51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134a52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134a529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134a52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134a53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134a539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134a53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134a54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134a549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134a54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134a55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134a559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134a55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134a56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134a56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134a56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134a57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134a57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134a57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134a58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134a58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134a58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134a59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134a59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134a59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134a5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134a5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134a5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134a5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134a5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134a5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134a5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134a5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134a5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134a5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134a5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134a5de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134a5e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134a5e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134a5ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134a5f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134a5f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134a5fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134a603a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134a608f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134a60e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134a61390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134a618e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134a61e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134a622d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134a62770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134a62c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134a630b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134a63550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134a639f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134a63e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134a64330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134a647d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134a64c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134a65110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134a655b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134a65a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134a65ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134a66390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134a668e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134a67000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134a67720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134a67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134a68560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134a68820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134a69010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134a692d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134a698e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.152.244 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.152.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a69590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a4b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a4b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a1e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a20960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a4d3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a1c7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a1d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a1d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134a1bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134a1dd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134a14d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134a0ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134a1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134a20f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134a2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134a68ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134a17ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134a181a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134a4d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134a4be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134a16310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134a165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134a16890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134a69d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134a6a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134a6a2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134a6a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134a6a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134a6ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134a6adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134a6b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134a6b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134a6b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134a6b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134a6bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134a6be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134a6c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134a6c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134a6c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134a6c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134a6cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134a6cec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134a6d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134a6d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134a6d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134a6d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134a6dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134a6df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134a6e200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134a6e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134a6e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134b06c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134b09000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134b092c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134b098d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134b0a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134b0a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134b0aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134b0aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134b0b340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134b0b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134b0bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134b0c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134b0c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134b0ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134b0cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134b0d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134b0d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134b0dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134b0e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134b0e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134b0ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134b0f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134b0f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134b0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134b10210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134b10760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134b10cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134b11200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134b11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134b11ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134b121f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134b12740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134b12c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134b131e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134b13730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134b13c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134b141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134b14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134b14c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134b151c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134b15710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134b15c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134b161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134b16700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134b16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134b171a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134b176f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134b17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134b18190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134b186e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134b18c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134b19180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134b196d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134b19c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134b1a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134b1a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134b1ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134b1b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134b1b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134b1baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134b1bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134b1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134b1c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134b1cd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134b1d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134b1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134b1db00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134b1dfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134b1e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134b1e8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134b1ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134b1f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134b1f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134b1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134b20000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134b204a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134b20940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134b20de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134b21280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134b21720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134b21bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134b22060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134b22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134b229a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134b22e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134b232e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134b23780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134b23c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134b240c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134b24560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134b24a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134b24ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134b25340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134b257e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134b25c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134b26120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134b265c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134b26a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134b26f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134b273a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134b27840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134b27ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134b28180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134b28620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134b28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134b28f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134b29400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134b298a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134b29d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134b2a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134b2a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134b2ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134b2afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134b2b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134b2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134b2bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134b2c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134b2c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134b2cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134b2d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134b2d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134b2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134b2de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134b2e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134b2e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134b2ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134b2f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134b2f520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134b2f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134b2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134b30300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134b307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134b30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134b310e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134b31580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134b31a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134b31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134b32360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134b328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134b32e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134b33350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134b338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134b33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134b34170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134b34780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134b34d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134b35580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134b35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134b35ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134b362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134b36900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134b370f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134b37590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134b37a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134b37ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134b38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134b38bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134b39120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134b39670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134b39bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134b3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134b3a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134b3abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134b3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134b3b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134b3bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134b3c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134b3c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134b3cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134b3d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134b3d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134b3db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134b3e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134b3e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134b3eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134b3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134b3f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134b3fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134b400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134b40600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134b40b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134b410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134b415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134b41b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134b42090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134b425e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134b42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134b43080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134b435d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134b43b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134b44070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134b445c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134b44b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134b45060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134b455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134b45b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134b46050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134b465a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134b46af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134b47040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134b47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134b47ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134b48030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134b48580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134b48ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134b49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134b49570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134b49ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134b4a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134b4a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134b4aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134b4b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134b4b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134b4b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134b4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134b4c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134b4c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134b4cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134b4d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134b4d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134b4d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134b4de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134b4e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134b4e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134b4ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134b4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134b4f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134b4fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134b501d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134b508f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134b51010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134b51730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134b519f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134b521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134b524a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134b52ab0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a6ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a6ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a6f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a6f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a6f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a6f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a6fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a6fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a70120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a703e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a706a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a70960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134a70f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134a71500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134a71b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134a71df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134a720b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134a72370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134a72630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134a728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134a72bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134a72e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134a73130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134a733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134a736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134a73970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134a73c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134a73ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134a741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134a74470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134a74730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134a749f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134a74cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134a74f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134a75230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134a754f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134a757b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134a75a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134a75d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134a75ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134a762b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134a76570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134a76830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134a76af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134a76db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134a77070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134a77330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134a775f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134a778b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134a77b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134a77e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134a780f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134a783b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134a78670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134a78930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134a78bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134a78eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134a79170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134a79430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134a796f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134a799b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134a79c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134a79f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134a7a1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134a7a4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134a7a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134a7aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134a7acf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134a7afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134a7b270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134a7b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134a7b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134a7bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134a7bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134a7c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134a7c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134a7c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134a7c870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134a7cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134a7cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134a7d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134a7d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134a7d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134a7d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134a7dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134a7de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134a7e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134a7e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134a7e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134a7e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134a7ec30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134a7eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134a7f1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134a7f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134a7f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134a7f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134a7fcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134a7ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134a80230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134a804f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134a807b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134a80a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134a80d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134a80ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134a812b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134a81570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134a81830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134a81af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134a81db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134a82070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134a82330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134a825f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134a828b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134a82b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134a82e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134a830f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134a833b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134a83670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134a83930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134a83bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134a83eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134a84170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134a84430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134a846f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134a849b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134a84c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134a84f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134a851f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134a854b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134a85770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134a85a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134a85cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134a85fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134a86270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134a86530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134a867f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134a86ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134a86d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134a87030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134a872f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134a875b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134a87870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134a87b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134a87df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134a880b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134a88370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134a88630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134a888f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134a88bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134a88e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134a89130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134a893f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134a896b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134a89970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134a89c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134a89ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134a8a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134a8a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134a8a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134a8a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134a8acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134a8af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134a8b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134a8b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134a8b7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134a8ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134a8bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134a8bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134a8c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134a8c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134a8c830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134a8caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134a8cdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134a8d070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134a8d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134a8d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134a8d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134a8db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134a8de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134a8e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134a8e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134a8e670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134a8e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134a8ebf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134a8eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134a8f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134a8f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134a8f6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134a8f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134a8fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134a8ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134a901f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134a904b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134a90770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134a90a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134a90cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134a90fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134a91270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134a91530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134a917f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134a91ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134a91d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134a92030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134a922f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134a925b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134a92870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134a92b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134a92df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134a930b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134a93370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134a93940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134a93c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134a93ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134a94180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134a94440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134a94700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134a949c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134a94c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134a94f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134a95200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134a954c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134a95780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134a95a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134a95d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134a95fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134a96280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134a96540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134a96800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134a96ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134a96d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134a97040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134a97590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134a97ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134a98030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134a98580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134a98ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134a99020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134a99570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134a99ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134a9a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134a9a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134a9aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134a9b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134a9b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134a9baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134a9bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134a9c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134a9ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134a9cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134a9d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134a9da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134a9dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134a9e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134a9ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134a9efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134a9f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134a9fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134a9ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134aa0500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134aa0a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134aa0fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134aa14f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134aa1a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134aa1f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134aa24e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134aa2a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134aa2f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134aa3240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134aa3500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134aa3a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134aa3f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134aa4400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134aa4900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134aa4e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134aa5300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134aa5800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134aa5d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134aa6200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134aa6700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134aa6c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134aa7100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134aa7600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134aa7b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134aa8510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134aa8c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134aa9350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134aa9a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134aa9d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134aaa520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134aaa7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134aaadf0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.822s
user	0m0.312s
sys	0m0.296s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4428 (e6e7c75d)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129f0a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129f0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129f0ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129f0b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129f0b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129f0ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129f0bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129f0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129f0c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129f0cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129f0d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129f0d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129f0e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129f0ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129f0f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129f0f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129f100b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129f107d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129f10ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129f11620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129f11d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129f12460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129f12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129f13420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129f13b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129f13e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129f140c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129f14530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129f14be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129f15050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129f154c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129f15a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129f15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129f16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129f165f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129f16ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129f17160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129f175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129f17a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129f17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129f18320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129f18790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129f18c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129f19070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129f194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129f19950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129f19dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129f1a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129f1aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129f1af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129f1b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129f1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129f1bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129f1c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129f1c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129f1cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129f1d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129f1d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129f1d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129f1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129f1e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129f1e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129f1ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129f1ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129f1f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129f1f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129f1fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129f20360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129f20860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129f20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129f21260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129f21760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129f21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129f22160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129f22710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129f22cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129f23270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129f23820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129f23dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129f24380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129f24930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129f24ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129f25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129f25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129f265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129f26b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129f27100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129f276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129f27c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129f28210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129f287c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129f28d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129f29320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129f298d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129f29e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129f2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129f1a3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129f2ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129f2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129f2b470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129f2ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129f2bfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129f2c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129f2cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129f2d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129f2d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129f2dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129f2e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129f2e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129f2ed50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129f2f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129f2f8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129f2fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129f30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129f30860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129f30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129f31260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129f31760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129f31c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129f32160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129f32660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129f32b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129f33060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129f33560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129f33a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129f33f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129f34460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129f34960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129f34e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129f35360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129f35860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129f35d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129f36260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129f36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129f36c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129f37160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129f37660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129f37b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129f38060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129f38560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129f38a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129f38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129f39460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129f39960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129f39e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129f3a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129f3a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129f3ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129f3b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129f3b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129f3bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129f3c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129f3c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129f3cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129f3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129f3d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129f3da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129f3df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129f3e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129f3e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129f3ee60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129f3f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129f3f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129f3fd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129f40260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129f40760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129f40c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129f41160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129f41660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129f41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129f42060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129f42560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129f42a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129f42f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129f43460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129f43960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129f43e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129f44360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129f44860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129f44d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129f45260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129f45760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129f45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129f46160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129f46660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129f46b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129f47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129f47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129f47a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129f47f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129f48460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129f48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129f48e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129f49410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129f499c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129f49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129f4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129f4ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129f4b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129f4b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129f4bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129f4c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129f4c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129f4ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129f4d2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129f4dab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129f4df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129f4e3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129f4e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129f4f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129f4f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129f4fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129f50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129f50580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129f50ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129f51020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129f51570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129f51ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129f52010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129f52560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129f52ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129f53000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129f53550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129f53aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129f53ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129f54540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129f54a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129f54fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129f55530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129f55a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129f55fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129f56520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129f56a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129f56fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129f57510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129f57a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129f57fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129f58500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129f58a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129f58fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129f594f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129f59a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129f59f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129f5a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129f5aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129f5af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129f5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129f5ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129f5bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129f5c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129f5ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129f5cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129f5da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129f5df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129f5e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129f5e9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129f5ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129f5f490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129f5f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129f5ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129f60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129f609d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129f60f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129f61470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129f619c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129f61e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129f62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129f627a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129f62c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129f630e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129f63580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129f63a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129f63ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129f64360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129f64800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129f64ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129f65140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129f655e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129f65a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129f65f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129f66470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129f66b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129f672b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129f679d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129f680f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129f683b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129f68ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129f68e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129f69470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.087.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.216 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x129e0abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x129e0b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x129e0b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x129e0b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x129e0bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x129e0c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x129e0c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x129e0cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x129e0cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x129e0d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x129e0d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x129e0df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x129e0ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x129e0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x129e0fa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x129e10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x129e10880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x129e10fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x129e116c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x129e11e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x129e125b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x129e12cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x129e133f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x129e13b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x129e14230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x129e144f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x129e147b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x129e14c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x129e15090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x129e15500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x129e15970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x129e15ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x129e16310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x129e165d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x129e16a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x129e16eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x129e17320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x129e17790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x129e17c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x129e18070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x129e184e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x129e18950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x129e18dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x129e19230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x129e196a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x129e19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x129e19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x129e1a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x129e1a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x129e1acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x129e1b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x129e1b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x129e1ba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x129e1be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x129e1c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x129e1c770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x129e1cce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x129e1d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x129e1d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x129e1dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x129e1df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x129e1e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x129e1e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x129e1ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x129e1f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x129e1f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x129e1f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x129e1fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x129e202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x129e20720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x129e20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x129e21000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x129e21470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x129e218e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x129e21d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x129e221c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x129e22630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x129e22aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x129e22f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x129e23380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x129e237f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x129e23c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x129e240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x129e24540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x129e249b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x129e24e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x129e25290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x129e25700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x129e25b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x129e25fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x129e26450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x129e268c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x129e26d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x129e271a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x129e27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x129e27a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x129e27ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x129e28360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x129e287d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x129e28c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x129e290b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x129e29520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x129e29990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x129e29e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x129e2a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x129e2a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x129e2ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x129e2afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x129e2b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x129e2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x129e2bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x129e2c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x129e2c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x129e2ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x129e2ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x129e2d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x129e2d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x129e2dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x129e2e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x129e2e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x129e2e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x129e2ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x129e2f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x129e2f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x129e2fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x129e2ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x129e30410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x129e30880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x129e30cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x129e31160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x129e315d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x129e31a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x129e31eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x129e32320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x129e32790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x129e32c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x129e33070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x129e334e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x129e33950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x129e33dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x129e34230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x129e346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x129e34b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x129e34f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x129e353f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x129e35860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x129e35cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x129e36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x129e365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x129e36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x129e36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x129e37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x129e37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x129e37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x129e38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x129e384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x129e38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x129e38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x129e39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x129e39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x129e39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x129e39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x129e3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x129e3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x129e3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x129e3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x129e3bd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x129e3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x129e3c2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x129e3c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x129e3cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x129e3d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x129e3d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x129e3d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x129e3dd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x129e3e1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x129e3e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x129e3eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x129e3ef30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x129e3f3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x129e3f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x129e3fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x129e400f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x129e40560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x129e409d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x129e40e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x129e412b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x129e41720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x129e41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x129e42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x129e42470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x129e428e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x129e42d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x129e431c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x129e43630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x129e43aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x129e43f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x129e44380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x129e447f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x129e44c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x129e450d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x129e45540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x129e45aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x129e45fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x129e46420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x129e46890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x129e46d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x129e47170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x129e47690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x129e47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x129e48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x129e489d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x129e48f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x129e49550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x129e49b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x129e4a0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x129e4a690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x129e4ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x129e4b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x129e4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x129e4bd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x129e4c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x129e4c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x129e4ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x129e4d490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x129e4da50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x129e4e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x129e4e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x129e4eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x129e4f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x129e4f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x129e4fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x129e50290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x129e50850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x129e50e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x129e513d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x129e51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x129e51f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x129e52510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x129e52ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x129e53090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x129e53650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x129e53c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x129e541d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x129e54790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x129e54d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x129e55310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x129e558d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x129e55e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x129e56450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x129e56a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x129e56fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x129e57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x129e57b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x129e58110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x129e586d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x129e58c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x129e59250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x129e59810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x129e59dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x129e5a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x129e5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x129e5af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x129e5b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x129e5ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x129e5c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x129e5c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x129e5cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x129e5d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x129e5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x129e5dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x129e5dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x129e5e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x129e5e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x129e5eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x129e5f3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x129e5f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x129e5fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x129e602d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x129e607d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x129e60cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x129e611d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x129e616d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x129e620e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x129e62800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x129e62f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x129e63640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x129e63900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x129e640f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x129e643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x129e649c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12a804f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12a805380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12a8057f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12a805c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12a8060d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12a806540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12a8069b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12a806e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12a807290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12a807700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12a807b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12a808210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12a808d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12a8094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12a809cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12a80a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12a80ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12a80b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12a80b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12a80c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12a80c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12a80cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12a80d6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12a80ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12a80e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12a80e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12a80ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12a80eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12a80f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12a80f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12a80fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12a810150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12a8105c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12a810880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12a810cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12a811160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12a8115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12a811a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12a811eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12a812320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12a812790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12a812c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12a813070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12a8134e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12a813950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12a813dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12a814230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12a8146a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12a814b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12a814f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12a8153f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12a815860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12a815cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12a816140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12a8165b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12a816a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12a816f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12a817490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12a817900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12a817d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12a8181e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12a818650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12a818ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12a818f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12a8193a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12a819810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12a819c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12a81a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12a81a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12a81a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12a81ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12a81b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12a81b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12a81bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12a81c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12a81c470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12a81c8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12a81cd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12a81d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12a81d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12a81daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12a81df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12a81e380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12a81e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12a81ec60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12a81f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12a81f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12a81f9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12a81fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12a820290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12a820700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12a820b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12a820fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12a821450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12a8218c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12a821d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12a8221a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12a822610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12a822a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12a822ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12a823360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12a8237d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12a823c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12a8244d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12a824790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12a824c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12a825070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12a8254e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12a825950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12a825dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12a826230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12a8266a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12a826b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12a826f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12a8273f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12a827860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12a827cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12a828140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12a8285b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12a828a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12a828e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12a829300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12a829770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12a829be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12a82a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12a82a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12a82a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12a82ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12a82b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12a82b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12a82baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12a82bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12a82c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12a82c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12a82ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12a82d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12a82d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12a82da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12a82de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12a82e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12a82e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12a82ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12a82f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12a82f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12a82f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12a82fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12a8301f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12a830660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12a830ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12a830f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12a8313b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12a831820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12a831c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12a832100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12a832570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12a8329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12a832e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12a8332c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12a833730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12a833ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12a834010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12a834480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12a8348f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12a834d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12a8351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12a835640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12a835ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12a835f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12a836390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12a836800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12a836c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12a8370e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12a837550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12a8379c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12a837e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12a8382a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12a838710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12a838b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12a838ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12a839460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12a8398d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12a839d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12a83a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12a83a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12a83aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12a83af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12a83b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12a83b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12a83bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12a83c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12a83c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12a83c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12a83ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12a83d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12a83d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12a83db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12a83dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12a83e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12a83e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12a83ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12a83f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12a83f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12a83fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12a83fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12a840350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12a8407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12a840c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12a8410a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12a841510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12a841980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12a842500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12a8427c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12a842a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12a842ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12a843360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12a8437d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12a843c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12a8440b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12a844520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12a844990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12a844e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12a845270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12a8456e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12a845b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12a845fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12a846430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12a8468a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12a846d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12a847180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12a8475f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12a847a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12a847ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12a848340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12a8487b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12a848c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12a849090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12a849500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12a849970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12a849de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12a84a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12a84a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12a84ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12a84afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12a84b410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12a84b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12a84bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12a84c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12a84c5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12a84ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12a84ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12a84d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12a84d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12a84dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12a84e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12a84e4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12a84e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12a84edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12a84f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12a84f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12a84fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12a84ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12a8503f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12a850860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12a850cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12a851140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12a8515b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12a851a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12a851e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12a852300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12a852770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12a852be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12a853050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12a8534c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12a853930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12a853da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12a854210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12a854680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12a854af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12a854f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12a8553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12a855840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12a855cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12a856120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12a856b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12a8572b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12a8579d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12a8580f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12a8583b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12a858820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12a858e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12a859430 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.895s
user	0m0.242s
sys	0m0.121s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
