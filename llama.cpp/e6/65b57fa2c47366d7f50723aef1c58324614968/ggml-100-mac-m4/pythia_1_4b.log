Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:312 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.4s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.672s
user	0m0.926s
sys	0m1.267s
++ nproc
+ make -j10
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  0%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  1%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  1%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  1%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  5%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  5%] Built target build_info
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Built target sha256
[  6%] Linking CXX shared library ../../bin/libggml-base.dylib
[  6%] Built target ggml-base
[  6%] Generate assembly for embedded Metal library
Embedding Metal library
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  8%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 11%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 12%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 13%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 13%] Built target ggml-blas
[ 13%] Built target ggml-cpu
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 14%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 16%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 17%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 18%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 20%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 20%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 26%] Linking CXX shared library ../bin/libllama.dylib
[ 26%] Built target llama-gguf-hash
[ 26%] Built target llama-gguf
[ 26%] Built target llama
[ 27%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 27%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 29%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 30%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 30%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 31%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 33%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 33%] Linking C executable ../bin/test-c
[ 33%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 35%] Linking CXX executable ../../bin/llama-quantize-stats
[ 35%] Linking CXX executable ../../bin/llama-simple
[ 35%] Built target llava
[ 36%] Linking CXX static library libcommon.a
[ 37%] Linking CXX static library libllava_static.a
[ 37%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 37%] Built target llama-simple-chat
[ 37%] Built target test-c
[ 37%] Built target llama-quantize-stats
[ 37%] Built target llama-simple
[ 37%] Built target common
[ 37%] Built target llava_static
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 37%] Built target llava_shared
[ 37%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 45%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 45%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 46%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 46%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 46%] Linking CXX executable ../bin/test-grammar-integration
[ 47%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 47%] Linking CXX executable ../bin/test-llama-grammar
[ 47%] Linking CXX executable ../bin/test-grammar-parser
[ 48%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 48%] Linking CXX executable ../bin/test-sampling
[ 49%] Linking CXX executable ../bin/test-log
[ 49%] Linking CXX executable ../bin/test-arg-parser
[ 49%] Built target test-tokenizer-1-bpe
[ 49%] Built target test-tokenizer-1-spm
[ 49%] Built target test-tokenizer-0
[ 49%] Built target test-grammar-integration
[ 49%] Built target test-grammar-parser
[ 49%] Built target test-json-schema-to-grammar
[ 49%] Built target test-llama-grammar
[ 49%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 50%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 50%] Built target test-arg-parser
[ 50%] Built target test-log
[ 51%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 56%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 57%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 57%] Linking CXX executable ../bin/test-backend-ops
[ 58%] Linking CXX executable ../bin/test-gguf
[ 58%] Linking CXX executable ../bin/test-chat-template
[ 58%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 59%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-autorelease
[ 60%] Linking CXX executable ../bin/test-model-load-cancel
[ 61%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 62%] Linking CXX executable ../bin/test-barrier
[ 63%] Linking CXX executable ../bin/test-quantize-perf
[ 63%] Linking CXX executable ../bin/test-rope
[ 64%] Linking CXX executable ../../bin/llama-batched-bench
[ 64%] Built target test-backend-ops
[ 64%] Built target test-chat-template
[ 64%] Built target test-model-load-cancel
[ 64%] Built target test-quantize-fns
[ 64%] Built target test-barrier
[ 64%] Built target test-gguf
[ 64%] Built target test-autorelease
[ 64%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 64%] Built target test-rope
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 64%] Built target test-quantize-perf
[ 66%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Built target llama-batched-bench
[ 67%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 69%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 69%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 70%] Linking CXX executable ../../bin/llama-batched
[ 71%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-embedding
[ 71%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 71%] Linking CXX executable ../../bin/llama-eval-callback
[ 71%] Linking CXX executable ../../bin/llama-gguf-split
[ 72%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 72%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 72%] Linking CXX executable ../../bin/llama-imatrix
[ 72%] Linking CXX executable ../../bin/llama-gritlm
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Linking CXX executable ../../bin/llama-lookahead
[ 73%] Built target llama-embedding
[ 73%] Built target llama-batched
[ 73%] Built target llama-gguf-split
[ 73%] Built target llama-eval-callback
[ 73%] Built target llama-gbnf-validator
[ 73%] Built target llama-gritlm
[ 73%] Built target llama-imatrix
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 74%] Built target llama-bench
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Built target llama-infill
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 77%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 77%] Built target llama-lookahead
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-create
[ 77%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 77%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup
[ 77%] Linking CXX executable ../../bin/llama-lookup-merge
[ 77%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 77%] Linking CXX executable ../../bin/llama-lookup-stats
[ 78%] Linking CXX executable ../../bin/llama-parallel
[ 78%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 81%] Linking CXX executable ../../bin/llama-cli
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Built target llama-lookup-merge
[ 82%] Built target llama-lookup
[ 83%] Linking CXX executable ../../bin/llama-retrieval
[ 83%] Generating loading.html.hpp
[ 83%] Built target llama-lookup-stats
[ 83%] Built target llama-passkey
[ 83%] Built target llama-parallel
[ 83%] Built target llama-lookup-create
[ 83%] Built target llama-quantize
[ 84%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 84%] Built target llama-cli
[ 85%] Generating index.html.gz.hpp
[ 85%] Built target llama-perplexity
[ 86%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 86%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 87%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 87%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 87%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 87%] Linking CXX executable ../../bin/llama-save-load-state
[ 88%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 88%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 88%] Built target llama-retrieval
[ 88%] Linking CXX executable ../../bin/llama-speculative
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 90%] Linking CXX executable ../../bin/llama-tokenize
[ 91%] Linking CXX executable ../../bin/llama-tts
[ 91%] Linking CXX executable ../../bin/llama-gen-docs
[ 92%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 92%] Linking CXX executable ../../bin/llama-run
[ 92%] Built target llama-save-load-state
[ 92%] Built target llama-speculative-simple
[ 92%] Built target llama-speculative
[ 92%] Built target llama-tokenize
[ 93%] Linking CXX executable ../../bin/llama-cvector-generator
[ 93%] Built target llama-tts
[ 93%] Built target llama-convert-llama2c-to-ggml
[ 93%] Built target llama-gen-docs
[ 94%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 95%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 96%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 96%] Built target llama-run
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 96%] Linking CXX executable ../../bin/llama-export-lora
[ 97%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 97%] Linking CXX executable ../../bin/llama-llava-cli
[ 97%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-cvector-generator
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-q8dot
[100%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.108s
user	0m6.251s
sys	0m9.513s

main: quantize time =  3703.11 ms
main:    total time =  3703.11 ms

main: quantize time =  1383.91 ms
main:    total time =  1383.91 ms

main: quantize time =  1320.24 ms
main:    total time =  1320.24 ms

main: quantize time =  2329.03 ms
main:    total time =  2329.03 ms

main: quantize time =  2202.07 ms
main:    total time =  2202.07 ms

main: quantize time =  5207.43 ms
main:    total time =  5207.43 ms

main: quantize time =  5669.60 ms
main:    total time =  5669.60 ms

main: quantize time =  6825.03 ms
main:    total time =  6825.03 ms

main: quantize time =  5741.91 ms
main:    total time =  5741.91 ms

main: quantize time =  4373.58 ms
main:    total time =  4373.58 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.199 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.384 I main: llama backend init
0.00.000.392 I main: load the model and apply lora adapter, if any
0.00.053.930 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.067.125 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.067.150 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.067.153 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.067.154 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.067.155 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.067.155 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.067.156 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.067.159 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.067.159 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.067.160 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.067.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.067.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.067.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.067.163 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.067.168 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.067.168 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.067.169 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.074.267 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.076.493 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.085.501 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.085.511 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.085.511 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.085.512 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.085.512 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.085.513 I llama_model_loader: - type  f32:  194 tensors
0.00.085.514 I llama_model_loader: - type  f16:   98 tensors
0.00.085.516 I print_info: file format = GGUF V3 (latest)
0.00.085.526 I print_info: file type   = all F32 (guessed)
0.00.085.527 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.118.569 I load: special tokens cache size = 25
0.00.125.890 I load: token to piece cache size = 0.2984 MB
0.00.125.893 I print_info: arch             = gptneox
0.00.125.893 I print_info: vocab_only       = 0
0.00.125.893 I print_info: n_ctx_train      = 2048
0.00.125.894 I print_info: n_embd           = 2048
0.00.125.894 I print_info: n_layer          = 24
0.00.125.898 I print_info: n_head           = 16
0.00.125.900 I print_info: n_head_kv        = 16
0.00.125.900 I print_info: n_rot            = 32
0.00.125.900 I print_info: n_swa            = 0
0.00.125.901 I print_info: n_embd_head_k    = 128
0.00.125.901 I print_info: n_embd_head_v    = 128
0.00.125.901 I print_info: n_gqa            = 1
0.00.125.906 I print_info: n_embd_k_gqa     = 2048
0.00.125.906 I print_info: n_embd_v_gqa     = 2048
0.00.125.907 I print_info: f_norm_eps       = 1.0e-05
0.00.125.907 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.125.907 I print_info: f_clamp_kqv      = 0.0e+00
0.00.125.908 I print_info: f_max_alibi_bias = 0.0e+00
0.00.125.908 I print_info: f_logit_scale    = 0.0e+00
0.00.125.910 I print_info: n_ff             = 8192
0.00.125.910 I print_info: n_expert         = 0
0.00.125.910 I print_info: n_expert_used    = 0
0.00.125.910 I print_info: causal attn      = 1
0.00.125.910 I print_info: pooling type     = 0
0.00.125.910 I print_info: rope type        = 2
0.00.125.911 I print_info: rope scaling     = linear
0.00.125.911 I print_info: freq_base_train  = 10000.0
0.00.125.911 I print_info: freq_scale_train = 1
0.00.125.911 I print_info: n_ctx_orig_yarn  = 2048
0.00.125.912 I print_info: rope_finetuned   = unknown
0.00.125.912 I print_info: ssm_d_conv       = 0
0.00.125.912 I print_info: ssm_d_inner      = 0
0.00.125.912 I print_info: ssm_d_state      = 0
0.00.125.912 I print_info: ssm_dt_rank      = 0
0.00.125.915 I print_info: ssm_dt_b_c_rms   = 0
0.00.125.915 I print_info: model type       = 1.4B
0.00.125.916 I print_info: model params     = 1.41 B
0.00.125.916 I print_info: general.name     = 1.4B
0.00.125.916 I print_info: vocab type       = BPE
0.00.125.917 I print_info: n_vocab          = 50304
0.00.125.921 I print_info: n_merges         = 50009
0.00.125.922 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.125.923 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.125.923 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.125.923 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.125.923 I print_info: LF token         = 128 'Ä'
0.00.125.924 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.125.925 I print_info: max token length = 1024
0.00.161.555 I load_tensors: offloading 24 repeating layers to GPU
0.00.161.559 I load_tensors: offloading output layer to GPU
0.00.161.559 I load_tensors: offloaded 25/25 layers to GPU
0.00.161.582 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.161.583 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.161.899 I llama_context: n_seq_max     = 1
0.00.161.900 I llama_context: n_ctx         = 2048
0.00.161.900 I llama_context: n_ctx_per_seq = 2048
0.00.161.900 I llama_context: n_batch       = 2048
0.00.161.901 I llama_context: n_ubatch      = 512
0.00.161.901 I llama_context: flash_attn    = 0
0.00.161.901 I llama_context: freq_base     = 10000.0
0.00.161.902 I llama_context: freq_scale    = 1
0.00.161.902 I ggml_metal_init: allocating
0.00.161.918 I ggml_metal_init: found device: Apple M4
0.00.161.923 I ggml_metal_init: picking default device: Apple M4
0.00.162.497 I ggml_metal_init: using embedded metal library
0.00.290.989 I ggml_metal_init: GPU name:   Apple M4
0.00.290.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.290.994 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.290.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.290.994 I ggml_metal_init: simdgroup reduction   = true
0.00.290.995 I ggml_metal_init: simdgroup matrix mul. = true
0.00.290.995 I ggml_metal_init: has residency sets    = true
0.00.290.995 I ggml_metal_init: has bfloat            = true
0.00.290.995 I ggml_metal_init: use bfloat            = true
0.00.290.996 I ggml_metal_init: hasUnifiedMemory      = true
0.00.290.997 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.359.311 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.394.898 I init:      Metal KV buffer size =   384.00 MiB
0.00.394.905 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.394.944 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.399.002 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.399.005 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.399.005 I llama_context: graph nodes  = 967
0.00.399.005 I llama_context: graph splits = 2
0.00.399.009 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.399.139 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.399.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.467.385 I main: llama threadpool init, n_threads = 4
0.00.467.431 I 
0.00.467.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.467.465 I 
0.00.467.534 I sampler seed: 1234
0.00.467.538 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.467.564 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.467.566 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.467.566 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.296.737 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55686.27 tokens per second)
0.02.296.738 I llama_perf_context_print:        load time =     412.30 ms
0.02.296.738 I llama_perf_context_print: prompt eval time =      43.89 ms /     7 tokens (    6.27 ms per token,   159.49 tokens per second)
0.02.296.739 I llama_perf_context_print:        eval time =    1782.26 ms /    63 runs   (   28.29 ms per token,    35.35 tokens per second)
0.02.296.741 I llama_perf_context_print:       total time =    1830.50 ms /    70 tokens
0.02.299.220 I ggml_metal_free: deallocating

real	0m2.605s
user	0m0.160s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.016.636 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.031.928 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.031.935 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.940 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.941 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.941 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.941 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.942 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.943 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.943 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.944 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.944 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.944 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.945 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.945 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.947 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.948 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.948 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.035.829 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.036.947 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.041.037 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.041.038 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.041.038 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.041.038 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.041.039 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.041.039 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.041.039 I llama_model_loader: - type  f32:  194 tensors
0.00.041.040 I llama_model_loader: - type q8_0:   98 tensors
0.00.041.040 I print_info: file format = GGUF V3 (latest)
0.00.041.041 I print_info: file type   = Q8_0
0.00.041.041 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.063.824 I load: special tokens cache size = 25
0.00.072.224 I load: token to piece cache size = 0.2984 MB
0.00.072.228 I print_info: arch             = gptneox
0.00.072.228 I print_info: vocab_only       = 0
0.00.072.229 I print_info: n_ctx_train      = 2048
0.00.072.229 I print_info: n_embd           = 2048
0.00.072.229 I print_info: n_layer          = 24
0.00.072.233 I print_info: n_head           = 16
0.00.072.234 I print_info: n_head_kv        = 16
0.00.072.234 I print_info: n_rot            = 32
0.00.072.235 I print_info: n_swa            = 0
0.00.072.235 I print_info: n_embd_head_k    = 128
0.00.072.238 I print_info: n_embd_head_v    = 128
0.00.072.239 I print_info: n_gqa            = 1
0.00.072.240 I print_info: n_embd_k_gqa     = 2048
0.00.072.241 I print_info: n_embd_v_gqa     = 2048
0.00.072.242 I print_info: f_norm_eps       = 1.0e-05
0.00.072.243 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.072.243 I print_info: f_clamp_kqv      = 0.0e+00
0.00.072.243 I print_info: f_max_alibi_bias = 0.0e+00
0.00.072.244 I print_info: f_logit_scale    = 0.0e+00
0.00.072.244 I print_info: n_ff             = 8192
0.00.072.245 I print_info: n_expert         = 0
0.00.072.245 I print_info: n_expert_used    = 0
0.00.072.245 I print_info: causal attn      = 1
0.00.072.246 I print_info: pooling type     = 0
0.00.072.247 I print_info: rope type        = 2
0.00.072.247 I print_info: rope scaling     = linear
0.00.072.247 I print_info: freq_base_train  = 10000.0
0.00.072.248 I print_info: freq_scale_train = 1
0.00.072.248 I print_info: n_ctx_orig_yarn  = 2048
0.00.072.248 I print_info: rope_finetuned   = unknown
0.00.072.248 I print_info: ssm_d_conv       = 0
0.00.072.249 I print_info: ssm_d_inner      = 0
0.00.072.249 I print_info: ssm_d_state      = 0
0.00.072.249 I print_info: ssm_dt_rank      = 0
0.00.072.249 I print_info: ssm_dt_b_c_rms   = 0
0.00.072.249 I print_info: model type       = 1.4B
0.00.072.250 I print_info: model params     = 1.41 B
0.00.072.250 I print_info: general.name     = 1.4B
0.00.072.250 I print_info: vocab type       = BPE
0.00.072.251 I print_info: n_vocab          = 50304
0.00.072.251 I print_info: n_merges         = 50009
0.00.072.251 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.072.251 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.072.252 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.072.252 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.072.252 I print_info: LF token         = 128 'Ä'
0.00.072.253 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.072.253 I print_info: max token length = 1024
0.01.366.414 I load_tensors: offloading 24 repeating layers to GPU
0.01.366.420 I load_tensors: offloading output layer to GPU
0.01.366.421 I load_tensors: offloaded 25/25 layers to GPU
0.01.366.448 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.366.449 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.367.362 I llama_context: n_seq_max     = 1
0.01.367.364 I llama_context: n_ctx         = 2048
0.01.367.364 I llama_context: n_ctx_per_seq = 2048
0.01.367.364 I llama_context: n_batch       = 2048
0.01.367.365 I llama_context: n_ubatch      = 512
0.01.367.365 I llama_context: flash_attn    = 0
0.01.367.365 I llama_context: freq_base     = 10000.0
0.01.367.366 I llama_context: freq_scale    = 1
0.01.367.367 I ggml_metal_init: allocating
0.01.367.429 I ggml_metal_init: found device: Apple M4
0.01.367.436 I ggml_metal_init: picking default device: Apple M4
0.01.368.502 I ggml_metal_init: using embedded metal library
0.01.372.124 I ggml_metal_init: GPU name:   Apple M4
0.01.372.126 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.372.127 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.372.128 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.372.128 I ggml_metal_init: simdgroup reduction   = true
0.01.372.128 I ggml_metal_init: simdgroup matrix mul. = true
0.01.372.128 I ggml_metal_init: has residency sets    = true
0.01.372.128 I ggml_metal_init: has bfloat            = true
0.01.372.129 I ggml_metal_init: use bfloat            = true
0.01.372.129 I ggml_metal_init: hasUnifiedMemory      = true
0.01.372.131 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.382.344 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.413.009 I init:      Metal KV buffer size =   384.00 MiB
0.01.413.014 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.413.048 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.417.531 I llama_context:      Metal compute buffer size =   102.25 MiB
0.01.417.533 I llama_context:        CPU compute buffer size =     8.01 MiB
0.01.417.533 I llama_context: graph nodes  = 967
0.01.417.533 I llama_context: graph splits = 2
0.01.417.539 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.417.663 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.417.663 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.476.401 I main: llama threadpool init, n_threads = 4
0.01.476.437 I 
0.01.476.458 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.476.458 I 
0.01.476.688 I sampler seed: 1234
0.01.476.692 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.476.711 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.476.711 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.476.711 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.585.694 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 45954.69 tokens per second)
0.02.585.695 I llama_perf_context_print:        load time =    1458.80 ms
0.02.585.696 I llama_perf_context_print: prompt eval time =      49.10 ms /     7 tokens (    7.01 ms per token,   142.57 tokens per second)
0.02.585.697 I llama_perf_context_print:        eval time =    1057.22 ms /    63 runs   (   16.78 ms per token,    59.59 tokens per second)
0.02.585.697 I llama_perf_context_print:       total time =    1110.25 ms /    70 tokens
0.02.588.854 I ggml_metal_free: deallocating

real	0m2.606s
user	0m0.120s
sys	0m0.260s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.013.619 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.030.593 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.030.598 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.603 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.030.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.604 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.030.604 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.030.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.030.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.030.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.030.607 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.030.608 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.030.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.030.608 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.030.609 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.030.611 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.030.611 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.611 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.034.770 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.035.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.040.030 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.040.032 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.040.032 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.040.033 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.040.033 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.040.033 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.040.034 I llama_model_loader: - type  f32:  194 tensors
0.00.040.034 I llama_model_loader: - type q4_0:   97 tensors
0.00.040.034 I llama_model_loader: - type q6_K:    1 tensors
0.00.040.035 I print_info: file format = GGUF V3 (latest)
0.00.040.035 I print_info: file type   = Q4_0
0.00.040.036 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.062.101 I load: special tokens cache size = 25
0.00.069.047 I load: token to piece cache size = 0.2984 MB
0.00.069.053 I print_info: arch             = gptneox
0.00.069.055 I print_info: vocab_only       = 0
0.00.069.055 I print_info: n_ctx_train      = 2048
0.00.069.056 I print_info: n_embd           = 2048
0.00.069.056 I print_info: n_layer          = 24
0.00.069.060 I print_info: n_head           = 16
0.00.069.060 I print_info: n_head_kv        = 16
0.00.069.061 I print_info: n_rot            = 32
0.00.069.061 I print_info: n_swa            = 0
0.00.069.061 I print_info: n_embd_head_k    = 128
0.00.069.061 I print_info: n_embd_head_v    = 128
0.00.069.062 I print_info: n_gqa            = 1
0.00.069.062 I print_info: n_embd_k_gqa     = 2048
0.00.069.063 I print_info: n_embd_v_gqa     = 2048
0.00.069.064 I print_info: f_norm_eps       = 1.0e-05
0.00.069.064 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.069.064 I print_info: f_clamp_kqv      = 0.0e+00
0.00.069.064 I print_info: f_max_alibi_bias = 0.0e+00
0.00.069.065 I print_info: f_logit_scale    = 0.0e+00
0.00.069.065 I print_info: n_ff             = 8192
0.00.069.066 I print_info: n_expert         = 0
0.00.069.066 I print_info: n_expert_used    = 0
0.00.069.066 I print_info: causal attn      = 1
0.00.069.066 I print_info: pooling type     = 0
0.00.069.066 I print_info: rope type        = 2
0.00.069.066 I print_info: rope scaling     = linear
0.00.069.067 I print_info: freq_base_train  = 10000.0
0.00.069.067 I print_info: freq_scale_train = 1
0.00.069.067 I print_info: n_ctx_orig_yarn  = 2048
0.00.069.067 I print_info: rope_finetuned   = unknown
0.00.069.067 I print_info: ssm_d_conv       = 0
0.00.069.068 I print_info: ssm_d_inner      = 0
0.00.069.068 I print_info: ssm_d_state      = 0
0.00.069.068 I print_info: ssm_dt_rank      = 0
0.00.069.068 I print_info: ssm_dt_b_c_rms   = 0
0.00.069.068 I print_info: model type       = 1.4B
0.00.069.069 I print_info: model params     = 1.41 B
0.00.069.069 I print_info: general.name     = 1.4B
0.00.069.069 I print_info: vocab type       = BPE
0.00.069.069 I print_info: n_vocab          = 50304
0.00.069.069 I print_info: n_merges         = 50009
0.00.069.070 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.069.070 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.069.070 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.069.070 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.069.071 I print_info: LF token         = 128 'Ä'
0.00.069.071 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.069.071 I print_info: max token length = 1024
0.00.626.941 I load_tensors: offloading 24 repeating layers to GPU
0.00.626.957 I load_tensors: offloading output layer to GPU
0.00.626.958 I load_tensors: offloaded 25/25 layers to GPU
0.00.626.996 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.626.997 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.628.385 I llama_context: n_seq_max     = 1
0.00.628.395 I llama_context: n_ctx         = 2048
0.00.628.395 I llama_context: n_ctx_per_seq = 2048
0.00.628.396 I llama_context: n_batch       = 2048
0.00.628.396 I llama_context: n_ubatch      = 512
0.00.628.397 I llama_context: flash_attn    = 0
0.00.628.399 I llama_context: freq_base     = 10000.0
0.00.628.399 I llama_context: freq_scale    = 1
0.00.628.405 I ggml_metal_init: allocating
0.00.628.539 I ggml_metal_init: found device: Apple M4
0.00.628.555 I ggml_metal_init: picking default device: Apple M4
0.00.630.468 I ggml_metal_init: using embedded metal library
0.00.635.878 I ggml_metal_init: GPU name:   Apple M4
0.00.635.887 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.635.888 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.635.889 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.635.890 I ggml_metal_init: simdgroup reduction   = true
0.00.635.890 I ggml_metal_init: simdgroup matrix mul. = true
0.00.635.890 I ggml_metal_init: has residency sets    = true
0.00.635.890 I ggml_metal_init: has bfloat            = true
0.00.635.891 I ggml_metal_init: use bfloat            = true
0.00.635.892 I ggml_metal_init: hasUnifiedMemory      = true
0.00.635.896 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.655.508 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.711.540 I init:      Metal KV buffer size =   384.00 MiB
0.00.711.548 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.711.626 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.716.049 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.716.051 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.716.051 I llama_context: graph nodes  = 967
0.00.716.051 I llama_context: graph splits = 2
0.00.716.058 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.716.186 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.716.186 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.771.825 I main: llama threadpool init, n_threads = 4
0.00.771.868 I 
0.00.771.891 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.771.893 I 
0.00.772.118 I sampler seed: 1234
0.00.772.123 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.772.161 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.772.163 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.772.163 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.459.621 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50786.84 tokens per second)
0.01.459.621 I llama_perf_context_print:        load time =     757.32 ms
0.01.459.622 I llama_perf_context_print: prompt eval time =      46.74 ms /     7 tokens (    6.68 ms per token,   149.76 tokens per second)
0.01.459.623 I llama_perf_context_print:        eval time =     637.73 ms /    63 runs   (   10.12 ms per token,    98.79 tokens per second)
0.01.459.623 I llama_perf_context_print:       total time =     688.68 ms /    70 tokens
0.01.463.604 I ggml_metal_free: deallocating

real	0m1.484s
user	0m0.126s
sys	0m0.184s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.008.617 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.515 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.516 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.516 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.516 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.517 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.517 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.518 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.518 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.518 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.519 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.519 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.519 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.520 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.524 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.524 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.524 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.341 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.428 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.259 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.261 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.261 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.262 I llama_model_loader: - type  f32:  194 tensors
0.00.034.263 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.034.263 I print_info: file format = GGUF V3 (latest)
0.00.034.264 I print_info: file type   = Q4_1
0.00.034.264 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.055.615 I load: special tokens cache size = 25
0.00.062.029 I load: token to piece cache size = 0.2984 MB
0.00.062.032 I print_info: arch             = gptneox
0.00.062.032 I print_info: vocab_only       = 0
0.00.062.032 I print_info: n_ctx_train      = 2048
0.00.062.032 I print_info: n_embd           = 2048
0.00.062.033 I print_info: n_layer          = 24
0.00.062.035 I print_info: n_head           = 16
0.00.062.036 I print_info: n_head_kv        = 16
0.00.062.036 I print_info: n_rot            = 32
0.00.062.037 I print_info: n_swa            = 0
0.00.062.037 I print_info: n_embd_head_k    = 128
0.00.062.037 I print_info: n_embd_head_v    = 128
0.00.062.037 I print_info: n_gqa            = 1
0.00.062.038 I print_info: n_embd_k_gqa     = 2048
0.00.062.039 I print_info: n_embd_v_gqa     = 2048
0.00.062.039 I print_info: f_norm_eps       = 1.0e-05
0.00.062.040 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.062.040 I print_info: f_clamp_kqv      = 0.0e+00
0.00.062.040 I print_info: f_max_alibi_bias = 0.0e+00
0.00.062.040 I print_info: f_logit_scale    = 0.0e+00
0.00.062.041 I print_info: n_ff             = 8192
0.00.062.041 I print_info: n_expert         = 0
0.00.062.041 I print_info: n_expert_used    = 0
0.00.062.041 I print_info: causal attn      = 1
0.00.062.041 I print_info: pooling type     = 0
0.00.062.043 I print_info: rope type        = 2
0.00.062.044 I print_info: rope scaling     = linear
0.00.062.045 I print_info: freq_base_train  = 10000.0
0.00.062.045 I print_info: freq_scale_train = 1
0.00.062.045 I print_info: n_ctx_orig_yarn  = 2048
0.00.062.045 I print_info: rope_finetuned   = unknown
0.00.062.046 I print_info: ssm_d_conv       = 0
0.00.062.046 I print_info: ssm_d_inner      = 0
0.00.062.046 I print_info: ssm_d_state      = 0
0.00.062.046 I print_info: ssm_dt_rank      = 0
0.00.062.046 I print_info: ssm_dt_b_c_rms   = 0
0.00.062.046 I print_info: model type       = 1.4B
0.00.062.047 I print_info: model params     = 1.41 B
0.00.062.047 I print_info: general.name     = 1.4B
0.00.062.047 I print_info: vocab type       = BPE
0.00.062.047 I print_info: n_vocab          = 50304
0.00.062.047 I print_info: n_merges         = 50009
0.00.062.049 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.062.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.062.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.062.049 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.062.049 I print_info: LF token         = 128 'Ä'
0.00.062.050 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.062.050 I print_info: max token length = 1024
0.00.649.080 I load_tensors: offloading 24 repeating layers to GPU
0.00.649.095 I load_tensors: offloading output layer to GPU
0.00.649.096 I load_tensors: offloaded 25/25 layers to GPU
0.00.649.130 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.649.131 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.650.660 I llama_context: n_seq_max     = 1
0.00.650.666 I llama_context: n_ctx         = 2048
0.00.650.666 I llama_context: n_ctx_per_seq = 2048
0.00.650.667 I llama_context: n_batch       = 2048
0.00.650.667 I llama_context: n_ubatch      = 512
0.00.650.668 I llama_context: flash_attn    = 0
0.00.650.669 I llama_context: freq_base     = 10000.0
0.00.650.670 I llama_context: freq_scale    = 1
0.00.650.677 I ggml_metal_init: allocating
0.00.650.773 I ggml_metal_init: found device: Apple M4
0.00.650.787 I ggml_metal_init: picking default device: Apple M4
0.00.652.539 I ggml_metal_init: using embedded metal library
0.00.659.154 I ggml_metal_init: GPU name:   Apple M4
0.00.659.159 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.659.160 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.659.161 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.659.161 I ggml_metal_init: simdgroup reduction   = true
0.00.659.161 I ggml_metal_init: simdgroup matrix mul. = true
0.00.659.162 I ggml_metal_init: has residency sets    = true
0.00.659.162 I ggml_metal_init: has bfloat            = true
0.00.659.162 I ggml_metal_init: use bfloat            = true
0.00.659.163 I ggml_metal_init: hasUnifiedMemory      = true
0.00.659.165 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.676.794 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.735.757 I init:      Metal KV buffer size =   384.00 MiB
0.00.735.764 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.735.812 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.740.577 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.740.579 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.740.579 I llama_context: graph nodes  = 967
0.00.740.580 I llama_context: graph splits = 2
0.00.740.586 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.740.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.740.712 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.794.520 I main: llama threadpool init, n_threads = 4
0.00.794.563 I 
0.00.794.590 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.794.591 I 
0.00.794.821 I sampler seed: 1234
0.00.794.826 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.794.849 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.794.851 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.794.851 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.523.943 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57723.58 tokens per second)
0.01.523.944 I llama_perf_context_print:        load time =     785.02 ms
0.01.523.944 I llama_perf_context_print: prompt eval time =      43.46 ms /     7 tokens (    6.21 ms per token,   161.08 tokens per second)
0.01.523.945 I llama_perf_context_print:        eval time =     682.86 ms /    63 runs   (   10.84 ms per token,    92.26 tokens per second)
0.01.523.947 I llama_perf_context_print:       total time =     730.31 ms /    70 tokens
0.01.528.017 I ggml_metal_free: deallocating

real	0m1.544s
user	0m0.123s
sys	0m0.198s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.009.821 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.706 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.028.710 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.712 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.712 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.712 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.713 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.714 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.715 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.718 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.718 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.590 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.681 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.495 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.496 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.496 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.497 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.497 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.497 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.037.498 I llama_model_loader: - type  f32:  194 tensors
0.00.037.498 I llama_model_loader: - type q5_0:   97 tensors
0.00.037.498 I llama_model_loader: - type q6_K:    1 tensors
0.00.037.499 I print_info: file format = GGUF V3 (latest)
0.00.037.499 I print_info: file type   = Q5_0
0.00.037.500 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.058.501 I load: special tokens cache size = 25
0.00.064.859 I load: token to piece cache size = 0.2984 MB
0.00.064.863 I print_info: arch             = gptneox
0.00.064.863 I print_info: vocab_only       = 0
0.00.064.863 I print_info: n_ctx_train      = 2048
0.00.064.863 I print_info: n_embd           = 2048
0.00.064.863 I print_info: n_layer          = 24
0.00.064.866 I print_info: n_head           = 16
0.00.064.867 I print_info: n_head_kv        = 16
0.00.064.868 I print_info: n_rot            = 32
0.00.064.869 I print_info: n_swa            = 0
0.00.064.869 I print_info: n_embd_head_k    = 128
0.00.064.869 I print_info: n_embd_head_v    = 128
0.00.064.870 I print_info: n_gqa            = 1
0.00.064.870 I print_info: n_embd_k_gqa     = 2048
0.00.064.871 I print_info: n_embd_v_gqa     = 2048
0.00.064.872 I print_info: f_norm_eps       = 1.0e-05
0.00.064.872 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.064.875 I print_info: f_clamp_kqv      = 0.0e+00
0.00.064.875 I print_info: f_max_alibi_bias = 0.0e+00
0.00.064.875 I print_info: f_logit_scale    = 0.0e+00
0.00.064.876 I print_info: n_ff             = 8192
0.00.064.876 I print_info: n_expert         = 0
0.00.064.876 I print_info: n_expert_used    = 0
0.00.064.876 I print_info: causal attn      = 1
0.00.064.876 I print_info: pooling type     = 0
0.00.064.878 I print_info: rope type        = 2
0.00.064.880 I print_info: rope scaling     = linear
0.00.064.880 I print_info: freq_base_train  = 10000.0
0.00.064.881 I print_info: freq_scale_train = 1
0.00.064.881 I print_info: n_ctx_orig_yarn  = 2048
0.00.064.881 I print_info: rope_finetuned   = unknown
0.00.064.881 I print_info: ssm_d_conv       = 0
0.00.064.881 I print_info: ssm_d_inner      = 0
0.00.064.885 I print_info: ssm_d_state      = 0
0.00.064.885 I print_info: ssm_dt_rank      = 0
0.00.064.885 I print_info: ssm_dt_b_c_rms   = 0
0.00.064.885 I print_info: model type       = 1.4B
0.00.064.886 I print_info: model params     = 1.41 B
0.00.064.886 I print_info: general.name     = 1.4B
0.00.064.887 I print_info: vocab type       = BPE
0.00.064.887 I print_info: n_vocab          = 50304
0.00.064.887 I print_info: n_merges         = 50009
0.00.064.887 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.064.887 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.064.887 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.064.888 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.064.888 I print_info: LF token         = 128 'Ä'
0.00.064.888 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.064.888 I print_info: max token length = 1024
0.00.731.874 I load_tensors: offloading 24 repeating layers to GPU
0.00.731.889 I load_tensors: offloading output layer to GPU
0.00.731.890 I load_tensors: offloaded 25/25 layers to GPU
0.00.731.925 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.731.931 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.733.357 I llama_context: n_seq_max     = 1
0.00.733.363 I llama_context: n_ctx         = 2048
0.00.733.363 I llama_context: n_ctx_per_seq = 2048
0.00.733.364 I llama_context: n_batch       = 2048
0.00.733.364 I llama_context: n_ubatch      = 512
0.00.733.364 I llama_context: flash_attn    = 0
0.00.733.366 I llama_context: freq_base     = 10000.0
0.00.733.367 I llama_context: freq_scale    = 1
0.00.733.373 I ggml_metal_init: allocating
0.00.733.461 I ggml_metal_init: found device: Apple M4
0.00.733.475 I ggml_metal_init: picking default device: Apple M4
0.00.735.281 I ggml_metal_init: using embedded metal library
0.00.741.832 I ggml_metal_init: GPU name:   Apple M4
0.00.741.836 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.741.837 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.741.837 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.741.838 I ggml_metal_init: simdgroup reduction   = true
0.00.741.838 I ggml_metal_init: simdgroup matrix mul. = true
0.00.741.839 I ggml_metal_init: has residency sets    = true
0.00.741.839 I ggml_metal_init: has bfloat            = true
0.00.741.839 I ggml_metal_init: use bfloat            = true
0.00.741.840 I ggml_metal_init: hasUnifiedMemory      = true
0.00.741.841 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.760.050 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.814.796 I init:      Metal KV buffer size =   384.00 MiB
0.00.814.802 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.814.846 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.819.580 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.819.582 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.819.582 I llama_context: graph nodes  = 967
0.00.819.583 I llama_context: graph splits = 2
0.00.819.590 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.819.721 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.819.722 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.879.389 I main: llama threadpool init, n_threads = 4
0.00.879.430 I 
0.00.879.453 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.879.453 I 
0.00.879.678 I sampler seed: 1234
0.00.879.683 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.879.703 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.879.703 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.879.703 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.670.480 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52321.30 tokens per second)
0.01.670.481 I llama_perf_context_print:        load time =     868.66 ms
0.01.670.482 I llama_perf_context_print: prompt eval time =      47.38 ms /     7 tokens (    6.77 ms per token,   147.76 tokens per second)
0.01.670.484 I llama_perf_context_print:        eval time =     740.48 ms /    63 runs   (   11.75 ms per token,    85.08 tokens per second)
0.01.670.484 I llama_perf_context_print:       total time =     791.99 ms /    70 tokens
0.01.674.581 I ggml_metal_free: deallocating

real	0m1.692s
user	0m0.124s
sys	0m0.221s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.008.972 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.513 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.517 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.519 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.520 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.520 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.520 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.521 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.522 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.522 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.522 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.523 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.523 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.523 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.524 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.525 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.526 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.526 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.348 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.081 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.082 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.083 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.084 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.084 I llama_model_loader: - type  f32:  194 tensors
0.00.025.085 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.085 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.086 I print_info: file format = GGUF V3 (latest)
0.00.025.086 I print_info: file type   = Q5_1
0.00.025.087 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.043.957 I load: special tokens cache size = 25
0.00.050.146 I load: token to piece cache size = 0.2984 MB
0.00.050.149 I print_info: arch             = gptneox
0.00.050.150 I print_info: vocab_only       = 0
0.00.050.150 I print_info: n_ctx_train      = 2048
0.00.050.150 I print_info: n_embd           = 2048
0.00.050.150 I print_info: n_layer          = 24
0.00.050.153 I print_info: n_head           = 16
0.00.050.154 I print_info: n_head_kv        = 16
0.00.050.154 I print_info: n_rot            = 32
0.00.050.154 I print_info: n_swa            = 0
0.00.050.155 I print_info: n_embd_head_k    = 128
0.00.050.155 I print_info: n_embd_head_v    = 128
0.00.050.156 I print_info: n_gqa            = 1
0.00.050.156 I print_info: n_embd_k_gqa     = 2048
0.00.050.157 I print_info: n_embd_v_gqa     = 2048
0.00.050.158 I print_info: f_norm_eps       = 1.0e-05
0.00.050.158 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.158 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.158 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.159 I print_info: f_logit_scale    = 0.0e+00
0.00.050.159 I print_info: n_ff             = 8192
0.00.050.159 I print_info: n_expert         = 0
0.00.050.160 I print_info: n_expert_used    = 0
0.00.050.160 I print_info: causal attn      = 1
0.00.050.160 I print_info: pooling type     = 0
0.00.050.160 I print_info: rope type        = 2
0.00.050.160 I print_info: rope scaling     = linear
0.00.050.161 I print_info: freq_base_train  = 10000.0
0.00.050.161 I print_info: freq_scale_train = 1
0.00.050.161 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.161 I print_info: rope_finetuned   = unknown
0.00.050.161 I print_info: ssm_d_conv       = 0
0.00.050.162 I print_info: ssm_d_inner      = 0
0.00.050.164 I print_info: ssm_d_state      = 0
0.00.050.164 I print_info: ssm_dt_rank      = 0
0.00.050.164 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.165 I print_info: model type       = 1.4B
0.00.050.165 I print_info: model params     = 1.41 B
0.00.050.165 I print_info: general.name     = 1.4B
0.00.050.166 I print_info: vocab type       = BPE
0.00.050.166 I print_info: n_vocab          = 50304
0.00.050.166 I print_info: n_merges         = 50009
0.00.050.166 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.167 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.167 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.167 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.167 I print_info: LF token         = 128 'Ä'
0.00.050.168 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.168 I print_info: max token length = 1024
0.00.621.028 I load_tensors: offloading 24 repeating layers to GPU
0.00.621.032 I load_tensors: offloading output layer to GPU
0.00.621.033 I load_tensors: offloaded 25/25 layers to GPU
0.00.621.060 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.621.063 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.622.554 I llama_context: n_seq_max     = 1
0.00.622.562 I llama_context: n_ctx         = 2048
0.00.622.562 I llama_context: n_ctx_per_seq = 2048
0.00.622.563 I llama_context: n_batch       = 2048
0.00.622.564 I llama_context: n_ubatch      = 512
0.00.622.565 I llama_context: flash_attn    = 0
0.00.622.566 I llama_context: freq_base     = 10000.0
0.00.622.566 I llama_context: freq_scale    = 1
0.00.622.568 I ggml_metal_init: allocating
0.00.622.633 I ggml_metal_init: found device: Apple M4
0.00.622.645 I ggml_metal_init: picking default device: Apple M4
0.00.624.183 I ggml_metal_init: using embedded metal library
0.00.630.229 I ggml_metal_init: GPU name:   Apple M4
0.00.630.233 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.234 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.234 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.235 I ggml_metal_init: simdgroup reduction   = true
0.00.630.235 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.235 I ggml_metal_init: has residency sets    = true
0.00.630.236 I ggml_metal_init: has bfloat            = true
0.00.630.236 I ggml_metal_init: use bfloat            = true
0.00.630.237 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.238 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.646.928 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.699.720 I init:      Metal KV buffer size =   384.00 MiB
0.00.699.728 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.699.762 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.703.901 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.703.904 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.703.904 I llama_context: graph nodes  = 967
0.00.703.904 I llama_context: graph splits = 2
0.00.703.909 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.704.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.704.039 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.760.120 I main: llama threadpool init, n_threads = 4
0.00.760.170 I 
0.00.760.193 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.194 I 
0.00.760.432 I sampler seed: 1234
0.00.760.436 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.447 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.447 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.448 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.596.391 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52052.79 tokens per second)
0.01.596.393 I llama_perf_context_print:        load time =     750.24 ms
0.01.596.394 I llama_perf_context_print: prompt eval time =      41.95 ms /     7 tokens (    5.99 ms per token,   166.85 tokens per second)
0.01.596.394 I llama_perf_context_print:        eval time =     791.04 ms /    63 runs   (   12.56 ms per token,    79.64 tokens per second)
0.01.596.395 I llama_perf_context_print:       total time =     837.18 ms /    70 tokens
0.01.600.373 I ggml_metal_free: deallocating

real	0m1.619s
user	0m0.120s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.039 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.009.732 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.251 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.257 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.258 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.259 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.259 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.260 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.260 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.261 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.262 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.263 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.264 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.264 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.264 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.265 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.266 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.267 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.267 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.043 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.087 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.906 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.907 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.908 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.909 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.909 I llama_model_loader: - type  f32:  194 tensors
0.00.024.909 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.910 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.910 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.910 I print_info: file format = GGUF V3 (latest)
0.00.024.911 I print_info: file type   = Q2_K - Medium
0.00.024.912 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.043.762 I load: special tokens cache size = 25
0.00.049.738 I load: token to piece cache size = 0.2984 MB
0.00.049.742 I print_info: arch             = gptneox
0.00.049.742 I print_info: vocab_only       = 0
0.00.049.742 I print_info: n_ctx_train      = 2048
0.00.049.742 I print_info: n_embd           = 2048
0.00.049.742 I print_info: n_layer          = 24
0.00.049.745 I print_info: n_head           = 16
0.00.049.746 I print_info: n_head_kv        = 16
0.00.049.746 I print_info: n_rot            = 32
0.00.049.746 I print_info: n_swa            = 0
0.00.049.747 I print_info: n_embd_head_k    = 128
0.00.049.747 I print_info: n_embd_head_v    = 128
0.00.049.748 I print_info: n_gqa            = 1
0.00.049.749 I print_info: n_embd_k_gqa     = 2048
0.00.049.752 I print_info: n_embd_v_gqa     = 2048
0.00.049.752 I print_info: f_norm_eps       = 1.0e-05
0.00.049.753 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.753 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.753 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.753 I print_info: f_logit_scale    = 0.0e+00
0.00.049.754 I print_info: n_ff             = 8192
0.00.049.754 I print_info: n_expert         = 0
0.00.049.754 I print_info: n_expert_used    = 0
0.00.049.754 I print_info: causal attn      = 1
0.00.049.754 I print_info: pooling type     = 0
0.00.049.755 I print_info: rope type        = 2
0.00.049.755 I print_info: rope scaling     = linear
0.00.049.755 I print_info: freq_base_train  = 10000.0
0.00.049.756 I print_info: freq_scale_train = 1
0.00.049.756 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.756 I print_info: rope_finetuned   = unknown
0.00.049.756 I print_info: ssm_d_conv       = 0
0.00.049.756 I print_info: ssm_d_inner      = 0
0.00.049.756 I print_info: ssm_d_state      = 0
0.00.049.756 I print_info: ssm_dt_rank      = 0
0.00.049.757 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.757 I print_info: model type       = 1.4B
0.00.049.757 I print_info: model params     = 1.41 B
0.00.049.757 I print_info: general.name     = 1.4B
0.00.049.758 I print_info: vocab type       = BPE
0.00.049.760 I print_info: n_vocab          = 50304
0.00.049.760 I print_info: n_merges         = 50009
0.00.049.760 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.760 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.760 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.760 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.761 I print_info: LF token         = 128 'Ä'
0.00.049.762 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.762 I print_info: max token length = 1024
0.00.353.946 I load_tensors: offloading 24 repeating layers to GPU
0.00.353.962 I load_tensors: offloading output layer to GPU
0.00.353.963 I load_tensors: offloaded 25/25 layers to GPU
0.00.353.997 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.353.998 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.355.595 I llama_context: n_seq_max     = 1
0.00.355.602 I llama_context: n_ctx         = 2048
0.00.355.602 I llama_context: n_ctx_per_seq = 2048
0.00.355.603 I llama_context: n_batch       = 2048
0.00.355.603 I llama_context: n_ubatch      = 512
0.00.355.603 I llama_context: flash_attn    = 0
0.00.355.606 I llama_context: freq_base     = 10000.0
0.00.355.606 I llama_context: freq_scale    = 1
0.00.355.609 I ggml_metal_init: allocating
0.00.355.708 I ggml_metal_init: found device: Apple M4
0.00.355.722 I ggml_metal_init: picking default device: Apple M4
0.00.357.517 I ggml_metal_init: using embedded metal library
0.00.363.187 I ggml_metal_init: GPU name:   Apple M4
0.00.363.202 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.363.203 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.363.203 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.363.204 I ggml_metal_init: simdgroup reduction   = true
0.00.363.204 I ggml_metal_init: simdgroup matrix mul. = true
0.00.363.204 I ggml_metal_init: has residency sets    = true
0.00.363.204 I ggml_metal_init: has bfloat            = true
0.00.363.205 I ggml_metal_init: use bfloat            = true
0.00.363.207 I ggml_metal_init: hasUnifiedMemory      = true
0.00.363.212 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.385.106 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.444.101 I init:      Metal KV buffer size =   384.00 MiB
0.00.444.111 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.444.147 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.448.706 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.448.708 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.448.709 I llama_context: graph nodes  = 967
0.00.448.709 I llama_context: graph splits = 2
0.00.448.716 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.448.844 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.448.845 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.505.719 I main: llama threadpool init, n_threads = 4
0.00.505.764 I 
0.00.505.790 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.505.790 I 
0.00.506.006 I sampler seed: 1234
0.00.506.011 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.506.031 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.506.031 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.506.031 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.176.578 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52514.79 tokens per second)
0.01.176.578 I llama_perf_context_print:        load time =     495.01 ms
0.01.176.579 I llama_perf_context_print: prompt eval time =      35.44 ms /     7 tokens (    5.06 ms per token,   197.51 tokens per second)
0.01.176.580 I llama_perf_context_print:        eval time =     632.28 ms /    63 runs   (   10.04 ms per token,    99.64 tokens per second)
0.01.176.580 I llama_perf_context_print:       total time =     671.84 ms /    70 tokens
0.01.180.469 I ggml_metal_free: deallocating

real	0m1.200s
user	0m0.124s
sys	0m0.172s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.907 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.665 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.670 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.672 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.673 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.673 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.673 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.674 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.675 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.675 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.676 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.679 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.680 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.680 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.440 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.513 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.272 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.274 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.274 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.275 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.275 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.275 I llama_model_loader: - type  f32:  194 tensors
0.00.025.276 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.276 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.276 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.276 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.277 I print_info: file format = GGUF V3 (latest)
0.00.025.282 I print_info: file type   = Q3_K - Medium
0.00.025.283 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.044.254 I load: special tokens cache size = 25
0.00.050.303 I load: token to piece cache size = 0.2984 MB
0.00.050.306 I print_info: arch             = gptneox
0.00.050.306 I print_info: vocab_only       = 0
0.00.050.306 I print_info: n_ctx_train      = 2048
0.00.050.307 I print_info: n_embd           = 2048
0.00.050.307 I print_info: n_layer          = 24
0.00.050.310 I print_info: n_head           = 16
0.00.050.310 I print_info: n_head_kv        = 16
0.00.050.311 I print_info: n_rot            = 32
0.00.050.311 I print_info: n_swa            = 0
0.00.050.311 I print_info: n_embd_head_k    = 128
0.00.050.311 I print_info: n_embd_head_v    = 128
0.00.050.312 I print_info: n_gqa            = 1
0.00.050.313 I print_info: n_embd_k_gqa     = 2048
0.00.050.313 I print_info: n_embd_v_gqa     = 2048
0.00.050.314 I print_info: f_norm_eps       = 1.0e-05
0.00.050.316 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.317 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.318 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.318 I print_info: f_logit_scale    = 0.0e+00
0.00.050.318 I print_info: n_ff             = 8192
0.00.050.319 I print_info: n_expert         = 0
0.00.050.320 I print_info: n_expert_used    = 0
0.00.050.320 I print_info: causal attn      = 1
0.00.050.320 I print_info: pooling type     = 0
0.00.050.320 I print_info: rope type        = 2
0.00.050.321 I print_info: rope scaling     = linear
0.00.050.321 I print_info: freq_base_train  = 10000.0
0.00.050.321 I print_info: freq_scale_train = 1
0.00.050.322 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.322 I print_info: rope_finetuned   = unknown
0.00.050.322 I print_info: ssm_d_conv       = 0
0.00.050.322 I print_info: ssm_d_inner      = 0
0.00.050.322 I print_info: ssm_d_state      = 0
0.00.050.322 I print_info: ssm_dt_rank      = 0
0.00.050.322 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.323 I print_info: model type       = 1.4B
0.00.050.324 I print_info: model params     = 1.41 B
0.00.050.324 I print_info: general.name     = 1.4B
0.00.050.325 I print_info: vocab type       = BPE
0.00.050.325 I print_info: n_vocab          = 50304
0.00.050.326 I print_info: n_merges         = 50009
0.00.050.327 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.327 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.327 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.327 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.327 I print_info: LF token         = 128 'Ä'
0.00.050.328 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.328 I print_info: max token length = 1024
0.00.453.510 I load_tensors: offloading 24 repeating layers to GPU
0.00.453.523 I load_tensors: offloading output layer to GPU
0.00.453.524 I load_tensors: offloaded 25/25 layers to GPU
0.00.453.559 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.453.564 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.455.073 I llama_context: n_seq_max     = 1
0.00.455.076 I llama_context: n_ctx         = 2048
0.00.455.077 I llama_context: n_ctx_per_seq = 2048
0.00.455.077 I llama_context: n_batch       = 2048
0.00.455.078 I llama_context: n_ubatch      = 512
0.00.455.078 I llama_context: flash_attn    = 0
0.00.455.080 I llama_context: freq_base     = 10000.0
0.00.455.081 I llama_context: freq_scale    = 1
0.00.455.083 I ggml_metal_init: allocating
0.00.455.145 I ggml_metal_init: found device: Apple M4
0.00.455.159 I ggml_metal_init: picking default device: Apple M4
0.00.456.955 I ggml_metal_init: using embedded metal library
0.00.462.541 I ggml_metal_init: GPU name:   Apple M4
0.00.462.561 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.462.562 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.462.563 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.462.563 I ggml_metal_init: simdgroup reduction   = true
0.00.462.564 I ggml_metal_init: simdgroup matrix mul. = true
0.00.462.564 I ggml_metal_init: has residency sets    = true
0.00.462.564 I ggml_metal_init: has bfloat            = true
0.00.462.565 I ggml_metal_init: use bfloat            = true
0.00.462.569 I ggml_metal_init: hasUnifiedMemory      = true
0.00.462.574 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.482.794 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.541.393 I init:      Metal KV buffer size =   384.00 MiB
0.00.541.399 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.541.437 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.545.987 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.545.989 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.545.990 I llama_context: graph nodes  = 967
0.00.545.990 I llama_context: graph splits = 2
0.00.545.995 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.546.125 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.546.126 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.303 I main: llama threadpool init, n_threads = 4
0.00.602.345 I 
0.00.602.373 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.373 I 
0.00.602.600 I sampler seed: 1234
0.00.602.604 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.602.615 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.602.615 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.602.615 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.343.861 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50641.94 tokens per second)
0.01.343.862 I llama_perf_context_print:        load time =     592.51 ms
0.01.343.863 I llama_perf_context_print: prompt eval time =      47.65 ms /     7 tokens (    6.81 ms per token,   146.90 tokens per second)
0.01.343.864 I llama_perf_context_print:        eval time =     690.77 ms /    63 runs   (   10.96 ms per token,    91.20 tokens per second)
0.01.343.864 I llama_perf_context_print:       total time =     742.44 ms /    70 tokens
0.01.347.548 I ggml_metal_free: deallocating

real	0m1.363s
user	0m0.122s
sys	0m0.185s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.041 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.079 I main: load the model and apply lora adapter, if any
0.00.008.684 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.276 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.287 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.288 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.289 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.289 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.289 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.290 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.291 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.291 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.292 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.292 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.292 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.292 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.293 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.294 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.295 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.295 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.235 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.356 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.279 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.280 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.280 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.281 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.281 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.281 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.282 I llama_model_loader: - type  f32:  194 tensors
0.00.025.282 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.282 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.282 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.283 I print_info: file format = GGUF V3 (latest)
0.00.025.284 I print_info: file type   = Q4_K - Medium
0.00.025.284 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.033 I load: special tokens cache size = 25
0.00.051.182 I load: token to piece cache size = 0.2984 MB
0.00.051.186 I print_info: arch             = gptneox
0.00.051.186 I print_info: vocab_only       = 0
0.00.051.186 I print_info: n_ctx_train      = 2048
0.00.051.186 I print_info: n_embd           = 2048
0.00.051.187 I print_info: n_layer          = 24
0.00.051.190 I print_info: n_head           = 16
0.00.051.190 I print_info: n_head_kv        = 16
0.00.051.191 I print_info: n_rot            = 32
0.00.051.191 I print_info: n_swa            = 0
0.00.051.191 I print_info: n_embd_head_k    = 128
0.00.051.191 I print_info: n_embd_head_v    = 128
0.00.051.194 I print_info: n_gqa            = 1
0.00.051.195 I print_info: n_embd_k_gqa     = 2048
0.00.051.195 I print_info: n_embd_v_gqa     = 2048
0.00.051.196 I print_info: f_norm_eps       = 1.0e-05
0.00.051.196 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.197 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.197 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.197 I print_info: f_logit_scale    = 0.0e+00
0.00.051.198 I print_info: n_ff             = 8192
0.00.051.198 I print_info: n_expert         = 0
0.00.051.198 I print_info: n_expert_used    = 0
0.00.051.198 I print_info: causal attn      = 1
0.00.051.198 I print_info: pooling type     = 0
0.00.051.199 I print_info: rope type        = 2
0.00.051.199 I print_info: rope scaling     = linear
0.00.051.199 I print_info: freq_base_train  = 10000.0
0.00.051.200 I print_info: freq_scale_train = 1
0.00.051.200 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.200 I print_info: rope_finetuned   = unknown
0.00.051.200 I print_info: ssm_d_conv       = 0
0.00.051.200 I print_info: ssm_d_inner      = 0
0.00.051.200 I print_info: ssm_d_state      = 0
0.00.051.202 I print_info: ssm_dt_rank      = 0
0.00.051.202 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.203 I print_info: model type       = 1.4B
0.00.051.203 I print_info: model params     = 1.41 B
0.00.051.203 I print_info: general.name     = 1.4B
0.00.051.204 I print_info: vocab type       = BPE
0.00.051.204 I print_info: n_vocab          = 50304
0.00.051.204 I print_info: n_merges         = 50009
0.00.051.204 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.204 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.205 I print_info: LF token         = 128 'Ä'
0.00.051.205 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.206 I print_info: max token length = 1024
0.00.542.114 I load_tensors: offloading 24 repeating layers to GPU
0.00.542.131 I load_tensors: offloading output layer to GPU
0.00.542.132 I load_tensors: offloaded 25/25 layers to GPU
0.00.542.167 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.542.179 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.543.187 I llama_context: n_seq_max     = 1
0.00.543.192 I llama_context: n_ctx         = 2048
0.00.543.192 I llama_context: n_ctx_per_seq = 2048
0.00.543.193 I llama_context: n_batch       = 2048
0.00.543.193 I llama_context: n_ubatch      = 512
0.00.543.194 I llama_context: flash_attn    = 0
0.00.543.196 I llama_context: freq_base     = 10000.0
0.00.543.196 I llama_context: freq_scale    = 1
0.00.543.198 I ggml_metal_init: allocating
0.00.543.275 I ggml_metal_init: found device: Apple M4
0.00.543.289 I ggml_metal_init: picking default device: Apple M4
0.00.545.091 I ggml_metal_init: using embedded metal library
0.00.551.188 I ggml_metal_init: GPU name:   Apple M4
0.00.551.193 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.551.194 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.551.195 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.551.200 I ggml_metal_init: simdgroup reduction   = true
0.00.551.200 I ggml_metal_init: simdgroup matrix mul. = true
0.00.551.200 I ggml_metal_init: has residency sets    = true
0.00.551.200 I ggml_metal_init: has bfloat            = true
0.00.551.201 I ggml_metal_init: use bfloat            = true
0.00.551.202 I ggml_metal_init: hasUnifiedMemory      = true
0.00.551.208 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.570.239 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.751 I init:      Metal KV buffer size =   384.00 MiB
0.00.628.757 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.628.793 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.633.251 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.633.253 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.633.253 I llama_context: graph nodes  = 967
0.00.633.253 I llama_context: graph splits = 2
0.00.633.259 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.633.384 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.633.385 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.689.711 I main: llama threadpool init, n_threads = 4
0.00.689.760 I 
0.00.689.787 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.689.788 I 
0.00.690.010 I sampler seed: 1234
0.00.690.015 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.690.036 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.690.037 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.690.037 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.440.698 I llama_perf_sampler_print:    sampling time =       1.45 ms /    71 runs   (    0.02 ms per token, 48931.77 tokens per second)
0.01.440.699 I llama_perf_context_print:        load time =     680.12 ms
0.01.440.700 I llama_perf_context_print: prompt eval time =      46.80 ms /     7 tokens (    6.69 ms per token,   149.59 tokens per second)
0.01.440.701 I llama_perf_context_print:        eval time =     700.84 ms /    63 runs   (   11.12 ms per token,    89.89 tokens per second)
0.01.440.701 I llama_perf_context_print:       total time =     751.89 ms /    70 tokens
0.01.443.768 I ggml_metal_free: deallocating

real	0m1.461s
user	0m0.121s
sys	0m0.208s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.087 I main: load the model and apply lora adapter, if any
0.00.010.059 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.754 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.759 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.760 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.761 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.761 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.761 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.762 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.765 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.765 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.765 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.766 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.766 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.766 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.771 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.772 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.773 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.775 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.539 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.526 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.188 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.189 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.189 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.190 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.190 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.191 I llama_model_loader: - type  f32:  194 tensors
0.00.025.191 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.191 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.192 I print_info: file format = GGUF V3 (latest)
0.00.025.193 I print_info: file type   = Q5_K - Medium
0.00.025.194 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.044.890 I load: special tokens cache size = 25
0.00.050.953 I load: token to piece cache size = 0.2984 MB
0.00.050.957 I print_info: arch             = gptneox
0.00.050.957 I print_info: vocab_only       = 0
0.00.050.957 I print_info: n_ctx_train      = 2048
0.00.050.957 I print_info: n_embd           = 2048
0.00.050.958 I print_info: n_layer          = 24
0.00.050.960 I print_info: n_head           = 16
0.00.050.961 I print_info: n_head_kv        = 16
0.00.050.962 I print_info: n_rot            = 32
0.00.050.962 I print_info: n_swa            = 0
0.00.050.962 I print_info: n_embd_head_k    = 128
0.00.050.962 I print_info: n_embd_head_v    = 128
0.00.050.963 I print_info: n_gqa            = 1
0.00.050.964 I print_info: n_embd_k_gqa     = 2048
0.00.050.965 I print_info: n_embd_v_gqa     = 2048
0.00.050.965 I print_info: f_norm_eps       = 1.0e-05
0.00.050.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.966 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.966 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.966 I print_info: f_logit_scale    = 0.0e+00
0.00.050.967 I print_info: n_ff             = 8192
0.00.050.967 I print_info: n_expert         = 0
0.00.050.967 I print_info: n_expert_used    = 0
0.00.050.967 I print_info: causal attn      = 1
0.00.050.968 I print_info: pooling type     = 0
0.00.050.968 I print_info: rope type        = 2
0.00.050.968 I print_info: rope scaling     = linear
0.00.050.968 I print_info: freq_base_train  = 10000.0
0.00.050.969 I print_info: freq_scale_train = 1
0.00.050.969 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.969 I print_info: rope_finetuned   = unknown
0.00.050.969 I print_info: ssm_d_conv       = 0
0.00.050.969 I print_info: ssm_d_inner      = 0
0.00.050.970 I print_info: ssm_d_state      = 0
0.00.050.970 I print_info: ssm_dt_rank      = 0
0.00.050.970 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.970 I print_info: model type       = 1.4B
0.00.050.971 I print_info: model params     = 1.41 B
0.00.050.971 I print_info: general.name     = 1.4B
0.00.050.971 I print_info: vocab type       = BPE
0.00.050.974 I print_info: n_vocab          = 50304
0.00.050.974 I print_info: n_merges         = 50009
0.00.050.974 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.974 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.974 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.975 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.975 I print_info: LF token         = 128 'Ä'
0.00.050.975 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.975 I print_info: max token length = 1024
0.00.624.050 I load_tensors: offloading 24 repeating layers to GPU
0.00.624.064 I load_tensors: offloading output layer to GPU
0.00.624.065 I load_tensors: offloaded 25/25 layers to GPU
0.00.624.095 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.624.097 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.625.742 I llama_context: n_seq_max     = 1
0.00.625.748 I llama_context: n_ctx         = 2048
0.00.625.749 I llama_context: n_ctx_per_seq = 2048
0.00.625.749 I llama_context: n_batch       = 2048
0.00.625.749 I llama_context: n_ubatch      = 512
0.00.625.750 I llama_context: flash_attn    = 0
0.00.625.752 I llama_context: freq_base     = 10000.0
0.00.625.753 I llama_context: freq_scale    = 1
0.00.625.762 I ggml_metal_init: allocating
0.00.625.843 I ggml_metal_init: found device: Apple M4
0.00.625.856 I ggml_metal_init: picking default device: Apple M4
0.00.627.487 I ggml_metal_init: using embedded metal library
0.00.633.841 I ggml_metal_init: GPU name:   Apple M4
0.00.633.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.633.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.633.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.633.847 I ggml_metal_init: simdgroup reduction   = true
0.00.633.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.633.847 I ggml_metal_init: has residency sets    = true
0.00.633.847 I ggml_metal_init: has bfloat            = true
0.00.633.848 I ggml_metal_init: use bfloat            = true
0.00.633.849 I ggml_metal_init: hasUnifiedMemory      = true
0.00.633.850 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.650.809 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.705.466 I init:      Metal KV buffer size =   384.00 MiB
0.00.705.473 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.705.552 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.709.747 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.709.749 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.709.749 I llama_context: graph nodes  = 967
0.00.709.750 I llama_context: graph splits = 2
0.00.709.754 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.709.882 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.709.883 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.776.537 I main: llama threadpool init, n_threads = 4
0.00.776.581 I 
0.00.776.608 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.776.608 I 
0.00.776.834 I sampler seed: 1234
0.00.776.839 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.776.849 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.776.850 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.776.852 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.624.948 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52553.66 tokens per second)
0.01.624.949 I llama_perf_context_print:        load time =     765.61 ms
0.01.624.950 I llama_perf_context_print: prompt eval time =      58.59 ms /     7 tokens (    8.37 ms per token,   119.48 tokens per second)
0.01.624.950 I llama_perf_context_print:        eval time =     786.51 ms /    63 runs   (   12.48 ms per token,    80.10 tokens per second)
0.01.624.951 I llama_perf_context_print:       total time =     849.28 ms /    70 tokens
0.01.629.052 I ggml_metal_free: deallocating

real	0m1.649s
user	0m0.120s
sys	0m0.225s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.042 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.079 I main: llama backend init
0.00.000.081 I main: load the model and apply lora adapter, if any
0.00.009.923 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.585 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.590 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.592 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.592 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.592 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.593 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.594 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.594 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.595 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.595 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.597 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.506 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.306 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.307 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.307 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.308 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.308 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.309 I llama_model_loader: - type  f32:  194 tensors
0.00.025.309 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.310 I print_info: file format = GGUF V3 (latest)
0.00.025.310 I print_info: file type   = Q6_K
0.00.025.311 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.045.146 I load: special tokens cache size = 25
0.00.050.986 I load: token to piece cache size = 0.2984 MB
0.00.050.989 I print_info: arch             = gptneox
0.00.050.989 I print_info: vocab_only       = 0
0.00.050.989 I print_info: n_ctx_train      = 2048
0.00.050.989 I print_info: n_embd           = 2048
0.00.050.990 I print_info: n_layer          = 24
0.00.050.993 I print_info: n_head           = 16
0.00.050.994 I print_info: n_head_kv        = 16
0.00.050.994 I print_info: n_rot            = 32
0.00.050.994 I print_info: n_swa            = 0
0.00.050.994 I print_info: n_embd_head_k    = 128
0.00.050.994 I print_info: n_embd_head_v    = 128
0.00.050.995 I print_info: n_gqa            = 1
0.00.050.996 I print_info: n_embd_k_gqa     = 2048
0.00.050.997 I print_info: n_embd_v_gqa     = 2048
0.00.050.997 I print_info: f_norm_eps       = 1.0e-05
0.00.050.998 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.998 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.000 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.000 I print_info: f_logit_scale    = 0.0e+00
0.00.051.001 I print_info: n_ff             = 8192
0.00.051.001 I print_info: n_expert         = 0
0.00.051.002 I print_info: n_expert_used    = 0
0.00.051.010 I print_info: causal attn      = 1
0.00.051.012 I print_info: pooling type     = 0
0.00.051.012 I print_info: rope type        = 2
0.00.051.012 I print_info: rope scaling     = linear
0.00.051.013 I print_info: freq_base_train  = 10000.0
0.00.051.013 I print_info: freq_scale_train = 1
0.00.051.013 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.013 I print_info: rope_finetuned   = unknown
0.00.051.014 I print_info: ssm_d_conv       = 0
0.00.051.014 I print_info: ssm_d_inner      = 0
0.00.051.014 I print_info: ssm_d_state      = 0
0.00.051.014 I print_info: ssm_dt_rank      = 0
0.00.051.014 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.014 I print_info: model type       = 1.4B
0.00.051.015 I print_info: model params     = 1.41 B
0.00.051.015 I print_info: general.name     = 1.4B
0.00.051.015 I print_info: vocab type       = BPE
0.00.051.015 I print_info: n_vocab          = 50304
0.00.051.016 I print_info: n_merges         = 50009
0.00.051.016 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.016 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.016 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.016 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.017 I print_info: LF token         = 128 'Ä'
0.00.051.017 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.017 I print_info: max token length = 1024
0.00.657.308 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.312 I load_tensors: offloading output layer to GPU
0.00.657.312 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.334 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.657.336 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.658.563 I llama_context: n_seq_max     = 1
0.00.658.565 I llama_context: n_ctx         = 2048
0.00.658.565 I llama_context: n_ctx_per_seq = 2048
0.00.658.566 I llama_context: n_batch       = 2048
0.00.658.566 I llama_context: n_ubatch      = 512
0.00.658.567 I llama_context: flash_attn    = 0
0.00.658.568 I llama_context: freq_base     = 10000.0
0.00.658.568 I llama_context: freq_scale    = 1
0.00.658.569 I ggml_metal_init: allocating
0.00.658.607 I ggml_metal_init: found device: Apple M4
0.00.658.621 I ggml_metal_init: picking default device: Apple M4
0.00.660.031 I ggml_metal_init: using embedded metal library
0.00.666.292 I ggml_metal_init: GPU name:   Apple M4
0.00.666.295 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.296 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.297 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.298 I ggml_metal_init: simdgroup reduction   = true
0.00.666.298 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.298 I ggml_metal_init: has residency sets    = true
0.00.666.298 I ggml_metal_init: has bfloat            = true
0.00.666.299 I ggml_metal_init: use bfloat            = true
0.00.666.299 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.301 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.970 I init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.740.449 I init:      Metal KV buffer size =   384.00 MiB
0.00.740.455 I llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.740.536 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.744.545 I llama_context:      Metal compute buffer size =   102.25 MiB
0.00.744.547 I llama_context:        CPU compute buffer size =     8.01 MiB
0.00.744.547 I llama_context: graph nodes  = 967
0.00.744.548 I llama_context: graph splits = 2
0.00.744.554 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.744.687 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.744.687 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.811.095 I main: llama threadpool init, n_threads = 4
0.00.811.134 I 
0.00.811.157 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.811.157 I 
0.00.811.372 I sampler seed: 1234
0.00.811.376 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.811.421 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.811.424 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.811.424 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.688.655 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55295.95 tokens per second)
0.01.688.656 I llama_perf_context_print:        load time =     800.21 ms
0.01.688.657 I llama_perf_context_print: prompt eval time =      54.28 ms /     7 tokens (    7.75 ms per token,   128.96 tokens per second)
0.01.688.657 I llama_perf_context_print:        eval time =     820.08 ms /    63 runs   (   13.02 ms per token,    76.82 tokens per second)
0.01.688.658 I llama_perf_context_print:       total time =     878.52 ms /    70 tokens
0.01.692.547 I ggml_metal_free: deallocating

real	0m1.710s
user	0m0.122s
sys	0m0.218s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.002.205 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.032.335 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.046.483 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.046.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.046.496 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.046.497 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.046.497 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.046.498 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.046.498 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.046.500 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.046.500 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.046.501 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.046.501 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.046.502 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.046.502 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.046.503 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.046.506 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.046.506 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.046.507 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.053.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.055.819 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.062.908 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.062.915 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.062.915 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.062.916 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.062.917 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.062.918 I llama_model_loader: - type  f32:  194 tensors
0.00.062.919 I llama_model_loader: - type  f16:   98 tensors
0.00.062.920 I print_info: file format = GGUF V3 (latest)
0.00.062.922 I print_info: file type   = all F32 (guessed)
0.00.062.926 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.085.675 I load: special tokens cache size = 25
0.00.091.822 I load: token to piece cache size = 0.2984 MB
0.00.091.826 I print_info: arch             = gptneox
0.00.091.827 I print_info: vocab_only       = 0
0.00.091.827 I print_info: n_ctx_train      = 2048
0.00.091.827 I print_info: n_embd           = 2048
0.00.091.827 I print_info: n_layer          = 24
0.00.091.832 I print_info: n_head           = 16
0.00.091.833 I print_info: n_head_kv        = 16
0.00.091.833 I print_info: n_rot            = 32
0.00.091.833 I print_info: n_swa            = 0
0.00.091.833 I print_info: n_embd_head_k    = 128
0.00.091.833 I print_info: n_embd_head_v    = 128
0.00.091.834 I print_info: n_gqa            = 1
0.00.091.834 I print_info: n_embd_k_gqa     = 2048
0.00.091.835 I print_info: n_embd_v_gqa     = 2048
0.00.091.836 I print_info: f_norm_eps       = 1.0e-05
0.00.091.838 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.838 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.838 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.838 I print_info: f_logit_scale    = 0.0e+00
0.00.091.839 I print_info: n_ff             = 8192
0.00.091.839 I print_info: n_expert         = 0
0.00.091.839 I print_info: n_expert_used    = 0
0.00.091.839 I print_info: causal attn      = 1
0.00.091.839 I print_info: pooling type     = 0
0.00.091.839 I print_info: rope type        = 2
0.00.091.840 I print_info: rope scaling     = linear
0.00.091.844 I print_info: freq_base_train  = 10000.0
0.00.091.844 I print_info: freq_scale_train = 1
0.00.091.844 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.844 I print_info: rope_finetuned   = unknown
0.00.091.845 I print_info: ssm_d_conv       = 0
0.00.091.845 I print_info: ssm_d_inner      = 0
0.00.091.845 I print_info: ssm_d_state      = 0
0.00.091.845 I print_info: ssm_dt_rank      = 0
0.00.091.845 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.846 I print_info: model type       = 1.4B
0.00.091.846 I print_info: model params     = 1.41 B
0.00.091.846 I print_info: general.name     = 1.4B
0.00.091.847 I print_info: vocab type       = BPE
0.00.091.847 I print_info: n_vocab          = 50304
0.00.091.847 I print_info: n_merges         = 50009
0.00.091.847 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.847 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.847 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.847 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.849 I print_info: LF token         = 128 'Ä'
0.00.091.849 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.850 I print_info: max token length = 1024
0.01.422.587 I load_tensors: offloading 24 repeating layers to GPU
0.01.422.595 I load_tensors: offloading output layer to GPU
0.01.422.597 I load_tensors: offloaded 25/25 layers to GPU
0.01.422.630 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.422.632 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.423.792 I llama_context: n_seq_max     = 1
0.01.423.794 I llama_context: n_ctx         = 128
0.01.423.794 I llama_context: n_ctx_per_seq = 128
0.01.423.794 I llama_context: n_batch       = 128
0.01.423.795 I llama_context: n_ubatch      = 128
0.01.423.795 I llama_context: flash_attn    = 0
0.01.423.795 I llama_context: freq_base     = 10000.0
0.01.423.795 I llama_context: freq_scale    = 1
0.01.423.796 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.423.797 I ggml_metal_init: allocating
0.01.423.844 I ggml_metal_init: found device: Apple M4
0.01.423.850 I ggml_metal_init: picking default device: Apple M4
0.01.424.905 I ggml_metal_init: using embedded metal library
0.01.428.747 I ggml_metal_init: GPU name:   Apple M4
0.01.428.750 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.428.750 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.428.751 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.428.751 I ggml_metal_init: simdgroup reduction   = true
0.01.428.752 I ggml_metal_init: simdgroup matrix mul. = true
0.01.428.752 I ggml_metal_init: has residency sets    = true
0.01.428.752 I ggml_metal_init: has bfloat            = true
0.01.428.752 I ggml_metal_init: use bfloat            = true
0.01.428.753 I ggml_metal_init: hasUnifiedMemory      = true
0.01.428.754 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.439.216 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.440.894 I init:      Metal KV buffer size =    24.00 MiB
0.01.440.896 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.440.921 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.442.554 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.442.556 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.442.556 I llama_context: graph nodes  = 967
0.01.442.556 I llama_context: graph splits = 2
0.01.442.558 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.442.558 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.478.150 I 
0.01.478.189 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.478.210 I perplexity: tokenizing the input ..
0.01.487.824 I perplexity: tokenization took 9.611 ms
0.01.487.847 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.606.294 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.607.623 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.607.635 I llama_perf_context_print:        load time =    1445.81 ms
0.01.607.636 I llama_perf_context_print: prompt eval time =     118.12 ms /   128 tokens (    0.92 ms per token,  1083.62 tokens per second)
0.01.607.636 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.607.637 I llama_perf_context_print:       total time =     129.49 ms /   129 tokens
0.01.608.159 I ggml_metal_free: deallocating

real	0m1.800s
user	0m0.107s
sys	0m0.233s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.936 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.747 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.016.752 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.754 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.754 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.755 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.755 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.755 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.756 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.756 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.757 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.757 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.757 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.757 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.758 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.759 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.760 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.760 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.585 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.625 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.524 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.525 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.526 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.526 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.025.527 I llama_model_loader: - type  f32:  194 tensors
0.00.025.527 I llama_model_loader: - type q8_0:   98 tensors
0.00.025.528 I print_info: file format = GGUF V3 (latest)
0.00.025.528 I print_info: file type   = Q8_0
0.00.025.529 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.339 I load: special tokens cache size = 25
0.00.052.402 I load: token to piece cache size = 0.2984 MB
0.00.052.408 I print_info: arch             = gptneox
0.00.052.408 I print_info: vocab_only       = 0
0.00.052.409 I print_info: n_ctx_train      = 2048
0.00.052.409 I print_info: n_embd           = 2048
0.00.052.409 I print_info: n_layer          = 24
0.00.052.413 I print_info: n_head           = 16
0.00.052.414 I print_info: n_head_kv        = 16
0.00.052.414 I print_info: n_rot            = 32
0.00.052.414 I print_info: n_swa            = 0
0.00.052.414 I print_info: n_embd_head_k    = 128
0.00.052.414 I print_info: n_embd_head_v    = 128
0.00.052.415 I print_info: n_gqa            = 1
0.00.052.418 I print_info: n_embd_k_gqa     = 2048
0.00.052.419 I print_info: n_embd_v_gqa     = 2048
0.00.052.419 I print_info: f_norm_eps       = 1.0e-05
0.00.052.419 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.052.419 I print_info: f_clamp_kqv      = 0.0e+00
0.00.052.420 I print_info: f_max_alibi_bias = 0.0e+00
0.00.052.420 I print_info: f_logit_scale    = 0.0e+00
0.00.052.420 I print_info: n_ff             = 8192
0.00.052.421 I print_info: n_expert         = 0
0.00.052.425 I print_info: n_expert_used    = 0
0.00.052.426 I print_info: causal attn      = 1
0.00.052.426 I print_info: pooling type     = 0
0.00.052.426 I print_info: rope type        = 2
0.00.052.427 I print_info: rope scaling     = linear
0.00.052.427 I print_info: freq_base_train  = 10000.0
0.00.052.427 I print_info: freq_scale_train = 1
0.00.052.427 I print_info: n_ctx_orig_yarn  = 2048
0.00.052.428 I print_info: rope_finetuned   = unknown
0.00.052.428 I print_info: ssm_d_conv       = 0
0.00.052.428 I print_info: ssm_d_inner      = 0
0.00.052.428 I print_info: ssm_d_state      = 0
0.00.052.428 I print_info: ssm_dt_rank      = 0
0.00.052.428 I print_info: ssm_dt_b_c_rms   = 0
0.00.052.428 I print_info: model type       = 1.4B
0.00.052.429 I print_info: model params     = 1.41 B
0.00.052.429 I print_info: general.name     = 1.4B
0.00.052.429 I print_info: vocab type       = BPE
0.00.052.430 I print_info: n_vocab          = 50304
0.00.052.430 I print_info: n_merges         = 50009
0.00.052.430 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.052.430 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.052.430 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.052.430 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.052.431 I print_info: LF token         = 128 'Ä'
0.00.052.431 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.052.431 I print_info: max token length = 1024
0.00.905.564 I load_tensors: offloading 24 repeating layers to GPU
0.00.905.567 I load_tensors: offloading output layer to GPU
0.00.905.568 I load_tensors: offloaded 25/25 layers to GPU
0.00.905.586 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.905.587 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.906.008 I llama_context: n_seq_max     = 1
0.00.906.009 I llama_context: n_ctx         = 128
0.00.906.009 I llama_context: n_ctx_per_seq = 128
0.00.906.010 I llama_context: n_batch       = 128
0.00.906.010 I llama_context: n_ubatch      = 128
0.00.906.010 I llama_context: flash_attn    = 0
0.00.906.010 I llama_context: freq_base     = 10000.0
0.00.906.011 I llama_context: freq_scale    = 1
0.00.906.011 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.906.012 I ggml_metal_init: allocating
0.00.906.035 I ggml_metal_init: found device: Apple M4
0.00.906.040 I ggml_metal_init: picking default device: Apple M4
0.00.906.668 I ggml_metal_init: using embedded metal library
0.00.909.696 I ggml_metal_init: GPU name:   Apple M4
0.00.909.698 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.909.698 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.909.699 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.909.700 I ggml_metal_init: simdgroup reduction   = true
0.00.909.700 I ggml_metal_init: simdgroup matrix mul. = true
0.00.909.700 I ggml_metal_init: has residency sets    = true
0.00.909.702 I ggml_metal_init: has bfloat            = true
0.00.909.702 I ggml_metal_init: use bfloat            = true
0.00.909.703 I ggml_metal_init: hasUnifiedMemory      = true
0.00.909.704 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.919.228 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.920.894 I init:      Metal KV buffer size =    24.00 MiB
0.00.920.898 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.920.922 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.922.639 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.922.640 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.922.640 I llama_context: graph nodes  = 967
0.00.922.640 I llama_context: graph splits = 2
0.00.922.642 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.922.642 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.945.336 I 
0.00.945.364 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.945.373 I perplexity: tokenizing the input ..
0.00.952.558 I perplexity: tokenization took 7.184 ms
0.00.952.567 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.089.589 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.090.954 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.090.964 I llama_perf_context_print:        load time =     936.40 ms
0.01.090.968 I llama_perf_context_print: prompt eval time =     136.80 ms /   128 tokens (    1.07 ms per token,   935.71 tokens per second)
0.01.090.970 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.090.970 I llama_perf_context_print:       total time =     145.63 ms /   129 tokens
0.01.091.506 I ggml_metal_free: deallocating

real	0m1.116s
user	0m0.080s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.014.583 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.023.200 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.023.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.209 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.210 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.210 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.211 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.212 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.213 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.213 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.214 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.215 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.216 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.216 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.172 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.127 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.032.132 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.132 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.133 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.133 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.133 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.032.134 I llama_model_loader: - type  f32:  194 tensors
0.00.032.134 I llama_model_loader: - type q4_0:   97 tensors
0.00.032.134 I llama_model_loader: - type q6_K:    1 tensors
0.00.032.135 I print_info: file format = GGUF V3 (latest)
0.00.032.135 I print_info: file type   = Q4_0
0.00.032.136 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.052.890 I load: special tokens cache size = 25
0.00.059.045 I load: token to piece cache size = 0.2984 MB
0.00.059.054 I print_info: arch             = gptneox
0.00.059.054 I print_info: vocab_only       = 0
0.00.059.054 I print_info: n_ctx_train      = 2048
0.00.059.054 I print_info: n_embd           = 2048
0.00.059.055 I print_info: n_layer          = 24
0.00.059.059 I print_info: n_head           = 16
0.00.059.059 I print_info: n_head_kv        = 16
0.00.059.060 I print_info: n_rot            = 32
0.00.059.060 I print_info: n_swa            = 0
0.00.059.060 I print_info: n_embd_head_k    = 128
0.00.059.060 I print_info: n_embd_head_v    = 128
0.00.059.061 I print_info: n_gqa            = 1
0.00.059.062 I print_info: n_embd_k_gqa     = 2048
0.00.059.062 I print_info: n_embd_v_gqa     = 2048
0.00.059.063 I print_info: f_norm_eps       = 1.0e-05
0.00.059.063 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.059.063 I print_info: f_clamp_kqv      = 0.0e+00
0.00.059.063 I print_info: f_max_alibi_bias = 0.0e+00
0.00.059.064 I print_info: f_logit_scale    = 0.0e+00
0.00.059.066 I print_info: n_ff             = 8192
0.00.059.068 I print_info: n_expert         = 0
0.00.059.068 I print_info: n_expert_used    = 0
0.00.059.068 I print_info: causal attn      = 1
0.00.059.068 I print_info: pooling type     = 0
0.00.059.068 I print_info: rope type        = 2
0.00.059.069 I print_info: rope scaling     = linear
0.00.059.072 I print_info: freq_base_train  = 10000.0
0.00.059.072 I print_info: freq_scale_train = 1
0.00.059.072 I print_info: n_ctx_orig_yarn  = 2048
0.00.059.073 I print_info: rope_finetuned   = unknown
0.00.059.073 I print_info: ssm_d_conv       = 0
0.00.059.073 I print_info: ssm_d_inner      = 0
0.00.059.073 I print_info: ssm_d_state      = 0
0.00.059.073 I print_info: ssm_dt_rank      = 0
0.00.059.073 I print_info: ssm_dt_b_c_rms   = 0
0.00.059.074 I print_info: model type       = 1.4B
0.00.059.074 I print_info: model params     = 1.41 B
0.00.059.074 I print_info: general.name     = 1.4B
0.00.059.075 I print_info: vocab type       = BPE
0.00.059.075 I print_info: n_vocab          = 50304
0.00.059.075 I print_info: n_merges         = 50009
0.00.059.075 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.059.075 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.059.075 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.059.076 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.059.076 I print_info: LF token         = 128 'Ä'
0.00.059.076 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.059.076 I print_info: max token length = 1024
0.00.610.437 I load_tensors: offloading 24 repeating layers to GPU
0.00.610.451 I load_tensors: offloading output layer to GPU
0.00.610.452 I load_tensors: offloaded 25/25 layers to GPU
0.00.610.488 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.610.490 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.611.555 I llama_context: n_seq_max     = 1
0.00.611.560 I llama_context: n_ctx         = 128
0.00.611.560 I llama_context: n_ctx_per_seq = 128
0.00.611.564 I llama_context: n_batch       = 128
0.00.611.565 I llama_context: n_ubatch      = 128
0.00.611.565 I llama_context: flash_attn    = 0
0.00.611.567 I llama_context: freq_base     = 10000.0
0.00.611.568 I llama_context: freq_scale    = 1
0.00.611.568 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.611.574 I ggml_metal_init: allocating
0.00.611.663 I ggml_metal_init: found device: Apple M4
0.00.611.678 I ggml_metal_init: picking default device: Apple M4
0.00.613.443 I ggml_metal_init: using embedded metal library
0.00.618.839 I ggml_metal_init: GPU name:   Apple M4
0.00.618.848 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.849 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.851 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.851 I ggml_metal_init: simdgroup reduction   = true
0.00.618.852 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.852 I ggml_metal_init: has residency sets    = true
0.00.618.852 I ggml_metal_init: has bfloat            = true
0.00.618.852 I ggml_metal_init: use bfloat            = true
0.00.618.854 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.857 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.637.760 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.641.402 I init:      Metal KV buffer size =    24.00 MiB
0.00.641.406 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.641.449 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.644.712 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.644.714 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.644.715 I llama_context: graph nodes  = 967
0.00.644.715 I llama_context: graph splits = 2
0.00.644.719 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.644.719 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.123 I 
0.00.674.221 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.239 I perplexity: tokenizing the input ..
0.00.683.369 I perplexity: tokenization took 9.128 ms
0.00.683.381 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.807.384 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.808.717 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.808.730 I llama_perf_context_print:        load time =     659.53 ms
0.00.808.732 I llama_perf_context_print: prompt eval time =     123.78 ms /   128 tokens (    0.97 ms per token,  1034.13 tokens per second)
0.00.808.732 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.808.733 I llama_perf_context_print:       total time =     134.61 ms /   129 tokens
0.00.809.293 I ggml_metal_free: deallocating

real	0m0.829s
user	0m0.094s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.872 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.614 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.618 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.620 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.621 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.621 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.622 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.623 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.623 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.624 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.625 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.626 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.626 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.626 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.630 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.630 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.630 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.524 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.335 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.336 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.337 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.338 I llama_model_loader: - type  f32:  194 tensors
0.00.024.338 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.339 I print_info: file format = GGUF V3 (latest)
0.00.024.340 I print_info: file type   = Q4_1
0.00.024.341 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.043.951 I load: special tokens cache size = 25
0.00.049.967 I load: token to piece cache size = 0.2984 MB
0.00.049.970 I print_info: arch             = gptneox
0.00.049.971 I print_info: vocab_only       = 0
0.00.049.971 I print_info: n_ctx_train      = 2048
0.00.049.971 I print_info: n_embd           = 2048
0.00.049.971 I print_info: n_layer          = 24
0.00.049.974 I print_info: n_head           = 16
0.00.049.975 I print_info: n_head_kv        = 16
0.00.049.975 I print_info: n_rot            = 32
0.00.049.975 I print_info: n_swa            = 0
0.00.049.975 I print_info: n_embd_head_k    = 128
0.00.049.976 I print_info: n_embd_head_v    = 128
0.00.049.979 I print_info: n_gqa            = 1
0.00.049.980 I print_info: n_embd_k_gqa     = 2048
0.00.049.980 I print_info: n_embd_v_gqa     = 2048
0.00.049.981 I print_info: f_norm_eps       = 1.0e-05
0.00.049.981 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.981 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.982 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.982 I print_info: f_logit_scale    = 0.0e+00
0.00.049.983 I print_info: n_ff             = 8192
0.00.049.983 I print_info: n_expert         = 0
0.00.049.983 I print_info: n_expert_used    = 0
0.00.049.983 I print_info: causal attn      = 1
0.00.049.983 I print_info: pooling type     = 0
0.00.049.984 I print_info: rope type        = 2
0.00.049.984 I print_info: rope scaling     = linear
0.00.049.984 I print_info: freq_base_train  = 10000.0
0.00.049.984 I print_info: freq_scale_train = 1
0.00.049.984 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.985 I print_info: rope_finetuned   = unknown
0.00.049.985 I print_info: ssm_d_conv       = 0
0.00.049.985 I print_info: ssm_d_inner      = 0
0.00.049.985 I print_info: ssm_d_state      = 0
0.00.049.985 I print_info: ssm_dt_rank      = 0
0.00.049.986 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.986 I print_info: model type       = 1.4B
0.00.049.987 I print_info: model params     = 1.41 B
0.00.049.987 I print_info: general.name     = 1.4B
0.00.049.988 I print_info: vocab type       = BPE
0.00.049.988 I print_info: n_vocab          = 50304
0.00.049.988 I print_info: n_merges         = 50009
0.00.049.988 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.988 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.989 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.989 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.989 I print_info: LF token         = 128 'Ä'
0.00.049.989 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.989 I print_info: max token length = 1024
0.00.628.665 I load_tensors: offloading 24 repeating layers to GPU
0.00.628.681 I load_tensors: offloading output layer to GPU
0.00.628.681 I load_tensors: offloaded 25/25 layers to GPU
0.00.628.714 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.628.715 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.630.068 I llama_context: n_seq_max     = 1
0.00.630.074 I llama_context: n_ctx         = 128
0.00.630.074 I llama_context: n_ctx_per_seq = 128
0.00.630.075 I llama_context: n_batch       = 128
0.00.630.075 I llama_context: n_ubatch      = 128
0.00.630.076 I llama_context: flash_attn    = 0
0.00.630.078 I llama_context: freq_base     = 10000.0
0.00.630.078 I llama_context: freq_scale    = 1
0.00.630.079 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.630.085 I ggml_metal_init: allocating
0.00.630.149 I ggml_metal_init: found device: Apple M4
0.00.630.161 I ggml_metal_init: picking default device: Apple M4
0.00.631.868 I ggml_metal_init: using embedded metal library
0.00.639.384 I ggml_metal_init: GPU name:   Apple M4
0.00.639.389 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.639.390 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.639.391 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.639.392 I ggml_metal_init: simdgroup reduction   = true
0.00.639.392 I ggml_metal_init: simdgroup matrix mul. = true
0.00.639.392 I ggml_metal_init: has residency sets    = true
0.00.639.393 I ggml_metal_init: has bfloat            = true
0.00.639.393 I ggml_metal_init: use bfloat            = true
0.00.639.394 I ggml_metal_init: hasUnifiedMemory      = true
0.00.639.395 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.657.517 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.661.086 I init:      Metal KV buffer size =    24.00 MiB
0.00.661.090 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.661.139 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.665.208 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.665.209 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.665.210 I llama_context: graph nodes  = 967
0.00.665.210 I llama_context: graph splits = 2
0.00.665.214 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.665.214 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.690.351 I 
0.00.690.423 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.690.443 I perplexity: tokenizing the input ..
0.00.702.698 I perplexity: tokenization took 12.253 ms
0.00.702.723 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.832.842 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.834.247 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.834.262 I llama_perf_context_print:        load time =     681.47 ms
0.00.834.264 I llama_perf_context_print: prompt eval time =     129.22 ms /   128 tokens (    1.01 ms per token,   990.60 tokens per second)
0.00.834.265 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.834.266 I llama_perf_context_print:       total time =     143.92 ms /   129 tokens
0.00.834.817 I ggml_metal_free: deallocating

real	0m0.849s
user	0m0.097s
sys	0m0.118s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.875 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.938 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.944 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.945 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.947 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.953 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.953 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.955 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.955 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.955 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.955 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.957 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.958 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.958 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.961 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.961 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.961 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.891 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.965 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.767 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.768 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.768 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.769 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.769 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.770 I llama_model_loader: - type  f32:  194 tensors
0.00.024.770 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.771 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.771 I print_info: file format = GGUF V3 (latest)
0.00.024.772 I print_info: file type   = Q5_0
0.00.024.773 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.044.412 I load: special tokens cache size = 25
0.00.050.214 I load: token to piece cache size = 0.2984 MB
0.00.050.217 I print_info: arch             = gptneox
0.00.050.217 I print_info: vocab_only       = 0
0.00.050.217 I print_info: n_ctx_train      = 2048
0.00.050.217 I print_info: n_embd           = 2048
0.00.050.217 I print_info: n_layer          = 24
0.00.050.220 I print_info: n_head           = 16
0.00.050.221 I print_info: n_head_kv        = 16
0.00.050.221 I print_info: n_rot            = 32
0.00.050.221 I print_info: n_swa            = 0
0.00.050.223 I print_info: n_embd_head_k    = 128
0.00.050.223 I print_info: n_embd_head_v    = 128
0.00.050.223 I print_info: n_gqa            = 1
0.00.050.224 I print_info: n_embd_k_gqa     = 2048
0.00.050.225 I print_info: n_embd_v_gqa     = 2048
0.00.050.226 I print_info: f_norm_eps       = 1.0e-05
0.00.050.228 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.228 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.228 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.228 I print_info: f_logit_scale    = 0.0e+00
0.00.050.229 I print_info: n_ff             = 8192
0.00.050.229 I print_info: n_expert         = 0
0.00.050.229 I print_info: n_expert_used    = 0
0.00.050.229 I print_info: causal attn      = 1
0.00.050.230 I print_info: pooling type     = 0
0.00.050.230 I print_info: rope type        = 2
0.00.050.230 I print_info: rope scaling     = linear
0.00.050.230 I print_info: freq_base_train  = 10000.0
0.00.050.232 I print_info: freq_scale_train = 1
0.00.050.232 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.233 I print_info: rope_finetuned   = unknown
0.00.050.233 I print_info: ssm_d_conv       = 0
0.00.050.233 I print_info: ssm_d_inner      = 0
0.00.050.233 I print_info: ssm_d_state      = 0
0.00.050.233 I print_info: ssm_dt_rank      = 0
0.00.050.233 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.234 I print_info: model type       = 1.4B
0.00.050.234 I print_info: model params     = 1.41 B
0.00.050.234 I print_info: general.name     = 1.4B
0.00.050.235 I print_info: vocab type       = BPE
0.00.050.235 I print_info: n_vocab          = 50304
0.00.050.236 I print_info: n_merges         = 50009
0.00.050.236 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.238 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.238 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.238 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.238 I print_info: LF token         = 128 'Ä'
0.00.050.238 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.239 I print_info: max token length = 1024
0.00.693.256 I load_tensors: offloading 24 repeating layers to GPU
0.00.693.271 I load_tensors: offloading output layer to GPU
0.00.693.272 I load_tensors: offloaded 25/25 layers to GPU
0.00.693.304 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.693.305 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.694.735 I llama_context: n_seq_max     = 1
0.00.694.741 I llama_context: n_ctx         = 128
0.00.694.741 I llama_context: n_ctx_per_seq = 128
0.00.694.746 I llama_context: n_batch       = 128
0.00.694.747 I llama_context: n_ubatch      = 128
0.00.694.752 I llama_context: flash_attn    = 0
0.00.694.754 I llama_context: freq_base     = 10000.0
0.00.694.755 I llama_context: freq_scale    = 1
0.00.694.756 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.694.758 I ggml_metal_init: allocating
0.00.694.840 I ggml_metal_init: found device: Apple M4
0.00.694.855 I ggml_metal_init: picking default device: Apple M4
0.00.696.360 I ggml_metal_init: using embedded metal library
0.00.702.732 I ggml_metal_init: GPU name:   Apple M4
0.00.702.737 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.702.738 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.702.739 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.702.739 I ggml_metal_init: simdgroup reduction   = true
0.00.702.739 I ggml_metal_init: simdgroup matrix mul. = true
0.00.702.740 I ggml_metal_init: has residency sets    = true
0.00.702.740 I ggml_metal_init: has bfloat            = true
0.00.702.740 I ggml_metal_init: use bfloat            = true
0.00.702.741 I ggml_metal_init: hasUnifiedMemory      = true
0.00.702.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.720.403 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.947 I init:      Metal KV buffer size =    24.00 MiB
0.00.723.953 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.724.020 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.727.361 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.727.363 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.727.363 I llama_context: graph nodes  = 967
0.00.727.364 I llama_context: graph splits = 2
0.00.727.368 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.727.370 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.756.488 I 
0.00.756.569 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.756.590 I perplexity: tokenizing the input ..
0.00.765.400 I perplexity: tokenization took 8.808 ms
0.00.765.412 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.899.529 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.900.880 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.900.894 I llama_perf_context_print:        load time =     747.60 ms
0.00.900.895 I llama_perf_context_print: prompt eval time =     133.88 ms /   128 tokens (    1.05 ms per token,   956.06 tokens per second)
0.00.900.896 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.900.896 I llama_perf_context_print:       total time =     144.41 ms /   129 tokens
0.00.901.450 I ggml_metal_free: deallocating

real	0m0.916s
user	0m0.092s
sys	0m0.141s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.476 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.371 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.373 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.373 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.373 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.380 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.380 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.223 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.996 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.997 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.997 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.998 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.998 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.998 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.999 I llama_model_loader: - type  f32:  194 tensors
0.00.025.999 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.000 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.000 I print_info: file format = GGUF V3 (latest)
0.00.026.001 I print_info: file type   = Q5_1
0.00.026.002 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.045.637 I load: special tokens cache size = 25
0.00.051.953 I load: token to piece cache size = 0.2984 MB
0.00.051.955 I print_info: arch             = gptneox
0.00.051.955 I print_info: vocab_only       = 0
0.00.051.956 I print_info: n_ctx_train      = 2048
0.00.051.956 I print_info: n_embd           = 2048
0.00.051.956 I print_info: n_layer          = 24
0.00.051.959 I print_info: n_head           = 16
0.00.051.960 I print_info: n_head_kv        = 16
0.00.051.960 I print_info: n_rot            = 32
0.00.051.961 I print_info: n_swa            = 0
0.00.051.961 I print_info: n_embd_head_k    = 128
0.00.051.963 I print_info: n_embd_head_v    = 128
0.00.051.964 I print_info: n_gqa            = 1
0.00.051.965 I print_info: n_embd_k_gqa     = 2048
0.00.051.965 I print_info: n_embd_v_gqa     = 2048
0.00.051.966 I print_info: f_norm_eps       = 1.0e-05
0.00.051.966 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.967 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.967 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.967 I print_info: f_logit_scale    = 0.0e+00
0.00.051.973 I print_info: n_ff             = 8192
0.00.051.974 I print_info: n_expert         = 0
0.00.051.974 I print_info: n_expert_used    = 0
0.00.051.974 I print_info: causal attn      = 1
0.00.051.975 I print_info: pooling type     = 0
0.00.051.975 I print_info: rope type        = 2
0.00.051.975 I print_info: rope scaling     = linear
0.00.051.976 I print_info: freq_base_train  = 10000.0
0.00.051.976 I print_info: freq_scale_train = 1
0.00.051.976 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.977 I print_info: rope_finetuned   = unknown
0.00.051.977 I print_info: ssm_d_conv       = 0
0.00.051.977 I print_info: ssm_d_inner      = 0
0.00.051.977 I print_info: ssm_d_state      = 0
0.00.051.977 I print_info: ssm_dt_rank      = 0
0.00.051.977 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.978 I print_info: model type       = 1.4B
0.00.051.978 I print_info: model params     = 1.41 B
0.00.051.978 I print_info: general.name     = 1.4B
0.00.051.979 I print_info: vocab type       = BPE
0.00.051.979 I print_info: n_vocab          = 50304
0.00.051.980 I print_info: n_merges         = 50009
0.00.051.980 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.981 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.981 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.981 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.981 I print_info: LF token         = 128 'Ä'
0.00.051.982 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.982 I print_info: max token length = 1024
0.00.611.468 I load_tensors: offloading 24 repeating layers to GPU
0.00.611.482 I load_tensors: offloading output layer to GPU
0.00.611.483 I load_tensors: offloaded 25/25 layers to GPU
0.00.611.514 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.611.515 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.612.990 I llama_context: n_seq_max     = 1
0.00.612.997 I llama_context: n_ctx         = 128
0.00.612.997 I llama_context: n_ctx_per_seq = 128
0.00.613.003 I llama_context: n_batch       = 128
0.00.613.003 I llama_context: n_ubatch      = 128
0.00.613.004 I llama_context: flash_attn    = 0
0.00.613.018 I llama_context: freq_base     = 10000.0
0.00.613.018 I llama_context: freq_scale    = 1
0.00.613.019 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.613.022 I ggml_metal_init: allocating
0.00.613.111 I ggml_metal_init: found device: Apple M4
0.00.613.124 I ggml_metal_init: picking default device: Apple M4
0.00.614.978 I ggml_metal_init: using embedded metal library
0.00.621.406 I ggml_metal_init: GPU name:   Apple M4
0.00.621.410 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.621.411 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.621.412 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.621.412 I ggml_metal_init: simdgroup reduction   = true
0.00.621.413 I ggml_metal_init: simdgroup matrix mul. = true
0.00.621.413 I ggml_metal_init: has residency sets    = true
0.00.621.413 I ggml_metal_init: has bfloat            = true
0.00.621.413 I ggml_metal_init: use bfloat            = true
0.00.621.414 I ggml_metal_init: hasUnifiedMemory      = true
0.00.621.417 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.065 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.600 I init:      Metal KV buffer size =    24.00 MiB
0.00.642.605 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.647 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.645.705 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.645.707 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.645.707 I llama_context: graph nodes  = 967
0.00.645.708 I llama_context: graph splits = 2
0.00.645.711 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.645.711 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.041 I 
0.00.673.124 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.144 I perplexity: tokenizing the input ..
0.00.685.779 I perplexity: tokenization took 12.633 ms
0.00.685.797 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.820.929 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.822.343 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.822.355 I llama_perf_context_print:        load time =     713.82 ms
0.00.822.356 I llama_perf_context_print: prompt eval time =     134.73 ms /   128 tokens (    1.05 ms per token,   950.08 tokens per second)
0.00.822.357 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.822.358 I llama_perf_context_print:       total time =     149.32 ms /   129 tokens
0.00.822.933 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.097s
sys	0m0.149s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.237 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.034 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.039 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.044 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.044 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.045 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.046 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.046 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.046 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.049 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.049 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.049 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.051 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.337 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.909 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.911 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.912 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.912 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.912 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.913 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.913 I llama_model_loader: - type  f32:  194 tensors
0.00.025.914 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.914 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.914 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.915 I print_info: file format = GGUF V3 (latest)
0.00.025.916 I print_info: file type   = Q2_K - Medium
0.00.025.917 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.050.643 I load: special tokens cache size = 25
0.00.057.338 I load: token to piece cache size = 0.2984 MB
0.00.057.341 I print_info: arch             = gptneox
0.00.057.341 I print_info: vocab_only       = 0
0.00.057.342 I print_info: n_ctx_train      = 2048
0.00.057.342 I print_info: n_embd           = 2048
0.00.057.342 I print_info: n_layer          = 24
0.00.057.346 I print_info: n_head           = 16
0.00.057.346 I print_info: n_head_kv        = 16
0.00.057.346 I print_info: n_rot            = 32
0.00.057.347 I print_info: n_swa            = 0
0.00.057.347 I print_info: n_embd_head_k    = 128
0.00.057.347 I print_info: n_embd_head_v    = 128
0.00.057.348 I print_info: n_gqa            = 1
0.00.057.348 I print_info: n_embd_k_gqa     = 2048
0.00.057.349 I print_info: n_embd_v_gqa     = 2048
0.00.057.350 I print_info: f_norm_eps       = 1.0e-05
0.00.057.350 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.057.350 I print_info: f_clamp_kqv      = 0.0e+00
0.00.057.350 I print_info: f_max_alibi_bias = 0.0e+00
0.00.057.350 I print_info: f_logit_scale    = 0.0e+00
0.00.057.351 I print_info: n_ff             = 8192
0.00.057.351 I print_info: n_expert         = 0
0.00.057.351 I print_info: n_expert_used    = 0
0.00.057.351 I print_info: causal attn      = 1
0.00.057.352 I print_info: pooling type     = 0
0.00.057.352 I print_info: rope type        = 2
0.00.057.352 I print_info: rope scaling     = linear
0.00.057.352 I print_info: freq_base_train  = 10000.0
0.00.057.352 I print_info: freq_scale_train = 1
0.00.057.353 I print_info: n_ctx_orig_yarn  = 2048
0.00.057.353 I print_info: rope_finetuned   = unknown
0.00.057.353 I print_info: ssm_d_conv       = 0
0.00.057.353 I print_info: ssm_d_inner      = 0
0.00.057.353 I print_info: ssm_d_state      = 0
0.00.057.353 I print_info: ssm_dt_rank      = 0
0.00.057.353 I print_info: ssm_dt_b_c_rms   = 0
0.00.057.354 I print_info: model type       = 1.4B
0.00.057.354 I print_info: model params     = 1.41 B
0.00.057.354 I print_info: general.name     = 1.4B
0.00.057.355 I print_info: vocab type       = BPE
0.00.057.355 I print_info: n_vocab          = 50304
0.00.057.355 I print_info: n_merges         = 50009
0.00.057.355 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.057.355 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.057.355 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.057.357 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.057.358 I print_info: LF token         = 128 'Ä'
0.00.057.358 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.057.358 I print_info: max token length = 1024
0.00.362.888 I load_tensors: offloading 24 repeating layers to GPU
0.00.362.901 I load_tensors: offloading output layer to GPU
0.00.362.901 I load_tensors: offloaded 25/25 layers to GPU
0.00.362.935 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.362.937 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.364.221 I llama_context: n_seq_max     = 1
0.00.364.227 I llama_context: n_ctx         = 128
0.00.364.227 I llama_context: n_ctx_per_seq = 128
0.00.364.228 I llama_context: n_batch       = 128
0.00.364.228 I llama_context: n_ubatch      = 128
0.00.364.229 I llama_context: flash_attn    = 0
0.00.364.231 I llama_context: freq_base     = 10000.0
0.00.364.231 I llama_context: freq_scale    = 1
0.00.364.232 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.364.235 I ggml_metal_init: allocating
0.00.364.314 I ggml_metal_init: found device: Apple M4
0.00.364.327 I ggml_metal_init: picking default device: Apple M4
0.00.366.224 I ggml_metal_init: using embedded metal library
0.00.371.834 I ggml_metal_init: GPU name:   Apple M4
0.00.371.844 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.371.845 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.371.846 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.371.846 I ggml_metal_init: simdgroup reduction   = true
0.00.371.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.371.847 I ggml_metal_init: has residency sets    = true
0.00.371.847 I ggml_metal_init: has bfloat            = true
0.00.371.848 I ggml_metal_init: use bfloat            = true
0.00.371.851 I ggml_metal_init: hasUnifiedMemory      = true
0.00.371.856 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.393.211 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.396.830 I init:      Metal KV buffer size =    24.00 MiB
0.00.396.837 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.396.883 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.400.256 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.400.257 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.400.258 I llama_context: graph nodes  = 967
0.00.400.258 I llama_context: graph splits = 2
0.00.400.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.400.262 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.432.098 I 
0.00.432.182 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.432.205 I perplexity: tokenizing the input ..
0.00.444.015 I perplexity: tokenization took 11.808 ms
0.00.444.031 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.576.199 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.577.545 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.577.558 I llama_perf_context_print:        load time =     422.85 ms
0.00.577.559 I llama_perf_context_print: prompt eval time =     131.62 ms /   128 tokens (    1.03 ms per token,   972.50 tokens per second)
0.00.577.560 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.577.560 I llama_perf_context_print:       total time =     145.47 ms /   129 tokens
0.00.578.116 I ggml_metal_free: deallocating

real	0m0.593s
user	0m0.104s
sys	0m0.093s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.446 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.680 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.686 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.687 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.688 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.688 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.689 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.689 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.690 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.693 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.693 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.693 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.694 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.696 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.697 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.698 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.698 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.644 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.649 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.421 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.422 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.422 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.423 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.423 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.423 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.424 I llama_model_loader: - type  f32:  194 tensors
0.00.025.424 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.424 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.424 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.425 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.425 I print_info: file format = GGUF V3 (latest)
0.00.025.426 I print_info: file type   = Q3_K - Medium
0.00.025.427 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.045.139 I load: special tokens cache size = 25
0.00.051.314 I load: token to piece cache size = 0.2984 MB
0.00.051.317 I print_info: arch             = gptneox
0.00.051.317 I print_info: vocab_only       = 0
0.00.051.317 I print_info: n_ctx_train      = 2048
0.00.051.317 I print_info: n_embd           = 2048
0.00.051.317 I print_info: n_layer          = 24
0.00.051.321 I print_info: n_head           = 16
0.00.051.321 I print_info: n_head_kv        = 16
0.00.051.322 I print_info: n_rot            = 32
0.00.051.322 I print_info: n_swa            = 0
0.00.051.322 I print_info: n_embd_head_k    = 128
0.00.051.322 I print_info: n_embd_head_v    = 128
0.00.051.323 I print_info: n_gqa            = 1
0.00.051.324 I print_info: n_embd_k_gqa     = 2048
0.00.051.325 I print_info: n_embd_v_gqa     = 2048
0.00.051.325 I print_info: f_norm_eps       = 1.0e-05
0.00.051.325 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.326 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.326 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.326 I print_info: f_logit_scale    = 0.0e+00
0.00.051.328 I print_info: n_ff             = 8192
0.00.051.329 I print_info: n_expert         = 0
0.00.051.329 I print_info: n_expert_used    = 0
0.00.051.329 I print_info: causal attn      = 1
0.00.051.329 I print_info: pooling type     = 0
0.00.051.329 I print_info: rope type        = 2
0.00.051.330 I print_info: rope scaling     = linear
0.00.051.331 I print_info: freq_base_train  = 10000.0
0.00.051.332 I print_info: freq_scale_train = 1
0.00.051.332 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.332 I print_info: rope_finetuned   = unknown
0.00.051.332 I print_info: ssm_d_conv       = 0
0.00.051.332 I print_info: ssm_d_inner      = 0
0.00.051.333 I print_info: ssm_d_state      = 0
0.00.051.333 I print_info: ssm_dt_rank      = 0
0.00.051.333 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.333 I print_info: model type       = 1.4B
0.00.051.334 I print_info: model params     = 1.41 B
0.00.051.334 I print_info: general.name     = 1.4B
0.00.051.334 I print_info: vocab type       = BPE
0.00.051.335 I print_info: n_vocab          = 50304
0.00.051.335 I print_info: n_merges         = 50009
0.00.051.335 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.335 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.335 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.336 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.336 I print_info: LF token         = 128 'Ä'
0.00.051.336 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.336 I print_info: max token length = 1024
0.00.451.134 I load_tensors: offloading 24 repeating layers to GPU
0.00.451.152 I load_tensors: offloading output layer to GPU
0.00.451.152 I load_tensors: offloaded 25/25 layers to GPU
0.00.451.185 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.451.186 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.452.645 I llama_context: n_seq_max     = 1
0.00.452.649 I llama_context: n_ctx         = 128
0.00.452.650 I llama_context: n_ctx_per_seq = 128
0.00.452.650 I llama_context: n_batch       = 128
0.00.452.651 I llama_context: n_ubatch      = 128
0.00.452.651 I llama_context: flash_attn    = 0
0.00.452.653 I llama_context: freq_base     = 10000.0
0.00.452.654 I llama_context: freq_scale    = 1
0.00.452.654 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.452.657 I ggml_metal_init: allocating
0.00.452.730 I ggml_metal_init: found device: Apple M4
0.00.452.745 I ggml_metal_init: picking default device: Apple M4
0.00.454.430 I ggml_metal_init: using embedded metal library
0.00.461.247 I ggml_metal_init: GPU name:   Apple M4
0.00.461.253 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.461.254 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.461.255 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.461.255 I ggml_metal_init: simdgroup reduction   = true
0.00.461.256 I ggml_metal_init: simdgroup matrix mul. = true
0.00.461.256 I ggml_metal_init: has residency sets    = true
0.00.461.256 I ggml_metal_init: has bfloat            = true
0.00.461.257 I ggml_metal_init: use bfloat            = true
0.00.461.258 I ggml_metal_init: hasUnifiedMemory      = true
0.00.461.260 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.479.670 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.483.289 I init:      Metal KV buffer size =    24.00 MiB
0.00.483.293 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.483.334 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.486.802 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.486.804 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.486.804 I llama_context: graph nodes  = 967
0.00.486.805 I llama_context: graph splits = 2
0.00.486.808 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.486.808 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.516.800 I 
0.00.516.882 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.516.905 I perplexity: tokenizing the input ..
0.00.526.583 I perplexity: tokenization took 9.676 ms
0.00.526.597 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.658.966 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.660.253 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.660.267 I llama_perf_context_print:        load time =     507.35 ms
0.00.660.268 I llama_perf_context_print: prompt eval time =     132.14 ms /   128 tokens (    1.03 ms per token,   968.68 tokens per second)
0.00.660.269 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.660.269 I llama_perf_context_print:       total time =     143.47 ms /   129 tokens
0.00.660.821 I ggml_metal_free: deallocating

real	0m0.675s
user	0m0.094s
sys	0m0.106s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.083 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.947 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.045 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.045 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.046 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.046 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.047 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.048 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.048 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.048 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.049 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.049 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.050 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.050 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.051 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.052 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.052 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.831 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.923 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.684 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.685 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.685 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.686 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.686 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.687 I llama_model_loader: - type  f32:  194 tensors
0.00.025.687 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.687 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.687 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.688 I print_info: file format = GGUF V3 (latest)
0.00.025.689 I print_info: file type   = Q4_K - Medium
0.00.025.689 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.045.328 I load: special tokens cache size = 25
0.00.051.525 I load: token to piece cache size = 0.2984 MB
0.00.051.527 I print_info: arch             = gptneox
0.00.051.528 I print_info: vocab_only       = 0
0.00.051.528 I print_info: n_ctx_train      = 2048
0.00.051.528 I print_info: n_embd           = 2048
0.00.051.528 I print_info: n_layer          = 24
0.00.051.531 I print_info: n_head           = 16
0.00.051.532 I print_info: n_head_kv        = 16
0.00.051.532 I print_info: n_rot            = 32
0.00.051.533 I print_info: n_swa            = 0
0.00.051.533 I print_info: n_embd_head_k    = 128
0.00.051.533 I print_info: n_embd_head_v    = 128
0.00.051.534 I print_info: n_gqa            = 1
0.00.051.534 I print_info: n_embd_k_gqa     = 2048
0.00.051.535 I print_info: n_embd_v_gqa     = 2048
0.00.051.536 I print_info: f_norm_eps       = 1.0e-05
0.00.051.536 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.051.536 I print_info: f_clamp_kqv      = 0.0e+00
0.00.051.536 I print_info: f_max_alibi_bias = 0.0e+00
0.00.051.536 I print_info: f_logit_scale    = 0.0e+00
0.00.051.537 I print_info: n_ff             = 8192
0.00.051.537 I print_info: n_expert         = 0
0.00.051.537 I print_info: n_expert_used    = 0
0.00.051.537 I print_info: causal attn      = 1
0.00.051.538 I print_info: pooling type     = 0
0.00.051.538 I print_info: rope type        = 2
0.00.051.538 I print_info: rope scaling     = linear
0.00.051.539 I print_info: freq_base_train  = 10000.0
0.00.051.541 I print_info: freq_scale_train = 1
0.00.051.541 I print_info: n_ctx_orig_yarn  = 2048
0.00.051.542 I print_info: rope_finetuned   = unknown
0.00.051.542 I print_info: ssm_d_conv       = 0
0.00.051.542 I print_info: ssm_d_inner      = 0
0.00.051.542 I print_info: ssm_d_state      = 0
0.00.051.542 I print_info: ssm_dt_rank      = 0
0.00.051.542 I print_info: ssm_dt_b_c_rms   = 0
0.00.051.543 I print_info: model type       = 1.4B
0.00.051.543 I print_info: model params     = 1.41 B
0.00.051.543 I print_info: general.name     = 1.4B
0.00.051.544 I print_info: vocab type       = BPE
0.00.051.544 I print_info: n_vocab          = 50304
0.00.051.544 I print_info: n_merges         = 50009
0.00.051.544 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.051.544 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.051.545 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.051.545 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.051.549 I print_info: LF token         = 128 'Ä'
0.00.051.550 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.051.550 I print_info: max token length = 1024
0.00.530.250 I load_tensors: offloading 24 repeating layers to GPU
0.00.530.261 I load_tensors: offloading output layer to GPU
0.00.530.262 I load_tensors: offloaded 25/25 layers to GPU
0.00.530.290 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.530.292 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.531.731 I llama_context: n_seq_max     = 1
0.00.531.737 I llama_context: n_ctx         = 128
0.00.531.738 I llama_context: n_ctx_per_seq = 128
0.00.531.739 I llama_context: n_batch       = 128
0.00.531.739 I llama_context: n_ubatch      = 128
0.00.531.739 I llama_context: flash_attn    = 0
0.00.531.741 I llama_context: freq_base     = 10000.0
0.00.531.742 I llama_context: freq_scale    = 1
0.00.531.742 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.531.745 I ggml_metal_init: allocating
0.00.531.813 I ggml_metal_init: found device: Apple M4
0.00.531.826 I ggml_metal_init: picking default device: Apple M4
0.00.533.483 I ggml_metal_init: using embedded metal library
0.00.538.892 I ggml_metal_init: GPU name:   Apple M4
0.00.538.903 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.538.904 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.538.905 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.538.905 I ggml_metal_init: simdgroup reduction   = true
0.00.538.906 I ggml_metal_init: simdgroup matrix mul. = true
0.00.538.906 I ggml_metal_init: has residency sets    = true
0.00.538.906 I ggml_metal_init: has bfloat            = true
0.00.538.906 I ggml_metal_init: use bfloat            = true
0.00.538.910 I ggml_metal_init: hasUnifiedMemory      = true
0.00.538.915 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.559.680 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.563.224 I init:      Metal KV buffer size =    24.00 MiB
0.00.563.234 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.563.281 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.566.856 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.566.858 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.566.859 I llama_context: graph nodes  = 967
0.00.566.859 I llama_context: graph splits = 2
0.00.566.863 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.566.865 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.594.930 I 
0.00.595.007 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.595.027 I perplexity: tokenizing the input ..
0.00.606.661 I perplexity: tokenization took 11.633 ms
0.00.606.678 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.740.385 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.741.804 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.741.818 I llama_perf_context_print:        load time =     584.97 ms
0.00.741.819 I llama_perf_context_print: prompt eval time =     133.45 ms /   128 tokens (    1.04 ms per token,   959.17 tokens per second)
0.00.741.819 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.820 I llama_perf_context_print:       total time =     146.89 ms /   129 tokens
0.00.742.373 I ggml_metal_free: deallocating

real	0m0.758s
user	0m0.096s
sys	0m0.126s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.098 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.921 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.776 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.780 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.786 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.786 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.787 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.787 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.787 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.788 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.788 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.789 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.789 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.789 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.790 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.790 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.792 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.793 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.548 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.593 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.424 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.425 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.426 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.426 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.426 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.427 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.427 I llama_model_loader: - type  f32:  194 tensors
0.00.024.428 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.428 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.428 I print_info: file format = GGUF V3 (latest)
0.00.024.429 I print_info: file type   = Q5_K - Medium
0.00.024.430 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.043.197 I load: special tokens cache size = 25
0.00.049.059 I load: token to piece cache size = 0.2984 MB
0.00.049.062 I print_info: arch             = gptneox
0.00.049.063 I print_info: vocab_only       = 0
0.00.049.063 I print_info: n_ctx_train      = 2048
0.00.049.063 I print_info: n_embd           = 2048
0.00.049.063 I print_info: n_layer          = 24
0.00.049.066 I print_info: n_head           = 16
0.00.049.067 I print_info: n_head_kv        = 16
0.00.049.067 I print_info: n_rot            = 32
0.00.049.067 I print_info: n_swa            = 0
0.00.049.068 I print_info: n_embd_head_k    = 128
0.00.049.068 I print_info: n_embd_head_v    = 128
0.00.049.069 I print_info: n_gqa            = 1
0.00.049.069 I print_info: n_embd_k_gqa     = 2048
0.00.049.070 I print_info: n_embd_v_gqa     = 2048
0.00.049.071 I print_info: f_norm_eps       = 1.0e-05
0.00.049.071 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.049.071 I print_info: f_clamp_kqv      = 0.0e+00
0.00.049.071 I print_info: f_max_alibi_bias = 0.0e+00
0.00.049.072 I print_info: f_logit_scale    = 0.0e+00
0.00.049.072 I print_info: n_ff             = 8192
0.00.049.073 I print_info: n_expert         = 0
0.00.049.073 I print_info: n_expert_used    = 0
0.00.049.073 I print_info: causal attn      = 1
0.00.049.073 I print_info: pooling type     = 0
0.00.049.073 I print_info: rope type        = 2
0.00.049.073 I print_info: rope scaling     = linear
0.00.049.074 I print_info: freq_base_train  = 10000.0
0.00.049.074 I print_info: freq_scale_train = 1
0.00.049.076 I print_info: n_ctx_orig_yarn  = 2048
0.00.049.076 I print_info: rope_finetuned   = unknown
0.00.049.077 I print_info: ssm_d_conv       = 0
0.00.049.077 I print_info: ssm_d_inner      = 0
0.00.049.077 I print_info: ssm_d_state      = 0
0.00.049.077 I print_info: ssm_dt_rank      = 0
0.00.049.077 I print_info: ssm_dt_b_c_rms   = 0
0.00.049.077 I print_info: model type       = 1.4B
0.00.049.078 I print_info: model params     = 1.41 B
0.00.049.078 I print_info: general.name     = 1.4B
0.00.049.078 I print_info: vocab type       = BPE
0.00.049.078 I print_info: n_vocab          = 50304
0.00.049.079 I print_info: n_merges         = 50009
0.00.049.082 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.049.083 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.049.083 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.049.083 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.049.083 I print_info: LF token         = 128 'Ä'
0.00.049.084 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.049.084 I print_info: max token length = 1024
0.00.608.988 I load_tensors: offloading 24 repeating layers to GPU
0.00.609.001 I load_tensors: offloading output layer to GPU
0.00.609.002 I load_tensors: offloaded 25/25 layers to GPU
0.00.609.036 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.609.037 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.610.633 I llama_context: n_seq_max     = 1
0.00.610.638 I llama_context: n_ctx         = 128
0.00.610.639 I llama_context: n_ctx_per_seq = 128
0.00.610.640 I llama_context: n_batch       = 128
0.00.610.640 I llama_context: n_ubatch      = 128
0.00.610.641 I llama_context: flash_attn    = 0
0.00.610.643 I llama_context: freq_base     = 10000.0
0.00.610.644 I llama_context: freq_scale    = 1
0.00.610.644 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.610.647 I ggml_metal_init: allocating
0.00.610.725 I ggml_metal_init: found device: Apple M4
0.00.610.739 I ggml_metal_init: picking default device: Apple M4
0.00.612.403 I ggml_metal_init: using embedded metal library
0.00.618.929 I ggml_metal_init: GPU name:   Apple M4
0.00.618.933 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.618.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.618.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.618.935 I ggml_metal_init: simdgroup reduction   = true
0.00.618.936 I ggml_metal_init: simdgroup matrix mul. = true
0.00.618.936 I ggml_metal_init: has residency sets    = true
0.00.618.936 I ggml_metal_init: has bfloat            = true
0.00.618.936 I ggml_metal_init: use bfloat            = true
0.00.618.937 I ggml_metal_init: hasUnifiedMemory      = true
0.00.618.940 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.636.445 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.639.988 I init:      Metal KV buffer size =    24.00 MiB
0.00.639.991 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.640.040 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.643.122 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.643.124 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.643.124 I llama_context: graph nodes  = 967
0.00.643.124 I llama_context: graph splits = 2
0.00.643.128 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.643.128 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.674.090 I 
0.00.674.163 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.674.185 I perplexity: tokenizing the input ..
0.00.683.481 I perplexity: tokenization took 9.294 ms
0.00.683.499 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.823.233 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.824.561 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.824.572 I llama_perf_context_print:        load time =     665.16 ms
0.00.824.573 I llama_perf_context_print: prompt eval time =     139.48 ms /   128 tokens (    1.09 ms per token,   917.71 tokens per second)
0.00.824.574 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.824.575 I llama_perf_context_print:       total time =     150.49 ms /   129 tokens
0.00.825.167 I ggml_metal_free: deallocating

real	0m0.839s
user	0m0.092s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.091 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.095 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.800 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.805 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.811 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.812 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.812 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.814 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.814 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.815 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.816 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.816 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.819 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.820 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.820 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.821 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.822 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.823 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.823 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.660 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.754 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.545 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.546 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.547 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.547 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.547 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.548 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.548 I llama_model_loader: - type  f32:  194 tensors
0.00.024.549 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.549 I print_info: file format = GGUF V3 (latest)
0.00.024.550 I print_info: file type   = Q6_K
0.00.024.554 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.044.168 I load: special tokens cache size = 25
0.00.050.290 I load: token to piece cache size = 0.2984 MB
0.00.050.293 I print_info: arch             = gptneox
0.00.050.293 I print_info: vocab_only       = 0
0.00.050.293 I print_info: n_ctx_train      = 2048
0.00.050.294 I print_info: n_embd           = 2048
0.00.050.294 I print_info: n_layer          = 24
0.00.050.297 I print_info: n_head           = 16
0.00.050.298 I print_info: n_head_kv        = 16
0.00.050.298 I print_info: n_rot            = 32
0.00.050.298 I print_info: n_swa            = 0
0.00.050.298 I print_info: n_embd_head_k    = 128
0.00.050.299 I print_info: n_embd_head_v    = 128
0.00.050.299 I print_info: n_gqa            = 1
0.00.050.300 I print_info: n_embd_k_gqa     = 2048
0.00.050.301 I print_info: n_embd_v_gqa     = 2048
0.00.050.301 I print_info: f_norm_eps       = 1.0e-05
0.00.050.304 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.304 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.304 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.304 I print_info: f_logit_scale    = 0.0e+00
0.00.050.305 I print_info: n_ff             = 8192
0.00.050.305 I print_info: n_expert         = 0
0.00.050.305 I print_info: n_expert_used    = 0
0.00.050.305 I print_info: causal attn      = 1
0.00.050.306 I print_info: pooling type     = 0
0.00.050.306 I print_info: rope type        = 2
0.00.050.306 I print_info: rope scaling     = linear
0.00.050.306 I print_info: freq_base_train  = 10000.0
0.00.050.307 I print_info: freq_scale_train = 1
0.00.050.307 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.307 I print_info: rope_finetuned   = unknown
0.00.050.307 I print_info: ssm_d_conv       = 0
0.00.050.307 I print_info: ssm_d_inner      = 0
0.00.050.308 I print_info: ssm_d_state      = 0
0.00.050.308 I print_info: ssm_dt_rank      = 0
0.00.050.308 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.308 I print_info: model type       = 1.4B
0.00.050.309 I print_info: model params     = 1.41 B
0.00.050.309 I print_info: general.name     = 1.4B
0.00.050.309 I print_info: vocab type       = BPE
0.00.050.310 I print_info: n_vocab          = 50304
0.00.050.310 I print_info: n_merges         = 50009
0.00.050.310 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.310 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.310 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.312 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.312 I print_info: LF token         = 128 'Ä'
0.00.050.313 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.313 I print_info: max token length = 1024
0.00.467.130 I load_tensors: offloading 24 repeating layers to GPU
0.00.467.135 I load_tensors: offloading output layer to GPU
0.00.467.136 I load_tensors: offloaded 25/25 layers to GPU
0.00.467.163 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.467.166 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.468.523 I llama_context: n_seq_max     = 1
0.00.468.525 I llama_context: n_ctx         = 128
0.00.468.526 I llama_context: n_ctx_per_seq = 128
0.00.468.526 I llama_context: n_batch       = 128
0.00.468.529 I llama_context: n_ubatch      = 128
0.00.468.529 I llama_context: flash_attn    = 0
0.00.468.530 I llama_context: freq_base     = 10000.0
0.00.468.530 I llama_context: freq_scale    = 1
0.00.468.531 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.468.533 I ggml_metal_init: allocating
0.00.468.589 I ggml_metal_init: found device: Apple M4
0.00.468.599 I ggml_metal_init: picking default device: Apple M4
0.00.469.922 I ggml_metal_init: using embedded metal library
0.00.475.460 I ggml_metal_init: GPU name:   Apple M4
0.00.475.463 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.475.464 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.475.464 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.475.465 I ggml_metal_init: simdgroup reduction   = true
0.00.475.465 I ggml_metal_init: simdgroup matrix mul. = true
0.00.475.465 I ggml_metal_init: has residency sets    = true
0.00.475.465 I ggml_metal_init: has bfloat            = true
0.00.475.466 I ggml_metal_init: use bfloat            = true
0.00.475.467 I ggml_metal_init: hasUnifiedMemory      = true
0.00.475.468 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.491.097 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.494.622 I init:      Metal KV buffer size =    24.00 MiB
0.00.494.627 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.494.669 I llama_context:        CPU  output buffer size =     0.19 MiB
0.00.497.733 I llama_context:      Metal compute buffer size =    25.56 MiB
0.00.497.734 I llama_context:        CPU compute buffer size =     1.06 MiB
0.00.497.735 I llama_context: graph nodes  = 967
0.00.497.735 I llama_context: graph splits = 2
0.00.497.738 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.497.738 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.532.547 I 
0.00.532.630 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.532.653 I perplexity: tokenizing the input ..
0.00.541.535 I perplexity: tokenization took 8.88 ms
0.00.541.548 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.680.403 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.681.891 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.681.906 I llama_perf_context_print:        load time =     523.44 ms
0.00.681.907 I llama_perf_context_print: prompt eval time =     138.62 ms /   128 tokens (    1.08 ms per token,   923.35 tokens per second)
0.00.681.908 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.681.908 I llama_perf_context_print:       total time =     149.37 ms /   129 tokens
0.00.682.445 I ggml_metal_free: deallocating

real	0m0.696s
user	0m0.089s
sys	0m0.113s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.253 I build: 4584 (e665b57f) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.025.956 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.041.025 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.041.031 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.041.033 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.041.033 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.041.034 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.041.038 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.041.039 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.041.040 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.041.040 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.041.042 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.041.042 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.041.042 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.041.043 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.041.043 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.041.045 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.041.045 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.041.045 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.049.116 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.051.282 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.058.210 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.058.212 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.058.212 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.058.213 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.058.213 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.058.214 I llama_model_loader: - type  f32:  194 tensors
0.00.058.215 I llama_model_loader: - type  f16:   98 tensors
0.00.058.216 I print_info: file format = GGUF V3 (latest)
0.00.058.216 I print_info: file type   = all F32 (guessed)
0.00.058.218 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.084.953 I load: special tokens cache size = 25
0.00.091.667 I load: token to piece cache size = 0.2984 MB
0.00.091.670 I print_info: arch             = gptneox
0.00.091.670 I print_info: vocab_only       = 0
0.00.091.670 I print_info: n_ctx_train      = 2048
0.00.091.671 I print_info: n_embd           = 2048
0.00.091.671 I print_info: n_layer          = 24
0.00.091.673 I print_info: n_head           = 16
0.00.091.674 I print_info: n_head_kv        = 16
0.00.091.674 I print_info: n_rot            = 32
0.00.091.674 I print_info: n_swa            = 0
0.00.091.674 I print_info: n_embd_head_k    = 128
0.00.091.675 I print_info: n_embd_head_v    = 128
0.00.091.675 I print_info: n_gqa            = 1
0.00.091.676 I print_info: n_embd_k_gqa     = 2048
0.00.091.677 I print_info: n_embd_v_gqa     = 2048
0.00.091.677 I print_info: f_norm_eps       = 1.0e-05
0.00.091.677 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.678 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.678 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.678 I print_info: f_logit_scale    = 0.0e+00
0.00.091.679 I print_info: n_ff             = 8192
0.00.091.681 I print_info: n_expert         = 0
0.00.091.681 I print_info: n_expert_used    = 0
0.00.091.681 I print_info: causal attn      = 1
0.00.091.681 I print_info: pooling type     = 0
0.00.091.681 I print_info: rope type        = 2
0.00.091.681 I print_info: rope scaling     = linear
0.00.091.682 I print_info: freq_base_train  = 10000.0
0.00.091.682 I print_info: freq_scale_train = 1
0.00.091.682 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.682 I print_info: rope_finetuned   = unknown
0.00.091.682 I print_info: ssm_d_conv       = 0
0.00.091.683 I print_info: ssm_d_inner      = 0
0.00.091.683 I print_info: ssm_d_state      = 0
0.00.091.683 I print_info: ssm_dt_rank      = 0
0.00.091.683 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.683 I print_info: model type       = 1.4B
0.00.091.684 I print_info: model params     = 1.41 B
0.00.091.684 I print_info: general.name     = 1.4B
0.00.091.684 I print_info: vocab type       = BPE
0.00.091.684 I print_info: n_vocab          = 50304
0.00.091.685 I print_info: n_merges         = 50009
0.00.091.685 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.685 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.685 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.685 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.685 I print_info: LF token         = 128 'Ä'
0.00.091.686 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.686 I print_info: max token length = 1024
0.01.324.232 I load_tensors: offloading 24 repeating layers to GPU
0.01.324.237 I load_tensors: offloading output layer to GPU
0.01.324.237 I load_tensors: offloaded 25/25 layers to GPU
0.01.324.263 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.324.264 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.325.344 I llama_context: n_seq_max     = 1
0.01.325.345 I llama_context: n_ctx         = 128
0.01.325.345 I llama_context: n_ctx_per_seq = 128
0.01.325.345 I llama_context: n_batch       = 128
0.01.325.346 I llama_context: n_ubatch      = 128
0.01.325.349 I llama_context: flash_attn    = 0
0.01.325.352 I llama_context: freq_base     = 10000.0
0.01.325.353 I llama_context: freq_scale    = 1
0.01.325.353 W llama_context: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.325.355 I ggml_metal_init: allocating
0.01.325.429 I ggml_metal_init: found device: Apple M4
0.01.325.434 I ggml_metal_init: picking default device: Apple M4
0.01.326.546 I ggml_metal_init: using embedded metal library
0.01.330.307 I ggml_metal_init: GPU name:   Apple M4
0.01.330.309 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.330.310 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.330.310 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.330.311 I ggml_metal_init: simdgroup reduction   = true
0.01.330.311 I ggml_metal_init: simdgroup matrix mul. = true
0.01.330.311 I ggml_metal_init: has residency sets    = true
0.01.330.311 I ggml_metal_init: has bfloat            = true
0.01.330.311 I ggml_metal_init: use bfloat            = true
0.01.330.312 I ggml_metal_init: hasUnifiedMemory      = true
0.01.330.313 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.340.612 I init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.342.285 I init:      Metal KV buffer size =    24.00 MiB
0.01.342.287 I llama_context: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.342.311 I llama_context:        CPU  output buffer size =     0.19 MiB
0.01.343.910 I llama_context:      Metal compute buffer size =    25.56 MiB
0.01.343.912 I llama_context:        CPU compute buffer size =     1.06 MiB
0.01.343.912 I llama_context: graph nodes  = 967
0.01.343.912 I llama_context: graph splits = 2
0.01.343.914 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.343.914 I 
0.01.343.951 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.343.952 I compute_imatrix: tokenizing the input ..
0.01.351.655 I compute_imatrix: tokenization took 7.703 ms
0.01.351.657 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.620.072 I compute_imatrix: 0.27 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.622.668 I llama_perf_context_print:        load time =    1594.11 ms
0.01.622.669 I llama_perf_context_print: prompt eval time =     266.68 ms /   128 tokens (    2.08 ms per token,   479.97 tokens per second)
0.01.622.669 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.622.670 I llama_perf_context_print:       total time =    1596.70 ms /   129 tokens
0.01.623.354 I ggml_metal_free: deallocating

real	0m1.811s
user	0m0.145s
sys	0m0.239s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4584 (e665b57f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15d404a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15d405190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15d405740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15d405cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15d4062a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15d406850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15d406e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15d4073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15d407960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15d407e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15d408360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15d408860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15d409380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15d409b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15d40a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15d40aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15d40b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15d40b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15d40bfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15d40c790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15d40ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15d40d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15d40dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15d40e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15d40ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15d40ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15d40f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15d4101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15d410730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15d4109f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15d410e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15d411150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15d4119e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15d411f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15d4121e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15d412680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15d412b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15d412fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15d413460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15d413900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15d413da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15d414240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15d4146e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15d414b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15d414e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15d415450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15d415a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15d416380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15d416990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15d416fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15d4175b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15d417bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15d4181d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15d4187e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15d418fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15d419470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15d419910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15d419bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15d41a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15d41a9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15d41ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15d41b130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15d41b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15d41ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15d41bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15d41c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15d41c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15d41ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15d41d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15d41d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15d41dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15d41df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15d41e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15d41e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15d41eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15d41f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15d41f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15d41fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15d4203f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15d420940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15d420e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15d4213e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15d421930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15d421e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15d4223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15d422920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15d422e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15d4233c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15d423910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15d423e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15d4243b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15d424900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15d424e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15d4253a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15d4258f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15d425e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15d426390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15d416070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15d426800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15d426fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15d427500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15d427a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15d427fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15d4284f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15d428a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15d428f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15d4294e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15d429a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15d429f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15d42a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15d42aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15d42af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15d42b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15d42b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15d42be00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15d42c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15d42c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15d42cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15d42d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15d42d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15d42d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15d42de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15d42e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15d42e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15d42ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15d42f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15d42f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15d42fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15d42fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15d430360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15d430800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15d430ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x15d431140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15d4315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15d431a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x15d431f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15d4323c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15d432860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x15d432d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15d4331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15d433640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15d433ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15d433f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15d434420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15d4348c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15d434d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15d435200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15d4356a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15d435b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15d435fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15d436480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15d436920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15d436dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15d437260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15d437700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15d437ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15d438040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15d4384e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15d438980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15d438e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15d4392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15d439760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15d439c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15d43a0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15d43a540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15d43a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15d43ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15d43b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15d43b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15d43bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15d43c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15d43c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15d43ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15d43cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15d43d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15d43d820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15d43dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15d43e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15d43e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15d43eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15d43ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15d43f3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15d43f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15d43fd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15d4401c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15d440660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15d440b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15d440fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15d441440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15d4418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15d441d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15d442220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15d4426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15d442c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15d443160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15d4436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15d443c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15d443ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15d4444d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15d444ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15d4450f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15d4458e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15d445d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15d446040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15d446650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15d446c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15d447450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15d4478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15d447d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15d448230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15d4489e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15d448f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15d449480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15d4499d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15d449f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15d44a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15d44a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15d44af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15d44b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15d44b9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15d44bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15d44c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15d44c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15d44cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15d44d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15d44d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15d44dee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15d44e430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15d44e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15d44eed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15d44f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15d44f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15d44fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15d450410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15d450960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15d450eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15d451400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15d451950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15d451ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15d4523f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15d452940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15d452e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15d4533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15d453930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15d453e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15d4543d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15d454920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15d454e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15d4553c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15d455910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15d455e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15d4563b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15d456900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15d456e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15d4573a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15d4578f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15d457e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15d458390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15d4588e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15d458e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15d459380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15d4598d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15d459e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15d45a370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15d45a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15d45ae10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15d45b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15d45b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15d45bca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15d45c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15d45c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15d45ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15d45cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15d45d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15d45d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15d45dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15d45e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15d45e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15d45eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15d45ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15d45f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15d45f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15d45fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15d460530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15d460c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15d461370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15d461a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15d461d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15d462540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15d462800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15d462e10 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
0.00.745.399 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.745.403 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x14be05340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x14be057b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x14be05c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x14be06090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x14be06500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x14be06970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x14be06de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x14be07250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x14be076c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x14be07b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x14be07fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x14be08690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x14be091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x14be09960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x14be0a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14be0a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14be0afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14be0b6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14be0bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14be0c520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14be0cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14be0d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14be0da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14be0e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14be0e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14be0eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14be0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14be0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14be0f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14be0fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14be10000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14be10530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x14be109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x14be10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x14be110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x14be11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x14be119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x14be11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x14be12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x14be12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x14be12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x14be12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x14be13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x14be138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x14be13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x14be141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x14be14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x14be14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x14be14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x14be15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x14be157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x14be15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x14be160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x14be16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x14be16990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x14be16e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x14be17370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x14be17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x14be17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x14be18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x14be185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x14be18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x14be18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x14be19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x14be19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x14be19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x14be1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x14be1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14be1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14be1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14be1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14be1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14be1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14be1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14be1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14be1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14be1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14be1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14be1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14be1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14be1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14be1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14be1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14be1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14be1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14be1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14be1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14be1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14be20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14be20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x14be20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x14be20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x14be213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x14be21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x14be21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x14be22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x14be22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x14be229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x14be22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x14be232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x14be23740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x14be23bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x14be24020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x14be24490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x14be24900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x14be24d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x14be251e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x14be25650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x14be25ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x14be25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x14be263a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x14be26810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x14be26c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x14be270f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x14be27560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x14be279d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x14be27e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x14be282b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x14be28720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x14be28b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x14be29000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x14be29470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x14be298e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x14be29d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x14be2a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x14be2a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14be2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14be2af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14be2b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14be2b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14be2bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14be2c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14be2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14be2c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14be2ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14be2d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14be2d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14be2db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14be2dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14be2e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14be2e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14be2ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14be2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14be2f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14be2fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14be2fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14be30360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14be307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14be30c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14be310b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14be31520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x14be31990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x14be31e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x14be32270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x14be326e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x14be32b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x14be32fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x14be33430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x14be338a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x14be33d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x14be34180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x14be345f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x14be34a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x14be34ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x14be35340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x14be357b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x14be363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x14be366a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x14be36960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x14be36dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x14be37240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x14be376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x14be37b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x14be37f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x14be38400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x14be38870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x14be38ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15d446300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15d444790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15d462ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15d444180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15d444da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15d417e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15d417870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15d419e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15d446910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15d40f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15d415d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15d416640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15d416c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15d415710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15d415100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15d418490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15d417260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15d40e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15d408b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15d41a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15d426ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15d462010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15d411410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15d4116d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15d446f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15d4453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15d40f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15d40fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15d40fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15d463270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15d463530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15d4637f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15d463ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15d464080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15d464340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15d464600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15d4648c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15d464b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15d464e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15d465100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15d4653c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15d465680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15d465940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15d465c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15d465ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15d466180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15d466440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15d466700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15d4669c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15d466c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15d466f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15d467200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15d4674c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15d467780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15d467a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15d467d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15d467fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15d468280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15d468540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15d468800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15d468ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15d468d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15d469040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15d469300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15d4695c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15d469880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15d469b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15d469e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15d46a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15d46a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15d46a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15d46a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15d46abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15d46ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15d46b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15d46b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15d46b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15d46b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15d46bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15d46bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15d46c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15d46c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15d46c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15d46ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15d46ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15d46cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15d46d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15d46d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15d46d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15d46da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15d46dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15d46e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15d46e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15d46e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15d46e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15d46eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15d46edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15d46f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15d46f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15d46f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15d46f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15d46fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15d46fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15d470100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15d4703c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15d470680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15d470940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15d470c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15d470ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15d471180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15d471440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15d471700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15d4719c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15d471c80 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 0
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15d4721f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15d4724b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15d4728f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15d472bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15d472e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15d473130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15d4733f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15d4736b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15d473970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15d473c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15d473ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15d4741b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15d474780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15d474d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15d475380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15d475640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15d475900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15d475bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15d475e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15d476140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15d476400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15d4766c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x15d476980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x15d476c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x15d476f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x15d4771c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15d477480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x15d477740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15d477a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x15d477cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15d477f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x15d478240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x15d478500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x15d4787c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15d478a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15d478d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15d479000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15d4792c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15d479580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15d479840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15d479b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15d479dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15d47a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15d47a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15d47a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15d47a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15d47ab80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15d47ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15d47b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15d47b3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15d47b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15d47b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15d47bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15d47bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15d47c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15d47c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15d47c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15d47c9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15d47cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15d47cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15d47d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15d47d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15d47d780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15d47da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15d47dd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15d47dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15d47e280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15d47e540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15d47e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15d47eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15d47ed80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15d47f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15d47f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15d47f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15d47f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15d47fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15d47fe00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x15d4800c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x15d480380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15d480640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15d480900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x15d480bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x15d480e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15d481140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15d481400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15d4816c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15d481980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15d481c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15d481f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15d4821c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15d482480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15d482740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15d482a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15d482cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15d482f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15d483240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15d483500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15d4837c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15d483a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15d483d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15d484000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15d4842c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15d484580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15d484840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15d484b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15d484dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15d485080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15d485340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15d485600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15d4858c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15d485b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15d485e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15d486100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15d4863c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15d486680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15d486940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15d486c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15d486ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15d487180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15d487440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15d487700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15d4879c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15d487c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15d487f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15d488200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15d4884c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15d488780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15d488a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15d488d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x15d488fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14be08950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14be35a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14be04df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14be38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14be0c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14be393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14be39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14be39940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14be39c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14be39ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14be3a180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14be3a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14be3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14be3a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14be3ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14be3af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14be3b200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x14be3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x14be3b780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x14be3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x14be3bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15d489280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15d489540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15d489800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15d489ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15d489d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15d48a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15d48a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15d48a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15d48a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15d48ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15d48ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15d48b0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15d48b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15d48b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15d48b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15d48bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15d48be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15d48c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15d48c400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15d48c6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15d48c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15d48cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15d48cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15d48d1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15d48d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15d48d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x15d48da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15d48dcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15d48df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x15d48e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15d48e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15d48e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15d48ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15d48ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15d48f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15d48f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15d48f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x15d48f840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15d48fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15d48fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15d490080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15d490340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x15d490600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15d4908c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15d490b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15d490e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15d491100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15d4913c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15d491680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15d491940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15d491c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15d491ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15d492180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15d492440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15d492700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15d4929c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15d492c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15d492f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15d493200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15d4937d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15d493a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15d493d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15d494010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15d4942d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15d494590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15d494850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15d494b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15d494dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15d495090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15d495350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15d495610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15d495b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15d4960b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15d496600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15d496b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x15d4970a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15d4975f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15d497b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x15d498090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15d4985e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15d498b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15d499080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15d4995d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15d499b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15d49a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x15d49a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x15d49ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15d49b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x15d49b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15d49bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15d49c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15d49c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15d49caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15d49d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15d49d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15d49dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15d49e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15d49e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15d49ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15d49f020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15d49f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15d49fac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15d4a0010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15d4a0560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15d4a0ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15d4a1000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15d4a1550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15d4a1aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15d4a1ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15d4a2540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15d4a2a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15d4a2fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15d4a3530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15d4a3a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15d4a3fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15d4a4520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15d4a47e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15d4a4aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15d4a4fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15d4a54a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15d4a59a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15d4a5ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15d4a63a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15d4a68a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x15d4a6da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15d4a72a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15d4a77a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15d4a7ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15d4a81a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15d4a86a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15d4a8ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x15d4a90a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x15d4a9ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x15d4aa1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x15d4aa8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x15d4ab010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x15d4ab2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x15d4abac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x15d4abd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15d4ac390 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 967
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.794s
user	0m0.299s
sys	0m0.332s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4584 (e665b57f)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 128 'Ä'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131707110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1317078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131707e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131708400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1317089b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x131708f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131709510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131709ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13170a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13170a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13170aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13170af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13170ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13170c240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13170ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x13170d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13170d890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13170dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13170e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13170eea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13170f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13170fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131710400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131710ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1317113c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131711680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131711c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131712900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131712e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131713100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1317135a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x131713860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x1317140f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131714630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1317148f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131714d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131715230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1317156d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131715b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x131716010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1317164b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131716950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131716df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x131717290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131717550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131717b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131718170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131718a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1317190a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1317196b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131719cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13171a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13171a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13171aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13171b6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13171bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13171c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13171c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13171c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13171d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13171d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13171d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13171dce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13171e180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13171e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13171eac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13171ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13171f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13171f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13171fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1317201e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x131720680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x131720b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131721070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1317215c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x131721b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x131722060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1317225b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131722b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131723050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1317235a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131723af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131724040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x131724590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x131724ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131725030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131725580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x131725ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x131726020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131726570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131726ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131727010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131727560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131727ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131728000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x131728550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131728aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131718780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131728f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1317296c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131729c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13172a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13172a6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13172ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13172b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13172b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13172bbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13172c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13172c690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13172cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x13172d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x13172d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x13172dbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x13172e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x13172e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x13172e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x13172ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x13172f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x13172f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x13172fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1317300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131730570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131730a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131730eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131731350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1317317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131731c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131732130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1317325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131732a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131732f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1317333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131733850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131733cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131734190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131734630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131734ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131734f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131735410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1317358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131735d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x1317361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131736690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x131736b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131736fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131737470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131737910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x131737db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x131738250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1317386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x131738b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x131739030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1317394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x131739970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x131739e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13173a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13173a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13173abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13173b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13173b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13173b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13173be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13173c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13173c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13173cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13173d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13173d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13173da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13173ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13173e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13173e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13173ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13173f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13173f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13173fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13173ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1317403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x131740870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x131740d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1317411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x131741650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x131741af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131741f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x131742430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1317428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131742d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131743210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1317436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131743b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131743ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131744490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x131744930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131744dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131745320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131745870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x131745dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131746310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1317465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131746be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1317471f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131747800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131747ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x131748490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x131748750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x131748d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x131749370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x131749b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13174a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13174a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13174a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13174b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13174b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13174bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13174c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13174c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13174cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13174d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x13174d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x13174db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x13174e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x13174e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x13174eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x13174f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x13174f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x13174fb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1317500a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1317505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x131750b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x131751090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1317515e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x131751b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x131752080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1317525d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x131752b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x131753070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1317535c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x131753b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x131754060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1317545b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x131754b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x131755050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1317555a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x131755af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x131756040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x131756590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x131756ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x131757030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x131757580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x131757ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x131758020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x131758570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x131758ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x131759010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x131759560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x131759ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13175a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13175a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13175aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13175aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13175b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13175ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13175bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13175c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13175ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13175cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13175d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13175da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13175df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13175e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13175e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13175ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13175f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13175f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13175fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x13175ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x131760410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1317608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x131760d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1317611f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x131761690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x131761b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x131761fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x131762520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x131762c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x131763360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x131763a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1317641a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x131764460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x131764c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x131764f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x131765520 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
0.00.111.262 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.111.266 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131746ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131748a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1317651d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131746890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1317474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13171a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x131719f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13171c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131749020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x131711940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x131718430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131718d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131717e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13171aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131719970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131710940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131706740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13171b1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13171cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1317291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x131764720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131713b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131713de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x131749630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x131747ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131711f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131712210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x1317124d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131765980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131765c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131765f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1317661c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131766480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131766740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x131766a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x131766cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x131766f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x131767240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x131767500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1317677c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x131767a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x131767d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x131768000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1317682c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x131768580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x131768840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x131768b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x131768dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x131769080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x131769340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x131769600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1317698c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x131769b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x131769e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13176a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13176a3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13176a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13176a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13176ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13176aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13176b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13176b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13176b700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13176b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13176bc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13176bf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13176c200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13176c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13176c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x13176ca40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13176cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13176cfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13176d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13176d540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13176d800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13176dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13176dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13176e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13176e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13176e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13176e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13176eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13176ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13176f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13176f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13176f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13176f900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13176fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13176fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131770140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131770400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x1317706c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131770980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131770c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131770f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1317711c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131771480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131771740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131771a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x131771cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x131771f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131772240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131772500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x1317727c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x131772a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131772d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131773000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1317732c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131773580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131773840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131773b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x131773dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x131774080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131774340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131774600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1317748c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x131774b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131774e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131775100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1317753c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131775680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131775940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131775c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x131775ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x131776180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131776440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131776700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x1317769c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x131776c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x131776f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x131777200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1317774c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x131777780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x131777a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x131777d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x131777fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x131778280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x131778540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x131778800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x131778ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x131778d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x131779040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x131779300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1317795c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x131779880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x131779b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x131779e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13177a0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13177a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13177a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13177a900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13177abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13177ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13177b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13177b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13177b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13177b980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13177bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13177bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13177c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13177c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13177c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13177ca00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13177ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13177cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x13177d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x13177d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x13177d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x13177da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x13177dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x13177e000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x13177e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x13177e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x13177e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x13177eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x13177edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x13177f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x13177f340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x13177f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x13177f8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x13177fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x13177fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x131780100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1317803c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x131780680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x131780940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x131780c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x131780ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x131781180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x131781440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x131781700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1317819c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x131781c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x131781f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x131782200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1317824c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x131782780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x131782a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x131782d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x131782fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x131783280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x131783540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13290bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13290c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13290c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13290cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13290cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13290d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13290d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13290dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13290e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13290eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13290edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13290f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13290f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13290fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x13290ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1329103f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x132910860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x132910cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x132911140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1329115b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x132911a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x132911e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x132912300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x132912770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x132912be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x132913050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1329134c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x132913930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x132913da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x132914210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x132914680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x132914af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x132914f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1329153d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x132915840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x132915cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x132916120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x132916590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x132916a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x132916e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1329172e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x132917750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x132917bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x132918030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1329184a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x132918910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x132918d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1329191f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x132919660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x132919ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x132919f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13291a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13291a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13291ac90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13291b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13291b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13291b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13291be50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13291c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13291c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13291cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13291d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x13291d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x13291d8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x13291dd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x13291e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x13291e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x13291eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x13291ef20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x13291f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x13291f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x13291fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1329200e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x132920550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1329209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x132920e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1329212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x132921710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x132921b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x132921ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x132922460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x132922ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1329235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x132923d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x132924430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1329246f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x132924b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x132925160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x132925770 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_context: n_seq_max     = 1
llama_context: n_ctx         = 2048
llama_context: n_ctx_per_seq = 2048
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: flash_attn    = 1
llama_context: freq_base     = 10000.0
llama_context: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x131783800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x131783ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x131783ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x131784160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x131784420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1317846e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1317849a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x131784c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x131784f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1317851e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1317854a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x131785760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x131785d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x131786300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x131786930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x131786bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x131786eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x131787170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x131787430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x1317876f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1317879b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x131787c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x131787f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x1317881f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1317884b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x131788770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x131788a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x131788cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x131788fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x131789270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x131789530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1317897f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x131789ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x131789d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x13178a030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x13178a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x13178a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x13178a870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x13178ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x13178adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x13178b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x13178b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x13178b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13178b8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13178bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13178be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13178c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13178c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13178c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13178c970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13178cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13178cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13178d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13178d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13178d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13178d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13178dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13178df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13178e230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x13178e4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x13178e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x13178ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x13178ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x13178eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x13178f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x13178f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x13178f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x13178faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x13178fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x131790070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x131790330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1317905f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1317908b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x131790b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x131790e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1317910f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1317913b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x131791670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x131791930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x131791bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x131791eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x131792170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x131792430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1317926f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1317929b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x131792c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x131792f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1317931f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x1317934b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x131793770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x131793a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x131793cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x131793fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x131794270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x131794530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1317947f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x131794ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x131794d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x131795030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1317952f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1317955b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x131795870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x131795b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x131795df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1317960b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x131796370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x131796630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1317968f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x131796bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x131796e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x131797130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1317973f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1317976b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x131797970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x131797c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x131797ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1317981b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x131798470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x131798730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1317989f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x131798cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x131798f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x131799230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1317994f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1317997b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x131799a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x131799d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x131799ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13179a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13179a570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13179a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13179aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13179adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13179b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13179b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13179b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13179b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13179bb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13179be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13179c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13179c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13179c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13179c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13179cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13179ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13179d170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13179d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13179d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13179d9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13179dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13179df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13179e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13179e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13179e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13179ea30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13179ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13179efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13179f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13179f530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13179f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13179fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13179fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x1317a0030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1317a02f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1317a05b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1317a0870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1317a0b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1317a0df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1317a10b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1317a1370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1317a1630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1317a18f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1317a1bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1317a1e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x1317a2130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1317a23f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1317a26b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1317a2970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1317a2c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1317a2ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1317a31b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1317a3470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1317a3730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x1317a39f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1317a3cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x1317a3f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x1317a4230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1317a44f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x1317a47b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x1317a4a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1317a4d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x1317a4ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1317a52b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x1317a5570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1317a5830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x1317a5af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x1317a5db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1317a6070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x1317a6330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1317a65f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x1317a68b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x1317a6b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1317a6e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1317a70f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1317a73b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1317a7670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1317a7930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1317a7bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x1317a7eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1317a8170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1317a8740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1317a8a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1317a8cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1317a8f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1317a9240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1317a9500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1317a97c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1317a9a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x1317a9d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1317aa000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1317aa2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1317aa580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1317aa840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1317aab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1317aadc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1317ab080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1317ab340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1317ab600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1317ab8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1317abe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1317ac360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1317ac8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1317ace00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1317ad350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1317ad8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1317addf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1317ae340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1317ae890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1317aede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1317af330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1317af880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1317afdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x1317b0320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1317b0870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1317b0dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1317b1310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x1317b1860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1317b1db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x1317b2300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1317b2850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1317b2da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1317b32f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x1317b3840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1317b3d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x1317b42e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1317b4830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1317b4d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1317b52d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1317b5820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1317b5d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1317b62c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1317b6810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1317b6d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1317b72b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1317b7800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1317b7d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x1317b82a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1317b8560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1317b8820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1317b8d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1317b9220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1317b9720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x1317b9c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1317ba120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1317ba620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x1317bab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1317bb020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1317bb520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1317bba20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1317bbf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1317bc420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1317bc920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x1317bce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1317bd830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1317bdf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1317be670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1317bed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1317bf050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x1317bf840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1317bfb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x1317c0110 | th_max = 1024 | th_width =   32
init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
init: layer   0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer   9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init: layer  23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048, dev = Metal
init:      Metal KV buffer size =   384.00 MiB
llama_context: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_context:        CPU  output buffer size =     0.19 MiB
llama_context:      Metal compute buffer size =   102.25 MiB
llama_context:        CPU compute buffer size =     8.01 MiB
llama_context: graph nodes  = 872
llama_context: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.983s
user	0m0.249s
sys	0m0.195s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
