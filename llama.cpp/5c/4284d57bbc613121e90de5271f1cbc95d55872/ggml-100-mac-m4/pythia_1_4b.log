Archive:  models-mnt/wikitext/wikitext-2-raw-v1.zip
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.test.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.valid.raw  
  inflating: models-mnt/wikitext/wikitext-2-raw/wiki.train.raw  
+ cmake -DCMAKE_BUILD_TYPE=Release -DLLAMA_FATAL_WARNINGS=ON -DGGML_METAL=ON -DGGML_METAL_USE_BF16=ON ..
-- The C compiler identification is AppleClang 16.0.0.16000026
-- The CXX compiler identification is AppleClang 16.0.0.16000026
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Found Git: /usr/bin/git (found version "2.39.5 (Apple Git-154)")
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success
-- Found Threads: TRUE
-- ccache found, compilation results will be cached. Disable with GGML_CCACHE=OFF.
-- CMAKE_SYSTEM_PROCESSOR: arm64
-- Including CPU backend
-- Accelerate framework found
-- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES) 
-- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES) 
-- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND) 
CMake Warning at ggml/src/ggml-cpu/CMakeLists.txt:53 (message):
  OpenMP not found
Call Stack (most recent call first):
  ggml/src/CMakeLists.txt:318 (ggml_add_cpu_backend_variant_impl)


-- ARM detected
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E
-- Performing Test GGML_COMPILER_SUPPORTS_FP16_FORMAT_I3E - Failed
-- ARM -mcpu not found, -mcpu=native will be used
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod
-- Performing Test GGML_MACHINE_SUPPORTS_dotprod - Success
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm
-- Performing Test GGML_MACHINE_SUPPORTS_i8mm - Success
-- Performing Test GGML_MACHINE_SUPPORTS_sve
-- Performing Test GGML_MACHINE_SUPPORTS_sve - Failed
-- ARM feature DOTPROD enabled
-- ARM feature MATMUL_INT8 enabled
-- ARM feature FMA enabled
-- ARM feature FP16_VECTOR_ARITHMETIC enabled
-- Adding CPU backend variant ggml-cpu: -mcpu=native+dotprod+i8mm+nosve 
-- Looking for dgemm_
-- Looking for dgemm_ - found
-- Found BLAS: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Libraries: /Library/Developer/CommandLineTools/SDKs/MacOSX15.1.sdk/System/Library/Frameworks/Accelerate.framework
-- BLAS found, Includes: 
-- Including BLAS backend
-- Metal framework found
-- The ASM compiler identification is AppleClang
-- Found assembler: /Library/Developer/CommandLineTools/usr/bin/cc
-- Including METAL backend
-- Configuring done (2.3s)
-- Generating done (0.2s)
-- Build files have been written to: /Users/ggml/work/llama.cpp/build-ci-release

real	0m2.526s
user	0m0.843s
sys	0m1.240s
++ nproc
+ make -j10
[  1%] Building C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o
[  2%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o
[  2%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o
[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o
[  3%] Building C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o
[  3%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o
[  4%] Building C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o
[  4%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o
[  4%] Built target sha256
[  5%] Built target sha1
[  5%] Built target xxhash
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o
[  5%] Built target build_info
[  5%] Linking CXX shared library ../../bin/libggml-base.dylib
[  5%] Built target ggml-base
[  5%] Generate assembly for embedded Metal library
Embedding Metal library
[  5%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o
[  6%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o
[  7%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o
[  8%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o
[  9%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o
[ 10%] Building CXX object ggml/src/ggml-blas/CMakeFiles/ggml-blas.dir/ggml-blas.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o
[ 10%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o
[ 10%] Linking CXX shared library ../../../bin/libggml-blas.dylib
[ 11%] Linking CXX shared library ../../bin/libggml-cpu.dylib
[ 11%] Building ASM object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/__/__/__/autogenerated/ggml-metal-embed.s.o
[ 12%] Building C object ggml/src/ggml-metal/CMakeFiles/ggml-metal.dir/ggml-metal.m.o
[ 12%] Built target ggml-cpu
[ 12%] Built target ggml-blas
[ 13%] Linking C shared library ../../../bin/libggml-metal.dylib
[ 13%] Built target ggml-metal
[ 13%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o
[ 14%] Linking CXX shared library ../../bin/libggml.dylib
[ 14%] Built target ggml
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o
[ 15%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o
[ 16%] Building CXX object examples/gguf/CMakeFiles/llama-gguf.dir/gguf.cpp.o
[ 16%] Building CXX object examples/gguf-hash/CMakeFiles/llama-gguf-hash.dir/gguf-hash.cpp.o
[ 17%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o
[ 18%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o
[ 19%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o
[ 21%] Linking CXX executable ../../bin/llama-gguf-hash
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o
[ 21%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o
[ 22%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o
[ 22%] Linking CXX executable ../../bin/llama-gguf
[ 23%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o
[ 24%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o
[ 25%] Built target llama-gguf-hash
[ 25%] Linking CXX shared library ../bin/libllama.dylib
[ 25%] Built target llama-gguf
[ 25%] Built target llama
[ 26%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o
[ 26%] Building CXX object examples/quantize-stats/CMakeFiles/llama-quantize-stats.dir/quantize-stats.cpp.o
[ 26%] Building C object tests/CMakeFiles/test-c.dir/test-c.c.o
[ 26%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o
[ 27%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o
[ 27%] Building CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o
[ 27%] Building CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o
[ 28%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o
[ 28%] Building CXX object examples/llava/CMakeFiles/llava.dir/llava.cpp.o
[ 29%] Linking C executable ../bin/test-c
[ 29%] Building CXX object examples/llava/CMakeFiles/llava.dir/clip.cpp.o
[ 29%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o
[ 30%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o
[ 33%] Linking CXX executable ../../bin/llama-quantize-stats
[ 33%] Linking CXX executable ../../bin/llama-simple-chat
[ 33%] Linking CXX executable ../../bin/llama-simple
[ 34%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o
[ 34%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o
[ 34%] Built target llava
[ 35%] Linking CXX static library libcommon.a
[ 35%] Built target llama-simple
[ 35%] Linking CXX static library libllava_static.a
[ 35%] Built target llama-simple-chat
[ 35%] Built target test-c
[ 36%] Linking CXX shared library ../../bin/libllava_shared.dylib
[ 36%] Built target llama-quantize-stats
[ 36%] Built target common
[ 36%] Built target llava_static
[ 36%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o
[ 36%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o
[ 36%] Built target llava_shared
[ 36%] Building CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o
[ 37%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o
[ 39%] Building CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o
[ 38%] Building CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o
[ 40%] Building CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o
[ 41%] Building CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o
[ 42%] Building CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o
[ 43%] Building CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o
[ 43%] Linking CXX executable ../bin/test-tokenizer-1-spm
[ 43%] Building CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o
[ 44%] Building CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o
[ 44%] Linking CXX executable ../bin/test-tokenizer-0
[ 44%] Linking CXX executable ../bin/test-tokenizer-1-bpe
[ 44%] Linking CXX executable ../bin/test-chat
[ 45%] Linking CXX executable ../bin/test-grammar-parser
[ 45%] Building CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o
[ 46%] Linking CXX executable ../bin/test-llama-grammar
[ 46%] Linking CXX executable ../bin/test-json-schema-to-grammar
[ 46%] Linking CXX executable ../bin/test-sampling
[ 47%] Linking CXX executable ../bin/test-grammar-integration
[ 48%] Building CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o
[ 48%] Built target test-tokenizer-0
[ 48%] Built target test-tokenizer-1-spm
[ 48%] Built target test-tokenizer-1-bpe
[ 48%] Built target test-chat
[ 48%] Built target test-llama-grammar
[ 48%] Linking CXX executable ../bin/test-log
[ 48%] Built target test-grammar-parser
[ 48%] Built target test-json-schema-to-grammar
[ 48%] Built target test-sampling
[ 49%] Building CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o
[ 49%] Building CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o
[ 49%] Built target test-grammar-integration
[ 50%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o
[ 51%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o
[ 52%] Building CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o
[ 53%] Building CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o
[ 53%] Built target test-log
[ 54%] Building CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o
[ 54%] Building CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o
[ 55%] Building CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o
[ 55%] Linking CXX executable ../bin/test-gguf
[ 56%] Linking CXX executable ../bin/test-chat-template
[ 57%] Linking CXX executable ../bin/test-arg-parser
[ 58%] Linking CXX executable ../bin/test-model-load-cancel
[ 59%] Linking CXX executable ../bin/test-backend-ops
[ 60%] Building CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o
[ 60%] Linking CXX executable ../bin/test-quantize-fns
[ 61%] Linking CXX executable ../bin/test-barrier
[ 61%] Linking CXX executable ../bin/test-autorelease
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o
[ 62%] Linking CXX executable ../bin/test-quantize-perf
[ 62%] Building CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o
[ 62%] Built target test-gguf
[ 62%] Built target test-chat-template
[ 62%] Built target test-arg-parser
[ 62%] Built target test-quantize-fns
[ 62%] Built target test-model-load-cancel
[ 62%] Built target test-backend-ops
[ 62%] Built target test-barrier
[ 62%] Built target test-autorelease
[ 63%] Linking CXX executable ../bin/test-rope
[ 63%] Built target test-quantize-perf
[ 63%] Building CXX object examples/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o
[ 64%] Building CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o
[ 65%] Building CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o
[ 65%] Building CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o
[ 66%] Building CXX object examples/gbnf-validator/CMakeFiles/llama-gbnf-validator.dir/gbnf-validator.cpp.o
[ 66%] Building CXX object examples/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o
[ 66%] Building CXX object examples/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o
[ 66%] Building CXX object examples/gritlm/CMakeFiles/llama-gritlm.dir/gritlm.cpp.o
[ 66%] Building CXX object examples/infill/CMakeFiles/llama-infill.dir/infill.cpp.o
[ 66%] Linking CXX executable ../../bin/llama-embedding
[ 67%] Linking CXX executable ../../bin/llama-batched-bench
[ 67%] Linking CXX executable ../../bin/llama-eval-callback
[ 69%] Linking CXX executable ../../bin/llama-batched
[ 69%] Linking CXX executable ../../bin/llama-gguf-split
[ 69%] Linking CXX executable ../../bin/llama-gbnf-validator
[ 70%] Linking CXX executable ../../bin/llama-imatrix
[ 71%] Linking CXX executable ../../bin/llama-gritlm
[ 71%] Built target test-rope
[ 72%] Linking CXX executable ../../bin/llama-infill
[ 72%] Building CXX object examples/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o
[ 72%] Built target llama-gguf-split
[ 72%] Built target llama-eval-callback
[ 72%] Built target llama-embedding
[ 72%] Built target llama-batched-bench
[ 72%] Built target llama-gbnf-validator
[ 72%] Built target llama-batched
[ 72%] Built target llama-imatrix
[ 72%] Built target llama-gritlm
[ 73%] Linking CXX executable ../../bin/llama-bench
[ 73%] Built target llama-infill
[ 73%] Building CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o
[ 74%] Building CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o
[ 75%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o
[ 76%] Building CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o
[ 77%] Building CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o
[ 75%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o
[ 78%] Building CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o
[ 79%] Building CXX object examples/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o
[ 79%] Linking CXX executable ../../bin/llama-lookup
[ 79%] Linking CXX executable ../../bin/llama-lookup-stats
[ 80%] Linking CXX executable ../../bin/llama-lookahead
[ 80%] Linking CXX executable ../../bin/llama-lookup-create
[ 80%] Linking CXX executable ../../bin/llama-lookup-merge
[ 80%] Linking CXX executable ../../bin/llama-cli
[ 80%] Linking CXX executable ../../bin/llama-parallel
[ 80%] Linking CXX executable ../../bin/llama-passkey
[ 80%] Linking CXX executable ../../bin/llama-perplexity
[ 80%] Built target llama-bench
[ 80%] Built target llama-lookup-stats
[ 80%] Built target llama-lookup-merge
[ 80%] Built target llama-lookup
[ 80%] Built target llama-lookahead
[ 80%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o
[ 80%] Built target llama-lookup-create
[ 80%] Built target llama-parallel
[ 80%] Built target llama-cli
[ 80%] Generating loading.html.hpp
[ 80%] Built target llama-passkey
[ 80%] Built target llama-perplexity
[ 81%] Building CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o
[ 81%] Building CXX object examples/run/CMakeFiles/llama-run.dir/run.cpp.o
[ 81%] Building CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o
[ 82%] Linking CXX executable ../../bin/llama-quantize
[ 82%] Building CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o
[ 82%] Building CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o
[ 83%] Generating index.html.gz.hpp
[ 83%] Building CXX object examples/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o
[ 84%] Building CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o
[ 84%] Building CXX object examples/tts/CMakeFiles/llama-tts.dir/tts.cpp.o
[ 84%] Linking CXX executable ../../bin/llama-save-load-state
[ 85%] Linking CXX executable ../../bin/llama-retrieval
[ 86%] Linking CXX executable ../../bin/llama-speculative
[ 87%] Building CXX object examples/run/CMakeFiles/llama-run.dir/linenoise.cpp/linenoise.cpp.o
[ 88%] Linking CXX executable ../../bin/llama-speculative-simple
[ 89%] Linking CXX executable ../../bin/llama-tokenize
[ 89%] Linking CXX executable ../../bin/llama-gen-docs
[ 90%] Linking CXX executable ../../bin/llama-tts
[ 90%] Linking CXX executable ../../bin/llama-run
[ 90%] Built target llama-quantize
[ 90%] Built target llama-save-load-state
[ 90%] Built target llama-speculative-simple
[ 90%] Built target llama-speculative
[ 90%] Built target llama-retrieval
[ 90%] Built target llama-tokenize
[ 91%] Building CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o
[ 91%] Built target llama-gen-docs
[ 91%] Built target llama-tts
[ 92%] Building CXX object examples/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o
[ 93%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-llava-cli.dir/llava-cli.cpp.o
[ 93%] Built target llama-run
[ 93%] Building CXX object examples/llava/CMakeFiles/llama-qwen2vl-cli.dir/qwen2vl-cli.cpp.o
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-minicpmv-cli.dir/minicpmv-cli.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-convert-llama2c-to-ggml
[ 94%] Building CXX object examples/llava/CMakeFiles/llama-llava-clip-quantize-cli.dir/clip-quantize-cli.cpp.o
[ 94%] Building CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o
[ 94%] Linking CXX executable ../../bin/llama-cvector-generator
[ 95%] Linking CXX executable ../../bin/llama-llava-cli
[ 95%] Linking CXX executable ../../bin/llama-export-lora
[ 96%] Linking CXX executable ../../bin/llama-qwen2vl-cli
[ 96%] Linking CXX executable ../../bin/llama-minicpmv-cli
[ 96%] Building CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o
[ 97%] Linking CXX executable ../../bin/llama-llava-clip-quantize-cli
[ 98%] Linking CXX executable ../../bin/llama-vdot
[ 98%] Built target llama-convert-llama2c-to-ggml
[ 99%] Linking CXX executable ../../bin/llama-q8dot
[ 99%] Built target llama-cvector-generator
[ 99%] Built target llama-export-lora
[ 99%] Built target llama-llava-cli
[ 99%] Built target llama-qwen2vl-cli
[ 99%] Built target llama-minicpmv-cli
[ 99%] Built target llama-llava-clip-quantize-cli
[ 99%] Built target llama-vdot
[ 99%] Built target llama-q8dot
[ 99%] Building CXX object examples/server/CMakeFiles/llama-server.dir/server.cpp.o
[100%] Linking CXX executable ../../bin/llama-server
[100%] Built target llama-server

real	0m3.206s
user	0m6.563s
sys	0m10.321s

main: quantize time =  4613.24 ms
main:    total time =  4613.24 ms

main: quantize time =  4277.93 ms
main:    total time =  4277.94 ms

main: quantize time =  3248.55 ms
main:    total time =  3248.55 ms

main: quantize time =  3087.09 ms
main:    total time =  3087.09 ms

main: quantize time =  2011.53 ms
main:    total time =  2011.53 ms

main: quantize time =  5511.40 ms
main:    total time =  5511.40 ms

main: quantize time =  6024.62 ms
main:    total time =  6024.62 ms

main: quantize time =  7263.81 ms
main:    total time =  7263.81 ms

main: quantize time =  6123.69 ms
main:    total time =  6123.69 ms

main: quantize time =  4374.03 ms
main:    total time =  4374.03 ms
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.255 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.422 I main: llama backend init
0.00.000.429 I main: load the model and apply lora adapter, if any
0.00.047.889 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.060.544 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.565 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.570 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.571 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.572 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.572 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.575 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.575 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.576 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.577 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.589 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.590 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.591 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.596 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.596 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.597 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.659 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.847 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.079.143 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.079.154 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.079.154 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.079.155 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.079.155 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.079.157 I llama_model_loader: - type  f32:  194 tensors
0.00.079.157 I llama_model_loader: - type  f16:   98 tensors
0.00.079.158 I print_info: file format = GGUF V3 (latest)
0.00.079.160 I print_info: file type   = all F32 (guessed)
0.00.079.163 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.094.325 I load: special tokens cache size = 25
0.00.102.948 I load: token to piece cache size = 0.2984 MB
0.00.102.952 I print_info: arch             = gptneox
0.00.102.952 I print_info: vocab_only       = 0
0.00.102.952 I print_info: n_ctx_train      = 2048
0.00.102.953 I print_info: n_embd           = 2048
0.00.102.953 I print_info: n_layer          = 24
0.00.102.958 I print_info: n_head           = 16
0.00.102.959 I print_info: n_head_kv        = 16
0.00.102.959 I print_info: n_rot            = 32
0.00.102.959 I print_info: n_swa            = 0
0.00.102.959 I print_info: n_embd_head_k    = 128
0.00.102.960 I print_info: n_embd_head_v    = 128
0.00.102.960 I print_info: n_gqa            = 1
0.00.102.961 I print_info: n_embd_k_gqa     = 2048
0.00.102.962 I print_info: n_embd_v_gqa     = 2048
0.00.102.963 I print_info: f_norm_eps       = 1.0e-05
0.00.102.963 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.102.963 I print_info: f_clamp_kqv      = 0.0e+00
0.00.102.964 I print_info: f_max_alibi_bias = 0.0e+00
0.00.102.964 I print_info: f_logit_scale    = 0.0e+00
0.00.102.964 I print_info: n_ff             = 8192
0.00.102.965 I print_info: n_expert         = 0
0.00.102.965 I print_info: n_expert_used    = 0
0.00.102.965 I print_info: causal attn      = 1
0.00.102.965 I print_info: pooling type     = 0
0.00.102.966 I print_info: rope type        = 2
0.00.102.966 I print_info: rope scaling     = linear
0.00.102.966 I print_info: freq_base_train  = 10000.0
0.00.102.967 I print_info: freq_scale_train = 1
0.00.102.967 I print_info: n_ctx_orig_yarn  = 2048
0.00.102.967 I print_info: rope_finetuned   = unknown
0.00.102.967 I print_info: ssm_d_conv       = 0
0.00.102.968 I print_info: ssm_d_inner      = 0
0.00.102.968 I print_info: ssm_d_state      = 0
0.00.102.968 I print_info: ssm_dt_rank      = 0
0.00.102.968 I print_info: ssm_dt_b_c_rms   = 0
0.00.102.968 I print_info: model type       = 1.4B
0.00.102.970 I print_info: model params     = 1.41 B
0.00.102.971 I print_info: general.name     = 1.4B
0.00.102.971 I print_info: vocab type       = BPE
0.00.102.971 I print_info: n_vocab          = 50304
0.00.102.972 I print_info: n_merges         = 50009
0.00.102.972 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.102.973 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.102.973 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.102.973 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.102.973 I print_info: LF token         = 187 'Ċ'
0.00.102.974 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.102.974 I print_info: max token length = 1024
0.00.102.974 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.148.489 I load_tensors: offloading 24 repeating layers to GPU
0.00.148.493 I load_tensors: offloading output layer to GPU
0.00.148.493 I load_tensors: offloaded 25/25 layers to GPU
0.00.148.516 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.148.517 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.149.139 I llama_init_from_model: n_seq_max     = 1
0.00.149.140 I llama_init_from_model: n_ctx         = 2048
0.00.149.140 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.149.140 I llama_init_from_model: n_batch       = 2048
0.00.149.140 I llama_init_from_model: n_ubatch      = 512
0.00.149.140 I llama_init_from_model: flash_attn    = 0
0.00.149.141 I llama_init_from_model: freq_base     = 10000.0
0.00.149.141 I llama_init_from_model: freq_scale    = 1
0.00.149.142 I ggml_metal_init: allocating
0.00.149.180 I ggml_metal_init: found device: Apple M4
0.00.149.187 I ggml_metal_init: picking default device: Apple M4
0.00.149.802 I ggml_metal_init: using embedded metal library
0.00.183.872 I ggml_metal_init: GPU name:   Apple M4
0.00.183.874 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.183.875 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.183.875 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.183.876 I ggml_metal_init: simdgroup reduction   = true
0.00.183.876 I ggml_metal_init: simdgroup matrix mul. = true
0.00.183.876 I ggml_metal_init: has residency sets    = true
0.00.183.876 I ggml_metal_init: has bfloat            = true
0.00.183.876 I ggml_metal_init: use bfloat            = true
0.00.183.877 I ggml_metal_init: hasUnifiedMemory      = true
0.00.183.878 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.326.809 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.355.456 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.355.462 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.355.486 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.359.451 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.359.453 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.359.454 I llama_init_from_model: graph nodes  = 967
0.00.359.454 I llama_init_from_model: graph splits = 2
0.00.359.463 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.359.590 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.359.591 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.426.679 I main: llama threadpool init, n_threads = 4
0.00.426.723 I 
0.00.426.739 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.426.740 I 
0.00.426.924 I sampler seed: 1234
0.00.426.928 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.426.953 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.426.954 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.426.954 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.260.459 I llama_perf_sampler_print:    sampling time =       1.22 ms /    71 runs   (    0.02 ms per token, 58196.72 tokens per second)
0.02.260.461 I llama_perf_context_print:        load time =     377.94 ms
0.02.260.462 I llama_perf_context_print: prompt eval time =      43.77 ms /     7 tokens (    6.25 ms per token,   159.93 tokens per second)
0.02.260.463 I llama_perf_context_print:        eval time =    1786.78 ms /    63 runs   (   28.36 ms per token,    35.26 tokens per second)
0.02.260.464 I llama_perf_context_print:       total time =    1834.62 ms /    70 tokens
0.02.260.661 I ggml_metal_free: deallocating

real	0m2.585s
user	0m0.135s
sys	0m0.143s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.009.820 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.025.846 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.852 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.853 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.854 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.856 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.856 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.857 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.858 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.858 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.859 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.859 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.859 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.860 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.860 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.862 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.862 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.862 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.727 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.745 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.851 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.034.853 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.853 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.853 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.854 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.854 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.855 I llama_model_loader: - type  f32:  194 tensors
0.00.034.855 I llama_model_loader: - type q8_0:   98 tensors
0.00.034.856 I print_info: file format = GGUF V3 (latest)
0.00.034.856 I print_info: file type   = Q8_0
0.00.034.857 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.044.182 I load: special tokens cache size = 25
0.00.050.523 I load: token to piece cache size = 0.2984 MB
0.00.050.528 I print_info: arch             = gptneox
0.00.050.528 I print_info: vocab_only       = 0
0.00.050.529 I print_info: n_ctx_train      = 2048
0.00.050.530 I print_info: n_embd           = 2048
0.00.050.530 I print_info: n_layer          = 24
0.00.050.537 I print_info: n_head           = 16
0.00.050.538 I print_info: n_head_kv        = 16
0.00.050.538 I print_info: n_rot            = 32
0.00.050.539 I print_info: n_swa            = 0
0.00.050.539 I print_info: n_embd_head_k    = 128
0.00.050.539 I print_info: n_embd_head_v    = 128
0.00.050.540 I print_info: n_gqa            = 1
0.00.050.540 I print_info: n_embd_k_gqa     = 2048
0.00.050.541 I print_info: n_embd_v_gqa     = 2048
0.00.050.542 I print_info: f_norm_eps       = 1.0e-05
0.00.050.542 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.542 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.543 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.543 I print_info: f_logit_scale    = 0.0e+00
0.00.050.544 I print_info: n_ff             = 8192
0.00.050.544 I print_info: n_expert         = 0
0.00.050.544 I print_info: n_expert_used    = 0
0.00.050.544 I print_info: causal attn      = 1
0.00.050.544 I print_info: pooling type     = 0
0.00.050.547 I print_info: rope type        = 2
0.00.050.547 I print_info: rope scaling     = linear
0.00.050.547 I print_info: freq_base_train  = 10000.0
0.00.050.548 I print_info: freq_scale_train = 1
0.00.050.548 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.548 I print_info: rope_finetuned   = unknown
0.00.050.548 I print_info: ssm_d_conv       = 0
0.00.050.549 I print_info: ssm_d_inner      = 0
0.00.050.549 I print_info: ssm_d_state      = 0
0.00.050.549 I print_info: ssm_dt_rank      = 0
0.00.050.549 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.550 I print_info: model type       = 1.4B
0.00.050.550 I print_info: model params     = 1.41 B
0.00.050.550 I print_info: general.name     = 1.4B
0.00.050.551 I print_info: vocab type       = BPE
0.00.050.551 I print_info: n_vocab          = 50304
0.00.050.551 I print_info: n_merges         = 50009
0.00.050.552 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.552 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.552 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.552 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.552 I print_info: LF token         = 187 'Ċ'
0.00.050.553 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.553 I print_info: max token length = 1024
0.00.050.553 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.162.281 I load_tensors: offloading 24 repeating layers to GPU
0.01.162.286 I load_tensors: offloading output layer to GPU
0.01.162.287 I load_tensors: offloaded 25/25 layers to GPU
0.01.162.309 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.162.310 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.163.216 I llama_init_from_model: n_seq_max     = 1
0.01.163.217 I llama_init_from_model: n_ctx         = 2048
0.01.163.218 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.163.218 I llama_init_from_model: n_batch       = 2048
0.01.163.219 I llama_init_from_model: n_ubatch      = 512
0.01.163.219 I llama_init_from_model: flash_attn    = 0
0.01.163.220 I llama_init_from_model: freq_base     = 10000.0
0.01.163.220 I llama_init_from_model: freq_scale    = 1
0.01.163.221 I ggml_metal_init: allocating
0.01.163.230 I ggml_metal_init: found device: Apple M4
0.01.163.236 I ggml_metal_init: picking default device: Apple M4
0.01.164.434 I ggml_metal_init: using embedded metal library
0.01.169.679 I ggml_metal_init: GPU name:   Apple M4
0.01.169.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.169.683 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.169.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.169.684 I ggml_metal_init: simdgroup reduction   = true
0.01.169.684 I ggml_metal_init: simdgroup matrix mul. = true
0.01.169.685 I ggml_metal_init: has residency sets    = true
0.01.169.685 I ggml_metal_init: has bfloat            = true
0.01.169.685 I ggml_metal_init: use bfloat            = true
0.01.169.686 I ggml_metal_init: hasUnifiedMemory      = true
0.01.169.687 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.184.827 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.229.222 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.229.228 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.229.257 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.233.697 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.233.699 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.233.699 I llama_init_from_model: graph nodes  = 967
0.01.233.699 I llama_init_from_model: graph splits = 2
0.01.233.704 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.233.819 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.233.820 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.291.995 I main: llama threadpool init, n_threads = 4
0.01.292.038 I 
0.01.292.054 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.292.054 I 
0.01.292.285 I sampler seed: 1234
0.01.292.293 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.292.307 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.292.309 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.292.309 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.388.787 I llama_perf_sampler_print:    sampling time =       1.39 ms /    71 runs   (    0.02 ms per token, 50969.13 tokens per second)
0.02.388.788 I llama_perf_context_print:        load time =    1281.47 ms
0.02.388.789 I llama_perf_context_print: prompt eval time =      49.48 ms /     7 tokens (    7.07 ms per token,   141.48 tokens per second)
0.02.388.789 I llama_perf_context_print:        eval time =    1044.21 ms /    63 runs   (   16.57 ms per token,    60.33 tokens per second)
0.02.388.790 I llama_perf_context_print:       total time =    1097.50 ms /    70 tokens
0.02.389.032 I ggml_metal_free: deallocating

real	0m2.409s
user	0m0.109s
sys	0m0.262s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.949 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.663 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.669 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.671 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.671 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.672 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.672 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.672 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.673 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.675 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.675 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.675 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.676 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.676 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.677 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.678 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.538 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.544 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.479 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.481 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.481 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.482 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.482 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.483 I llama_model_loader: - type  f32:  194 tensors
0.00.026.483 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.483 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.484 I print_info: file format = GGUF V3 (latest)
0.00.026.484 I print_info: file type   = Q4_0
0.00.026.486 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.856 I load: special tokens cache size = 25
0.00.040.808 I load: token to piece cache size = 0.2984 MB
0.00.040.811 I print_info: arch             = gptneox
0.00.040.811 I print_info: vocab_only       = 0
0.00.040.812 I print_info: n_ctx_train      = 2048
0.00.040.812 I print_info: n_embd           = 2048
0.00.040.812 I print_info: n_layer          = 24
0.00.040.816 I print_info: n_head           = 16
0.00.040.817 I print_info: n_head_kv        = 16
0.00.040.817 I print_info: n_rot            = 32
0.00.040.817 I print_info: n_swa            = 0
0.00.040.818 I print_info: n_embd_head_k    = 128
0.00.040.818 I print_info: n_embd_head_v    = 128
0.00.040.819 I print_info: n_gqa            = 1
0.00.040.819 I print_info: n_embd_k_gqa     = 2048
0.00.040.820 I print_info: n_embd_v_gqa     = 2048
0.00.040.821 I print_info: f_norm_eps       = 1.0e-05
0.00.040.823 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.823 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.823 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.824 I print_info: f_logit_scale    = 0.0e+00
0.00.040.824 I print_info: n_ff             = 8192
0.00.040.825 I print_info: n_expert         = 0
0.00.040.825 I print_info: n_expert_used    = 0
0.00.040.826 I print_info: causal attn      = 1
0.00.040.826 I print_info: pooling type     = 0
0.00.040.826 I print_info: rope type        = 2
0.00.040.826 I print_info: rope scaling     = linear
0.00.040.827 I print_info: freq_base_train  = 10000.0
0.00.040.829 I print_info: freq_scale_train = 1
0.00.040.829 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.830 I print_info: rope_finetuned   = unknown
0.00.040.830 I print_info: ssm_d_conv       = 0
0.00.040.830 I print_info: ssm_d_inner      = 0
0.00.040.830 I print_info: ssm_d_state      = 0
0.00.040.830 I print_info: ssm_dt_rank      = 0
0.00.040.830 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.831 I print_info: model type       = 1.4B
0.00.040.831 I print_info: model params     = 1.41 B
0.00.040.831 I print_info: general.name     = 1.4B
0.00.040.835 I print_info: vocab type       = BPE
0.00.040.837 I print_info: n_vocab          = 50304
0.00.040.837 I print_info: n_merges         = 50009
0.00.040.837 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.837 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.837 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.837 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.838 I print_info: LF token         = 187 'Ċ'
0.00.040.838 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.838 I print_info: max token length = 1024
0.00.040.839 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.555.594 I load_tensors: offloading 24 repeating layers to GPU
0.00.555.612 I load_tensors: offloading output layer to GPU
0.00.555.613 I load_tensors: offloaded 25/25 layers to GPU
0.00.555.644 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.555.645 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.556.912 I llama_init_from_model: n_seq_max     = 1
0.00.556.918 I llama_init_from_model: n_ctx         = 2048
0.00.556.918 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.556.919 I llama_init_from_model: n_batch       = 2048
0.00.556.919 I llama_init_from_model: n_ubatch      = 512
0.00.556.920 I llama_init_from_model: flash_attn    = 0
0.00.556.922 I llama_init_from_model: freq_base     = 10000.0
0.00.556.922 I llama_init_from_model: freq_scale    = 1
0.00.556.924 I ggml_metal_init: allocating
0.00.557.009 I ggml_metal_init: found device: Apple M4
0.00.557.024 I ggml_metal_init: picking default device: Apple M4
0.00.558.624 I ggml_metal_init: using embedded metal library
0.00.563.111 I ggml_metal_init: GPU name:   Apple M4
0.00.563.119 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.563.119 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.563.120 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.563.121 I ggml_metal_init: simdgroup reduction   = true
0.00.563.121 I ggml_metal_init: simdgroup matrix mul. = true
0.00.563.121 I ggml_metal_init: has residency sets    = true
0.00.563.122 I ggml_metal_init: has bfloat            = true
0.00.563.122 I ggml_metal_init: use bfloat            = true
0.00.563.123 I ggml_metal_init: hasUnifiedMemory      = true
0.00.563.125 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.576.203 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.607.000 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.607.008 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.607.044 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.612.477 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.612.479 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.612.479 I llama_init_from_model: graph nodes  = 967
0.00.612.479 I llama_init_from_model: graph splits = 2
0.00.612.486 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.612.602 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.612.602 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.668.548 I main: llama threadpool init, n_threads = 4
0.00.668.598 I 
0.00.668.613 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.668.613 I 
0.00.668.790 I sampler seed: 1234
0.00.668.794 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.668.838 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.668.840 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.668.840 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.354.547 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51711.58 tokens per second)
0.01.354.548 I llama_perf_context_print:        load time =     656.90 ms
0.01.354.549 I llama_perf_context_print: prompt eval time =      49.25 ms /     7 tokens (    7.04 ms per token,   142.14 tokens per second)
0.01.354.549 I llama_perf_context_print:        eval time =     633.59 ms /    63 runs   (   10.06 ms per token,    99.43 tokens per second)
0.01.354.551 I llama_perf_context_print:       total time =     686.70 ms /    70 tokens
0.01.354.780 I ggml_metal_free: deallocating

real	0m1.375s
user	0m0.103s
sys	0m0.171s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.055 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.009.121 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.056 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.060 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.066 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.066 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.068 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.068 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.068 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.069 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.070 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.070 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.070 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.071 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.071 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.073 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.074 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.076 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.076 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.959 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.951 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.817 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.818 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.818 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.819 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.819 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.820 I llama_model_loader: - type  f32:  194 tensors
0.00.024.820 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.820 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.821 I print_info: file format = GGUF V3 (latest)
0.00.024.821 I print_info: file type   = Q4_1
0.00.024.822 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.032.687 I load: special tokens cache size = 25
0.00.038.749 I load: token to piece cache size = 0.2984 MB
0.00.038.752 I print_info: arch             = gptneox
0.00.038.752 I print_info: vocab_only       = 0
0.00.038.752 I print_info: n_ctx_train      = 2048
0.00.038.752 I print_info: n_embd           = 2048
0.00.038.752 I print_info: n_layer          = 24
0.00.038.755 I print_info: n_head           = 16
0.00.038.756 I print_info: n_head_kv        = 16
0.00.038.756 I print_info: n_rot            = 32
0.00.038.756 I print_info: n_swa            = 0
0.00.038.756 I print_info: n_embd_head_k    = 128
0.00.038.756 I print_info: n_embd_head_v    = 128
0.00.038.758 I print_info: n_gqa            = 1
0.00.038.759 I print_info: n_embd_k_gqa     = 2048
0.00.038.760 I print_info: n_embd_v_gqa     = 2048
0.00.038.761 I print_info: f_norm_eps       = 1.0e-05
0.00.038.765 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.765 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.766 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.766 I print_info: f_logit_scale    = 0.0e+00
0.00.038.766 I print_info: n_ff             = 8192
0.00.038.767 I print_info: n_expert         = 0
0.00.038.767 I print_info: n_expert_used    = 0
0.00.038.767 I print_info: causal attn      = 1
0.00.038.767 I print_info: pooling type     = 0
0.00.038.767 I print_info: rope type        = 2
0.00.038.768 I print_info: rope scaling     = linear
0.00.038.769 I print_info: freq_base_train  = 10000.0
0.00.038.770 I print_info: freq_scale_train = 1
0.00.038.770 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.770 I print_info: rope_finetuned   = unknown
0.00.038.770 I print_info: ssm_d_conv       = 0
0.00.038.770 I print_info: ssm_d_inner      = 0
0.00.038.770 I print_info: ssm_d_state      = 0
0.00.038.771 I print_info: ssm_dt_rank      = 0
0.00.038.771 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.772 I print_info: model type       = 1.4B
0.00.038.772 I print_info: model params     = 1.41 B
0.00.038.772 I print_info: general.name     = 1.4B
0.00.038.773 I print_info: vocab type       = BPE
0.00.038.773 I print_info: n_vocab          = 50304
0.00.038.773 I print_info: n_merges         = 50009
0.00.038.774 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.774 I print_info: LF token         = 187 'Ċ'
0.00.038.775 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.775 I print_info: max token length = 1024
0.00.038.776 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.970 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.985 I load_tensors: offloading output layer to GPU
0.00.641.986 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.019 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.642.020 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.643.462 I llama_init_from_model: n_seq_max     = 1
0.00.643.465 I llama_init_from_model: n_ctx         = 2048
0.00.643.465 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.643.466 I llama_init_from_model: n_batch       = 2048
0.00.643.466 I llama_init_from_model: n_ubatch      = 512
0.00.643.467 I llama_init_from_model: flash_attn    = 0
0.00.643.469 I llama_init_from_model: freq_base     = 10000.0
0.00.643.469 I llama_init_from_model: freq_scale    = 1
0.00.643.474 I ggml_metal_init: allocating
0.00.643.575 I ggml_metal_init: found device: Apple M4
0.00.643.588 I ggml_metal_init: picking default device: Apple M4
0.00.645.459 I ggml_metal_init: using embedded metal library
0.00.652.050 I ggml_metal_init: GPU name:   Apple M4
0.00.652.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.652.056 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.652.056 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.652.057 I ggml_metal_init: simdgroup reduction   = true
0.00.652.057 I ggml_metal_init: simdgroup matrix mul. = true
0.00.652.058 I ggml_metal_init: has residency sets    = true
0.00.652.058 I ggml_metal_init: has bfloat            = true
0.00.652.058 I ggml_metal_init: use bfloat            = true
0.00.652.059 I ggml_metal_init: hasUnifiedMemory      = true
0.00.652.061 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.534 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.727.411 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.727.418 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.727.441 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.731.527 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.731.529 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.731.530 I llama_init_from_model: graph nodes  = 967
0.00.731.530 I llama_init_from_model: graph splits = 2
0.00.731.536 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.731.661 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.731.661 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.784.446 I main: llama threadpool init, n_threads = 4
0.00.784.492 I 
0.00.784.508 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.784.508 I 
0.00.784.688 I sampler seed: 1234
0.00.784.692 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.784.712 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.784.712 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.784.712 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.501.024 I llama_perf_sampler_print:    sampling time =       1.28 ms /    71 runs   (    0.02 ms per token, 55555.56 tokens per second)
0.01.501.025 I llama_perf_context_print:        load time =     774.62 ms
0.01.501.025 I llama_perf_context_print: prompt eval time =      39.27 ms /     7 tokens (    5.61 ms per token,   178.27 tokens per second)
0.01.501.026 I llama_perf_context_print:        eval time =     674.24 ms /    63 runs   (   10.70 ms per token,    93.44 tokens per second)
0.01.501.026 I llama_perf_context_print:       total time =     717.28 ms /    70 tokens
0.01.501.324 I ggml_metal_free: deallocating

real	0m1.519s
user	0m0.110s
sys	0m0.213s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.818 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.662 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.666 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.668 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.668 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.669 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.669 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.671 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.672 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.672 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.673 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.674 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.675 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.675 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.676 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.678 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.679 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.679 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.381 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.366 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.076 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.077 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.077 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.078 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.078 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.078 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.079 I llama_model_loader: - type  f32:  194 tensors
0.00.026.079 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.079 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.080 I print_info: file format = GGUF V3 (latest)
0.00.026.080 I print_info: file type   = Q5_0
0.00.026.081 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.812 I load: special tokens cache size = 25
0.00.039.851 I load: token to piece cache size = 0.2984 MB
0.00.039.854 I print_info: arch             = gptneox
0.00.039.854 I print_info: vocab_only       = 0
0.00.039.854 I print_info: n_ctx_train      = 2048
0.00.039.854 I print_info: n_embd           = 2048
0.00.039.854 I print_info: n_layer          = 24
0.00.039.857 I print_info: n_head           = 16
0.00.039.858 I print_info: n_head_kv        = 16
0.00.039.858 I print_info: n_rot            = 32
0.00.039.858 I print_info: n_swa            = 0
0.00.039.860 I print_info: n_embd_head_k    = 128
0.00.039.860 I print_info: n_embd_head_v    = 128
0.00.039.861 I print_info: n_gqa            = 1
0.00.039.861 I print_info: n_embd_k_gqa     = 2048
0.00.039.862 I print_info: n_embd_v_gqa     = 2048
0.00.039.863 I print_info: f_norm_eps       = 1.0e-05
0.00.039.863 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.863 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.863 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.863 I print_info: f_logit_scale    = 0.0e+00
0.00.039.864 I print_info: n_ff             = 8192
0.00.039.864 I print_info: n_expert         = 0
0.00.039.864 I print_info: n_expert_used    = 0
0.00.039.865 I print_info: causal attn      = 1
0.00.039.865 I print_info: pooling type     = 0
0.00.039.868 I print_info: rope type        = 2
0.00.039.868 I print_info: rope scaling     = linear
0.00.039.869 I print_info: freq_base_train  = 10000.0
0.00.039.869 I print_info: freq_scale_train = 1
0.00.039.869 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.869 I print_info: rope_finetuned   = unknown
0.00.039.869 I print_info: ssm_d_conv       = 0
0.00.039.869 I print_info: ssm_d_inner      = 0
0.00.039.870 I print_info: ssm_d_state      = 0
0.00.039.870 I print_info: ssm_dt_rank      = 0
0.00.039.870 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.870 I print_info: model type       = 1.4B
0.00.039.871 I print_info: model params     = 1.41 B
0.00.039.871 I print_info: general.name     = 1.4B
0.00.039.871 I print_info: vocab type       = BPE
0.00.039.872 I print_info: n_vocab          = 50304
0.00.039.872 I print_info: n_merges         = 50009
0.00.039.872 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.872 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.873 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.873 I print_info: LF token         = 187 'Ċ'
0.00.039.873 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.873 I print_info: max token length = 1024
0.00.039.874 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.630.312 I load_tensors: offloading 24 repeating layers to GPU
0.00.630.327 I load_tensors: offloading output layer to GPU
0.00.630.329 I load_tensors: offloaded 25/25 layers to GPU
0.00.630.362 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.630.364 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.631.956 I llama_init_from_model: n_seq_max     = 1
0.00.631.958 I llama_init_from_model: n_ctx         = 2048
0.00.631.959 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.631.960 I llama_init_from_model: n_batch       = 2048
0.00.631.960 I llama_init_from_model: n_ubatch      = 512
0.00.631.960 I llama_init_from_model: flash_attn    = 0
0.00.631.963 I llama_init_from_model: freq_base     = 10000.0
0.00.631.963 I llama_init_from_model: freq_scale    = 1
0.00.631.966 I ggml_metal_init: allocating
0.00.632.051 I ggml_metal_init: found device: Apple M4
0.00.632.064 I ggml_metal_init: picking default device: Apple M4
0.00.634.001 I ggml_metal_init: using embedded metal library
0.00.640.676 I ggml_metal_init: GPU name:   Apple M4
0.00.640.680 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.640.681 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.640.682 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.640.682 I ggml_metal_init: simdgroup reduction   = true
0.00.640.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.640.683 I ggml_metal_init: has residency sets    = true
0.00.640.683 I ggml_metal_init: has bfloat            = true
0.00.640.683 I ggml_metal_init: use bfloat            = true
0.00.640.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.640.685 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.658.483 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.710.750 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.710.759 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.710.779 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.715.120 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.715.122 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.715.122 I llama_init_from_model: graph nodes  = 967
0.00.715.122 I llama_init_from_model: graph splits = 2
0.00.715.129 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.715.242 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.715.243 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.775.382 I main: llama threadpool init, n_threads = 4
0.00.775.423 I 
0.00.775.445 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.775.445 I 
0.00.775.615 I sampler seed: 1234
0.00.775.620 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.775.668 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.775.670 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.775.671 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.562.119 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52014.65 tokens per second)
0.01.562.119 I llama_perf_context_print:        load time =     764.87 ms
0.01.562.120 I llama_perf_context_print: prompt eval time =      50.69 ms /     7 tokens (    7.24 ms per token,   138.10 tokens per second)
0.01.562.122 I llama_perf_context_print:        eval time =     732.85 ms /    63 runs   (   11.63 ms per token,    85.97 tokens per second)
0.01.562.122 I llama_perf_context_print:       total time =     787.43 ms /    70 tokens
0.01.562.405 I ggml_metal_free: deallocating

real	0m1.580s
user	0m0.109s
sys	0m0.217s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.083 I main: llama backend init
0.00.000.085 I main: load the model and apply lora adapter, if any
0.00.008.710 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.660 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.665 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.666 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.667 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.667 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.668 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.669 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.669 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.670 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.670 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.670 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.671 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.671 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.674 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.675 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.675 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.457 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.460 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.239 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.240 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.241 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.242 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.242 I llama_model_loader: - type  f32:  194 tensors
0.00.025.243 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.243 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.243 I print_info: file format = GGUF V3 (latest)
0.00.025.244 I print_info: file type   = Q5_1
0.00.025.245 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.007 I load: special tokens cache size = 25
0.00.039.019 I load: token to piece cache size = 0.2984 MB
0.00.039.021 I print_info: arch             = gptneox
0.00.039.022 I print_info: vocab_only       = 0
0.00.039.022 I print_info: n_ctx_train      = 2048
0.00.039.022 I print_info: n_embd           = 2048
0.00.039.022 I print_info: n_layer          = 24
0.00.039.025 I print_info: n_head           = 16
0.00.039.026 I print_info: n_head_kv        = 16
0.00.039.028 I print_info: n_rot            = 32
0.00.039.028 I print_info: n_swa            = 0
0.00.039.029 I print_info: n_embd_head_k    = 128
0.00.039.029 I print_info: n_embd_head_v    = 128
0.00.039.030 I print_info: n_gqa            = 1
0.00.039.031 I print_info: n_embd_k_gqa     = 2048
0.00.039.031 I print_info: n_embd_v_gqa     = 2048
0.00.039.032 I print_info: f_norm_eps       = 1.0e-05
0.00.039.032 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.032 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.032 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.033 I print_info: f_logit_scale    = 0.0e+00
0.00.039.033 I print_info: n_ff             = 8192
0.00.039.033 I print_info: n_expert         = 0
0.00.039.034 I print_info: n_expert_used    = 0
0.00.039.034 I print_info: causal attn      = 1
0.00.039.034 I print_info: pooling type     = 0
0.00.039.036 I print_info: rope type        = 2
0.00.039.037 I print_info: rope scaling     = linear
0.00.039.038 I print_info: freq_base_train  = 10000.0
0.00.039.038 I print_info: freq_scale_train = 1
0.00.039.038 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.039 I print_info: rope_finetuned   = unknown
0.00.039.041 I print_info: ssm_d_conv       = 0
0.00.039.041 I print_info: ssm_d_inner      = 0
0.00.039.041 I print_info: ssm_d_state      = 0
0.00.039.041 I print_info: ssm_dt_rank      = 0
0.00.039.041 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.041 I print_info: model type       = 1.4B
0.00.039.042 I print_info: model params     = 1.41 B
0.00.039.042 I print_info: general.name     = 1.4B
0.00.039.047 I print_info: vocab type       = BPE
0.00.039.047 I print_info: n_vocab          = 50304
0.00.039.047 I print_info: n_merges         = 50009
0.00.039.047 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.049 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.049 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.049 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.049 I print_info: LF token         = 187 'Ċ'
0.00.039.050 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.050 I print_info: max token length = 1024
0.00.039.050 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.679.434 I load_tensors: offloading 24 repeating layers to GPU
0.00.679.450 I load_tensors: offloading output layer to GPU
0.00.679.452 I load_tensors: offloaded 25/25 layers to GPU
0.00.679.484 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.679.486 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.681.155 I llama_init_from_model: n_seq_max     = 1
0.00.681.158 I llama_init_from_model: n_ctx         = 2048
0.00.681.159 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.681.159 I llama_init_from_model: n_batch       = 2048
0.00.681.160 I llama_init_from_model: n_ubatch      = 512
0.00.681.160 I llama_init_from_model: flash_attn    = 0
0.00.681.162 I llama_init_from_model: freq_base     = 10000.0
0.00.681.162 I llama_init_from_model: freq_scale    = 1
0.00.681.165 I ggml_metal_init: allocating
0.00.681.218 I ggml_metal_init: found device: Apple M4
0.00.681.237 I ggml_metal_init: picking default device: Apple M4
0.00.683.358 I ggml_metal_init: using embedded metal library
0.00.689.979 I ggml_metal_init: GPU name:   Apple M4
0.00.689.982 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.689.983 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.689.984 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.689.985 I ggml_metal_init: simdgroup reduction   = true
0.00.689.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.689.985 I ggml_metal_init: has residency sets    = true
0.00.689.985 I ggml_metal_init: has bfloat            = true
0.00.689.986 I ggml_metal_init: use bfloat            = true
0.00.689.987 I ggml_metal_init: hasUnifiedMemory      = true
0.00.689.989 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.707.302 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.754.015 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.754.021 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.754.041 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.758.241 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.758.243 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.758.243 I llama_init_from_model: graph nodes  = 967
0.00.758.243 I llama_init_from_model: graph splits = 2
0.00.758.248 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.758.390 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.758.390 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.816.575 I main: llama threadpool init, n_threads = 4
0.00.816.619 I 
0.00.816.644 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.816.646 I 
0.00.816.816 I sampler seed: 1234
0.00.816.820 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.816.832 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.816.832 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.816.833 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.654.913 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52748.89 tokens per second)
0.01.654.913 I llama_perf_context_print:        load time =     807.16 ms
0.01.654.914 I llama_perf_context_print: prompt eval time =      52.12 ms /     7 tokens (    7.45 ms per token,   134.31 tokens per second)
0.01.654.915 I llama_perf_context_print:        eval time =     783.09 ms /    63 runs   (   12.43 ms per token,    80.45 tokens per second)
0.01.654.915 I llama_perf_context_print:       total time =     839.04 ms /    70 tokens
0.01.655.193 I ggml_metal_free: deallocating

real	0m1.671s
user	0m0.109s
sys	0m0.230s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.089 I main: llama backend init
0.00.000.091 I main: load the model and apply lora adapter, if any
0.00.009.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.487 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.493 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.494 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.495 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.495 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.496 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.497 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.497 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.498 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.498 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.498 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.500 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.500 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.502 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.502 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.503 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.358 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.342 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.086 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.088 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.088 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.089 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.089 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.089 I llama_model_loader: - type  f32:  194 tensors
0.00.025.090 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.090 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.090 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.091 I print_info: file format = GGUF V3 (latest)
0.00.025.091 I print_info: file type   = Q2_K - Medium
0.00.025.092 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.377 I load: special tokens cache size = 25
0.00.039.573 I load: token to piece cache size = 0.2984 MB
0.00.039.576 I print_info: arch             = gptneox
0.00.039.576 I print_info: vocab_only       = 0
0.00.039.576 I print_info: n_ctx_train      = 2048
0.00.039.577 I print_info: n_embd           = 2048
0.00.039.577 I print_info: n_layer          = 24
0.00.039.580 I print_info: n_head           = 16
0.00.039.581 I print_info: n_head_kv        = 16
0.00.039.583 I print_info: n_rot            = 32
0.00.039.583 I print_info: n_swa            = 0
0.00.039.583 I print_info: n_embd_head_k    = 128
0.00.039.583 I print_info: n_embd_head_v    = 128
0.00.039.584 I print_info: n_gqa            = 1
0.00.039.585 I print_info: n_embd_k_gqa     = 2048
0.00.039.586 I print_info: n_embd_v_gqa     = 2048
0.00.039.586 I print_info: f_norm_eps       = 1.0e-05
0.00.039.586 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.587 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.587 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.587 I print_info: f_logit_scale    = 0.0e+00
0.00.039.587 I print_info: n_ff             = 8192
0.00.039.588 I print_info: n_expert         = 0
0.00.039.588 I print_info: n_expert_used    = 0
0.00.039.588 I print_info: causal attn      = 1
0.00.039.588 I print_info: pooling type     = 0
0.00.039.589 I print_info: rope type        = 2
0.00.039.589 I print_info: rope scaling     = linear
0.00.039.590 I print_info: freq_base_train  = 10000.0
0.00.039.590 I print_info: freq_scale_train = 1
0.00.039.590 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.590 I print_info: rope_finetuned   = unknown
0.00.039.591 I print_info: ssm_d_conv       = 0
0.00.039.591 I print_info: ssm_d_inner      = 0
0.00.039.591 I print_info: ssm_d_state      = 0
0.00.039.591 I print_info: ssm_dt_rank      = 0
0.00.039.591 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.591 I print_info: model type       = 1.4B
0.00.039.592 I print_info: model params     = 1.41 B
0.00.039.592 I print_info: general.name     = 1.4B
0.00.039.592 I print_info: vocab type       = BPE
0.00.039.592 I print_info: n_vocab          = 50304
0.00.039.594 I print_info: n_merges         = 50009
0.00.039.594 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.595 I print_info: LF token         = 187 'Ċ'
0.00.039.596 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.596 I print_info: max token length = 1024
0.00.039.596 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.372.661 I load_tensors: offloading 24 repeating layers to GPU
0.00.372.678 I load_tensors: offloading output layer to GPU
0.00.372.679 I load_tensors: offloaded 25/25 layers to GPU
0.00.372.707 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.372.709 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.374.267 I llama_init_from_model: n_seq_max     = 1
0.00.374.274 I llama_init_from_model: n_ctx         = 2048
0.00.374.274 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.374.275 I llama_init_from_model: n_batch       = 2048
0.00.374.275 I llama_init_from_model: n_ubatch      = 512
0.00.374.275 I llama_init_from_model: flash_attn    = 0
0.00.374.277 I llama_init_from_model: freq_base     = 10000.0
0.00.374.278 I llama_init_from_model: freq_scale    = 1
0.00.374.280 I ggml_metal_init: allocating
0.00.374.384 I ggml_metal_init: found device: Apple M4
0.00.374.398 I ggml_metal_init: picking default device: Apple M4
0.00.376.268 I ggml_metal_init: using embedded metal library
0.00.381.750 I ggml_metal_init: GPU name:   Apple M4
0.00.381.763 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.381.764 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.381.765 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.381.765 I ggml_metal_init: simdgroup reduction   = true
0.00.381.766 I ggml_metal_init: simdgroup matrix mul. = true
0.00.381.766 I ggml_metal_init: has residency sets    = true
0.00.381.766 I ggml_metal_init: has bfloat            = true
0.00.381.767 I ggml_metal_init: use bfloat            = true
0.00.381.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.381.775 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.403.206 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.462.060 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.462.066 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.462.088 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.467.061 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.467.064 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.467.064 I llama_init_from_model: graph nodes  = 967
0.00.467.064 I llama_init_from_model: graph splits = 2
0.00.467.069 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.467.185 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.467.185 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.526.942 I main: llama threadpool init, n_threads = 4
0.00.526.985 I 
0.00.527.000 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.527.000 I 
0.00.527.178 I sampler seed: 1234
0.00.527.182 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.527.200 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.527.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.527.200 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.214.101 I llama_perf_sampler_print:    sampling time =       1.32 ms /    71 runs   (    0.02 ms per token, 53584.91 tokens per second)
0.01.214.102 I llama_perf_context_print:        load time =     516.37 ms
0.01.214.103 I llama_perf_context_print: prompt eval time =      44.21 ms /     7 tokens (    6.32 ms per token,   158.35 tokens per second)
0.01.214.103 I llama_perf_context_print:        eval time =     639.73 ms /    63 runs   (   10.15 ms per token,    98.48 tokens per second)
0.01.214.104 I llama_perf_context_print:       total time =     687.85 ms /    70 tokens
0.01.214.331 I ggml_metal_free: deallocating

real	0m1.231s
user	0m0.111s
sys	0m0.175s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.008.727 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.235 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.238 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.238 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.238 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.239 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.239 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.240 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.241 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.010 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.023.704 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.704 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.705 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.705 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.705 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.706 I llama_model_loader: - type  f32:  194 tensors
0.00.023.706 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.706 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.707 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.707 I llama_model_loader: - type q6_K:    1 tensors
0.00.023.707 I print_info: file format = GGUF V3 (latest)
0.00.023.708 I print_info: file type   = Q3_K - Medium
0.00.023.709 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.031.447 I load: special tokens cache size = 25
0.00.037.544 I load: token to piece cache size = 0.2984 MB
0.00.037.546 I print_info: arch             = gptneox
0.00.037.546 I print_info: vocab_only       = 0
0.00.037.547 I print_info: n_ctx_train      = 2048
0.00.037.547 I print_info: n_embd           = 2048
0.00.037.547 I print_info: n_layer          = 24
0.00.037.549 I print_info: n_head           = 16
0.00.037.550 I print_info: n_head_kv        = 16
0.00.037.550 I print_info: n_rot            = 32
0.00.037.551 I print_info: n_swa            = 0
0.00.037.551 I print_info: n_embd_head_k    = 128
0.00.037.551 I print_info: n_embd_head_v    = 128
0.00.037.552 I print_info: n_gqa            = 1
0.00.037.552 I print_info: n_embd_k_gqa     = 2048
0.00.037.553 I print_info: n_embd_v_gqa     = 2048
0.00.037.554 I print_info: f_norm_eps       = 1.0e-05
0.00.037.554 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.037.554 I print_info: f_clamp_kqv      = 0.0e+00
0.00.037.554 I print_info: f_max_alibi_bias = 0.0e+00
0.00.037.555 I print_info: f_logit_scale    = 0.0e+00
0.00.037.557 I print_info: n_ff             = 8192
0.00.037.557 I print_info: n_expert         = 0
0.00.037.558 I print_info: n_expert_used    = 0
0.00.037.558 I print_info: causal attn      = 1
0.00.037.558 I print_info: pooling type     = 0
0.00.037.558 I print_info: rope type        = 2
0.00.037.558 I print_info: rope scaling     = linear
0.00.037.559 I print_info: freq_base_train  = 10000.0
0.00.037.559 I print_info: freq_scale_train = 1
0.00.037.559 I print_info: n_ctx_orig_yarn  = 2048
0.00.037.559 I print_info: rope_finetuned   = unknown
0.00.037.561 I print_info: ssm_d_conv       = 0
0.00.037.561 I print_info: ssm_d_inner      = 0
0.00.037.561 I print_info: ssm_d_state      = 0
0.00.037.561 I print_info: ssm_dt_rank      = 0
0.00.037.561 I print_info: ssm_dt_b_c_rms   = 0
0.00.037.561 I print_info: model type       = 1.4B
0.00.037.562 I print_info: model params     = 1.41 B
0.00.037.562 I print_info: general.name     = 1.4B
0.00.037.562 I print_info: vocab type       = BPE
0.00.037.563 I print_info: n_vocab          = 50304
0.00.037.563 I print_info: n_merges         = 50009
0.00.037.564 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.037.565 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.037.565 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.037.566 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.037.566 I print_info: LF token         = 187 'Ċ'
0.00.037.567 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.037.567 I print_info: max token length = 1024
0.00.037.568 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.444.375 I load_tensors: offloading 24 repeating layers to GPU
0.00.444.390 I load_tensors: offloading output layer to GPU
0.00.444.391 I load_tensors: offloaded 25/25 layers to GPU
0.00.444.426 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.444.427 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.446.163 I llama_init_from_model: n_seq_max     = 1
0.00.446.166 I llama_init_from_model: n_ctx         = 2048
0.00.446.166 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.446.167 I llama_init_from_model: n_batch       = 2048
0.00.446.167 I llama_init_from_model: n_ubatch      = 512
0.00.446.168 I llama_init_from_model: flash_attn    = 0
0.00.446.170 I llama_init_from_model: freq_base     = 10000.0
0.00.446.170 I llama_init_from_model: freq_scale    = 1
0.00.446.173 I ggml_metal_init: allocating
0.00.446.280 I ggml_metal_init: found device: Apple M4
0.00.446.295 I ggml_metal_init: picking default device: Apple M4
0.00.448.233 I ggml_metal_init: using embedded metal library
0.00.453.676 I ggml_metal_init: GPU name:   Apple M4
0.00.453.692 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.693 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.694 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.695 I ggml_metal_init: simdgroup reduction   = true
0.00.453.695 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.695 I ggml_metal_init: has residency sets    = true
0.00.453.695 I ggml_metal_init: has bfloat            = true
0.00.453.696 I ggml_metal_init: use bfloat            = true
0.00.453.698 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.701 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.160 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.537.023 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.537.029 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.537.058 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.541.246 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.541.249 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.541.249 I llama_init_from_model: graph nodes  = 967
0.00.541.249 I llama_init_from_model: graph splits = 2
0.00.541.254 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.541.381 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.541.382 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.596.186 I main: llama threadpool init, n_threads = 4
0.00.596.230 I 
0.00.596.246 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.596.246 I 
0.00.596.419 I sampler seed: 1234
0.00.596.423 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.596.442 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.596.442 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.596.442 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.348.423 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51561.37 tokens per second)
0.01.348.424 I llama_perf_context_print:        load time =     586.76 ms
0.01.348.424 I llama_perf_context_print: prompt eval time =      50.07 ms /     7 tokens (    7.15 ms per token,   139.81 tokens per second)
0.01.348.425 I llama_perf_context_print:        eval time =     698.99 ms /    63 runs   (   11.10 ms per token,    90.13 tokens per second)
0.01.348.425 I llama_perf_context_print:       total time =     752.93 ms /    70 tokens
0.01.348.650 I ggml_metal_free: deallocating

real	0m1.366s
user	0m0.110s
sys	0m0.192s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.010.258 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.744 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.749 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.750 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.751 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.751 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.752 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.753 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.753 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.754 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.754 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.754 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.755 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.755 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.757 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.759 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.759 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.403 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.407 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.036 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.037 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.037 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.038 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.038 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.038 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.039 I llama_model_loader: - type  f32:  194 tensors
0.00.025.039 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.039 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.039 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.040 I print_info: file format = GGUF V3 (latest)
0.00.025.040 I print_info: file type   = Q4_K - Medium
0.00.025.041 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.032.821 I load: special tokens cache size = 25
0.00.038.999 I load: token to piece cache size = 0.2984 MB
0.00.039.002 I print_info: arch             = gptneox
0.00.039.002 I print_info: vocab_only       = 0
0.00.039.002 I print_info: n_ctx_train      = 2048
0.00.039.002 I print_info: n_embd           = 2048
0.00.039.003 I print_info: n_layer          = 24
0.00.039.005 I print_info: n_head           = 16
0.00.039.006 I print_info: n_head_kv        = 16
0.00.039.006 I print_info: n_rot            = 32
0.00.039.006 I print_info: n_swa            = 0
0.00.039.007 I print_info: n_embd_head_k    = 128
0.00.039.008 I print_info: n_embd_head_v    = 128
0.00.039.009 I print_info: n_gqa            = 1
0.00.039.010 I print_info: n_embd_k_gqa     = 2048
0.00.039.011 I print_info: n_embd_v_gqa     = 2048
0.00.039.011 I print_info: f_norm_eps       = 1.0e-05
0.00.039.012 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.012 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.012 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.012 I print_info: f_logit_scale    = 0.0e+00
0.00.039.013 I print_info: n_ff             = 8192
0.00.039.013 I print_info: n_expert         = 0
0.00.039.013 I print_info: n_expert_used    = 0
0.00.039.013 I print_info: causal attn      = 1
0.00.039.013 I print_info: pooling type     = 0
0.00.039.014 I print_info: rope type        = 2
0.00.039.014 I print_info: rope scaling     = linear
0.00.039.014 I print_info: freq_base_train  = 10000.0
0.00.039.016 I print_info: freq_scale_train = 1
0.00.039.016 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.017 I print_info: rope_finetuned   = unknown
0.00.039.017 I print_info: ssm_d_conv       = 0
0.00.039.017 I print_info: ssm_d_inner      = 0
0.00.039.017 I print_info: ssm_d_state      = 0
0.00.039.017 I print_info: ssm_dt_rank      = 0
0.00.039.017 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.018 I print_info: model type       = 1.4B
0.00.039.018 I print_info: model params     = 1.41 B
0.00.039.018 I print_info: general.name     = 1.4B
0.00.039.018 I print_info: vocab type       = BPE
0.00.039.019 I print_info: n_vocab          = 50304
0.00.039.019 I print_info: n_merges         = 50009
0.00.039.020 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.020 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.021 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.021 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.021 I print_info: LF token         = 187 'Ċ'
0.00.039.021 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.021 I print_info: max token length = 1024
0.00.039.022 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.517.549 I load_tensors: offloading 24 repeating layers to GPU
0.00.517.564 I load_tensors: offloading output layer to GPU
0.00.517.564 I load_tensors: offloaded 25/25 layers to GPU
0.00.517.595 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.517.596 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.519.317 I llama_init_from_model: n_seq_max     = 1
0.00.519.324 I llama_init_from_model: n_ctx         = 2048
0.00.519.324 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.519.325 I llama_init_from_model: n_batch       = 2048
0.00.519.325 I llama_init_from_model: n_ubatch      = 512
0.00.519.325 I llama_init_from_model: flash_attn    = 0
0.00.519.328 I llama_init_from_model: freq_base     = 10000.0
0.00.519.328 I llama_init_from_model: freq_scale    = 1
0.00.519.330 I ggml_metal_init: allocating
0.00.519.445 I ggml_metal_init: found device: Apple M4
0.00.519.460 I ggml_metal_init: picking default device: Apple M4
0.00.521.377 I ggml_metal_init: using embedded metal library
0.00.528.060 I ggml_metal_init: GPU name:   Apple M4
0.00.528.065 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.528.066 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.528.067 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.528.068 I ggml_metal_init: simdgroup reduction   = true
0.00.528.068 I ggml_metal_init: simdgroup matrix mul. = true
0.00.528.068 I ggml_metal_init: has residency sets    = true
0.00.528.069 I ggml_metal_init: has bfloat            = true
0.00.528.069 I ggml_metal_init: use bfloat            = true
0.00.528.070 I ggml_metal_init: hasUnifiedMemory      = true
0.00.528.072 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.812 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.600.292 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.600.299 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.600.323 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.604.909 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.604.912 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.604.912 I llama_init_from_model: graph nodes  = 967
0.00.604.912 I llama_init_from_model: graph splits = 2
0.00.604.918 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.605.040 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.605.041 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.661.894 I main: llama threadpool init, n_threads = 4
0.00.661.940 I 
0.00.661.956 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.661.956 I 
0.00.662.134 I sampler seed: 1234
0.00.662.139 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.662.149 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.662.151 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.662.151 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.431.933 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.431.934 I llama_perf_context_print:        load time =     650.94 ms
0.01.431.935 I llama_perf_context_print: prompt eval time =      57.44 ms /     7 tokens (    8.21 ms per token,   121.87 tokens per second)
0.01.431.936 I llama_perf_context_print:        eval time =     709.32 ms /    63 runs   (   11.26 ms per token,    88.82 tokens per second)
0.01.431.937 I llama_perf_context_print:       total time =     770.74 ms /    70 tokens
0.01.432.202 I ggml_metal_free: deallocating

real	0m1.450s
user	0m0.109s
sys	0m0.191s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.081 I main: llama backend init
0.00.000.083 I main: load the model and apply lora adapter, if any
0.00.010.144 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.917 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.922 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.924 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.924 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.925 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.925 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.925 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.926 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.926 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.927 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.927 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.928 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.928 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.930 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.935 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.935 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.935 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.784 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.787 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.670 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.671 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.671 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.672 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.672 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.672 I llama_model_loader: - type  f32:  194 tensors
0.00.025.672 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.673 I llama_model_loader: - type q6_K:   37 tensors
0.00.025.673 I print_info: file format = GGUF V3 (latest)
0.00.025.673 I print_info: file type   = Q5_K - Medium
0.00.025.674 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.033.574 I load: special tokens cache size = 25
0.00.039.879 I load: token to piece cache size = 0.2984 MB
0.00.039.882 I print_info: arch             = gptneox
0.00.039.882 I print_info: vocab_only       = 0
0.00.039.882 I print_info: n_ctx_train      = 2048
0.00.039.882 I print_info: n_embd           = 2048
0.00.039.883 I print_info: n_layer          = 24
0.00.039.885 I print_info: n_head           = 16
0.00.039.886 I print_info: n_head_kv        = 16
0.00.039.886 I print_info: n_rot            = 32
0.00.039.886 I print_info: n_swa            = 0
0.00.039.886 I print_info: n_embd_head_k    = 128
0.00.039.886 I print_info: n_embd_head_v    = 128
0.00.039.887 I print_info: n_gqa            = 1
0.00.039.888 I print_info: n_embd_k_gqa     = 2048
0.00.039.889 I print_info: n_embd_v_gqa     = 2048
0.00.039.889 I print_info: f_norm_eps       = 1.0e-05
0.00.039.889 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.890 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.890 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.890 I print_info: f_logit_scale    = 0.0e+00
0.00.039.891 I print_info: n_ff             = 8192
0.00.039.891 I print_info: n_expert         = 0
0.00.039.891 I print_info: n_expert_used    = 0
0.00.039.891 I print_info: causal attn      = 1
0.00.039.891 I print_info: pooling type     = 0
0.00.039.893 I print_info: rope type        = 2
0.00.039.895 I print_info: rope scaling     = linear
0.00.039.895 I print_info: freq_base_train  = 10000.0
0.00.039.895 I print_info: freq_scale_train = 1
0.00.039.896 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.896 I print_info: rope_finetuned   = unknown
0.00.039.896 I print_info: ssm_d_conv       = 0
0.00.039.896 I print_info: ssm_d_inner      = 0
0.00.039.896 I print_info: ssm_d_state      = 0
0.00.039.897 I print_info: ssm_dt_rank      = 0
0.00.039.897 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.897 I print_info: model type       = 1.4B
0.00.039.897 I print_info: model params     = 1.41 B
0.00.039.898 I print_info: general.name     = 1.4B
0.00.039.898 I print_info: vocab type       = BPE
0.00.039.898 I print_info: n_vocab          = 50304
0.00.039.898 I print_info: n_merges         = 50009
0.00.039.900 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.900 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.900 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.900 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: LF token         = 187 'Ċ'
0.00.039.901 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.901 I print_info: max token length = 1024
0.00.039.902 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.624.001 I load_tensors: offloading 24 repeating layers to GPU
0.00.624.009 I load_tensors: offloading output layer to GPU
0.00.624.010 I load_tensors: offloaded 25/25 layers to GPU
0.00.624.027 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.624.028 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.624.852 I llama_init_from_model: n_seq_max     = 1
0.00.624.856 I llama_init_from_model: n_ctx         = 2048
0.00.624.856 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.624.857 I llama_init_from_model: n_batch       = 2048
0.00.624.857 I llama_init_from_model: n_ubatch      = 512
0.00.624.858 I llama_init_from_model: flash_attn    = 0
0.00.624.859 I llama_init_from_model: freq_base     = 10000.0
0.00.624.859 I llama_init_from_model: freq_scale    = 1
0.00.624.861 I ggml_metal_init: allocating
0.00.624.896 I ggml_metal_init: found device: Apple M4
0.00.624.906 I ggml_metal_init: picking default device: Apple M4
0.00.625.998 I ggml_metal_init: using embedded metal library
0.00.630.224 I ggml_metal_init: GPU name:   Apple M4
0.00.630.231 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.630.231 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.630.232 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.630.233 I ggml_metal_init: simdgroup reduction   = true
0.00.630.233 I ggml_metal_init: simdgroup matrix mul. = true
0.00.630.233 I ggml_metal_init: has residency sets    = true
0.00.630.233 I ggml_metal_init: has bfloat            = true
0.00.630.234 I ggml_metal_init: use bfloat            = true
0.00.630.235 I ggml_metal_init: hasUnifiedMemory      = true
0.00.630.237 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.645.161 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.676.375 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.676.384 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.676.420 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.681.151 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.681.153 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.681.154 I llama_init_from_model: graph nodes  = 967
0.00.681.154 I llama_init_from_model: graph splits = 2
0.00.681.159 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.681.294 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.681.294 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.745.857 I main: llama threadpool init, n_threads = 4
0.00.745.902 I 
0.00.745.917 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.745.917 I 
0.00.746.082 I sampler seed: 1234
0.00.746.087 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.746.127 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.746.130 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.746.130 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.594.068 I llama_perf_sampler_print:    sampling time =       1.38 ms /    71 runs   (    0.02 ms per token, 51486.58 tokens per second)
0.01.594.068 I llama_perf_context_print:        load time =     734.99 ms
0.01.594.069 I llama_perf_context_print: prompt eval time =      51.55 ms /     7 tokens (    7.36 ms per token,   135.79 tokens per second)
0.01.594.070 I llama_perf_context_print:        eval time =     793.98 ms /    63 runs   (   12.60 ms per token,    79.35 tokens per second)
0.01.594.070 I llama_perf_context_print:       total time =     848.93 ms /    70 tokens
0.01.594.327 I ggml_metal_free: deallocating

real	0m1.612s
user	0m0.105s
sys	0m0.154s
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.057 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.009.454 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.527 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.532 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.534 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.534 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.535 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.535 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.535 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.536 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.537 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.537 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.537 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.538 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.538 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.538 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.569 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.572 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.572 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.419 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.359 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.072 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.074 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.074 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.075 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.075 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.076 I llama_model_loader: - type  f32:  194 tensors
0.00.025.076 I llama_model_loader: - type q6_K:   98 tensors
0.00.025.077 I print_info: file format = GGUF V3 (latest)
0.00.025.077 I print_info: file type   = Q6_K
0.00.025.079 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.033.537 I load: special tokens cache size = 25
0.00.039.864 I load: token to piece cache size = 0.2984 MB
0.00.039.867 I print_info: arch             = gptneox
0.00.039.868 I print_info: vocab_only       = 0
0.00.039.868 I print_info: n_ctx_train      = 2048
0.00.039.868 I print_info: n_embd           = 2048
0.00.039.868 I print_info: n_layer          = 24
0.00.039.872 I print_info: n_head           = 16
0.00.039.873 I print_info: n_head_kv        = 16
0.00.039.873 I print_info: n_rot            = 32
0.00.039.873 I print_info: n_swa            = 0
0.00.039.873 I print_info: n_embd_head_k    = 128
0.00.039.874 I print_info: n_embd_head_v    = 128
0.00.039.877 I print_info: n_gqa            = 1
0.00.039.877 I print_info: n_embd_k_gqa     = 2048
0.00.039.878 I print_info: n_embd_v_gqa     = 2048
0.00.039.879 I print_info: f_norm_eps       = 1.0e-05
0.00.039.879 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.883 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.884 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.884 I print_info: f_logit_scale    = 0.0e+00
0.00.039.885 I print_info: n_ff             = 8192
0.00.039.885 I print_info: n_expert         = 0
0.00.039.885 I print_info: n_expert_used    = 0
0.00.039.885 I print_info: causal attn      = 1
0.00.039.885 I print_info: pooling type     = 0
0.00.039.885 I print_info: rope type        = 2
0.00.039.886 I print_info: rope scaling     = linear
0.00.039.886 I print_info: freq_base_train  = 10000.0
0.00.039.886 I print_info: freq_scale_train = 1
0.00.039.887 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.888 I print_info: rope_finetuned   = unknown
0.00.039.888 I print_info: ssm_d_conv       = 0
0.00.039.888 I print_info: ssm_d_inner      = 0
0.00.039.888 I print_info: ssm_d_state      = 0
0.00.039.889 I print_info: ssm_dt_rank      = 0
0.00.039.889 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.889 I print_info: model type       = 1.4B
0.00.039.889 I print_info: model params     = 1.41 B
0.00.039.890 I print_info: general.name     = 1.4B
0.00.039.890 I print_info: vocab type       = BPE
0.00.039.890 I print_info: n_vocab          = 50304
0.00.039.890 I print_info: n_merges         = 50009
0.00.039.891 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.891 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.891 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.891 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.892 I print_info: LF token         = 187 'Ċ'
0.00.039.892 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.892 I print_info: max token length = 1024
0.00.039.893 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.642.750 I load_tensors: offloading 24 repeating layers to GPU
0.00.642.755 I load_tensors: offloading output layer to GPU
0.00.642.756 I load_tensors: offloaded 25/25 layers to GPU
0.00.642.789 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.642.791 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.644.241 I llama_init_from_model: n_seq_max     = 1
0.00.644.244 I llama_init_from_model: n_ctx         = 2048
0.00.644.244 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.644.244 I llama_init_from_model: n_batch       = 2048
0.00.644.245 I llama_init_from_model: n_ubatch      = 512
0.00.644.245 I llama_init_from_model: flash_attn    = 0
0.00.644.247 I llama_init_from_model: freq_base     = 10000.0
0.00.644.247 I llama_init_from_model: freq_scale    = 1
0.00.644.248 I ggml_metal_init: allocating
0.00.644.314 I ggml_metal_init: found device: Apple M4
0.00.644.325 I ggml_metal_init: picking default device: Apple M4
0.00.645.970 I ggml_metal_init: using embedded metal library
0.00.651.913 I ggml_metal_init: GPU name:   Apple M4
0.00.651.917 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.918 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.919 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.920 I ggml_metal_init: simdgroup reduction   = true
0.00.651.920 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.920 I ggml_metal_init: has residency sets    = true
0.00.651.921 I ggml_metal_init: has bfloat            = true
0.00.651.921 I ggml_metal_init: use bfloat            = true
0.00.651.922 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.926 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.668.559 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.723.736 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.723.742 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.723.764 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.728.401 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.728.404 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.728.404 I llama_init_from_model: graph nodes  = 967
0.00.728.404 I llama_init_from_model: graph splits = 2
0.00.728.410 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.728.535 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.728.535 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.791.000 I main: llama threadpool init, n_threads = 4
0.00.791.044 I 
0.00.791.060 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.791.061 I 
0.00.791.239 I sampler seed: 1234
0.00.791.244 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.791.264 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.791.265 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.791.265 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.677.140 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50283.29 tokens per second)
0.01.677.141 I llama_perf_context_print:        load time =     780.85 ms
0.01.677.141 I llama_perf_context_print: prompt eval time =      54.07 ms /     7 tokens (    7.72 ms per token,   129.45 tokens per second)
0.01.677.142 I llama_perf_context_print:        eval time =     828.80 ms /    63 runs   (   13.16 ms per token,    76.01 tokens per second)
0.01.677.143 I llama_perf_context_print:       total time =     886.83 ms /    70 tokens
0.01.677.362 I ggml_metal_free: deallocating

real	0m1.696s
user	0m0.108s
sys	0m0.201s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.003.328 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.033.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.047.421 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.436 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.444 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.445 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.446 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.446 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.446 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.449 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.450 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.450 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.451 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.452 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.452 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.453 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.456 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.458 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.458 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.054.574 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.056.705 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.065.791 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.065.797 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.065.798 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.065.798 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.065.799 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.065.800 I llama_model_loader: - type  f32:  194 tensors
0.00.065.800 I llama_model_loader: - type  f16:   98 tensors
0.00.065.802 I print_info: file format = GGUF V3 (latest)
0.00.065.803 I print_info: file type   = all F32 (guessed)
0.00.065.804 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.081.470 I load: special tokens cache size = 25
0.00.091.269 I load: token to piece cache size = 0.2984 MB
0.00.091.273 I print_info: arch             = gptneox
0.00.091.273 I print_info: vocab_only       = 0
0.00.091.273 I print_info: n_ctx_train      = 2048
0.00.091.274 I print_info: n_embd           = 2048
0.00.091.274 I print_info: n_layer          = 24
0.00.091.279 I print_info: n_head           = 16
0.00.091.280 I print_info: n_head_kv        = 16
0.00.091.280 I print_info: n_rot            = 32
0.00.091.280 I print_info: n_swa            = 0
0.00.091.280 I print_info: n_embd_head_k    = 128
0.00.091.283 I print_info: n_embd_head_v    = 128
0.00.091.284 I print_info: n_gqa            = 1
0.00.091.285 I print_info: n_embd_k_gqa     = 2048
0.00.091.288 I print_info: n_embd_v_gqa     = 2048
0.00.091.288 I print_info: f_norm_eps       = 1.0e-05
0.00.091.289 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.091.289 I print_info: f_clamp_kqv      = 0.0e+00
0.00.091.289 I print_info: f_max_alibi_bias = 0.0e+00
0.00.091.289 I print_info: f_logit_scale    = 0.0e+00
0.00.091.290 I print_info: n_ff             = 8192
0.00.091.290 I print_info: n_expert         = 0
0.00.091.291 I print_info: n_expert_used    = 0
0.00.091.291 I print_info: causal attn      = 1
0.00.091.291 I print_info: pooling type     = 0
0.00.091.291 I print_info: rope type        = 2
0.00.091.291 I print_info: rope scaling     = linear
0.00.091.292 I print_info: freq_base_train  = 10000.0
0.00.091.292 I print_info: freq_scale_train = 1
0.00.091.292 I print_info: n_ctx_orig_yarn  = 2048
0.00.091.293 I print_info: rope_finetuned   = unknown
0.00.091.293 I print_info: ssm_d_conv       = 0
0.00.091.293 I print_info: ssm_d_inner      = 0
0.00.091.293 I print_info: ssm_d_state      = 0
0.00.091.293 I print_info: ssm_dt_rank      = 0
0.00.091.294 I print_info: ssm_dt_b_c_rms   = 0
0.00.091.294 I print_info: model type       = 1.4B
0.00.091.294 I print_info: model params     = 1.41 B
0.00.091.295 I print_info: general.name     = 1.4B
0.00.091.295 I print_info: vocab type       = BPE
0.00.091.296 I print_info: n_vocab          = 50304
0.00.091.296 I print_info: n_merges         = 50009
0.00.091.296 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.091.296 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.091.297 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.091.297 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.091.297 I print_info: LF token         = 187 'Ċ'
0.00.091.297 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.091.298 I print_info: max token length = 1024
0.00.091.298 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.469.838 I load_tensors: offloading 24 repeating layers to GPU
0.01.469.842 I load_tensors: offloading output layer to GPU
0.01.469.843 I load_tensors: offloaded 25/25 layers to GPU
0.01.469.858 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.469.859 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.470.926 I llama_init_from_model: n_seq_max     = 1
0.01.470.928 I llama_init_from_model: n_ctx         = 128
0.01.470.928 I llama_init_from_model: n_ctx_per_seq = 128
0.01.470.928 I llama_init_from_model: n_batch       = 128
0.01.470.928 I llama_init_from_model: n_ubatch      = 128
0.01.470.929 I llama_init_from_model: flash_attn    = 0
0.01.470.929 I llama_init_from_model: freq_base     = 10000.0
0.01.470.930 I llama_init_from_model: freq_scale    = 1
0.01.470.930 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.470.931 I ggml_metal_init: allocating
0.01.470.994 I ggml_metal_init: found device: Apple M4
0.01.471.002 I ggml_metal_init: picking default device: Apple M4
0.01.472.126 I ggml_metal_init: using embedded metal library
0.01.476.286 I ggml_metal_init: GPU name:   Apple M4
0.01.476.288 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.476.289 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.476.290 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.476.290 I ggml_metal_init: simdgroup reduction   = true
0.01.476.290 I ggml_metal_init: simdgroup matrix mul. = true
0.01.476.290 I ggml_metal_init: has residency sets    = true
0.01.476.290 I ggml_metal_init: has bfloat            = true
0.01.476.290 I ggml_metal_init: use bfloat            = true
0.01.476.291 I ggml_metal_init: hasUnifiedMemory      = true
0.01.476.292 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.487.601 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.489.445 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.489.451 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.489.465 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.491.160 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.491.161 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.491.162 I llama_init_from_model: graph nodes  = 967
0.01.491.162 I llama_init_from_model: graph splits = 2
0.01.491.163 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.491.163 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.525.901 I 
0.01.525.927 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.525.952 I perplexity: tokenizing the input ..
0.01.531.087 I perplexity: tokenization took 5.134 ms
0.01.531.107 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.650.316 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.653.092 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.653.110 I llama_perf_context_print:        load time =    1491.95 ms
0.01.653.111 I llama_perf_context_print: prompt eval time =     118.94 ms /   128 tokens (    0.93 ms per token,  1076.15 tokens per second)
0.01.653.112 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.653.112 I llama_perf_context_print:       total time =     127.21 ms /   129 tokens
0.01.653.518 I ggml_metal_free: deallocating

real	0m1.842s
user	0m0.110s
sys	0m0.272s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.260 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.419 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.426 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.427 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.435 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.435 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.435 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.436 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.436 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.437 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.437 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.438 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.438 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.438 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.439 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.441 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.441 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.441 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.280 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.316 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.174 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.175 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.176 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.176 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.176 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.177 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.027.177 I llama_model_loader: - type  f32:  194 tensors
0.00.027.178 I llama_model_loader: - type q8_0:   98 tensors
0.00.027.178 I print_info: file format = GGUF V3 (latest)
0.00.027.179 I print_info: file type   = Q8_0
0.00.027.180 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.403 I load: special tokens cache size = 25
0.00.041.523 I load: token to piece cache size = 0.2984 MB
0.00.041.527 I print_info: arch             = gptneox
0.00.041.527 I print_info: vocab_only       = 0
0.00.041.528 I print_info: n_ctx_train      = 2048
0.00.041.528 I print_info: n_embd           = 2048
0.00.041.528 I print_info: n_layer          = 24
0.00.041.532 I print_info: n_head           = 16
0.00.041.533 I print_info: n_head_kv        = 16
0.00.041.533 I print_info: n_rot            = 32
0.00.041.536 I print_info: n_swa            = 0
0.00.041.536 I print_info: n_embd_head_k    = 128
0.00.041.536 I print_info: n_embd_head_v    = 128
0.00.041.537 I print_info: n_gqa            = 1
0.00.041.537 I print_info: n_embd_k_gqa     = 2048
0.00.041.538 I print_info: n_embd_v_gqa     = 2048
0.00.041.577 I print_info: f_norm_eps       = 1.0e-05
0.00.041.580 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.580 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.580 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.580 I print_info: f_logit_scale    = 0.0e+00
0.00.041.582 I print_info: n_ff             = 8192
0.00.041.583 I print_info: n_expert         = 0
0.00.041.583 I print_info: n_expert_used    = 0
0.00.041.583 I print_info: causal attn      = 1
0.00.041.583 I print_info: pooling type     = 0
0.00.041.583 I print_info: rope type        = 2
0.00.041.583 I print_info: rope scaling     = linear
0.00.041.584 I print_info: freq_base_train  = 10000.0
0.00.041.584 I print_info: freq_scale_train = 1
0.00.041.584 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.584 I print_info: rope_finetuned   = unknown
0.00.041.584 I print_info: ssm_d_conv       = 0
0.00.041.585 I print_info: ssm_d_inner      = 0
0.00.041.585 I print_info: ssm_d_state      = 0
0.00.041.586 I print_info: ssm_dt_rank      = 0
0.00.041.586 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.586 I print_info: model type       = 1.4B
0.00.041.587 I print_info: model params     = 1.41 B
0.00.041.587 I print_info: general.name     = 1.4B
0.00.041.587 I print_info: vocab type       = BPE
0.00.041.588 I print_info: n_vocab          = 50304
0.00.041.588 I print_info: n_merges         = 50009
0.00.041.588 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.588 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.588 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.588 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.589 I print_info: LF token         = 187 'Ċ'
0.00.041.589 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.589 I print_info: max token length = 1024
0.00.041.590 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.041.561 I load_tensors: offloading 24 repeating layers to GPU
0.01.041.569 I load_tensors: offloading output layer to GPU
0.01.041.569 I load_tensors: offloaded 25/25 layers to GPU
0.01.041.594 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.041.597 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.01.043.021 I llama_init_from_model: n_seq_max     = 1
0.01.043.023 I llama_init_from_model: n_ctx         = 128
0.01.043.023 I llama_init_from_model: n_ctx_per_seq = 128
0.01.043.023 I llama_init_from_model: n_batch       = 128
0.01.043.024 I llama_init_from_model: n_ubatch      = 128
0.01.043.024 I llama_init_from_model: flash_attn    = 0
0.01.043.025 I llama_init_from_model: freq_base     = 10000.0
0.01.043.025 I llama_init_from_model: freq_scale    = 1
0.01.043.026 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.043.027 I ggml_metal_init: allocating
0.01.043.074 I ggml_metal_init: found device: Apple M4
0.01.043.082 I ggml_metal_init: picking default device: Apple M4
0.01.044.468 I ggml_metal_init: using embedded metal library
0.01.049.595 I ggml_metal_init: GPU name:   Apple M4
0.01.049.598 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.049.599 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.049.600 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.049.600 I ggml_metal_init: simdgroup reduction   = true
0.01.049.600 I ggml_metal_init: simdgroup matrix mul. = true
0.01.049.601 I ggml_metal_init: has residency sets    = true
0.01.049.601 I ggml_metal_init: has bfloat            = true
0.01.049.601 I ggml_metal_init: use bfloat            = true
0.01.049.602 I ggml_metal_init: hasUnifiedMemory      = true
0.01.049.609 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.064.712 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.068.073 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.068.079 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.068.129 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.071.283 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.071.285 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.071.285 I llama_init_from_model: graph nodes  = 967
0.01.071.285 I llama_init_from_model: graph splits = 2
0.01.071.288 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.071.288 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.096.241 I 
0.01.096.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.096.343 I perplexity: tokenizing the input ..
0.01.103.694 I perplexity: tokenization took 7.348 ms
0.01.103.717 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.229.253 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.230.571 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.230.588 I llama_perf_context_print:        load time =    1086.97 ms
0.01.230.589 I llama_perf_context_print: prompt eval time =     124.59 ms /   128 tokens (    0.97 ms per token,  1027.35 tokens per second)
0.01.230.590 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.230.590 I llama_perf_context_print:       total time =     134.35 ms /   129 tokens
0.01.230.998 I ggml_metal_free: deallocating

real	0m1.245s
user	0m0.077s
sys	0m0.169s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.048 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.027.510 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.027.519 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.027.520 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.027.521 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.027.521 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.027.522 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.027.522 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.027.523 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.027.526 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.027.526 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.027.527 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.027.527 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.027.527 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.027.528 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.027.530 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.027.530 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.027.530 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.031.227 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.032.258 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.036.145 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.036.146 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.036.147 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.036.147 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.036.147 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.036.148 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.036.148 I llama_model_loader: - type  f32:  194 tensors
0.00.036.148 I llama_model_loader: - type q4_0:   97 tensors
0.00.036.149 I llama_model_loader: - type q6_K:    1 tensors
0.00.036.150 I print_info: file format = GGUF V3 (latest)
0.00.036.150 I print_info: file type   = Q4_0
0.00.036.151 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.044.034 I load: special tokens cache size = 25
0.00.050.185 I load: token to piece cache size = 0.2984 MB
0.00.050.189 I print_info: arch             = gptneox
0.00.050.189 I print_info: vocab_only       = 0
0.00.050.190 I print_info: n_ctx_train      = 2048
0.00.050.190 I print_info: n_embd           = 2048
0.00.050.190 I print_info: n_layer          = 24
0.00.050.194 I print_info: n_head           = 16
0.00.050.194 I print_info: n_head_kv        = 16
0.00.050.195 I print_info: n_rot            = 32
0.00.050.196 I print_info: n_swa            = 0
0.00.050.197 I print_info: n_embd_head_k    = 128
0.00.050.197 I print_info: n_embd_head_v    = 128
0.00.050.197 I print_info: n_gqa            = 1
0.00.050.198 I print_info: n_embd_k_gqa     = 2048
0.00.050.199 I print_info: n_embd_v_gqa     = 2048
0.00.050.200 I print_info: f_norm_eps       = 1.0e-05
0.00.050.200 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.050.200 I print_info: f_clamp_kqv      = 0.0e+00
0.00.050.200 I print_info: f_max_alibi_bias = 0.0e+00
0.00.050.200 I print_info: f_logit_scale    = 0.0e+00
0.00.050.203 I print_info: n_ff             = 8192
0.00.050.203 I print_info: n_expert         = 0
0.00.050.203 I print_info: n_expert_used    = 0
0.00.050.203 I print_info: causal attn      = 1
0.00.050.203 I print_info: pooling type     = 0
0.00.050.203 I print_info: rope type        = 2
0.00.050.204 I print_info: rope scaling     = linear
0.00.050.204 I print_info: freq_base_train  = 10000.0
0.00.050.204 I print_info: freq_scale_train = 1
0.00.050.206 I print_info: n_ctx_orig_yarn  = 2048
0.00.050.206 I print_info: rope_finetuned   = unknown
0.00.050.206 I print_info: ssm_d_conv       = 0
0.00.050.206 I print_info: ssm_d_inner      = 0
0.00.050.206 I print_info: ssm_d_state      = 0
0.00.050.206 I print_info: ssm_dt_rank      = 0
0.00.050.207 I print_info: ssm_dt_b_c_rms   = 0
0.00.050.208 I print_info: model type       = 1.4B
0.00.050.208 I print_info: model params     = 1.41 B
0.00.050.208 I print_info: general.name     = 1.4B
0.00.050.209 I print_info: vocab type       = BPE
0.00.050.209 I print_info: n_vocab          = 50304
0.00.050.209 I print_info: n_merges         = 50009
0.00.050.209 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.050.209 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.050.210 I print_info: LF token         = 187 'Ċ'
0.00.050.214 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.050.216 I print_info: max token length = 1024
0.00.050.216 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.585.396 I load_tensors: offloading 24 repeating layers to GPU
0.00.585.410 I load_tensors: offloading output layer to GPU
0.00.585.411 I load_tensors: offloaded 25/25 layers to GPU
0.00.585.447 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.585.448 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.586.889 I llama_init_from_model: n_seq_max     = 1
0.00.586.891 I llama_init_from_model: n_ctx         = 128
0.00.586.892 I llama_init_from_model: n_ctx_per_seq = 128
0.00.586.892 I llama_init_from_model: n_batch       = 128
0.00.586.892 I llama_init_from_model: n_ubatch      = 128
0.00.586.893 I llama_init_from_model: flash_attn    = 0
0.00.586.895 I llama_init_from_model: freq_base     = 10000.0
0.00.586.896 I llama_init_from_model: freq_scale    = 1
0.00.586.896 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.586.899 I ggml_metal_init: allocating
0.00.587.026 I ggml_metal_init: found device: Apple M4
0.00.587.041 I ggml_metal_init: picking default device: Apple M4
0.00.588.967 I ggml_metal_init: using embedded metal library
0.00.595.348 I ggml_metal_init: GPU name:   Apple M4
0.00.595.357 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.595.358 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.595.359 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.595.359 I ggml_metal_init: simdgroup reduction   = true
0.00.595.360 I ggml_metal_init: simdgroup matrix mul. = true
0.00.595.360 I ggml_metal_init: has residency sets    = true
0.00.595.360 I ggml_metal_init: has bfloat            = true
0.00.595.361 I ggml_metal_init: use bfloat            = true
0.00.595.362 I ggml_metal_init: hasUnifiedMemory      = true
0.00.595.366 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.614.457 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.618.113 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.618.121 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.618.148 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.449 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.451 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.451 I llama_init_from_model: graph nodes  = 967
0.00.621.452 I llama_init_from_model: graph splits = 2
0.00.621.455 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.455 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.789 I 
0.00.645.838 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.867 I perplexity: tokenizing the input ..
0.00.653.265 I perplexity: tokenization took 7.396 ms
0.00.653.286 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.790.540 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.791.973 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.791.989 I llama_perf_context_print:        load time =     629.73 ms
0.00.791.990 I llama_perf_context_print: prompt eval time =     136.38 ms /   128 tokens (    1.07 ms per token,   938.58 tokens per second)
0.00.791.991 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.791.991 I llama_perf_context_print:       total time =     146.20 ms /   129 tokens
0.00.792.324 I ggml_metal_free: deallocating

real	0m0.808s
user	0m0.080s
sys	0m0.116s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.004 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.918 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.924 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.925 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.926 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.926 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.926 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.927 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.927 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.928 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.928 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.929 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.929 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.929 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.930 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.933 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.933 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.934 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.736 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.810 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.764 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.766 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.767 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.768 I llama_model_loader: - type  f32:  194 tensors
0.00.024.768 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.769 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.769 I print_info: file format = GGUF V3 (latest)
0.00.024.770 I print_info: file type   = Q4_1
0.00.024.777 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.132 I load: special tokens cache size = 25
0.00.039.247 I load: token to piece cache size = 0.2984 MB
0.00.039.254 I print_info: arch             = gptneox
0.00.039.254 I print_info: vocab_only       = 0
0.00.039.254 I print_info: n_ctx_train      = 2048
0.00.039.256 I print_info: n_embd           = 2048
0.00.039.256 I print_info: n_layer          = 24
0.00.039.261 I print_info: n_head           = 16
0.00.039.262 I print_info: n_head_kv        = 16
0.00.039.262 I print_info: n_rot            = 32
0.00.039.263 I print_info: n_swa            = 0
0.00.039.265 I print_info: n_embd_head_k    = 128
0.00.039.265 I print_info: n_embd_head_v    = 128
0.00.039.265 I print_info: n_gqa            = 1
0.00.039.266 I print_info: n_embd_k_gqa     = 2048
0.00.039.266 I print_info: n_embd_v_gqa     = 2048
0.00.039.267 I print_info: f_norm_eps       = 1.0e-05
0.00.039.267 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.268 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.268 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.268 I print_info: f_logit_scale    = 0.0e+00
0.00.039.269 I print_info: n_ff             = 8192
0.00.039.269 I print_info: n_expert         = 0
0.00.039.269 I print_info: n_expert_used    = 0
0.00.039.269 I print_info: causal attn      = 1
0.00.039.269 I print_info: pooling type     = 0
0.00.039.271 I print_info: rope type        = 2
0.00.039.271 I print_info: rope scaling     = linear
0.00.039.271 I print_info: freq_base_train  = 10000.0
0.00.039.271 I print_info: freq_scale_train = 1
0.00.039.272 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.272 I print_info: rope_finetuned   = unknown
0.00.039.272 I print_info: ssm_d_conv       = 0
0.00.039.272 I print_info: ssm_d_inner      = 0
0.00.039.272 I print_info: ssm_d_state      = 0
0.00.039.272 I print_info: ssm_dt_rank      = 0
0.00.039.272 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.273 I print_info: model type       = 1.4B
0.00.039.274 I print_info: model params     = 1.41 B
0.00.039.274 I print_info: general.name     = 1.4B
0.00.039.275 I print_info: vocab type       = BPE
0.00.039.275 I print_info: n_vocab          = 50304
0.00.039.275 I print_info: n_merges         = 50009
0.00.039.275 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.275 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.275 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.275 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.276 I print_info: LF token         = 187 'Ċ'
0.00.039.276 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.276 I print_info: max token length = 1024
0.00.039.276 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.641.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.641.821 I load_tensors: offloading output layer to GPU
0.00.641.822 I load_tensors: offloaded 25/25 layers to GPU
0.00.641.856 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.641.858 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.643.256 I llama_init_from_model: n_seq_max     = 1
0.00.643.259 I llama_init_from_model: n_ctx         = 128
0.00.643.260 I llama_init_from_model: n_ctx_per_seq = 128
0.00.643.261 I llama_init_from_model: n_batch       = 128
0.00.643.261 I llama_init_from_model: n_ubatch      = 128
0.00.643.262 I llama_init_from_model: flash_attn    = 0
0.00.643.264 I llama_init_from_model: freq_base     = 10000.0
0.00.643.265 I llama_init_from_model: freq_scale    = 1
0.00.643.266 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.643.268 I ggml_metal_init: allocating
0.00.643.370 I ggml_metal_init: found device: Apple M4
0.00.643.385 I ggml_metal_init: picking default device: Apple M4
0.00.645.187 I ggml_metal_init: using embedded metal library
0.00.651.798 I ggml_metal_init: GPU name:   Apple M4
0.00.651.806 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.651.807 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.651.807 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.651.808 I ggml_metal_init: simdgroup reduction   = true
0.00.651.808 I ggml_metal_init: simdgroup matrix mul. = true
0.00.651.809 I ggml_metal_init: has residency sets    = true
0.00.651.809 I ggml_metal_init: has bfloat            = true
0.00.651.809 I ggml_metal_init: use bfloat            = true
0.00.651.811 I ggml_metal_init: hasUnifiedMemory      = true
0.00.651.814 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.670.357 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.673.929 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.673.932 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.673.962 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.677.302 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.677.304 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.677.305 I llama_init_from_model: graph nodes  = 967
0.00.677.305 I llama_init_from_model: graph splits = 2
0.00.677.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.677.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.704.881 I 
0.00.704.944 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.704.970 I perplexity: tokenizing the input ..
0.00.712.630 I perplexity: tokenization took 7.656 ms
0.00.712.653 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.847.103 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.848.452 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.848.472 I llama_perf_context_print:        load time =     695.87 ms
0.00.848.473 I llama_perf_context_print: prompt eval time =     133.47 ms /   128 tokens (    1.04 ms per token,   959.00 tokens per second)
0.00.848.474 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.848.474 I llama_perf_context_print:       total time =     143.60 ms /   129 tokens
0.00.848.875 I ggml_metal_free: deallocating

real	0m0.863s
user	0m0.081s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.925 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.219 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.229 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.183 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.242 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.169 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.170 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.170 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.171 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.171 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.172 I llama_model_loader: - type  f32:  194 tensors
0.00.025.172 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.173 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.173 I print_info: file format = GGUF V3 (latest)
0.00.025.174 I print_info: file type   = Q5_0
0.00.025.175 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.033.466 I load: special tokens cache size = 25
0.00.039.493 I load: token to piece cache size = 0.2984 MB
0.00.039.498 I print_info: arch             = gptneox
0.00.039.498 I print_info: vocab_only       = 0
0.00.039.498 I print_info: n_ctx_train      = 2048
0.00.039.498 I print_info: n_embd           = 2048
0.00.039.498 I print_info: n_layer          = 24
0.00.039.503 I print_info: n_head           = 16
0.00.039.504 I print_info: n_head_kv        = 16
0.00.039.504 I print_info: n_rot            = 32
0.00.039.504 I print_info: n_swa            = 0
0.00.039.504 I print_info: n_embd_head_k    = 128
0.00.039.505 I print_info: n_embd_head_v    = 128
0.00.039.505 I print_info: n_gqa            = 1
0.00.039.506 I print_info: n_embd_k_gqa     = 2048
0.00.039.507 I print_info: n_embd_v_gqa     = 2048
0.00.039.507 I print_info: f_norm_eps       = 1.0e-05
0.00.039.511 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.511 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.511 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.511 I print_info: f_logit_scale    = 0.0e+00
0.00.039.512 I print_info: n_ff             = 8192
0.00.039.512 I print_info: n_expert         = 0
0.00.039.512 I print_info: n_expert_used    = 0
0.00.039.512 I print_info: causal attn      = 1
0.00.039.512 I print_info: pooling type     = 0
0.00.039.513 I print_info: rope type        = 2
0.00.039.513 I print_info: rope scaling     = linear
0.00.039.513 I print_info: freq_base_train  = 10000.0
0.00.039.513 I print_info: freq_scale_train = 1
0.00.039.513 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.514 I print_info: rope_finetuned   = unknown
0.00.039.514 I print_info: ssm_d_conv       = 0
0.00.039.514 I print_info: ssm_d_inner      = 0
0.00.039.514 I print_info: ssm_d_state      = 0
0.00.039.514 I print_info: ssm_dt_rank      = 0
0.00.039.514 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.515 I print_info: model type       = 1.4B
0.00.039.515 I print_info: model params     = 1.41 B
0.00.039.515 I print_info: general.name     = 1.4B
0.00.039.516 I print_info: vocab type       = BPE
0.00.039.516 I print_info: n_vocab          = 50304
0.00.039.516 I print_info: n_merges         = 50009
0.00.039.516 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.516 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: LF token         = 187 'Ċ'
0.00.039.517 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.517 I print_info: max token length = 1024
0.00.039.518 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.613.076 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.087 I load_tensors: offloading output layer to GPU
0.00.613.088 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.120 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.613.122 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.614.760 I llama_init_from_model: n_seq_max     = 1
0.00.614.764 I llama_init_from_model: n_ctx         = 128
0.00.614.765 I llama_init_from_model: n_ctx_per_seq = 128
0.00.614.765 I llama_init_from_model: n_batch       = 128
0.00.614.765 I llama_init_from_model: n_ubatch      = 128
0.00.614.766 I llama_init_from_model: flash_attn    = 0
0.00.614.768 I llama_init_from_model: freq_base     = 10000.0
0.00.614.768 I llama_init_from_model: freq_scale    = 1
0.00.614.769 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.614.771 I ggml_metal_init: allocating
0.00.614.804 I ggml_metal_init: found device: Apple M4
0.00.614.815 I ggml_metal_init: picking default device: Apple M4
0.00.616.222 I ggml_metal_init: using embedded metal library
0.00.622.517 I ggml_metal_init: GPU name:   Apple M4
0.00.622.521 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.522 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.523 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.523 I ggml_metal_init: simdgroup reduction   = true
0.00.622.524 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.524 I ggml_metal_init: has residency sets    = true
0.00.622.524 I ggml_metal_init: has bfloat            = true
0.00.622.525 I ggml_metal_init: use bfloat            = true
0.00.622.525 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.530 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.639.393 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.642.960 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.642.963 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.642.990 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.646.188 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.646.190 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.646.190 I llama_init_from_model: graph nodes  = 967
0.00.646.191 I llama_init_from_model: graph splits = 2
0.00.646.193 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.646.193 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.676.018 I 
0.00.676.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.676.111 I perplexity: tokenizing the input ..
0.00.681.450 I perplexity: tokenization took 5.339 ms
0.00.681.462 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.819.233 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.820.577 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.820.599 I llama_perf_context_print:        load time =     667.08 ms
0.00.820.600 I llama_perf_context_print: prompt eval time =     137.54 ms /   128 tokens (    1.07 ms per token,   930.62 tokens per second)
0.00.820.601 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.820.602 I llama_perf_context_print:       total time =     144.59 ms /   129 tokens
0.00.820.941 I ggml_metal_free: deallocating

real	0m0.835s
user	0m0.076s
sys	0m0.127s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.969 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.047 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.052 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.054 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.060 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.061 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.061 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.062 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.063 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.063 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.063 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.063 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.064 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.064 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.066 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.066 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.066 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.852 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.890 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.695 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.697 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.697 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.697 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.698 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.698 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.699 I llama_model_loader: - type  f32:  194 tensors
0.00.025.699 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.699 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.700 I print_info: file format = GGUF V3 (latest)
0.00.025.700 I print_info: file type   = Q5_1
0.00.025.702 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.033.583 I load: special tokens cache size = 25
0.00.039.691 I load: token to piece cache size = 0.2984 MB
0.00.039.694 I print_info: arch             = gptneox
0.00.039.694 I print_info: vocab_only       = 0
0.00.039.694 I print_info: n_ctx_train      = 2048
0.00.039.695 I print_info: n_embd           = 2048
0.00.039.695 I print_info: n_layer          = 24
0.00.039.699 I print_info: n_head           = 16
0.00.039.700 I print_info: n_head_kv        = 16
0.00.039.700 I print_info: n_rot            = 32
0.00.039.700 I print_info: n_swa            = 0
0.00.039.700 I print_info: n_embd_head_k    = 128
0.00.039.700 I print_info: n_embd_head_v    = 128
0.00.039.701 I print_info: n_gqa            = 1
0.00.039.702 I print_info: n_embd_k_gqa     = 2048
0.00.039.702 I print_info: n_embd_v_gqa     = 2048
0.00.039.703 I print_info: f_norm_eps       = 1.0e-05
0.00.039.704 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.704 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.704 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.704 I print_info: f_logit_scale    = 0.0e+00
0.00.039.705 I print_info: n_ff             = 8192
0.00.039.705 I print_info: n_expert         = 0
0.00.039.705 I print_info: n_expert_used    = 0
0.00.039.706 I print_info: causal attn      = 1
0.00.039.706 I print_info: pooling type     = 0
0.00.039.706 I print_info: rope type        = 2
0.00.039.709 I print_info: rope scaling     = linear
0.00.039.709 I print_info: freq_base_train  = 10000.0
0.00.039.709 I print_info: freq_scale_train = 1
0.00.039.709 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.710 I print_info: rope_finetuned   = unknown
0.00.039.710 I print_info: ssm_d_conv       = 0
0.00.039.710 I print_info: ssm_d_inner      = 0
0.00.039.710 I print_info: ssm_d_state      = 0
0.00.039.710 I print_info: ssm_dt_rank      = 0
0.00.039.710 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.711 I print_info: model type       = 1.4B
0.00.039.711 I print_info: model params     = 1.41 B
0.00.039.711 I print_info: general.name     = 1.4B
0.00.039.712 I print_info: vocab type       = BPE
0.00.039.712 I print_info: n_vocab          = 50304
0.00.039.712 I print_info: n_merges         = 50009
0.00.039.712 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.712 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.712 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.713 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.714 I print_info: LF token         = 187 'Ċ'
0.00.039.717 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.717 I print_info: max token length = 1024
0.00.039.718 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.676.596 I load_tensors: offloading 24 repeating layers to GPU
0.00.676.609 I load_tensors: offloading output layer to GPU
0.00.676.610 I load_tensors: offloaded 25/25 layers to GPU
0.00.676.641 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.676.642 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.678.216 I llama_init_from_model: n_seq_max     = 1
0.00.678.221 I llama_init_from_model: n_ctx         = 128
0.00.678.222 I llama_init_from_model: n_ctx_per_seq = 128
0.00.678.222 I llama_init_from_model: n_batch       = 128
0.00.678.223 I llama_init_from_model: n_ubatch      = 128
0.00.678.223 I llama_init_from_model: flash_attn    = 0
0.00.678.224 I llama_init_from_model: freq_base     = 10000.0
0.00.678.225 I llama_init_from_model: freq_scale    = 1
0.00.678.225 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.678.227 I ggml_metal_init: allocating
0.00.678.317 I ggml_metal_init: found device: Apple M4
0.00.678.331 I ggml_metal_init: picking default device: Apple M4
0.00.680.016 I ggml_metal_init: using embedded metal library
0.00.686.393 I ggml_metal_init: GPU name:   Apple M4
0.00.686.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.686.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.686.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.686.399 I ggml_metal_init: simdgroup reduction   = true
0.00.686.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.686.400 I ggml_metal_init: has residency sets    = true
0.00.686.400 I ggml_metal_init: has bfloat            = true
0.00.686.400 I ggml_metal_init: use bfloat            = true
0.00.686.401 I ggml_metal_init: hasUnifiedMemory      = true
0.00.686.403 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.704.064 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.707.719 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.707.723 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.707.750 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.710.885 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.710.887 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.710.888 I llama_init_from_model: graph nodes  = 967
0.00.710.888 I llama_init_from_model: graph splits = 2
0.00.710.891 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.710.892 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.176 I 
0.00.741.236 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.277 I perplexity: tokenizing the input ..
0.00.748.389 I perplexity: tokenization took 7.109 ms
0.00.748.408 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.884.694 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.886.038 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.886.052 I llama_perf_context_print:        load time =     731.20 ms
0.00.886.053 I llama_perf_context_print: prompt eval time =     135.29 ms /   128 tokens (    1.06 ms per token,   946.12 tokens per second)
0.00.886.054 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.886.054 I llama_perf_context_print:       total time =     144.88 ms /   129 tokens
0.00.886.471 I ggml_metal_free: deallocating

real	0m0.902s
user	0m0.080s
sys	0m0.167s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.112 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.964 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.786 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.793 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.799 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.800 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.800 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.801 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.801 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.802 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.802 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.802 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.803 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.803 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.803 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.803 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.805 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.805 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.806 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.677 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.680 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.547 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.549 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.549 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.550 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.550 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.550 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.551 I llama_model_loader: - type  f32:  194 tensors
0.00.024.551 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.552 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.552 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.553 I print_info: file format = GGUF V3 (latest)
0.00.024.557 I print_info: file type   = Q2_K - Medium
0.00.024.558 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.503 I load: special tokens cache size = 25
0.00.038.569 I load: token to piece cache size = 0.2984 MB
0.00.038.573 I print_info: arch             = gptneox
0.00.038.573 I print_info: vocab_only       = 0
0.00.038.574 I print_info: n_ctx_train      = 2048
0.00.038.574 I print_info: n_embd           = 2048
0.00.038.574 I print_info: n_layer          = 24
0.00.038.578 I print_info: n_head           = 16
0.00.038.579 I print_info: n_head_kv        = 16
0.00.038.579 I print_info: n_rot            = 32
0.00.038.580 I print_info: n_swa            = 0
0.00.038.582 I print_info: n_embd_head_k    = 128
0.00.038.582 I print_info: n_embd_head_v    = 128
0.00.038.583 I print_info: n_gqa            = 1
0.00.038.584 I print_info: n_embd_k_gqa     = 2048
0.00.038.584 I print_info: n_embd_v_gqa     = 2048
0.00.038.585 I print_info: f_norm_eps       = 1.0e-05
0.00.038.585 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.585 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.585 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.586 I print_info: f_logit_scale    = 0.0e+00
0.00.038.586 I print_info: n_ff             = 8192
0.00.038.586 I print_info: n_expert         = 0
0.00.038.586 I print_info: n_expert_used    = 0
0.00.038.587 I print_info: causal attn      = 1
0.00.038.587 I print_info: pooling type     = 0
0.00.038.587 I print_info: rope type        = 2
0.00.038.587 I print_info: rope scaling     = linear
0.00.038.587 I print_info: freq_base_train  = 10000.0
0.00.038.588 I print_info: freq_scale_train = 1
0.00.038.588 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.588 I print_info: rope_finetuned   = unknown
0.00.038.588 I print_info: ssm_d_conv       = 0
0.00.038.588 I print_info: ssm_d_inner      = 0
0.00.038.588 I print_info: ssm_d_state      = 0
0.00.038.588 I print_info: ssm_dt_rank      = 0
0.00.038.589 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.589 I print_info: model type       = 1.4B
0.00.038.591 I print_info: model params     = 1.41 B
0.00.038.591 I print_info: general.name     = 1.4B
0.00.038.591 I print_info: vocab type       = BPE
0.00.038.591 I print_info: n_vocab          = 50304
0.00.038.591 I print_info: n_merges         = 50009
0.00.038.592 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.593 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.593 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.593 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.593 I print_info: LF token         = 187 'Ċ'
0.00.038.594 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.594 I print_info: max token length = 1024
0.00.038.594 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.371.513 I load_tensors: offloading 24 repeating layers to GPU
0.00.371.529 I load_tensors: offloading output layer to GPU
0.00.371.530 I load_tensors: offloaded 25/25 layers to GPU
0.00.371.561 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.371.562 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.373.286 I llama_init_from_model: n_seq_max     = 1
0.00.373.289 I llama_init_from_model: n_ctx         = 128
0.00.373.290 I llama_init_from_model: n_ctx_per_seq = 128
0.00.373.290 I llama_init_from_model: n_batch       = 128
0.00.373.290 I llama_init_from_model: n_ubatch      = 128
0.00.373.291 I llama_init_from_model: flash_attn    = 0
0.00.373.293 I llama_init_from_model: freq_base     = 10000.0
0.00.373.293 I llama_init_from_model: freq_scale    = 1
0.00.373.294 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.373.296 I ggml_metal_init: allocating
0.00.373.365 I ggml_metal_init: found device: Apple M4
0.00.373.378 I ggml_metal_init: picking default device: Apple M4
0.00.375.168 I ggml_metal_init: using embedded metal library
0.00.380.671 I ggml_metal_init: GPU name:   Apple M4
0.00.380.687 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.380.688 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.380.689 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.380.689 I ggml_metal_init: simdgroup reduction   = true
0.00.380.690 I ggml_metal_init: simdgroup matrix mul. = true
0.00.380.690 I ggml_metal_init: has residency sets    = true
0.00.380.690 I ggml_metal_init: has bfloat            = true
0.00.380.691 I ggml_metal_init: use bfloat            = true
0.00.380.692 I ggml_metal_init: hasUnifiedMemory      = true
0.00.380.696 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.401.358 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.404.973 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.404.986 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.405.037 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.408.549 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.408.551 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.408.552 I llama_init_from_model: graph nodes  = 967
0.00.408.552 I llama_init_from_model: graph splits = 2
0.00.408.555 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.408.556 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.861 I 
0.00.438.928 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.958 I perplexity: tokenizing the input ..
0.00.446.004 I perplexity: tokenization took 7.043 ms
0.00.446.026 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.636 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.580.978 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.580.991 I llama_perf_context_print:        load time =     429.89 ms
0.00.580.992 I llama_perf_context_print: prompt eval time =     132.72 ms /   128 tokens (    1.04 ms per token,   964.44 tokens per second)
0.00.580.992 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.580.993 I llama_perf_context_print:       total time =     142.13 ms /   129 tokens
0.00.581.359 I ggml_metal_free: deallocating

real	0m0.595s
user	0m0.081s
sys	0m0.100s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.716 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.586 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.592 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.594 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.595 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.595 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.595 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.596 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.597 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.597 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.599 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.600 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.602 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.449 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.498 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.334 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.335 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.336 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.337 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.337 I llama_model_loader: - type  f32:  194 tensors
0.00.024.338 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.338 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.338 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.338 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.339 I print_info: file format = GGUF V3 (latest)
0.00.024.340 I print_info: file type   = Q3_K - Medium
0.00.024.341 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.310 I load: special tokens cache size = 25
0.00.038.536 I load: token to piece cache size = 0.2984 MB
0.00.038.540 I print_info: arch             = gptneox
0.00.038.540 I print_info: vocab_only       = 0
0.00.038.540 I print_info: n_ctx_train      = 2048
0.00.038.541 I print_info: n_embd           = 2048
0.00.038.541 I print_info: n_layer          = 24
0.00.038.545 I print_info: n_head           = 16
0.00.038.548 I print_info: n_head_kv        = 16
0.00.038.548 I print_info: n_rot            = 32
0.00.038.548 I print_info: n_swa            = 0
0.00.038.548 I print_info: n_embd_head_k    = 128
0.00.038.549 I print_info: n_embd_head_v    = 128
0.00.038.550 I print_info: n_gqa            = 1
0.00.038.551 I print_info: n_embd_k_gqa     = 2048
0.00.038.552 I print_info: n_embd_v_gqa     = 2048
0.00.038.552 I print_info: f_norm_eps       = 1.0e-05
0.00.038.553 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.553 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.553 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.555 I print_info: f_logit_scale    = 0.0e+00
0.00.038.556 I print_info: n_ff             = 8192
0.00.038.556 I print_info: n_expert         = 0
0.00.038.556 I print_info: n_expert_used    = 0
0.00.038.556 I print_info: causal attn      = 1
0.00.038.556 I print_info: pooling type     = 0
0.00.038.556 I print_info: rope type        = 2
0.00.038.557 I print_info: rope scaling     = linear
0.00.038.557 I print_info: freq_base_train  = 10000.0
0.00.038.557 I print_info: freq_scale_train = 1
0.00.038.558 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.558 I print_info: rope_finetuned   = unknown
0.00.038.559 I print_info: ssm_d_conv       = 0
0.00.038.559 I print_info: ssm_d_inner      = 0
0.00.038.560 I print_info: ssm_d_state      = 0
0.00.038.560 I print_info: ssm_dt_rank      = 0
0.00.038.560 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.560 I print_info: model type       = 1.4B
0.00.038.561 I print_info: model params     = 1.41 B
0.00.038.561 I print_info: general.name     = 1.4B
0.00.038.561 I print_info: vocab type       = BPE
0.00.038.561 I print_info: n_vocab          = 50304
0.00.038.561 I print_info: n_merges         = 50009
0.00.038.562 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.562 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.562 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.562 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.562 I print_info: LF token         = 187 'Ċ'
0.00.038.563 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.563 I print_info: max token length = 1024
0.00.038.564 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.446.288 I load_tensors: offloading 24 repeating layers to GPU
0.00.446.301 I load_tensors: offloading output layer to GPU
0.00.446.302 I load_tensors: offloaded 25/25 layers to GPU
0.00.446.332 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.446.334 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.448.013 I llama_init_from_model: n_seq_max     = 1
0.00.448.015 I llama_init_from_model: n_ctx         = 128
0.00.448.016 I llama_init_from_model: n_ctx_per_seq = 128
0.00.448.016 I llama_init_from_model: n_batch       = 128
0.00.448.017 I llama_init_from_model: n_ubatch      = 128
0.00.448.017 I llama_init_from_model: flash_attn    = 0
0.00.448.020 I llama_init_from_model: freq_base     = 10000.0
0.00.448.020 I llama_init_from_model: freq_scale    = 1
0.00.448.021 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.448.023 I ggml_metal_init: allocating
0.00.448.095 I ggml_metal_init: found device: Apple M4
0.00.448.109 I ggml_metal_init: picking default device: Apple M4
0.00.449.914 I ggml_metal_init: using embedded metal library
0.00.456.047 I ggml_metal_init: GPU name:   Apple M4
0.00.456.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.456.053 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.456.054 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.456.055 I ggml_metal_init: simdgroup reduction   = true
0.00.456.055 I ggml_metal_init: simdgroup matrix mul. = true
0.00.456.056 I ggml_metal_init: has residency sets    = true
0.00.456.056 I ggml_metal_init: has bfloat            = true
0.00.456.056 I ggml_metal_init: use bfloat            = true
0.00.456.057 I ggml_metal_init: hasUnifiedMemory      = true
0.00.456.060 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.474.760 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.478.278 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.478.282 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.478.318 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.481.684 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.481.686 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.481.687 I llama_init_from_model: graph nodes  = 967
0.00.481.687 I llama_init_from_model: graph splits = 2
0.00.481.690 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.481.690 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.509.258 I 
0.00.509.319 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.509.349 I perplexity: tokenizing the input ..
0.00.516.864 I perplexity: tokenization took 7.511 ms
0.00.516.890 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.659.227 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.660.586 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.660.600 I llama_perf_context_print:        load time =     500.53 ms
0.00.660.601 I llama_perf_context_print: prompt eval time =     141.49 ms /   128 tokens (    1.11 ms per token,   904.65 tokens per second)
0.00.660.601 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.660.602 I llama_perf_context_print:       total time =     151.35 ms /   129 tokens
0.00.660.991 I ggml_metal_free: deallocating

real	0m0.675s
user	0m0.080s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.868 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.561 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.567 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.569 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.569 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.570 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.570 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.571 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.572 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.573 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.573 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.573 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.574 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.576 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.576 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.576 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.496 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.499 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.395 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.396 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.397 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.397 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.397 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.398 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.398 I llama_model_loader: - type  f32:  194 tensors
0.00.025.399 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.399 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.399 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.400 I print_info: file format = GGUF V3 (latest)
0.00.025.400 I print_info: file type   = Q4_K - Medium
0.00.025.401 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.565 I load: special tokens cache size = 25
0.00.039.656 I load: token to piece cache size = 0.2984 MB
0.00.039.659 I print_info: arch             = gptneox
0.00.039.659 I print_info: vocab_only       = 0
0.00.039.660 I print_info: n_ctx_train      = 2048
0.00.039.660 I print_info: n_embd           = 2048
0.00.039.660 I print_info: n_layer          = 24
0.00.039.663 I print_info: n_head           = 16
0.00.039.664 I print_info: n_head_kv        = 16
0.00.039.665 I print_info: n_rot            = 32
0.00.039.665 I print_info: n_swa            = 0
0.00.039.665 I print_info: n_embd_head_k    = 128
0.00.039.665 I print_info: n_embd_head_v    = 128
0.00.039.666 I print_info: n_gqa            = 1
0.00.039.667 I print_info: n_embd_k_gqa     = 2048
0.00.039.667 I print_info: n_embd_v_gqa     = 2048
0.00.039.668 I print_info: f_norm_eps       = 1.0e-05
0.00.039.668 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.669 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.669 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.669 I print_info: f_logit_scale    = 0.0e+00
0.00.039.670 I print_info: n_ff             = 8192
0.00.039.670 I print_info: n_expert         = 0
0.00.039.670 I print_info: n_expert_used    = 0
0.00.039.670 I print_info: causal attn      = 1
0.00.039.670 I print_info: pooling type     = 0
0.00.039.670 I print_info: rope type        = 2
0.00.039.671 I print_info: rope scaling     = linear
0.00.039.671 I print_info: freq_base_train  = 10000.0
0.00.039.672 I print_info: freq_scale_train = 1
0.00.039.672 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.672 I print_info: rope_finetuned   = unknown
0.00.039.672 I print_info: ssm_d_conv       = 0
0.00.039.672 I print_info: ssm_d_inner      = 0
0.00.039.673 I print_info: ssm_d_state      = 0
0.00.039.673 I print_info: ssm_dt_rank      = 0
0.00.039.673 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.673 I print_info: model type       = 1.4B
0.00.039.673 I print_info: model params     = 1.41 B
0.00.039.673 I print_info: general.name     = 1.4B
0.00.039.674 I print_info: vocab type       = BPE
0.00.039.674 I print_info: n_vocab          = 50304
0.00.039.675 I print_info: n_merges         = 50009
0.00.039.675 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.675 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.675 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.675 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.676 I print_info: LF token         = 187 'Ċ'
0.00.039.679 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.679 I print_info: max token length = 1024
0.00.039.679 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.538.066 I load_tensors: offloading 24 repeating layers to GPU
0.00.538.079 I load_tensors: offloading output layer to GPU
0.00.538.080 I load_tensors: offloaded 25/25 layers to GPU
0.00.538.111 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.538.112 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.539.751 I llama_init_from_model: n_seq_max     = 1
0.00.539.754 I llama_init_from_model: n_ctx         = 128
0.00.539.755 I llama_init_from_model: n_ctx_per_seq = 128
0.00.539.755 I llama_init_from_model: n_batch       = 128
0.00.539.756 I llama_init_from_model: n_ubatch      = 128
0.00.539.756 I llama_init_from_model: flash_attn    = 0
0.00.539.758 I llama_init_from_model: freq_base     = 10000.0
0.00.539.758 I llama_init_from_model: freq_scale    = 1
0.00.539.759 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.539.761 I ggml_metal_init: allocating
0.00.539.809 I ggml_metal_init: found device: Apple M4
0.00.539.821 I ggml_metal_init: picking default device: Apple M4
0.00.542.230 I ggml_metal_init: using embedded metal library
0.00.549.258 I ggml_metal_init: GPU name:   Apple M4
0.00.549.263 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.549.264 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.549.265 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.549.266 I ggml_metal_init: simdgroup reduction   = true
0.00.549.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.549.267 I ggml_metal_init: has residency sets    = true
0.00.549.267 I ggml_metal_init: has bfloat            = true
0.00.549.267 I ggml_metal_init: use bfloat            = true
0.00.549.268 I ggml_metal_init: hasUnifiedMemory      = true
0.00.549.270 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.567.588 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.571.323 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.571.327 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.571.355 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.574.647 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.574.649 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.574.649 I llama_init_from_model: graph nodes  = 967
0.00.574.650 I llama_init_from_model: graph splits = 2
0.00.574.653 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.574.653 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.600.580 I 
0.00.600.635 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.600.663 I perplexity: tokenizing the input ..
0.00.606.279 I perplexity: tokenization took 5.616 ms
0.00.606.290 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.739.659 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.740.988 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.741.004 I llama_perf_context_print:        load time =     590.70 ms
0.00.741.005 I llama_perf_context_print: prompt eval time =     133.13 ms /   128 tokens (    1.04 ms per token,   961.47 tokens per second)
0.00.741.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.741.006 I llama_perf_context_print:       total time =     140.43 ms /   129 tokens
0.00.741.422 I ggml_metal_free: deallocating

real	0m0.757s
user	0m0.077s
sys	0m0.138s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.954 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.783 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.015.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.791 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.796 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.796 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.796 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.797 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.798 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.798 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.799 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.800 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.800 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.800 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.801 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.802 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.803 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.803 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.635 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.639 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.531 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.532 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.533 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.533 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.533 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.534 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.534 I llama_model_loader: - type  f32:  194 tensors
0.00.024.535 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.535 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.536 I print_info: file format = GGUF V3 (latest)
0.00.024.540 I print_info: file type   = Q5_K - Medium
0.00.024.541 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.624 I load: special tokens cache size = 25
0.00.038.762 I load: token to piece cache size = 0.2984 MB
0.00.038.767 I print_info: arch             = gptneox
0.00.038.767 I print_info: vocab_only       = 0
0.00.038.767 I print_info: n_ctx_train      = 2048
0.00.038.767 I print_info: n_embd           = 2048
0.00.038.768 I print_info: n_layer          = 24
0.00.038.772 I print_info: n_head           = 16
0.00.038.773 I print_info: n_head_kv        = 16
0.00.038.773 I print_info: n_rot            = 32
0.00.038.773 I print_info: n_swa            = 0
0.00.038.774 I print_info: n_embd_head_k    = 128
0.00.038.774 I print_info: n_embd_head_v    = 128
0.00.038.775 I print_info: n_gqa            = 1
0.00.038.776 I print_info: n_embd_k_gqa     = 2048
0.00.038.777 I print_info: n_embd_v_gqa     = 2048
0.00.038.778 I print_info: f_norm_eps       = 1.0e-05
0.00.038.778 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.778 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.780 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.780 I print_info: f_logit_scale    = 0.0e+00
0.00.038.781 I print_info: n_ff             = 8192
0.00.038.781 I print_info: n_expert         = 0
0.00.038.781 I print_info: n_expert_used    = 0
0.00.038.781 I print_info: causal attn      = 1
0.00.038.782 I print_info: pooling type     = 0
0.00.038.783 I print_info: rope type        = 2
0.00.038.783 I print_info: rope scaling     = linear
0.00.038.784 I print_info: freq_base_train  = 10000.0
0.00.038.784 I print_info: freq_scale_train = 1
0.00.038.784 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.785 I print_info: rope_finetuned   = unknown
0.00.038.785 I print_info: ssm_d_conv       = 0
0.00.038.785 I print_info: ssm_d_inner      = 0
0.00.038.785 I print_info: ssm_d_state      = 0
0.00.038.785 I print_info: ssm_dt_rank      = 0
0.00.038.785 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.786 I print_info: model type       = 1.4B
0.00.038.786 I print_info: model params     = 1.41 B
0.00.038.786 I print_info: general.name     = 1.4B
0.00.038.787 I print_info: vocab type       = BPE
0.00.038.787 I print_info: n_vocab          = 50304
0.00.038.788 I print_info: n_merges         = 50009
0.00.038.788 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.788 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.789 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.789 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.789 I print_info: LF token         = 187 'Ċ'
0.00.038.789 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.789 I print_info: max token length = 1024
0.00.038.790 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.583.804 I load_tensors: offloading 24 repeating layers to GPU
0.00.583.820 I load_tensors: offloading output layer to GPU
0.00.583.821 I load_tensors: offloaded 25/25 layers to GPU
0.00.583.852 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.583.854 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.585.608 I llama_init_from_model: n_seq_max     = 1
0.00.585.612 I llama_init_from_model: n_ctx         = 128
0.00.585.612 I llama_init_from_model: n_ctx_per_seq = 128
0.00.585.612 I llama_init_from_model: n_batch       = 128
0.00.585.613 I llama_init_from_model: n_ubatch      = 128
0.00.585.613 I llama_init_from_model: flash_attn    = 0
0.00.585.614 I llama_init_from_model: freq_base     = 10000.0
0.00.585.615 I llama_init_from_model: freq_scale    = 1
0.00.585.616 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.585.628 I ggml_metal_init: allocating
0.00.585.692 I ggml_metal_init: found device: Apple M4
0.00.585.704 I ggml_metal_init: picking default device: Apple M4
0.00.587.180 I ggml_metal_init: using embedded metal library
0.00.593.457 I ggml_metal_init: GPU name:   Apple M4
0.00.593.461 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.593.462 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.593.463 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.593.463 I ggml_metal_init: simdgroup reduction   = true
0.00.593.464 I ggml_metal_init: simdgroup matrix mul. = true
0.00.593.464 I ggml_metal_init: has residency sets    = true
0.00.593.464 I ggml_metal_init: has bfloat            = true
0.00.593.464 I ggml_metal_init: use bfloat            = true
0.00.593.465 I ggml_metal_init: hasUnifiedMemory      = true
0.00.593.467 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.611.094 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.614.498 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.614.514 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.614.551 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.617.669 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.617.671 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.617.671 I llama_init_from_model: graph nodes  = 967
0.00.617.671 I llama_init_from_model: graph splits = 2
0.00.617.674 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.617.675 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.647.753 I 
0.00.647.820 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.647.851 I perplexity: tokenizing the input ..
0.00.652.508 I perplexity: tokenization took 4.655 ms
0.00.652.520 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.792.113 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.793.452 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.793.467 I llama_perf_context_print:        load time =     638.79 ms
0.00.793.468 I llama_perf_context_print: prompt eval time =     139.36 ms /   128 tokens (    1.09 ms per token,   918.46 tokens per second)
0.00.793.469 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.793.469 I llama_perf_context_print:       total time =     145.72 ms /   129 tokens
0.00.793.843 I ggml_metal_free: deallocating

real	0m0.807s
user	0m0.076s
sys	0m0.129s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.105 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.133 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.874 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.879 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.881 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.882 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.882 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.888 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.888 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.889 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.889 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.890 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.890 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.890 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.891 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.891 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.893 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.894 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.895 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.700 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.708 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.539 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.541 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.542 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.542 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.543 I llama_model_loader: - type  f32:  194 tensors
0.00.024.543 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.543 I print_info: file format = GGUF V3 (latest)
0.00.024.544 I print_info: file type   = Q6_K
0.00.024.545 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.675 I load: special tokens cache size = 25
0.00.038.504 I load: token to piece cache size = 0.2984 MB
0.00.038.507 I print_info: arch             = gptneox
0.00.038.507 I print_info: vocab_only       = 0
0.00.038.508 I print_info: n_ctx_train      = 2048
0.00.038.508 I print_info: n_embd           = 2048
0.00.038.508 I print_info: n_layer          = 24
0.00.038.512 I print_info: n_head           = 16
0.00.038.513 I print_info: n_head_kv        = 16
0.00.038.514 I print_info: n_rot            = 32
0.00.038.514 I print_info: n_swa            = 0
0.00.038.514 I print_info: n_embd_head_k    = 128
0.00.038.514 I print_info: n_embd_head_v    = 128
0.00.038.515 I print_info: n_gqa            = 1
0.00.038.516 I print_info: n_embd_k_gqa     = 2048
0.00.038.519 I print_info: n_embd_v_gqa     = 2048
0.00.038.520 I print_info: f_norm_eps       = 1.0e-05
0.00.038.520 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.520 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.521 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.521 I print_info: f_logit_scale    = 0.0e+00
0.00.038.521 I print_info: n_ff             = 8192
0.00.038.522 I print_info: n_expert         = 0
0.00.038.522 I print_info: n_expert_used    = 0
0.00.038.522 I print_info: causal attn      = 1
0.00.038.522 I print_info: pooling type     = 0
0.00.038.522 I print_info: rope type        = 2
0.00.038.523 I print_info: rope scaling     = linear
0.00.038.523 I print_info: freq_base_train  = 10000.0
0.00.038.523 I print_info: freq_scale_train = 1
0.00.038.524 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.524 I print_info: rope_finetuned   = unknown
0.00.038.524 I print_info: ssm_d_conv       = 0
0.00.038.525 I print_info: ssm_d_inner      = 0
0.00.038.526 I print_info: ssm_d_state      = 0
0.00.038.526 I print_info: ssm_dt_rank      = 0
0.00.038.526 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.526 I print_info: model type       = 1.4B
0.00.038.526 I print_info: model params     = 1.41 B
0.00.038.527 I print_info: general.name     = 1.4B
0.00.038.527 I print_info: vocab type       = BPE
0.00.038.527 I print_info: n_vocab          = 50304
0.00.038.527 I print_info: n_merges         = 50009
0.00.038.528 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.528 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.528 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.528 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.528 I print_info: LF token         = 187 'Ċ'
0.00.038.529 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.529 I print_info: max token length = 1024
0.00.038.530 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.00.600.762 I load_tensors: offloading 24 repeating layers to GPU
0.00.600.778 I load_tensors: offloading output layer to GPU
0.00.600.779 I load_tensors: offloaded 25/25 layers to GPU
0.00.600.814 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.600.816 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.602.462 I llama_init_from_model: n_seq_max     = 1
0.00.602.464 I llama_init_from_model: n_ctx         = 128
0.00.602.464 I llama_init_from_model: n_ctx_per_seq = 128
0.00.602.465 I llama_init_from_model: n_batch       = 128
0.00.602.465 I llama_init_from_model: n_ubatch      = 128
0.00.602.465 I llama_init_from_model: flash_attn    = 0
0.00.602.467 I llama_init_from_model: freq_base     = 10000.0
0.00.602.467 I llama_init_from_model: freq_scale    = 1
0.00.602.468 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.602.470 I ggml_metal_init: allocating
0.00.602.521 I ggml_metal_init: found device: Apple M4
0.00.602.534 I ggml_metal_init: picking default device: Apple M4
0.00.604.069 I ggml_metal_init: using embedded metal library
0.00.610.097 I ggml_metal_init: GPU name:   Apple M4
0.00.610.100 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.610.101 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.610.101 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.610.102 I ggml_metal_init: simdgroup reduction   = true
0.00.610.102 I ggml_metal_init: simdgroup matrix mul. = true
0.00.610.102 I ggml_metal_init: has residency sets    = true
0.00.610.103 I ggml_metal_init: has bfloat            = true
0.00.610.103 I ggml_metal_init: use bfloat            = true
0.00.610.104 I ggml_metal_init: hasUnifiedMemory      = true
0.00.610.106 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.626.557 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.629.889 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.629.892 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.629.919 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.633.087 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.633.088 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.633.089 I llama_init_from_model: graph nodes  = 967
0.00.633.089 I llama_init_from_model: graph splits = 2
0.00.633.092 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.633.092 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.669.728 I 
0.00.669.793 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.669.822 I perplexity: tokenizing the input ..
0.00.676.748 I perplexity: tokenization took 6.924 ms
0.00.676.761 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.815.740 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.817.125 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.817.141 I llama_perf_context_print:        load time =     660.59 ms
0.00.817.142 I llama_perf_context_print: prompt eval time =     138.75 ms /   128 tokens (    1.08 ms per token,   922.54 tokens per second)
0.00.817.143 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.817.143 I llama_perf_context_print:       total time =     147.42 ms /   129 tokens
0.00.817.539 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.077s
sys	0m0.133s
+ ./bin/llama-imatrix --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.358 I build: 4699 (5c4284d5) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.019.876 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.342 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.346 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.348 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.353 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.353 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.355 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.358 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.358 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.361 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.362 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.362 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.363 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.364 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.365 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.365 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.042.616 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.044.538 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.050.939 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.050.941 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.050.941 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.050.942 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.050.942 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.050.943 I llama_model_loader: - type  f32:  194 tensors
0.00.050.943 I llama_model_loader: - type  f16:   98 tensors
0.00.050.944 I print_info: file format = GGUF V3 (latest)
0.00.050.945 I print_info: file type   = all F32 (guessed)
0.00.050.946 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.062.468 I load: special tokens cache size = 25
0.00.070.390 I load: token to piece cache size = 0.2984 MB
0.00.070.393 I print_info: arch             = gptneox
0.00.070.393 I print_info: vocab_only       = 0
0.00.070.393 I print_info: n_ctx_train      = 2048
0.00.070.393 I print_info: n_embd           = 2048
0.00.070.393 I print_info: n_layer          = 24
0.00.070.396 I print_info: n_head           = 16
0.00.070.398 I print_info: n_head_kv        = 16
0.00.070.398 I print_info: n_rot            = 32
0.00.070.398 I print_info: n_swa            = 0
0.00.070.398 I print_info: n_embd_head_k    = 128
0.00.070.398 I print_info: n_embd_head_v    = 128
0.00.070.399 I print_info: n_gqa            = 1
0.00.070.400 I print_info: n_embd_k_gqa     = 2048
0.00.070.402 I print_info: n_embd_v_gqa     = 2048
0.00.070.403 I print_info: f_norm_eps       = 1.0e-05
0.00.070.403 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.070.404 I print_info: f_clamp_kqv      = 0.0e+00
0.00.070.404 I print_info: f_max_alibi_bias = 0.0e+00
0.00.070.404 I print_info: f_logit_scale    = 0.0e+00
0.00.070.405 I print_info: n_ff             = 8192
0.00.070.405 I print_info: n_expert         = 0
0.00.070.405 I print_info: n_expert_used    = 0
0.00.070.405 I print_info: causal attn      = 1
0.00.070.405 I print_info: pooling type     = 0
0.00.070.406 I print_info: rope type        = 2
0.00.070.406 I print_info: rope scaling     = linear
0.00.070.406 I print_info: freq_base_train  = 10000.0
0.00.070.407 I print_info: freq_scale_train = 1
0.00.070.407 I print_info: n_ctx_orig_yarn  = 2048
0.00.070.407 I print_info: rope_finetuned   = unknown
0.00.070.407 I print_info: ssm_d_conv       = 0
0.00.070.407 I print_info: ssm_d_inner      = 0
0.00.070.408 I print_info: ssm_d_state      = 0
0.00.070.408 I print_info: ssm_dt_rank      = 0
0.00.070.408 I print_info: ssm_dt_b_c_rms   = 0
0.00.070.408 I print_info: model type       = 1.4B
0.00.070.409 I print_info: model params     = 1.41 B
0.00.070.410 I print_info: general.name     = 1.4B
0.00.070.411 I print_info: vocab type       = BPE
0.00.070.411 I print_info: n_vocab          = 50304
0.00.070.411 I print_info: n_merges         = 50009
0.00.070.412 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.070.412 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.070.412 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.070.412 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.070.412 I print_info: LF token         = 187 'Ċ'
0.00.070.413 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.070.413 I print_info: max token length = 1024
0.00.070.414 I load_tensors: loading model tensors, this can take a while... (mmap = true)
0.01.325.531 I load_tensors: offloading 24 repeating layers to GPU
0.01.325.537 I load_tensors: offloading output layer to GPU
0.01.325.538 I load_tensors: offloaded 25/25 layers to GPU
0.01.325.562 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.325.564 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.01.326.641 I llama_init_from_model: n_seq_max     = 1
0.01.326.642 I llama_init_from_model: n_ctx         = 128
0.01.326.642 I llama_init_from_model: n_ctx_per_seq = 128
0.01.326.643 I llama_init_from_model: n_batch       = 128
0.01.326.643 I llama_init_from_model: n_ubatch      = 128
0.01.326.644 I llama_init_from_model: flash_attn    = 0
0.01.326.644 I llama_init_from_model: freq_base     = 10000.0
0.01.326.644 I llama_init_from_model: freq_scale    = 1
0.01.326.645 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.326.646 I ggml_metal_init: allocating
0.01.326.674 I ggml_metal_init: found device: Apple M4
0.01.326.680 I ggml_metal_init: picking default device: Apple M4
0.01.327.702 I ggml_metal_init: using embedded metal library
0.01.331.629 I ggml_metal_init: GPU name:   Apple M4
0.01.331.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.331.632 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.331.633 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.331.633 I ggml_metal_init: simdgroup reduction   = true
0.01.331.633 I ggml_metal_init: simdgroup matrix mul. = true
0.01.331.633 I ggml_metal_init: has residency sets    = true
0.01.331.633 I ggml_metal_init: has bfloat            = true
0.01.331.634 I ggml_metal_init: use bfloat            = true
0.01.331.634 I ggml_metal_init: hasUnifiedMemory      = true
0.01.331.635 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.342.399 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.344.156 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.344.159 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.344.172 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.345.779 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.345.780 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.345.781 I llama_init_from_model: graph nodes  = 967
0.01.345.781 I llama_init_from_model: graph splits = 2
0.01.345.782 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.345.783 I 
0.01.345.809 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.345.811 I compute_imatrix: tokenizing the input ..
0.01.349.900 I compute_imatrix: tokenization took 4.088 ms
0.01.349.902 I compute_imatrix: computing over 1 chunks with batch_size 128
0.01.613.104 I compute_imatrix: 0.26 seconds per pass - ETA 0.00 minutes
[1]10.1498,
Final estimate: PPL = 10.1498 +/- 3.22650

0.01.615.599 I llama_perf_context_print:        load time =    1593.23 ms
0.01.615.599 I llama_perf_context_print: prompt eval time =     261.48 ms /   128 tokens (    2.04 ms per token,   489.52 tokens per second)
0.01.615.600 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.615.601 I llama_perf_context_print:       total time =    1595.72 ms /   129 tokens
0.01.616.109 I ggml_metal_free: deallocating

real	0m1.797s
user	0m0.125s
sys	0m0.251s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4699 (5c4284d5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134a06240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134a068c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134a06e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134a07420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134a079d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134a07f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134a08530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134a08ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134a09090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134a09590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134a09a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134a09f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134a0aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134a0b260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134a0ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134a0c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134a0c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134a0cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134a0d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134a0dec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134a0e5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134a0ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134a0f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134a0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134a103e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134a106a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134a10cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134a11920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134a11e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134a12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134a125c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134a12880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134a13110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134a13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134a13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134a13db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134a14250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134a146f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134a14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134a15030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134a154d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134a15970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134a15e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134a162b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134a16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134a16b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134a17190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134a17ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134a180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134a186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134a18ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134a192f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134a19900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134a19f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134a1a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134a1aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134a1b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134a1b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134a1b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134a1c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134a1c3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134a1c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134a1cd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134a1d1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134a1d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134a1dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134a1df80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134a1e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134a1e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134a1ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134a1f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134a1f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134a1fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134a20090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134a205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134a20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134a21080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134a215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134a21b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134a22070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134a225c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134a22b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134a23060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134a235b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134a23b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134a24050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134a245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134a24af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134a25040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134a25590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134a25ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134a26030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134a26580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134a26ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134a27020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134a27570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134a27ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134a177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134a27f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134a286e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134a28c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134a29180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134a296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134a29c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134a2a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134a2a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134a2ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134a2b160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134a2b6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134a2bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134a2c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134a2c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134a2cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134a2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134a2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134a2d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134a2de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134a2e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134a2e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134a2ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134a2f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134a2f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134a2fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134a2fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134a30370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134a30810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134a30cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134a31150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134a315f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134a31a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134a31f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134a323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134a32870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134a32d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134a331b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134a33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134a33af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134a33f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134a34430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134a348d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134a34d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134a35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134a356b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134a35b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134a35ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134a36490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134a36930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134a36dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134a37270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134a37710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134a37bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134a38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134a384f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134a38990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134a38e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134a392d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134a39770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134a39c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134a3a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134a3a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134a3a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134a3ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134a3b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134a3b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134a3bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134a3c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134a3c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134a3ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134a3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134a3d390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134a3d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134a3dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134a3e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134a3e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134a3eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134a3ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134a3f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134a3f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134a3fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134a401d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134a40670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134a40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134a40fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134a41450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134a418f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134a41d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134a42230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134a426d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134a42b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134a43010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134a434b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134a43950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134a43df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134a44340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134a44890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134a44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134a45330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134a455f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134a45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134a46210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134a46820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134a47010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134a474b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134a47770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134a47d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134a48390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134a48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134a49020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134a494c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134a49960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134a4a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134a4a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134a4abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134a4b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134a4b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134a4bba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134a4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134a4c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134a4cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134a4d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134a4d630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134a4db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134a4e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134a4e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134a4eb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134a4f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134a4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134a4fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134a500b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134a50600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134a50b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134a510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134a515f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134a51b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134a52090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134a525e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134a52b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134a53080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134a535d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134a53b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134a54070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134a545c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134a54b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134a55060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134a555b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134a55b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134a56050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134a565a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134a56af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134a57040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134a57590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134a57ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134a58030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134a58580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134a58ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134a59020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134a59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134a59ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134a5a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134a5a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134a5aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134a5b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134a5b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134a5baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134a5bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134a5c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134a5ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134a5cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134a5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134a5d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134a5dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134a5e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134a5e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134a5eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134a5ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134a5f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134a5f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134a5fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134a60210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134a606b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134a60b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134a60ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134a61540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134a61c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134a62380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134a62aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134a631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134a63480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134a63c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134a63f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134a64540 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.749.136 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.749.140 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134c04e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134c052c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134c05730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134c05ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134c06010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134c06480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134c068f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134c06d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134c071d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134c07640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134c07ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134c081a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134c08cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134c09470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134c09c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134c0a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134c0aac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134c0b1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134c0b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134c0c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134c0c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134c0ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134c0d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134c0dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134c0e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134c0e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134c0e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134c0edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134c0f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134c0f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134c0fb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134c10040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134c104b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134c10770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134c10be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134c11050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134c114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134c11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134c11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134c12210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134c12680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134c12af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134c12f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134c133d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134c13840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134c13cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134c14120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134c14590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134c14a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134c14e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134c152e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134c15750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134c15bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134c16030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134c164a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134c16910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134c16e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134c17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134c177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134c17c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134c180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134c18540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134c189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134c18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134c19290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134c19700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134c19b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134c19fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134c1a450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134c1a8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134c1ad30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134c1b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134c1b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134c1ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134c1bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134c1c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134c1c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134c1cc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134c1d0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134c1d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134c1d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134c1de00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134c1e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134c1e6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134c1eb50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134c1efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134c1f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134c1f8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134c1fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134c20180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134c205f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134c20a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134c20ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134c21340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134c217b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134c21c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134c22090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134c22500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134c22970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134c22de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134c23250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134c236c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134c23b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134c23fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134c24410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134c24880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134c24cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134c25160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134c255d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134c25a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134c25eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134c26320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134c26790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134c26c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134c27070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134c274e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134c27950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134c27dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134c28230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134c286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134c28b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134c28f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134c293f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134c29860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134c29cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134c2a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134c2a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134c2aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134c2ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134c2b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134c2b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134c2bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134c2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134c2c4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134c2c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134c2cda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134c2d210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134c2d680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134c2daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134c2df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134c2e3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134c2e840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134c2ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134c2f120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134c2f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134c2fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134c2fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134c302e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134c30750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134c30bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134c31030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134c314a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134c31910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134c31d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134c321f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134c32660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134c32ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134c32f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134c333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134c33820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134c33c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134c34100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134c34570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134c349e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134c34e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134c352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134c35ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134c361b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134c36470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134c368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134c36d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134c371c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134c37630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134c37aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134c37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134c38380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134c387f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134c38c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134c390d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134c39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134c399b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134c39e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134c3a290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134c3a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134c3ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134c3afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134c3b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134c3b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134c3bd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134c3c1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134c3c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134c3ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134c3cef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134c3d360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134c3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134c3dc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134c3e0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134c3e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134c3e990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134c3ee00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134c3f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134c3f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134c3fc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134c40150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134c405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134c40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134c40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134c41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134c41830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134c41d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134c428b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134c42b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134c43130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134c436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134c43cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134c44270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134c44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134c44df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134c453b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134c45970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134c45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134c464f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134c46ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134c47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134c47630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134c47bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134c481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134c48770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134c48d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134c492f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134c498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134c49e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134c4a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134c4a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134c4afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134c4b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134c4bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134c4c0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134c4c6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134c4cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134c4d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134c4d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134c4ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134c4e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134c4e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134c4eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134c4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134c4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134c50030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134c505f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134c50bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134c51170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134c51730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134c51cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134c522b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134c52870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134c52e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134c533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134c539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134c53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134c54530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134c54af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134c550b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134c55670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134c55c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134c561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134c567b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134c56d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134c57270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134c57770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134c57c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134c58170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134c58670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134c58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134c59070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134c59570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134c59a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134c59f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134c5a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134c5a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134c5ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134c5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134c5b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134c5c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134c5c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134c5d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134c5d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134c5daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134c5e290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134c5e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134c5eb60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134b058e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134b05d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134b08a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134b08ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134b09340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134b097b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134b09c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134b0a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134b0a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134b0a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134b0ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134b0b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134b0bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134b0c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134b0cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134b0d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134b0dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134b0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134b0ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134b0f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134b0fab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134b101d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134b108f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134b11010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134b11730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134b119f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134b11cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134b12120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134b12590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134b12a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134b12f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134b13410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134b13880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134b13b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134b13fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134b14420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134b14980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134b14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134b15380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134b15880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134b15d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134b16280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134b16780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134b16c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134b17180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134b175f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134b17a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134b17ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134b18340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134b187b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134b18c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134b19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134b19500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134b19970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134b19de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134b1a5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134b1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134b1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134b1b320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134b1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134b1bfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134b1c450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134b1c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134b1cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134b1d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134b1d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134b1db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134b1e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134b1e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134b1e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134b1edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134b1f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134b1f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134b1fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134b201d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134b20720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134b20c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134b211c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134b21710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134b21c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134b221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134b22700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134b22c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134b231a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134b236f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134b23c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134b24190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134b246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134b24c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134b25180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134b256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134b25c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134b26170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134b266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134b26c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134b27160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134b276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134b27c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134b28150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134b286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134b28bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134b29140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134b29690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134b29be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134b2a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134b2a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134b2abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134b2b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134b2b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134b2bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134b2c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134b2c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134b2cbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134b2d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134b2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134b2d990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134b2de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134b2e2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134b2e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134b2ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134b2f0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134b2f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134b2f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134b2fe90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134b30330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134b307d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134b30c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134b31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134b315b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134b31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134b31ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134b32390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134b32830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134b32cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134b33170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134b33610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134b33ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134b33f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134b343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134b34890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134b34d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134b351d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134b35670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134b35b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134b35fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134b36450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134b368f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134b36d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134b37230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134b376d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134b37b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134b38010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134b384b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134b38950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134b38df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134b39290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134b39730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134b39bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134b3a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134b3a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134b3a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134b3ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134b3b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134b3b790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134b3bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134b3c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134b3c570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134b3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134b3ceb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134b3d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134b3d7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134b3dc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134b3e130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134b3e5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134b3ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134b3ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134b3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134b3f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134b3fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134b40190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134b40630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134b40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134b40f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134b41410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134b418b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134b41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134b421f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134b42690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134b42b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134b42fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134b43470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134b43910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134b43db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134b44300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134b44850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134b44da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134b452f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134b455b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134b45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x10f704080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x10f7044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x10f704960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x10f704dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x10f705240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x10f7056b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x10f705b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x10f705f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x10f706400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x10f706870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x10f706ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x10f707890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x10f707b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x10f707e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x10f708280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x10f7086f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x10f708b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x10f708fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x10f709440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x10f7098b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x10f709d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x10f70a190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x10f70a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x10f70aa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x10f70aee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x10f70b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x10f70b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x10f70bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x10f70c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x10f70c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x10f70c980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x10f70cdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x10f70d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x10f70d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x10f70db40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x10f70dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x10f70e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x10f70e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x10f70ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x10f70f170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x10f70f5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x10f70fa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x10f70fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x10f710330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x10f7107a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x10f710c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x10f711080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x10f7114f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x10f711960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x10f711dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x10f712240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x10f7126b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x10f712b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x10f712f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x10f713400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x10f713870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x10f713ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x10f714150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x10f7145c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x10f714a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x10f714ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x10f715310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x10f715780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x10f715bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x10f716060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x10f7164d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x10f716940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x10f716db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x10f717220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x10f717690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x10f717b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x10f717f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x10f7183e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x10f718850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x10f718cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x10f719130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x10f7195a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x10f719a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x10f719e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x10f71a2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x10f71a760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x10f71abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x10f71b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x10f71b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x10f71bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x10f71c640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x10f71cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x10f71d480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x10f71d740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x10f71dbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x10f71e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x10f71e7c0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.793s
user	0m0.281s
sys	0m0.339s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4699 (5c4284d5)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1528070e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x1528077f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x152807da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x152808350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x152808900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x152808eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x152809460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x152809a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x152809fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x15280a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x15280a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x15280aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x15280b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x15280c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x15280c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x15280d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x15280d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x15280df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x15280e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x15280edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x15280f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x15280fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x152810350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x152810bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x152811310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1528115d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x152811be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x152812850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x152812d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x152813050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1528134f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x1528137b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x152814040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x152814580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x152814840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x152814ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x152815180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x152815620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x152815ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x152815f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x152816400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1528168a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x152816d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1528171e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x1528174a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x152817ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1528180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1528189e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x152818ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x152819600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x152819c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15281a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15281a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15281ae40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15281b630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x15281bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x15281bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x15281c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x15281c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15281d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x15281d2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x15281d790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x15281dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x15281e0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x15281e570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15281ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15281eeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15281f350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x15281f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15281fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x152820130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1528205d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x152820a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x152820fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x152821510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x152821a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x152821fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x152822500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x152822a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x152822fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1528234f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x152823a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x152823f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1528244e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x152824a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x152824f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1528254d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x152825a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x152825f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1528264c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x152826a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x152826f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1528274b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x152827a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x152827f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x1528284a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1528289f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1528186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x152828e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x152829610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x152829b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15282a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15282a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15282ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15282b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x15282b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x15282bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15282c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15282c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15282cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x15282d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15282d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x15282db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15282dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15282e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15282e900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15282eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15282f240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15282f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15282fb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x152830020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x1528304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x152830960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x152830e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x1528312a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x152831740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x152831be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x152832080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x152832520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x1528329c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x152832e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x152833300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x1528337a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x152833c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1528340e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x152834580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x152834a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x152834ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x152835360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x152835800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x152835ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x152836140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1528365e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x152836a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x152836f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1528373c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x152837860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x152837d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1528381a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x152838640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x152838ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x152838f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x152839420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x1528398c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x152839d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15283a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15283a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15283ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15283afe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15283b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15283b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15283bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15283c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15283c700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15283cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15283d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15283d4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15283d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x15283de20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15283e2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15283e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15283ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15283f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x15283f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15283f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15283fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x152840320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1528407c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x152840c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x152841100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x1528415a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x152841a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x152841ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x152842380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x152842820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x152842cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x152843160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x152843600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x152843aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x152843f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1528443e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x152844880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x152844d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x152845270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x1528457c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x152845d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x152846260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x152846520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x152846b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x152847140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x152847750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x152847f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1528483e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x1528486a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x152848cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x1528492c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x152849ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x152849f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15284a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15284a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15284b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15284b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15284bae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15284c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x15284c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15284cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15284d020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x15284d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x15284dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x15284e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x15284e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x15284eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x15284f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15284f550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15284faa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x15284fff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x152850540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x152850a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x152850fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x152851530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x152851a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x152851fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x152852520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x152852a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x152852fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x152853510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x152853a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x152853fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x152854500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x152854a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x152854fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1528554f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x152855a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x152855f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1528564e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x152856a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x152856f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1528574d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x152857a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x152857f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x1528584c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x152858a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x152858f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x1528594b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x152859a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x152859f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15285a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15285a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15285af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15285b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15285b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x15285bf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x15285c480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15285c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15285cf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15285d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15285d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x15285de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x15285e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15285e7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15285ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x15285f0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15285f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15285fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x15285fec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x152860360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x152860800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x152860ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x152861140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1528615e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x152861a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x152861f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x152862470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x152862b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1528632b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x1528639d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1528640f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1528643b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x152864ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x152864e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x152865470 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.099.192 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.196 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x147304bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x147305030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x1473054a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x147305910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x147305d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x1473061f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x147306660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x147306ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x147306f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x1473073b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x147307820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x147307ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x147308a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x1473091b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1473099c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x14730a0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x14730a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x14730af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x14730b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x14730be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x14730c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x14730cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x14730d370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x14730da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x14730e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x14730e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x14730e730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x14730eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x14730f010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x14730f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x14730f8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x14730fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x147310290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x147310550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1473109c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x147310e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x1473112a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x147311710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x147311b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x147311ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x147312460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1473128d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x147312d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1473131b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x147313620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x147313a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x147313f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x147314370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1473147e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x147314c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1473150c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x147315530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1473159a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x147315e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x147316280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1473166f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x147316c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x147317160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1473175d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x147317a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x147317eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x147318320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x147318790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x147318c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x147319070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1473194e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x147319950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x147319dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x14731a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x14731a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x14731ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x14731af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x14731b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x14731b860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x14731bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x14731c140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x14731c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x14731ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x14731ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x14731d300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x14731d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x14731dbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x14731e050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x14731e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x14731e930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x14731eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x14731f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x14731f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x14731faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x14731ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1473203d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x147320840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x147320cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x147321120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x147321590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x147321a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x147321e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1473222e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x147322750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x147322bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x147323030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x1473234a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x147323910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x147323d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1473241f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x147324660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x147324ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x147324f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1473253b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x147325820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x147325c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x147326100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x147326570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x1473269e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x147326e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1473272c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x147327730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x147327ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x147328010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x147328480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1473288f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x147328d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1473291d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x147329640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x147329ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x147329f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x14732a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x14732a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x14732ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x14732b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x14732b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x14732b9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x14732be30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x14732c2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x14732c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x14732cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x14732cff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x14732d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x14732d8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x14732dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x14732e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x14732e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x14732ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x14732ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x14732f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x14732f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x14732fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1473300c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x147330530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1473309a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x147330e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x147331280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1473316f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x147331b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x147331fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x147332440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1473328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x147332d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x147333190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x147333600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x147333a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x147333ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x147334350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1473347c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x147334c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x1473350a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x147335cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x147335f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x147336250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x1473366c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x147336b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x147336fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x147337410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x147337880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x147337cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x147338160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1473385d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x147338a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x147338eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x147339320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x147339790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x147339c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x14733a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x14733a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x14733a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x14733adc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x14733b230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x14733b6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x14733bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x14733bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x14733c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x14733c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x14733ccd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x14733d140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x14733d5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x14733da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x14733de90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x14733e300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x14733e770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x14733ebe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x14733f050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x14733f4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x14733fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x14733ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x1473403a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x147340810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x147340c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1473410f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x147341610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x147341b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x147342690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x147342950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x147342f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1473434d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x147343a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x147344050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x147344610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x147344bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x147345190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x147345750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x147345d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x1473462d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x147346890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x147346e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x147347410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1473479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x147347f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x147348550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x147348b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1473490d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x147349690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x147349c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x14734a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x14734a7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x14734ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x14734b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x14734b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x14734bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x14734c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x14734ca50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x14734d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x14734d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x14734db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x14734e150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x14734e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x14734ecd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x14734f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x14734f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x14734fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1473503d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x147350990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x147350f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x147351510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x147351ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x147352090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x147352650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x147352c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1473531d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x147353790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x147353d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x147354310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x1473548d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x147354e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x147355450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x147355a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x147355fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x147356590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x147356b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x147357050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x147357550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x147357a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x147357f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x147358450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x147358950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x147358e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x147359350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x147359850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x147359d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x14735a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x14735a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x14735ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x14735b150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x14735b650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x14735c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x14735c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x14735cea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x14735d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x14735d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x14735e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x14735e330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x14735e940 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x15170b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x15170b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x15170b910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x15170bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x15170f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x15170f3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x15170f680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x15170faf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x15170ff60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x151710470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x1517108e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x151710f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x151711a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x151712230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x151712a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x151713160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x151713880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x151713fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1517146c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x151714e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1517155b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x151715cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x1517163f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x151716b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x151717230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1517174f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1517177b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x151717c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x151718090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x151718500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x151718a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x151718f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x151719380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x151719640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x151719ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x151719f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15171a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15171a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15171ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x15171b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x15171b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15171bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15171c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15171c780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15171cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x15171d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x15171d560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x15171d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15171de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15171e2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x15171e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15171eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15171f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x15171f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x15171f8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1517200b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x151720550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x151720810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x151720e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x151721610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x151721ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x151721f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1517223f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x151722890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x151722d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1517231d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x151723670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x151723b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x151723fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x151724450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1517248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x151724d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x151725230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x151725780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x151725cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x151726220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x151726770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x151726cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x151727210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x151727760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x151727cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x151728200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x151728750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x151728ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x1517291f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x151729740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x151729c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x15172a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15172a730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x15172ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x15172b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15172b720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x15172bc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15172c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15172c710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15172cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x15172d1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15172d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x15172dc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x15172e1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15172e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15172ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15172f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x15172f6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15172fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x151730180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1517306d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x151730c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x151731170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1517316c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x151731c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x151732160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1517326b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x151732b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x151732ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x151733490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x151733930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x151733dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x151734270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x151734710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x151734bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x151735050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1517354f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x151735990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x151735e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1517362d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x151736770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x151736c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1517370b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x151737550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x1517379f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x151737e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x151738330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x1517387d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x151738c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x151739110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1517395b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x151739a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x151739ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x15173a390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15173a830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15173acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x15173b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15173b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x15173bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x15173bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x15173c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15173c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15173cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15173d1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x15173d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x15173db10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x15173dfb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15173e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15173e8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x15173ed90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15173f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15173f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15173fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x151740010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1517404b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x151740950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x151740df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x151741290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x151741730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x151741bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x151742070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x151742510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1517429b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x151742e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1517432f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x151743790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x151743c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x1517440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x151744570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x151744a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x151744eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x151745350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1517457f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x151745c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x151746130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x1517465d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x151746a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x151746f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1517473b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x151747850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x151747cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x151748190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x151748630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x151748ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x151748f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x151749410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x1517498b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x151749e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x15174a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x15174a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x15174adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x15174b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x15174b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15174bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x15174c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15174cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x15174cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x15174d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x15174d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15174de50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15174e640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x15174eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15174ef80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x15174f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15174fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x151750120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x151750670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x151750bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x151751110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x151751660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x151751bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x151752100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x151752650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x151752ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x1517530f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x151753640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x151753b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1517540e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x151754630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x151754b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1517550d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x151755620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x151755b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1517560c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x151756610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x151756b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1517570b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x151757600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x151757b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1517580a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x1517585f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x151758b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x151759090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x1517595e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x151759b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15175a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15175a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15175ab20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15175b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15175b5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15175bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15175c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15175c5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15175cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15175d050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15175d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15175daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15175e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x15175e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x15175eae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x15175f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15175f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x15175fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x151760020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x151760570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x151760ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x151761010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x151761560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x151761ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x151762000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x151762550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x1517629f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x151762e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x151763330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1517637d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x151763c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x151764110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x1517645b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x151764a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x151764ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x151765390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x151765830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x151765cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x151766170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x151766610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x151766ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x151767000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x151767720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x151767e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x151768560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x151768c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x151768f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x151769730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x1517699f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x15176a000 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.957s
user	0m0.229s
sys	0m0.192s
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
