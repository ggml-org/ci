### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.23 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.86 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.24 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.71 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.44 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.34 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.51 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.08 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.35 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    1.04 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.34 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.35 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.28 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.12 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.22 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.26 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.18 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.45 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  180.16 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.89 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.71 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 224.21 sec*proc (28 tests)

Total Test time (real) = 224.22 sec

real	3m44.310s
user	7m34.963s
sys	0m6.290s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.18 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.31 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.05 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.11 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.23 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.16 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.93 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.17 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.17 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.19 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.31 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.36 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.75 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.46 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.30 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  52.22 sec*proc (28 tests)

Total Test time (real) =  52.23 sec

real	0m52.237s
user	1m13.373s
sys	0m5.740s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.132 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.020.093 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.024.320 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.024.328 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.024.330 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.024.331 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.024.332 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.024.333 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.024.333 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.024.335 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.024.336 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.024.336 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.024.337 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.024.337 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.024.341 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.024.342 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.024.346 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.024.346 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.024.347 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.024.347 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.024.348 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.029.560 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.030.873 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.030.875 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.030.875 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.030.876 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.030.876 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.030.877 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.030.877 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.030.878 I llama_model_loader: - type  f32:  124 tensors
0.00.030.878 I llama_model_loader: - type  f16:   73 tensors
0.00.035.329 I llm_load_vocab: special tokens cache size = 5
0.00.037.506 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.037.510 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.037.510 I llm_load_print_meta: arch             = bert
0.00.037.511 I llm_load_print_meta: vocab type       = WPM
0.00.037.511 I llm_load_print_meta: n_vocab          = 30522
0.00.037.511 I llm_load_print_meta: n_merges         = 0
0.00.037.511 I llm_load_print_meta: vocab_only       = 0
0.00.037.512 I llm_load_print_meta: n_ctx_train      = 512
0.00.037.512 I llm_load_print_meta: n_embd           = 384
0.00.037.512 I llm_load_print_meta: n_layer          = 12
0.00.037.517 I llm_load_print_meta: n_head           = 12
0.00.037.518 I llm_load_print_meta: n_head_kv        = 12
0.00.037.518 I llm_load_print_meta: n_rot            = 32
0.00.037.518 I llm_load_print_meta: n_swa            = 0
0.00.037.521 I llm_load_print_meta: n_embd_head_k    = 32
0.00.037.521 I llm_load_print_meta: n_embd_head_v    = 32
0.00.037.522 I llm_load_print_meta: n_gqa            = 1
0.00.037.523 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.037.524 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.037.524 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.037.525 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.037.526 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.037.526 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.037.526 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.037.527 I llm_load_print_meta: n_ff             = 1536
0.00.037.527 I llm_load_print_meta: n_expert         = 0
0.00.037.527 I llm_load_print_meta: n_expert_used    = 0
0.00.037.528 I llm_load_print_meta: causal attn      = 0
0.00.037.528 I llm_load_print_meta: pooling type     = 2
0.00.037.528 I llm_load_print_meta: rope type        = 2
0.00.037.528 I llm_load_print_meta: rope scaling     = linear
0.00.037.530 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.037.531 I llm_load_print_meta: freq_scale_train = 1
0.00.037.531 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.037.531 I llm_load_print_meta: rope_finetuned   = unknown
0.00.037.532 I llm_load_print_meta: ssm_d_conv       = 0
0.00.037.532 I llm_load_print_meta: ssm_d_inner      = 0
0.00.037.532 I llm_load_print_meta: ssm_d_state      = 0
0.00.037.532 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.037.532 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.037.533 I llm_load_print_meta: model type       = 33M
0.00.037.533 I llm_load_print_meta: model ftype      = F16
0.00.037.534 I llm_load_print_meta: model params     = 33.21 M
0.00.037.542 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.037.543 I llm_load_print_meta: general.name     = Bge Small
0.00.037.544 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.037.544 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.037.545 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.037.545 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.037.548 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.037.548 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.037.549 I llm_load_print_meta: max token length = 21
0.00.039.440 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.039.441 I llm_load_tensors: offloading output layer to GPU
0.00.039.441 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.039.465 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.039.467 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.040.018 I llama_new_context_with_model: n_seq_max     = 1
0.00.040.019 I llama_new_context_with_model: n_ctx         = 512
0.00.040.019 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.040.020 I llama_new_context_with_model: n_batch       = 2048
0.00.040.020 I llama_new_context_with_model: n_ubatch      = 2048
0.00.040.020 I llama_new_context_with_model: flash_attn    = 0
0.00.040.021 I llama_new_context_with_model: freq_base     = 10000.0
0.00.040.021 I llama_new_context_with_model: freq_scale    = 1
0.00.040.022 I ggml_metal_init: allocating
0.00.040.026 I ggml_metal_init: found device: Apple M4
0.00.040.029 I ggml_metal_init: picking default device: Apple M4
0.00.040.878 I ggml_metal_init: using embedded metal library
0.00.045.501 I ggml_metal_init: GPU name:   Apple M4
0.00.045.503 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.045.504 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.045.504 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.045.505 I ggml_metal_init: simdgroup reduction   = true
0.00.045.505 I ggml_metal_init: simdgroup matrix mul. = true
0.00.045.505 I ggml_metal_init: has bfloat            = true
0.00.045.505 I ggml_metal_init: use bfloat            = true
0.00.045.506 I ggml_metal_init: hasUnifiedMemory      = true
0.00.045.506 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.057.677 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.058.365 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.058.367 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.058.368 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.059.155 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.059.156 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.059.157 I llama_new_context_with_model: graph nodes  = 429
0.00.059.157 I llama_new_context_with_model: graph splits = 2
0.00.059.177 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.059.178 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.066.677 I 
0.00.066.706 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.067.420 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.072.788 I llama_perf_context_print:        load time =      46.58 ms
0.00.072.790 I llama_perf_context_print: prompt eval time =       5.21 ms /     9 tokens (    0.58 ms per token,  1725.79 tokens per second)
0.00.072.791 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.072.792 I llama_perf_context_print:       total time =       6.11 ms /    10 tokens
0.00.072.918 I ggml_metal_free: deallocating

real	0m0.251s
user	0m0.050s
sys	0m0.031s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.001 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.012.196 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.012.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.012.202 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.012.202 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.012.205 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.012.205 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.012.205 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.012.207 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.012.207 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.012.207 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.012.208 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.012.208 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.012.211 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.012.211 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.012.211 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.012.212 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.012.212 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.012.213 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.012.213 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.014.953 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.015.606 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.015.607 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.015.607 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.015.607 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.015.607 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.015.608 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.015.608 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.015.608 I llama_model_loader: - type  f32:  124 tensors
0.00.015.609 I llama_model_loader: - type q8_0:   73 tensors
0.00.018.222 I llm_load_vocab: special tokens cache size = 5
0.00.019.597 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.019.600 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.019.600 I llm_load_print_meta: arch             = bert
0.00.019.601 I llm_load_print_meta: vocab type       = WPM
0.00.019.601 I llm_load_print_meta: n_vocab          = 30522
0.00.019.601 I llm_load_print_meta: n_merges         = 0
0.00.019.601 I llm_load_print_meta: vocab_only       = 0
0.00.019.601 I llm_load_print_meta: n_ctx_train      = 512
0.00.019.602 I llm_load_print_meta: n_embd           = 384
0.00.019.602 I llm_load_print_meta: n_layer          = 12
0.00.019.605 I llm_load_print_meta: n_head           = 12
0.00.019.605 I llm_load_print_meta: n_head_kv        = 12
0.00.019.606 I llm_load_print_meta: n_rot            = 32
0.00.019.606 I llm_load_print_meta: n_swa            = 0
0.00.019.606 I llm_load_print_meta: n_embd_head_k    = 32
0.00.019.606 I llm_load_print_meta: n_embd_head_v    = 32
0.00.019.606 I llm_load_print_meta: n_gqa            = 1
0.00.019.607 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.019.608 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.019.608 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.019.609 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.019.609 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.019.609 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.019.609 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.019.609 I llm_load_print_meta: n_ff             = 1536
0.00.019.610 I llm_load_print_meta: n_expert         = 0
0.00.019.610 I llm_load_print_meta: n_expert_used    = 0
0.00.019.610 I llm_load_print_meta: causal attn      = 0
0.00.019.610 I llm_load_print_meta: pooling type     = 2
0.00.019.610 I llm_load_print_meta: rope type        = 2
0.00.019.610 I llm_load_print_meta: rope scaling     = linear
0.00.019.611 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.019.611 I llm_load_print_meta: freq_scale_train = 1
0.00.019.611 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.019.612 I llm_load_print_meta: rope_finetuned   = unknown
0.00.019.614 I llm_load_print_meta: ssm_d_conv       = 0
0.00.019.614 I llm_load_print_meta: ssm_d_inner      = 0
0.00.019.614 I llm_load_print_meta: ssm_d_state      = 0
0.00.019.614 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.019.614 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.019.614 I llm_load_print_meta: model type       = 33M
0.00.019.615 I llm_load_print_meta: model ftype      = Q8_0
0.00.019.615 I llm_load_print_meta: model params     = 33.21 M
0.00.019.616 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.019.616 I llm_load_print_meta: general.name     = Bge Small
0.00.019.616 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.019.616 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.019.617 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.019.617 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.019.617 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.019.620 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.019.620 I llm_load_print_meta: max token length = 21
0.00.020.919 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.020.920 I llm_load_tensors: offloading output layer to GPU
0.00.020.920 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.020.925 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.020.926 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.021.281 I llama_new_context_with_model: n_seq_max     = 1
0.00.021.282 I llama_new_context_with_model: n_ctx         = 512
0.00.021.282 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.021.283 I llama_new_context_with_model: n_batch       = 2048
0.00.021.283 I llama_new_context_with_model: n_ubatch      = 2048
0.00.021.283 I llama_new_context_with_model: flash_attn    = 0
0.00.021.284 I llama_new_context_with_model: freq_base     = 10000.0
0.00.021.284 I llama_new_context_with_model: freq_scale    = 1
0.00.021.284 I ggml_metal_init: allocating
0.00.021.291 I ggml_metal_init: found device: Apple M4
0.00.021.293 I ggml_metal_init: picking default device: Apple M4
0.00.022.021 I ggml_metal_init: using embedded metal library
0.00.024.693 I ggml_metal_init: GPU name:   Apple M4
0.00.024.695 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.024.695 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.024.696 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.024.696 I ggml_metal_init: simdgroup reduction   = true
0.00.024.696 I ggml_metal_init: simdgroup matrix mul. = true
0.00.024.696 I ggml_metal_init: has bfloat            = true
0.00.024.697 I ggml_metal_init: use bfloat            = true
0.00.024.697 I ggml_metal_init: hasUnifiedMemory      = true
0.00.024.699 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.035.164 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12
0.00.035.742 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.035.744 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.035.745 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.036.416 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.036.417 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.036.417 I llama_new_context_with_model: graph nodes  = 429
0.00.036.417 I llama_new_context_with_model: graph splits = 2
0.00.036.431 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.036.431 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.042.008 I 
0.00.042.037 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.042.631 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.047.211 I llama_perf_context_print:        load time =      32.00 ms
0.00.047.212 I llama_perf_context_print: prompt eval time =       4.45 ms /     9 tokens (    0.49 ms per token,  2022.93 tokens per second)
0.00.047.213 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.047.213 I llama_perf_context_print:       total time =       5.20 ms /    10 tokens
0.00.047.420 I ggml_metal_free: deallocating

real	0m0.060s
user	0m0.031s
sys	0m0.017s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.127 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.623 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.034.019 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.034.024 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.034.027 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.034.028 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.034.029 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.034.030 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.034.030 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.034.032 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.034.033 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.034.033 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.034.034 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.034.035 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.034.038 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.034.039 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.034.039 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.034.040 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.034.041 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.687 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.982 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.754 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.756 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.756 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.757 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.757 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.758 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.758 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.049.758 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.759 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.759 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.759 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.760 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.049.760 I llama_model_loader: - type  f32:   40 tensors
0.00.049.761 I llama_model_loader: - type  f16:   30 tensors
0.00.068.501 W llm_load_vocab: empty token at index 5
0.00.073.226 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.074.553 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.074.587 I llm_load_vocab: special tokens cache size = 5
0.00.340.065 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.340.069 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.340.069 I llm_load_print_meta: arch             = jina-bert-v2
0.00.340.070 I llm_load_print_meta: vocab type       = BPE
0.00.340.070 I llm_load_print_meta: n_vocab          = 61056
0.00.340.070 I llm_load_print_meta: n_merges         = 39382
0.00.340.070 I llm_load_print_meta: vocab_only       = 0
0.00.340.071 I llm_load_print_meta: n_ctx_train      = 8192
0.00.340.071 I llm_load_print_meta: n_embd           = 384
0.00.340.071 I llm_load_print_meta: n_layer          = 4
0.00.340.074 I llm_load_print_meta: n_head           = 12
0.00.340.074 I llm_load_print_meta: n_head_kv        = 12
0.00.340.075 I llm_load_print_meta: n_rot            = 32
0.00.340.075 I llm_load_print_meta: n_swa            = 0
0.00.340.075 I llm_load_print_meta: n_embd_head_k    = 32
0.00.340.075 I llm_load_print_meta: n_embd_head_v    = 32
0.00.340.076 I llm_load_print_meta: n_gqa            = 1
0.00.340.076 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.340.077 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.340.077 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.340.078 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.340.078 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.340.079 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.340.079 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.340.079 I llm_load_print_meta: n_ff             = 1536
0.00.340.079 I llm_load_print_meta: n_expert         = 0
0.00.340.080 I llm_load_print_meta: n_expert_used    = 0
0.00.340.080 I llm_load_print_meta: causal attn      = 0
0.00.340.080 I llm_load_print_meta: pooling type     = -1
0.00.340.080 I llm_load_print_meta: rope type        = -1
0.00.340.080 I llm_load_print_meta: rope scaling     = linear
0.00.340.081 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.340.081 I llm_load_print_meta: freq_scale_train = 1
0.00.340.084 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.340.084 I llm_load_print_meta: rope_finetuned   = unknown
0.00.340.085 I llm_load_print_meta: ssm_d_conv       = 0
0.00.340.085 I llm_load_print_meta: ssm_d_inner      = 0
0.00.340.085 I llm_load_print_meta: ssm_d_state      = 0
0.00.340.085 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.340.085 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.340.085 I llm_load_print_meta: model type       = 33M
0.00.340.086 I llm_load_print_meta: model ftype      = F16
0.00.340.086 I llm_load_print_meta: model params     = 32.90 M
0.00.340.086 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.340.087 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.340.087 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.340.089 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.340.089 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.340.089 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.340.089 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.340.090 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.340.090 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.340.090 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.340.090 I llm_load_print_meta: max token length = 45
0.00.341.033 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.341.034 I llm_load_tensors: offloading output layer to GPU
0.00.341.034 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.341.056 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.341.057 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.341.794 I llama_new_context_with_model: n_seq_max     = 1
0.00.341.795 I llama_new_context_with_model: n_ctx         = 8192
0.00.341.796 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.341.796 I llama_new_context_with_model: n_batch       = 2048
0.00.341.796 I llama_new_context_with_model: n_ubatch      = 2048
0.00.341.796 I llama_new_context_with_model: flash_attn    = 0
0.00.341.797 I llama_new_context_with_model: freq_base     = 10000.0
0.00.341.797 I llama_new_context_with_model: freq_scale    = 1
0.00.341.797 I ggml_metal_init: allocating
0.00.341.801 I ggml_metal_init: found device: Apple M4
0.00.341.802 I ggml_metal_init: picking default device: Apple M4
0.00.342.630 I ggml_metal_init: using embedded metal library
0.00.345.336 I ggml_metal_init: GPU name:   Apple M4
0.00.345.337 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.345.338 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.345.338 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.345.338 I ggml_metal_init: simdgroup reduction   = true
0.00.345.338 I ggml_metal_init: simdgroup matrix mul. = true
0.00.345.338 I ggml_metal_init: has bfloat            = true
0.00.345.339 I ggml_metal_init: use bfloat            = true
0.00.345.339 I ggml_metal_init: hasUnifiedMemory      = true
0.00.345.340 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.354.987 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4
0.00.357.429 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.357.431 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.357.432 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.358.055 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.358.056 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.358.057 I llama_new_context_with_model: graph nodes  = 154
0.00.358.057 I llama_new_context_with_model: graph splits = 2
0.00.358.075 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.358.075 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.370.650 I 
0.00.370.686 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.370.907 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.370.908 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.370.917 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.370.918 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.370.921 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.370.921 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.371.432 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.375.053 I llama_perf_context_print:        load time =     348.02 ms
0.00.375.055 I llama_perf_context_print: prompt eval time =       3.61 ms /    62 tokens (    0.06 ms per token, 17160.25 tokens per second)
0.00.375.055 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.375.057 I llama_perf_context_print:       total time =       4.41 ms /    63 tokens
0.00.375.254 I ggml_metal_free: deallocating

real	0m1.111s
user	0m0.347s
sys	0m0.046s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.122 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.266 I main: llama backend init
0.00.000.276 I main: load the model and apply lora adapter, if any
0.00.036.859 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.047.898 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.047.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.047.916 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.047.917 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.047.918 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.047.918 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.047.919 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.047.921 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.047.922 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.047.922 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.047.923 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.047.924 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.047.924 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.047.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.047.930 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.047.930 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.047.931 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.855 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.059.192 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.066.703 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.066.706 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.066.706 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.066.707 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.066.707 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.066.708 I llama_model_loader: - type  f32:  194 tensors
0.00.066.708 I llama_model_loader: - type  f16:   98 tensors
0.00.097.926 I llm_load_vocab: special tokens cache size = 25
0.00.104.585 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.104.588 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.104.588 I llm_load_print_meta: arch             = gptneox
0.00.104.588 I llm_load_print_meta: vocab type       = BPE
0.00.104.588 I llm_load_print_meta: n_vocab          = 50304
0.00.104.589 I llm_load_print_meta: n_merges         = 50009
0.00.104.589 I llm_load_print_meta: vocab_only       = 0
0.00.104.589 I llm_load_print_meta: n_ctx_train      = 2048
0.00.104.589 I llm_load_print_meta: n_embd           = 2048
0.00.104.589 I llm_load_print_meta: n_layer          = 24
0.00.104.593 I llm_load_print_meta: n_head           = 16
0.00.104.593 I llm_load_print_meta: n_head_kv        = 16
0.00.104.594 I llm_load_print_meta: n_rot            = 32
0.00.104.594 I llm_load_print_meta: n_swa            = 0
0.00.104.594 I llm_load_print_meta: n_embd_head_k    = 128
0.00.104.594 I llm_load_print_meta: n_embd_head_v    = 128
0.00.104.595 I llm_load_print_meta: n_gqa            = 1
0.00.104.595 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.104.596 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.104.596 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.104.597 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.104.597 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.104.597 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.104.597 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.104.599 I llm_load_print_meta: n_ff             = 8192
0.00.104.600 I llm_load_print_meta: n_expert         = 0
0.00.104.600 I llm_load_print_meta: n_expert_used    = 0
0.00.104.600 I llm_load_print_meta: causal attn      = 1
0.00.104.600 I llm_load_print_meta: pooling type     = 0
0.00.104.600 I llm_load_print_meta: rope type        = 2
0.00.104.600 I llm_load_print_meta: rope scaling     = linear
0.00.104.601 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.104.601 I llm_load_print_meta: freq_scale_train = 1
0.00.104.601 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.104.601 I llm_load_print_meta: rope_finetuned   = unknown
0.00.104.602 I llm_load_print_meta: ssm_d_conv       = 0
0.00.104.602 I llm_load_print_meta: ssm_d_inner      = 0
0.00.104.604 I llm_load_print_meta: ssm_d_state      = 0
0.00.104.604 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.104.604 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.104.604 I llm_load_print_meta: model type       = 1.4B
0.00.104.604 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.104.605 I llm_load_print_meta: model params     = 1.41 B
0.00.104.606 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.104.606 I llm_load_print_meta: general.name     = 1.4B
0.00.104.606 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.104.606 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.104.606 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.104.606 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.104.607 I llm_load_print_meta: LF token         = 128 ''
0.00.104.607 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.104.607 I llm_load_print_meta: max token length = 1024
0.00.107.279 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.107.279 I llm_load_tensors: offloading output layer to GPU
0.00.107.280 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.107.298 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.107.299 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.108.246 I llama_new_context_with_model: n_seq_max     = 1
0.00.108.247 I llama_new_context_with_model: n_ctx         = 2048
0.00.108.247 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.108.248 I llama_new_context_with_model: n_batch       = 2048
0.00.108.248 I llama_new_context_with_model: n_ubatch      = 512
0.00.108.248 I llama_new_context_with_model: flash_attn    = 0
0.00.108.248 I llama_new_context_with_model: freq_base     = 10000.0
0.00.108.249 I llama_new_context_with_model: freq_scale    = 1
0.00.108.249 I ggml_metal_init: allocating
0.00.108.253 I ggml_metal_init: found device: Apple M4
0.00.108.255 I ggml_metal_init: picking default device: Apple M4
0.00.108.931 I ggml_metal_init: using embedded metal library
0.00.118.398 I ggml_metal_init: GPU name:   Apple M4
0.00.118.400 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.118.400 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.118.401 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.118.401 I ggml_metal_init: simdgroup reduction   = true
0.00.118.401 I ggml_metal_init: simdgroup matrix mul. = true
0.00.118.401 I ggml_metal_init: has bfloat            = true
0.00.118.401 I ggml_metal_init: use bfloat            = true
0.00.118.402 I ggml_metal_init: hasUnifiedMemory      = true
0.00.118.402 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.142.437 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.162.355 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.162.361 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.162.382 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.163.340 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.163.342 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.163.343 I llama_new_context_with_model: graph nodes  = 967
0.00.163.343 I llama_new_context_with_model: graph splits = 2
0.00.163.368 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.163.495 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.163.496 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.246.150 I main: llama threadpool init, n_threads = 4
0.00.246.187 I 
0.00.246.223 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.246.225 I 
0.00.246.297 I sampler seed: 1234
0.00.246.301 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.246.336 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.246.337 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.246.338 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.093.716 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56618.82 tokens per second)
0.02.093.717 I llama_perf_context_print:        load time =     209.28 ms
0.02.093.718 I llama_perf_context_print: prompt eval time =      43.76 ms /     7 tokens (    6.25 ms per token,   159.96 tokens per second)
0.02.093.719 I llama_perf_context_print:        eval time =    1800.64 ms /    63 runs   (   28.58 ms per token,    34.99 tokens per second)
0.02.093.719 I llama_perf_context_print:       total time =    1847.57 ms /    70 tokens
0.02.093.916 I ggml_metal_free: deallocating

real	0m2.378s
user	0m0.146s
sys	0m0.105s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.597 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.024.906 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.036.656 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.036.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.036.680 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.036.680 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.036.681 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.036.681 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.036.682 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.036.684 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.036.685 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.036.685 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.036.686 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.036.686 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.036.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.036.688 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.036.695 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.036.696 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.036.696 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.046.044 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.048.398 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.056.031 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.056.034 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.056.034 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.056.035 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.056.035 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.056.036 I llama_model_loader: - type  f32:  194 tensors
0.00.056.036 I llama_model_loader: - type  f16:   98 tensors
0.00.085.820 I llm_load_vocab: special tokens cache size = 25
0.00.092.504 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.092.507 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.092.507 I llm_load_print_meta: arch             = gptneox
0.00.092.507 I llm_load_print_meta: vocab type       = BPE
0.00.092.508 I llm_load_print_meta: n_vocab          = 50304
0.00.092.508 I llm_load_print_meta: n_merges         = 50009
0.00.092.508 I llm_load_print_meta: vocab_only       = 0
0.00.092.508 I llm_load_print_meta: n_ctx_train      = 2048
0.00.092.508 I llm_load_print_meta: n_embd           = 2048
0.00.092.508 I llm_load_print_meta: n_layer          = 24
0.00.092.511 I llm_load_print_meta: n_head           = 16
0.00.092.512 I llm_load_print_meta: n_head_kv        = 16
0.00.092.512 I llm_load_print_meta: n_rot            = 32
0.00.092.512 I llm_load_print_meta: n_swa            = 0
0.00.092.514 I llm_load_print_meta: n_embd_head_k    = 128
0.00.092.514 I llm_load_print_meta: n_embd_head_v    = 128
0.00.092.515 I llm_load_print_meta: n_gqa            = 1
0.00.092.515 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.092.516 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.092.517 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.092.517 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.092.517 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.092.517 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.092.517 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.092.518 I llm_load_print_meta: n_ff             = 8192
0.00.092.518 I llm_load_print_meta: n_expert         = 0
0.00.092.518 I llm_load_print_meta: n_expert_used    = 0
0.00.092.519 I llm_load_print_meta: causal attn      = 1
0.00.092.519 I llm_load_print_meta: pooling type     = 0
0.00.092.519 I llm_load_print_meta: rope type        = 2
0.00.092.519 I llm_load_print_meta: rope scaling     = linear
0.00.092.519 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.092.520 I llm_load_print_meta: freq_scale_train = 1
0.00.092.520 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.092.520 I llm_load_print_meta: rope_finetuned   = unknown
0.00.092.520 I llm_load_print_meta: ssm_d_conv       = 0
0.00.092.520 I llm_load_print_meta: ssm_d_inner      = 0
0.00.092.520 I llm_load_print_meta: ssm_d_state      = 0
0.00.092.520 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.092.522 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.092.523 I llm_load_print_meta: model type       = 1.4B
0.00.092.523 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.092.523 I llm_load_print_meta: model params     = 1.41 B
0.00.092.524 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.092.524 I llm_load_print_meta: general.name     = 1.4B
0.00.092.524 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.092.524 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.092.525 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.092.525 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.092.529 I llm_load_print_meta: LF token         = 128 ''
0.00.092.529 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.092.531 I llm_load_print_meta: max token length = 1024
0.00.094.621 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.094.622 I llm_load_tensors: offloading output layer to GPU
0.00.094.622 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.094.628 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.094.628 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.095.570 I llama_new_context_with_model: n_seq_max     = 1
0.00.095.571 I llama_new_context_with_model: n_ctx         = 128
0.00.095.571 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.095.571 I llama_new_context_with_model: n_batch       = 128
0.00.095.572 I llama_new_context_with_model: n_ubatch      = 128
0.00.095.572 I llama_new_context_with_model: flash_attn    = 0
0.00.095.572 I llama_new_context_with_model: freq_base     = 10000.0
0.00.095.573 I llama_new_context_with_model: freq_scale    = 1
0.00.095.573 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.095.573 I ggml_metal_init: allocating
0.00.095.582 I ggml_metal_init: found device: Apple M4
0.00.095.584 I ggml_metal_init: picking default device: Apple M4
0.00.096.236 I ggml_metal_init: using embedded metal library
0.00.098.895 I ggml_metal_init: GPU name:   Apple M4
0.00.098.896 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.098.897 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.098.897 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.098.898 I ggml_metal_init: simdgroup reduction   = true
0.00.098.898 I ggml_metal_init: simdgroup matrix mul. = true
0.00.098.898 I ggml_metal_init: has bfloat            = true
0.00.098.898 I ggml_metal_init: use bfloat            = true
0.00.098.898 I ggml_metal_init: hasUnifiedMemory      = true
0.00.098.899 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.108.808 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.110.125 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.110.127 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.110.142 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.111.006 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.111.007 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.111.008 I llama_new_context_with_model: graph nodes  = 967
0.00.111.008 I llama_new_context_with_model: graph splits = 2
0.00.111.015 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.111.016 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.007.883 I 
0.01.007.921 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.007.956 I perplexity: tokenizing the input ..
0.01.019.967 I perplexity: tokenization took 12.008 ms
0.01.019.971 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.141.628 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.143.705 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.143.726 I llama_perf_context_print:        load time =     982.96 ms
0.01.143.728 I llama_perf_context_print: prompt eval time =     121.27 ms /   128 tokens (    0.95 ms per token,  1055.51 tokens per second)
0.01.143.729 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.143.736 I llama_perf_context_print:       total time =     135.84 ms /   129 tokens
0.01.144.453 I ggml_metal_free: deallocating

real	0m1.337s
user	0m0.125s
sys	0m0.198s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.040 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.009.703 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.196 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.025.201 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.203 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.205 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.206 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.206 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.206 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.208 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.208 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.208 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.209 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.209 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.209 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.210 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.211 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.212 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.212 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.169 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.293 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.391 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.392 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.393 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.393 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.393 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.394 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.034.394 I llama_model_loader: - type  f32:  194 tensors
0.00.034.395 I llama_model_loader: - type q8_0:   98 tensors
0.00.058.320 I llm_load_vocab: special tokens cache size = 25
0.00.064.351 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.064.355 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.064.356 I llm_load_print_meta: arch             = gptneox
0.00.064.356 I llm_load_print_meta: vocab type       = BPE
0.00.064.356 I llm_load_print_meta: n_vocab          = 50304
0.00.064.357 I llm_load_print_meta: n_merges         = 50009
0.00.064.357 I llm_load_print_meta: vocab_only       = 0
0.00.064.357 I llm_load_print_meta: n_ctx_train      = 2048
0.00.064.357 I llm_load_print_meta: n_embd           = 2048
0.00.064.357 I llm_load_print_meta: n_layer          = 24
0.00.064.362 I llm_load_print_meta: n_head           = 16
0.00.064.363 I llm_load_print_meta: n_head_kv        = 16
0.00.064.363 I llm_load_print_meta: n_rot            = 32
0.00.064.363 I llm_load_print_meta: n_swa            = 0
0.00.064.365 I llm_load_print_meta: n_embd_head_k    = 128
0.00.064.365 I llm_load_print_meta: n_embd_head_v    = 128
0.00.064.366 I llm_load_print_meta: n_gqa            = 1
0.00.064.367 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.064.367 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.064.368 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.064.368 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.064.368 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.064.368 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.064.369 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.064.370 I llm_load_print_meta: n_ff             = 8192
0.00.064.370 I llm_load_print_meta: n_expert         = 0
0.00.064.370 I llm_load_print_meta: n_expert_used    = 0
0.00.064.370 I llm_load_print_meta: causal attn      = 1
0.00.064.370 I llm_load_print_meta: pooling type     = 0
0.00.064.371 I llm_load_print_meta: rope type        = 2
0.00.064.372 I llm_load_print_meta: rope scaling     = linear
0.00.064.373 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.064.373 I llm_load_print_meta: freq_scale_train = 1
0.00.064.373 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.064.373 I llm_load_print_meta: rope_finetuned   = unknown
0.00.064.373 I llm_load_print_meta: ssm_d_conv       = 0
0.00.064.374 I llm_load_print_meta: ssm_d_inner      = 0
0.00.064.374 I llm_load_print_meta: ssm_d_state      = 0
0.00.064.374 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.064.374 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.064.374 I llm_load_print_meta: model type       = 1.4B
0.00.064.374 I llm_load_print_meta: model ftype      = Q8_0
0.00.064.375 I llm_load_print_meta: model params     = 1.41 B
0.00.064.375 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.064.375 I llm_load_print_meta: general.name     = 1.4B
0.00.064.376 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.064.376 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.064.380 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.064.380 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.064.380 I llm_load_print_meta: LF token         = 128 ''
0.00.064.381 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.064.381 I llm_load_print_meta: max token length = 1024
0.00.066.885 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.066.886 I llm_load_tensors: offloading output layer to GPU
0.00.066.886 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.066.897 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.066.898 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.067.865 I llama_new_context_with_model: n_seq_max     = 1
0.00.067.866 I llama_new_context_with_model: n_ctx         = 2048
0.00.067.866 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.067.866 I llama_new_context_with_model: n_batch       = 2048
0.00.067.866 I llama_new_context_with_model: n_ubatch      = 512
0.00.067.866 I llama_new_context_with_model: flash_attn    = 0
0.00.067.867 I llama_new_context_with_model: freq_base     = 10000.0
0.00.067.867 I llama_new_context_with_model: freq_scale    = 1
0.00.067.868 I ggml_metal_init: allocating
0.00.067.871 I ggml_metal_init: found device: Apple M4
0.00.067.873 I ggml_metal_init: picking default device: Apple M4
0.00.068.629 I ggml_metal_init: using embedded metal library
0.00.071.366 I ggml_metal_init: GPU name:   Apple M4
0.00.071.368 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.071.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.071.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.071.369 I ggml_metal_init: simdgroup reduction   = true
0.00.071.369 I ggml_metal_init: simdgroup matrix mul. = true
0.00.071.369 I ggml_metal_init: has bfloat            = true
0.00.071.369 I ggml_metal_init: use bfloat            = true
0.00.071.370 I ggml_metal_init: hasUnifiedMemory      = true
0.00.071.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.081.327 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.106.712 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.106.721 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.106.746 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.963 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.107.965 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.107.965 I llama_new_context_with_model: graph nodes  = 967
0.00.107.966 I llama_new_context_with_model: graph splits = 2
0.00.107.987 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.108.122 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.108.123 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.533.312 I main: llama threadpool init, n_threads = 4
0.01.533.358 I 
0.01.533.392 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.533.394 I 
0.01.533.634 I sampler seed: 1234
0.01.533.639 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.533.653 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.533.654 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.533.654 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.632.413 I llama_perf_sampler_print:    sampling time =       1.41 ms /    71 runs   (    0.02 ms per token, 50212.16 tokens per second)
0.02.632.414 I llama_perf_context_print:        load time =    1523.60 ms
0.02.632.414 I llama_perf_context_print: prompt eval time =      39.60 ms /     7 tokens (    5.66 ms per token,   176.79 tokens per second)
0.02.632.415 I llama_perf_context_print:        eval time =    1056.51 ms /    63 runs   (   16.77 ms per token,    59.63 tokens per second)
0.02.632.416 I llama_perf_context_print:       total time =    1099.10 ms /    70 tokens
0.02.632.631 I ggml_metal_free: deallocating

real	0m2.652s
user	0m0.116s
sys	0m0.225s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.133 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.012.131 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.285 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.020.290 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.293 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.020.294 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.294 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.020.294 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.020.295 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.020.296 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.020.296 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.020.297 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.020.297 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.020.298 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.020.298 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.020.298 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.020.300 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.020.300 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.020.301 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.025.783 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.229 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.233 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.235 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.235 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.236 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.236 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.237 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.032.237 I llama_model_loader: - type  f32:  194 tensors
0.00.032.238 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.690 I llm_load_vocab: special tokens cache size = 25
0.00.062.638 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.641 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.642 I llm_load_print_meta: arch             = gptneox
0.00.062.642 I llm_load_print_meta: vocab type       = BPE
0.00.062.642 I llm_load_print_meta: n_vocab          = 50304
0.00.062.642 I llm_load_print_meta: n_merges         = 50009
0.00.062.643 I llm_load_print_meta: vocab_only       = 0
0.00.062.643 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.643 I llm_load_print_meta: n_embd           = 2048
0.00.062.643 I llm_load_print_meta: n_layer          = 24
0.00.062.647 I llm_load_print_meta: n_head           = 16
0.00.062.648 I llm_load_print_meta: n_head_kv        = 16
0.00.062.648 I llm_load_print_meta: n_rot            = 32
0.00.062.648 I llm_load_print_meta: n_swa            = 0
0.00.062.649 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.649 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.649 I llm_load_print_meta: n_gqa            = 1
0.00.062.650 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.651 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.652 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.652 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.652 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.652 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.652 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.653 I llm_load_print_meta: n_ff             = 8192
0.00.062.653 I llm_load_print_meta: n_expert         = 0
0.00.062.653 I llm_load_print_meta: n_expert_used    = 0
0.00.062.653 I llm_load_print_meta: causal attn      = 1
0.00.062.654 I llm_load_print_meta: pooling type     = 0
0.00.062.654 I llm_load_print_meta: rope type        = 2
0.00.062.657 I llm_load_print_meta: rope scaling     = linear
0.00.062.657 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.658 I llm_load_print_meta: freq_scale_train = 1
0.00.062.658 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.658 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.658 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.658 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.658 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.658 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.659 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.659 I llm_load_print_meta: model type       = 1.4B
0.00.062.659 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.660 I llm_load_print_meta: model params     = 1.41 B
0.00.062.660 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.660 I llm_load_print_meta: general.name     = 1.4B
0.00.062.661 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.661 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.661 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.662 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.663 I llm_load_print_meta: LF token         = 128 ''
0.00.062.664 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.664 I llm_load_print_meta: max token length = 1024
0.00.065.011 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.012 I llm_load_tensors: offloading output layer to GPU
0.00.065.012 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.023 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.025 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.017 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.018 I llama_new_context_with_model: n_ctx         = 128
0.00.066.018 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.018 I llama_new_context_with_model: n_batch       = 128
0.00.066.018 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.018 I llama_new_context_with_model: flash_attn    = 0
0.00.066.019 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.019 I llama_new_context_with_model: freq_scale    = 1
0.00.066.019 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.020 I ggml_metal_init: allocating
0.00.066.026 I ggml_metal_init: found device: Apple M4
0.00.066.029 I ggml_metal_init: picking default device: Apple M4
0.00.066.648 I ggml_metal_init: using embedded metal library
0.00.069.078 I ggml_metal_init: GPU name:   Apple M4
0.00.069.080 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.080 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.080 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.081 I ggml_metal_init: simdgroup reduction   = true
0.00.069.081 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.081 I ggml_metal_init: has bfloat            = true
0.00.069.081 I ggml_metal_init: use bfloat            = true
0.00.069.082 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.083 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.344 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.080.727 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.080.730 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.080.749 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.081.683 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.081.684 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.081.684 I llama_new_context_with_model: graph nodes  = 967
0.00.081.684 I llama_new_context_with_model: graph splits = 2
0.00.081.698 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.081.698 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.851.943 I 
0.00.851.999 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.852.019 I perplexity: tokenizing the input ..
0.00.859.898 I perplexity: tokenization took 7.877 ms
0.00.859.901 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.983.842 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.00.984.993 I Final estimate: PPL = 10.1362 +/- 3.22437

0.00.985.004 I llama_perf_context_print:        load time =     839.81 ms
0.00.985.005 I llama_perf_context_print: prompt eval time =     123.72 ms /   128 tokens (    0.97 ms per token,  1034.63 tokens per second)
0.00.985.006 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.985.006 I llama_perf_context_print:       total time =     133.06 ms /   129 tokens
0.00.985.472 I ggml_metal_free: deallocating

real	0m1.004s
user	0m0.091s
sys	0m0.153s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.043 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.074 I main: llama backend init
0.00.000.076 I main: load the model and apply lora adapter, if any
0.00.015.817 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.031.554 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.031.560 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.031.562 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.031.562 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.031.565 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.031.565 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.031.565 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.031.566 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.031.567 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.031.567 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.031.568 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.031.568 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.031.568 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.031.569 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.031.572 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.031.573 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.031.573 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.217 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.702 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.044.258 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.044.260 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.044.260 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.044.261 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.044.261 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.044.262 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.044.262 I llama_model_loader: - type  f32:  194 tensors
0.00.044.263 I llama_model_loader: - type q4_0:   97 tensors
0.00.044.263 I llama_model_loader: - type q6_K:    1 tensors
0.00.084.959 I llm_load_vocab: special tokens cache size = 25
0.00.094.373 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.094.379 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.094.379 I llm_load_print_meta: arch             = gptneox
0.00.094.380 I llm_load_print_meta: vocab type       = BPE
0.00.094.386 I llm_load_print_meta: n_vocab          = 50304
0.00.094.386 I llm_load_print_meta: n_merges         = 50009
0.00.094.389 I llm_load_print_meta: vocab_only       = 0
0.00.094.389 I llm_load_print_meta: n_ctx_train      = 2048
0.00.094.389 I llm_load_print_meta: n_embd           = 2048
0.00.094.389 I llm_load_print_meta: n_layer          = 24
0.00.094.393 I llm_load_print_meta: n_head           = 16
0.00.094.394 I llm_load_print_meta: n_head_kv        = 16
0.00.094.394 I llm_load_print_meta: n_rot            = 32
0.00.094.394 I llm_load_print_meta: n_swa            = 0
0.00.094.394 I llm_load_print_meta: n_embd_head_k    = 128
0.00.094.395 I llm_load_print_meta: n_embd_head_v    = 128
0.00.094.395 I llm_load_print_meta: n_gqa            = 1
0.00.094.397 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.094.397 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.094.398 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.094.399 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.094.399 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.094.399 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.094.399 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.094.400 I llm_load_print_meta: n_ff             = 8192
0.00.094.400 I llm_load_print_meta: n_expert         = 0
0.00.094.401 I llm_load_print_meta: n_expert_used    = 0
0.00.094.402 I llm_load_print_meta: causal attn      = 1
0.00.094.403 I llm_load_print_meta: pooling type     = 0
0.00.094.404 I llm_load_print_meta: rope type        = 2
0.00.094.404 I llm_load_print_meta: rope scaling     = linear
0.00.094.404 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.094.405 I llm_load_print_meta: freq_scale_train = 1
0.00.094.405 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.094.405 I llm_load_print_meta: rope_finetuned   = unknown
0.00.094.406 I llm_load_print_meta: ssm_d_conv       = 0
0.00.094.406 I llm_load_print_meta: ssm_d_inner      = 0
0.00.094.406 I llm_load_print_meta: ssm_d_state      = 0
0.00.094.408 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.094.408 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.094.408 I llm_load_print_meta: model type       = 1.4B
0.00.094.409 I llm_load_print_meta: model ftype      = Q4_0
0.00.094.409 I llm_load_print_meta: model params     = 1.41 B
0.00.094.410 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.094.410 I llm_load_print_meta: general.name     = 1.4B
0.00.094.411 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.094.411 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.094.411 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.094.413 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.094.414 I llm_load_print_meta: LF token         = 128 ''
0.00.094.414 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.094.414 I llm_load_print_meta: max token length = 1024
0.00.097.193 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.097.194 I llm_load_tensors: offloading output layer to GPU
0.00.097.194 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.097.205 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.097.207 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.098.437 I llama_new_context_with_model: n_seq_max     = 1
0.00.098.438 I llama_new_context_with_model: n_ctx         = 2048
0.00.098.438 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.098.438 I llama_new_context_with_model: n_batch       = 2048
0.00.098.439 I llama_new_context_with_model: n_ubatch      = 512
0.00.098.439 I llama_new_context_with_model: flash_attn    = 0
0.00.098.439 I llama_new_context_with_model: freq_base     = 10000.0
0.00.098.440 I llama_new_context_with_model: freq_scale    = 1
0.00.098.440 I ggml_metal_init: allocating
0.00.098.444 I ggml_metal_init: found device: Apple M4
0.00.098.447 I ggml_metal_init: picking default device: Apple M4
0.00.099.316 I ggml_metal_init: using embedded metal library
0.00.102.618 I ggml_metal_init: GPU name:   Apple M4
0.00.102.620 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.102.621 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.102.621 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.102.622 I ggml_metal_init: simdgroup reduction   = true
0.00.102.622 I ggml_metal_init: simdgroup matrix mul. = true
0.00.102.622 I ggml_metal_init: has bfloat            = true
0.00.102.624 I ggml_metal_init: use bfloat            = true
0.00.102.624 I ggml_metal_init: hasUnifiedMemory      = true
0.00.102.625 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.114.036 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.135.761 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.135.767 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.135.786 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.136.830 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.136.831 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.136.832 I llama_new_context_with_model: graph nodes  = 967
0.00.136.832 I llama_new_context_with_model: graph splits = 2
0.00.136.844 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.136.988 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.136.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.759.887 I main: llama threadpool init, n_threads = 4
0.00.759.982 I 
0.00.760.052 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.760.054 I 
0.00.760.636 I sampler seed: 1234
0.00.760.644 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.760.686 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.760.688 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.760.688 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.458.982 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59117.40 tokens per second)
0.01.458.983 I llama_perf_context_print:        load time =     744.06 ms
0.01.458.983 I llama_perf_context_print: prompt eval time =      49.72 ms /     7 tokens (    7.10 ms per token,   140.78 tokens per second)
0.01.458.984 I llama_perf_context_print:        eval time =     645.64 ms /    63 runs   (   10.25 ms per token,    97.58 tokens per second)
0.01.458.984 I llama_perf_context_print:       total time =     699.10 ms /    70 tokens
0.01.459.128 I ggml_metal_free: deallocating

real	0m1.499s
user	0m0.154s
sys	0m0.180s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.389 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.469 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.473 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.475 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.475 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.476 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.476 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.476 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.477 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.477 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.478 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.478 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.478 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.479 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.479 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.480 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.481 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.481 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.359 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.390 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.289 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.290 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.290 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.291 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.291 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.024.292 I llama_model_loader: - type  f32:  194 tensors
0.00.024.292 I llama_model_loader: - type q4_0:   97 tensors
0.00.024.293 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.079 I llm_load_vocab: special tokens cache size = 25
0.00.051.127 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.129 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.130 I llm_load_print_meta: arch             = gptneox
0.00.051.130 I llm_load_print_meta: vocab type       = BPE
0.00.051.130 I llm_load_print_meta: n_vocab          = 50304
0.00.051.131 I llm_load_print_meta: n_merges         = 50009
0.00.051.131 I llm_load_print_meta: vocab_only       = 0
0.00.051.131 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.131 I llm_load_print_meta: n_embd           = 2048
0.00.051.131 I llm_load_print_meta: n_layer          = 24
0.00.051.134 I llm_load_print_meta: n_head           = 16
0.00.051.137 I llm_load_print_meta: n_head_kv        = 16
0.00.051.137 I llm_load_print_meta: n_rot            = 32
0.00.051.137 I llm_load_print_meta: n_swa            = 0
0.00.051.137 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.137 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.138 I llm_load_print_meta: n_gqa            = 1
0.00.051.139 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.139 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.140 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.140 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.140 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.140 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.141 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.141 I llm_load_print_meta: n_ff             = 8192
0.00.051.141 I llm_load_print_meta: n_expert         = 0
0.00.051.142 I llm_load_print_meta: n_expert_used    = 0
0.00.051.142 I llm_load_print_meta: causal attn      = 1
0.00.051.142 I llm_load_print_meta: pooling type     = 0
0.00.051.142 I llm_load_print_meta: rope type        = 2
0.00.051.142 I llm_load_print_meta: rope scaling     = linear
0.00.051.143 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.143 I llm_load_print_meta: freq_scale_train = 1
0.00.051.143 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.145 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.145 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.146 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.146 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.146 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.146 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.146 I llm_load_print_meta: model type       = 1.4B
0.00.051.147 I llm_load_print_meta: model ftype      = Q4_0
0.00.051.147 I llm_load_print_meta: model params     = 1.41 B
0.00.051.148 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.051.148 I llm_load_print_meta: general.name     = 1.4B
0.00.051.148 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.150 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.150 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.150 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.151 I llm_load_print_meta: LF token         = 128 ''
0.00.051.151 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.151 I llm_load_print_meta: max token length = 1024
0.00.053.051 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.052 I llm_load_tensors: offloading output layer to GPU
0.00.053.052 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.062 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.053.063 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.053.950 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.950 I llama_new_context_with_model: n_ctx         = 128
0.00.053.951 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.951 I llama_new_context_with_model: n_batch       = 128
0.00.053.951 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.951 I llama_new_context_with_model: flash_attn    = 0
0.00.053.952 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.952 I llama_new_context_with_model: freq_scale    = 1
0.00.053.952 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.953 I ggml_metal_init: allocating
0.00.053.958 I ggml_metal_init: found device: Apple M4
0.00.053.960 I ggml_metal_init: picking default device: Apple M4
0.00.054.537 I ggml_metal_init: using embedded metal library
0.00.056.861 I ggml_metal_init: GPU name:   Apple M4
0.00.056.863 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.863 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.864 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.864 I ggml_metal_init: simdgroup reduction   = true
0.00.056.864 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.864 I ggml_metal_init: has bfloat            = true
0.00.056.865 I ggml_metal_init: use bfloat            = true
0.00.056.865 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.866 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.842 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.139 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.141 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.155 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.075 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.076 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.076 I llama_new_context_with_model: graph nodes  = 967
0.00.069.077 I llama_new_context_with_model: graph splits = 2
0.00.069.089 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.633.364 I 
0.00.633.405 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.633.418 I perplexity: tokenizing the input ..
0.00.641.286 I perplexity: tokenization took 7.865 ms
0.00.641.293 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.764.007 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.765.180 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.765.196 I llama_perf_context_print:        load time =     623.97 ms
0.00.765.197 I llama_perf_context_print: prompt eval time =     122.49 ms /   128 tokens (    0.96 ms per token,  1044.99 tokens per second)
0.00.765.198 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.765.198 I llama_perf_context_print:       total time =     131.84 ms /   129 tokens
0.00.765.501 I ggml_metal_free: deallocating

real	0m0.780s
user	0m0.079s
sys	0m0.116s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.828 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.025.296 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.025.301 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.025.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.025.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.025.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.025.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.025.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.025.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.025.311 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.025.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.025.312 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.025.313 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.025.313 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.025.313 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.025.315 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.025.315 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.025.316 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.029.436 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.030.494 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.034.681 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.034.682 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.034.682 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.034.683 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.034.683 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.034.683 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.034.684 I llama_model_loader: - type  f32:  194 tensors
0.00.034.684 I llama_model_loader: - type q4_1:   97 tensors
0.00.034.684 I llama_model_loader: - type q6_K:    1 tensors
0.00.056.424 I llm_load_vocab: special tokens cache size = 25
0.00.062.469 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.472 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.473 I llm_load_print_meta: arch             = gptneox
0.00.062.473 I llm_load_print_meta: vocab type       = BPE
0.00.062.473 I llm_load_print_meta: n_vocab          = 50304
0.00.062.473 I llm_load_print_meta: n_merges         = 50009
0.00.062.474 I llm_load_print_meta: vocab_only       = 0
0.00.062.474 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.474 I llm_load_print_meta: n_embd           = 2048
0.00.062.474 I llm_load_print_meta: n_layer          = 24
0.00.062.477 I llm_load_print_meta: n_head           = 16
0.00.062.478 I llm_load_print_meta: n_head_kv        = 16
0.00.062.478 I llm_load_print_meta: n_rot            = 32
0.00.062.478 I llm_load_print_meta: n_swa            = 0
0.00.062.481 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.481 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.482 I llm_load_print_meta: n_gqa            = 1
0.00.062.483 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.484 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.484 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.484 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.485 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.485 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.485 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.487 I llm_load_print_meta: n_ff             = 8192
0.00.062.487 I llm_load_print_meta: n_expert         = 0
0.00.062.487 I llm_load_print_meta: n_expert_used    = 0
0.00.062.488 I llm_load_print_meta: causal attn      = 1
0.00.062.488 I llm_load_print_meta: pooling type     = 0
0.00.062.488 I llm_load_print_meta: rope type        = 2
0.00.062.488 I llm_load_print_meta: rope scaling     = linear
0.00.062.489 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.489 I llm_load_print_meta: freq_scale_train = 1
0.00.062.489 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.489 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.489 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.490 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.490 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.490 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.490 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.490 I llm_load_print_meta: model type       = 1.4B
0.00.062.490 I llm_load_print_meta: model ftype      = Q4_1
0.00.062.491 I llm_load_print_meta: model params     = 1.41 B
0.00.062.495 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.062.496 I llm_load_print_meta: general.name     = 1.4B
0.00.062.496 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.496 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.496 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.496 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.497 I llm_load_print_meta: LF token         = 128 ''
0.00.062.497 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.497 I llm_load_print_meta: max token length = 1024
0.00.064.500 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.064.501 I llm_load_tensors: offloading output layer to GPU
0.00.064.501 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.064.511 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.064.512 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.065.395 I llama_new_context_with_model: n_seq_max     = 1
0.00.065.396 I llama_new_context_with_model: n_ctx         = 2048
0.00.065.396 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.065.396 I llama_new_context_with_model: n_batch       = 2048
0.00.065.397 I llama_new_context_with_model: n_ubatch      = 512
0.00.065.397 I llama_new_context_with_model: flash_attn    = 0
0.00.065.397 I llama_new_context_with_model: freq_base     = 10000.0
0.00.065.397 I llama_new_context_with_model: freq_scale    = 1
0.00.065.398 I ggml_metal_init: allocating
0.00.065.401 I ggml_metal_init: found device: Apple M4
0.00.065.403 I ggml_metal_init: picking default device: Apple M4
0.00.066.052 I ggml_metal_init: using embedded metal library
0.00.068.526 I ggml_metal_init: GPU name:   Apple M4
0.00.068.528 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.068.528 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.068.528 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.068.529 I ggml_metal_init: simdgroup reduction   = true
0.00.068.529 I ggml_metal_init: simdgroup matrix mul. = true
0.00.068.529 I ggml_metal_init: has bfloat            = true
0.00.068.529 I ggml_metal_init: use bfloat            = true
0.00.068.530 I ggml_metal_init: hasUnifiedMemory      = true
0.00.068.530 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.078.665 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.099.157 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.099.161 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.099.181 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.100.245 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.100.246 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.100.247 I llama_new_context_with_model: graph nodes  = 967
0.00.100.247 I llama_new_context_with_model: graph splits = 2
0.00.100.262 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.100.410 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.411 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.767.581 I main: llama threadpool init, n_threads = 4
0.00.767.620 I 
0.00.767.651 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.767.652 I 
0.00.767.880 I sampler seed: 1234
0.00.767.884 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.767.927 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.767.931 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.767.931 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.488.499 I llama_perf_sampler_print:    sampling time =       1.07 ms /    71 runs   (    0.02 ms per token, 66479.40 tokens per second)
0.01.488.500 I llama_perf_context_print:        load time =     758.75 ms
0.01.488.500 I llama_perf_context_print: prompt eval time =      39.70 ms /     7 tokens (    5.67 ms per token,   176.34 tokens per second)
0.01.488.502 I llama_perf_context_print:        eval time =     678.06 ms /    63 runs   (   10.76 ms per token,    92.91 tokens per second)
0.01.488.502 I llama_perf_context_print:       total time =     720.92 ms /    70 tokens
0.01.488.693 I ggml_metal_free: deallocating

real	0m1.505s
user	0m0.112s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.895 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.995 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.014.999 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.001 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.001 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.002 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.002 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.002 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.003 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.003 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.004 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.004 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.004 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.005 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.005 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.007 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.007 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.007 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.895 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.004 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.021 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.022 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.024.023 I llama_model_loader: - type  f32:  194 tensors
0.00.024.023 I llama_model_loader: - type q4_1:   97 tensors
0.00.024.023 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.684 I llm_load_vocab: special tokens cache size = 25
0.00.050.487 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.489 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.490 I llm_load_print_meta: arch             = gptneox
0.00.050.490 I llm_load_print_meta: vocab type       = BPE
0.00.050.490 I llm_load_print_meta: n_vocab          = 50304
0.00.050.490 I llm_load_print_meta: n_merges         = 50009
0.00.050.490 I llm_load_print_meta: vocab_only       = 0
0.00.050.491 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.491 I llm_load_print_meta: n_embd           = 2048
0.00.050.491 I llm_load_print_meta: n_layer          = 24
0.00.050.494 I llm_load_print_meta: n_head           = 16
0.00.050.495 I llm_load_print_meta: n_head_kv        = 16
0.00.050.495 I llm_load_print_meta: n_rot            = 32
0.00.050.495 I llm_load_print_meta: n_swa            = 0
0.00.050.497 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.497 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.497 I llm_load_print_meta: n_gqa            = 1
0.00.050.498 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.499 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.499 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.500 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.500 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.500 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.500 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.501 I llm_load_print_meta: n_ff             = 8192
0.00.050.503 I llm_load_print_meta: n_expert         = 0
0.00.050.503 I llm_load_print_meta: n_expert_used    = 0
0.00.050.503 I llm_load_print_meta: causal attn      = 1
0.00.050.503 I llm_load_print_meta: pooling type     = 0
0.00.050.503 I llm_load_print_meta: rope type        = 2
0.00.050.504 I llm_load_print_meta: rope scaling     = linear
0.00.050.504 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.505 I llm_load_print_meta: freq_scale_train = 1
0.00.050.505 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.506 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.506 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.506 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.506 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.506 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.506 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.507 I llm_load_print_meta: model type       = 1.4B
0.00.050.507 I llm_load_print_meta: model ftype      = Q4_1
0.00.050.507 I llm_load_print_meta: model params     = 1.41 B
0.00.050.508 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.050.508 I llm_load_print_meta: general.name     = 1.4B
0.00.050.508 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.509 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.509 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.509 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.509 I llm_load_print_meta: LF token         = 128 ''
0.00.050.509 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.510 I llm_load_print_meta: max token length = 1024
0.00.052.553 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.554 I llm_load_tensors: offloading output layer to GPU
0.00.052.554 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.565 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.052.566 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.053.469 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.470 I llama_new_context_with_model: n_ctx         = 128
0.00.053.470 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.470 I llama_new_context_with_model: n_batch       = 128
0.00.053.470 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.470 I llama_new_context_with_model: flash_attn    = 0
0.00.053.471 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.471 I llama_new_context_with_model: freq_scale    = 1
0.00.053.472 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.472 I ggml_metal_init: allocating
0.00.053.478 I ggml_metal_init: found device: Apple M4
0.00.053.482 I ggml_metal_init: picking default device: Apple M4
0.00.054.073 I ggml_metal_init: using embedded metal library
0.00.056.396 I ggml_metal_init: GPU name:   Apple M4
0.00.056.397 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.398 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.398 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.398 I ggml_metal_init: simdgroup reduction   = true
0.00.056.399 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.399 I ggml_metal_init: has bfloat            = true
0.00.056.399 I ggml_metal_init: use bfloat            = true
0.00.056.399 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.400 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.391 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.714 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.717 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.730 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.698 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.699 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.699 I llama_new_context_with_model: graph nodes  = 967
0.00.068.699 I llama_new_context_with_model: graph splits = 2
0.00.068.712 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.713 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.772 I 
0.00.662.824 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.843 I perplexity: tokenizing the input ..
0.00.670.733 I perplexity: tokenization took 7.888 ms
0.00.670.740 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.793.931 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.795.165 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.795.189 I llama_perf_context_print:        load time =     653.87 ms
0.00.795.190 I llama_perf_context_print: prompt eval time =     122.93 ms /   128 tokens (    0.96 ms per token,  1041.23 tokens per second)
0.00.795.191 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.795.191 I llama_perf_context_print:       total time =     132.42 ms /   129 tokens
0.00.795.557 I ggml_metal_free: deallocating

real	0m0.809s
user	0m0.079s
sys	0m0.104s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.015.160 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.032.870 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.032.874 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.032.875 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.032.876 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.032.876 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.032.876 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.032.876 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.032.877 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.032.878 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.032.878 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.032.878 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.032.879 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.032.879 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.032.881 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.032.884 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.032.884 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.032.884 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.037.351 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.038.533 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.042.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.042.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.042.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.042.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.042.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.042.966 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.042.967 I llama_model_loader: - type  f32:  194 tensors
0.00.042.967 I llama_model_loader: - type q5_0:   97 tensors
0.00.042.967 I llama_model_loader: - type q6_K:    1 tensors
0.00.069.870 I llm_load_vocab: special tokens cache size = 25
0.00.077.987 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.077.990 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.077.990 I llm_load_print_meta: arch             = gptneox
0.00.077.991 I llm_load_print_meta: vocab type       = BPE
0.00.077.991 I llm_load_print_meta: n_vocab          = 50304
0.00.077.991 I llm_load_print_meta: n_merges         = 50009
0.00.077.991 I llm_load_print_meta: vocab_only       = 0
0.00.077.991 I llm_load_print_meta: n_ctx_train      = 2048
0.00.077.992 I llm_load_print_meta: n_embd           = 2048
0.00.077.992 I llm_load_print_meta: n_layer          = 24
0.00.077.997 I llm_load_print_meta: n_head           = 16
0.00.077.997 I llm_load_print_meta: n_head_kv        = 16
0.00.077.999 I llm_load_print_meta: n_rot            = 32
0.00.077.999 I llm_load_print_meta: n_swa            = 0
0.00.077.999 I llm_load_print_meta: n_embd_head_k    = 128
0.00.078.000 I llm_load_print_meta: n_embd_head_v    = 128
0.00.078.000 I llm_load_print_meta: n_gqa            = 1
0.00.078.001 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.078.002 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.078.002 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.078.002 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.078.003 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.078.003 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.078.003 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.078.004 I llm_load_print_meta: n_ff             = 8192
0.00.078.004 I llm_load_print_meta: n_expert         = 0
0.00.078.004 I llm_load_print_meta: n_expert_used    = 0
0.00.078.005 I llm_load_print_meta: causal attn      = 1
0.00.078.006 I llm_load_print_meta: pooling type     = 0
0.00.078.007 I llm_load_print_meta: rope type        = 2
0.00.078.007 I llm_load_print_meta: rope scaling     = linear
0.00.078.007 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.078.008 I llm_load_print_meta: freq_scale_train = 1
0.00.078.008 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.078.008 I llm_load_print_meta: rope_finetuned   = unknown
0.00.078.008 I llm_load_print_meta: ssm_d_conv       = 0
0.00.078.008 I llm_load_print_meta: ssm_d_inner      = 0
0.00.078.008 I llm_load_print_meta: ssm_d_state      = 0
0.00.078.009 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.078.009 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.078.009 I llm_load_print_meta: model type       = 1.4B
0.00.078.009 I llm_load_print_meta: model ftype      = Q5_0
0.00.078.010 I llm_load_print_meta: model params     = 1.41 B
0.00.078.010 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.078.011 I llm_load_print_meta: general.name     = 1.4B
0.00.078.011 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.078.011 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.078.011 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.078.011 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.078.012 I llm_load_print_meta: LF token         = 128 ''
0.00.078.012 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.078.012 I llm_load_print_meta: max token length = 1024
0.00.080.386 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.080.387 I llm_load_tensors: offloading output layer to GPU
0.00.080.388 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.080.398 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.080.399 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.081.516 I llama_new_context_with_model: n_seq_max     = 1
0.00.081.517 I llama_new_context_with_model: n_ctx         = 2048
0.00.081.518 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.081.518 I llama_new_context_with_model: n_batch       = 2048
0.00.081.518 I llama_new_context_with_model: n_ubatch      = 512
0.00.081.518 I llama_new_context_with_model: flash_attn    = 0
0.00.081.519 I llama_new_context_with_model: freq_base     = 10000.0
0.00.081.519 I llama_new_context_with_model: freq_scale    = 1
0.00.081.520 I ggml_metal_init: allocating
0.00.081.527 I ggml_metal_init: found device: Apple M4
0.00.081.530 I ggml_metal_init: picking default device: Apple M4
0.00.082.230 I ggml_metal_init: using embedded metal library
0.00.086.070 I ggml_metal_init: GPU name:   Apple M4
0.00.086.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.086.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.086.074 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.086.074 I ggml_metal_init: simdgroup reduction   = true
0.00.086.075 I ggml_metal_init: simdgroup matrix mul. = true
0.00.086.075 I ggml_metal_init: has bfloat            = true
0.00.086.075 I ggml_metal_init: use bfloat            = true
0.00.086.076 I ggml_metal_init: hasUnifiedMemory      = true
0.00.086.078 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.099.153 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.121.963 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.121.970 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.121.987 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.122.984 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.122.986 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.122.986 I llama_new_context_with_model: graph nodes  = 967
0.00.122.986 I llama_new_context_with_model: graph splits = 2
0.00.123.001 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.123.145 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.123.146 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.801.848 I main: llama threadpool init, n_threads = 4
0.00.801.889 I 
0.00.801.920 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.801.920 I 
0.00.802.138 I sampler seed: 1234
0.00.802.144 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.802.179 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.802.180 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.802.180 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.591.837 I llama_perf_sampler_print:    sampling time =       1.23 ms /    71 runs   (    0.02 ms per token, 57629.87 tokens per second)
0.01.591.837 I llama_perf_context_print:        load time =     786.68 ms
0.01.591.838 I llama_perf_context_print: prompt eval time =      43.07 ms /     7 tokens (    6.15 ms per token,   162.52 tokens per second)
0.01.591.838 I llama_perf_context_print:        eval time =     743.60 ms /    63 runs   (   11.80 ms per token,    84.72 tokens per second)
0.01.591.839 I llama_perf_context_print:       total time =     789.99 ms /    70 tokens
0.01.592.007 I ggml_metal_free: deallocating

real	0m1.610s
user	0m0.127s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.095 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.401 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.359 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.364 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.373 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.374 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.374 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.375 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.375 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.375 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.376 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.376 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.377 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.378 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.378 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.322 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.355 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.291 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.292 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.292 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.293 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.293 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.293 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.025.294 I llama_model_loader: - type  f32:  194 tensors
0.00.025.294 I llama_model_loader: - type q5_0:   97 tensors
0.00.025.294 I llama_model_loader: - type q6_K:    1 tensors
0.00.046.554 I llm_load_vocab: special tokens cache size = 25
0.00.052.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.394 I llm_load_print_meta: arch             = gptneox
0.00.052.394 I llm_load_print_meta: vocab type       = BPE
0.00.052.395 I llm_load_print_meta: n_vocab          = 50304
0.00.052.395 I llm_load_print_meta: n_merges         = 50009
0.00.052.395 I llm_load_print_meta: vocab_only       = 0
0.00.052.395 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.395 I llm_load_print_meta: n_embd           = 2048
0.00.052.396 I llm_load_print_meta: n_layer          = 24
0.00.052.398 I llm_load_print_meta: n_head           = 16
0.00.052.399 I llm_load_print_meta: n_head_kv        = 16
0.00.052.399 I llm_load_print_meta: n_rot            = 32
0.00.052.399 I llm_load_print_meta: n_swa            = 0
0.00.052.400 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.400 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.400 I llm_load_print_meta: n_gqa            = 1
0.00.052.401 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.404 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.405 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.405 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.405 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.406 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.406 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.406 I llm_load_print_meta: n_ff             = 8192
0.00.052.407 I llm_load_print_meta: n_expert         = 0
0.00.052.408 I llm_load_print_meta: n_expert_used    = 0
0.00.052.408 I llm_load_print_meta: causal attn      = 1
0.00.052.409 I llm_load_print_meta: pooling type     = 0
0.00.052.409 I llm_load_print_meta: rope type        = 2
0.00.052.409 I llm_load_print_meta: rope scaling     = linear
0.00.052.409 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.410 I llm_load_print_meta: freq_scale_train = 1
0.00.052.410 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.410 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.410 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.410 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.410 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.410 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.411 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.411 I llm_load_print_meta: model type       = 1.4B
0.00.052.411 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.412 I llm_load_print_meta: model params     = 1.41 B
0.00.052.412 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.413 I llm_load_print_meta: general.name     = 1.4B
0.00.052.413 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.413 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.413 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.413 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.414 I llm_load_print_meta: LF token         = 128 ''
0.00.052.414 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.414 I llm_load_print_meta: max token length = 1024
0.00.054.532 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.533 I llm_load_tensors: offloading output layer to GPU
0.00.054.533 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.539 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.539 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.499 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.499 I llama_new_context_with_model: n_ctx         = 128
0.00.055.500 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.055.500 I llama_new_context_with_model: n_batch       = 128
0.00.055.500 I llama_new_context_with_model: n_ubatch      = 128
0.00.055.500 I llama_new_context_with_model: flash_attn    = 0
0.00.055.501 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.501 I llama_new_context_with_model: freq_scale    = 1
0.00.055.501 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.055.502 I ggml_metal_init: allocating
0.00.055.505 I ggml_metal_init: found device: Apple M4
0.00.055.507 I ggml_metal_init: picking default device: Apple M4
0.00.056.081 I ggml_metal_init: using embedded metal library
0.00.058.494 I ggml_metal_init: GPU name:   Apple M4
0.00.058.495 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.496 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.496 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.496 I ggml_metal_init: simdgroup reduction   = true
0.00.058.497 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.497 I ggml_metal_init: has bfloat            = true
0.00.058.497 I ggml_metal_init: use bfloat            = true
0.00.058.497 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.498 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.669 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.069.932 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.069.934 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.069.948 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.070.794 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.070.795 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.070.795 I llama_new_context_with_model: graph nodes  = 967
0.00.070.795 I llama_new_context_with_model: graph splits = 2
0.00.070.803 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.070.803 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.666.045 I 
0.00.666.084 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.666.095 I perplexity: tokenizing the input ..
0.00.673.728 I perplexity: tokenization took 7.631 ms
0.00.673.732 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.808.065 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.809.345 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.809.362 I llama_perf_context_print:        load time =     655.64 ms
0.00.809.363 I llama_perf_context_print: prompt eval time =     134.08 ms /   128 tokens (    1.05 ms per token,   954.64 tokens per second)
0.00.809.363 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.809.367 I llama_perf_context_print:       total time =     143.32 ms /   129 tokens
0.00.809.712 I ggml_metal_free: deallocating

real	0m0.825s
user	0m0.080s
sys	0m0.091s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.036 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.065 I main: llama backend init
0.00.000.067 I main: load the model and apply lora adapter, if any
0.00.008.626 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.163 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.168 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.169 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.170 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.173 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.173 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.173 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.177 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.177 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.178 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.179 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.182 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.183 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.183 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.147 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.203 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.134 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.135 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.135 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.136 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.136 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.137 I llama_model_loader: - type  f32:  194 tensors
0.00.025.137 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.138 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.969 I llm_load_vocab: special tokens cache size = 25
0.00.051.930 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.933 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.933 I llm_load_print_meta: arch             = gptneox
0.00.051.933 I llm_load_print_meta: vocab type       = BPE
0.00.051.934 I llm_load_print_meta: n_vocab          = 50304
0.00.051.934 I llm_load_print_meta: n_merges         = 50009
0.00.051.934 I llm_load_print_meta: vocab_only       = 0
0.00.051.934 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.934 I llm_load_print_meta: n_embd           = 2048
0.00.051.935 I llm_load_print_meta: n_layer          = 24
0.00.051.937 I llm_load_print_meta: n_head           = 16
0.00.051.938 I llm_load_print_meta: n_head_kv        = 16
0.00.051.938 I llm_load_print_meta: n_rot            = 32
0.00.051.938 I llm_load_print_meta: n_swa            = 0
0.00.051.938 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.938 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.939 I llm_load_print_meta: n_gqa            = 1
0.00.051.940 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.941 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.941 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.943 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.945 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.945 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.945 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.946 I llm_load_print_meta: n_ff             = 8192
0.00.051.946 I llm_load_print_meta: n_expert         = 0
0.00.051.946 I llm_load_print_meta: n_expert_used    = 0
0.00.051.946 I llm_load_print_meta: causal attn      = 1
0.00.051.946 I llm_load_print_meta: pooling type     = 0
0.00.051.946 I llm_load_print_meta: rope type        = 2
0.00.051.947 I llm_load_print_meta: rope scaling     = linear
0.00.051.947 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.947 I llm_load_print_meta: freq_scale_train = 1
0.00.051.948 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.948 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.948 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.948 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.948 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.948 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.948 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.949 I llm_load_print_meta: model type       = 1.4B
0.00.051.949 I llm_load_print_meta: model ftype      = Q5_1
0.00.051.949 I llm_load_print_meta: model params     = 1.41 B
0.00.051.950 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.051.950 I llm_load_print_meta: general.name     = 1.4B
0.00.051.951 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.951 I llm_load_print_meta: LF token         = 128 ''
0.00.051.952 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.952 I llm_load_print_meta: max token length = 1024
0.00.053.993 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.994 I llm_load_tensors: offloading output layer to GPU
0.00.053.994 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.004 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.054.005 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.054.901 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.902 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.902 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.902 I llama_new_context_with_model: n_batch       = 2048
0.00.054.903 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.903 I llama_new_context_with_model: flash_attn    = 0
0.00.054.903 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.903 I llama_new_context_with_model: freq_scale    = 1
0.00.054.904 I ggml_metal_init: allocating
0.00.054.907 I ggml_metal_init: found device: Apple M4
0.00.054.912 I ggml_metal_init: picking default device: Apple M4
0.00.055.537 I ggml_metal_init: using embedded metal library
0.00.057.946 I ggml_metal_init: GPU name:   Apple M4
0.00.057.948 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.948 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.948 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.949 I ggml_metal_init: simdgroup reduction   = true
0.00.057.949 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.949 I ggml_metal_init: has bfloat            = true
0.00.057.949 I ggml_metal_init: use bfloat            = true
0.00.057.950 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.950 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.865 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.531 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.538 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.557 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.544 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.545 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.545 I llama_new_context_with_model: graph nodes  = 967
0.00.088.546 I llama_new_context_with_model: graph splits = 2
0.00.088.561 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.702 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.703 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.820.183 I main: llama threadpool init, n_threads = 4
0.00.820.228 I 
0.00.820.257 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.820.258 I 
0.00.820.510 I sampler seed: 1234
0.00.820.517 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.820.554 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.820.555 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.820.555 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.661.662 I llama_perf_sampler_print:    sampling time =       1.43 ms /    71 runs   (    0.02 ms per token, 49581.01 tokens per second)
0.01.661.663 I llama_perf_context_print:        load time =     811.55 ms
0.01.661.664 I llama_perf_context_print: prompt eval time =      42.00 ms /     7 tokens (    6.00 ms per token,   166.68 tokens per second)
0.01.661.665 I llama_perf_context_print:        eval time =     796.19 ms /    63 runs   (   12.64 ms per token,    79.13 tokens per second)
0.01.661.665 I llama_perf_context_print:       total time =     841.48 ms /    70 tokens
0.01.661.847 I ggml_metal_free: deallocating

real	0m1.677s
user	0m0.111s
sys	0m0.172s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.094 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.548 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.817 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.014.822 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.824 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.824 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.825 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.825 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.825 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.826 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.827 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.827 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.827 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.828 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.828 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.829 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.830 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.830 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.830 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.608 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.658 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.569 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.570 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.570 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.570 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.571 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.571 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.571 I llama_model_loader: - type  f32:  194 tensors
0.00.023.572 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.572 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.875 I llm_load_vocab: special tokens cache size = 25
0.00.050.785 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.789 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.790 I llm_load_print_meta: arch             = gptneox
0.00.050.790 I llm_load_print_meta: vocab type       = BPE
0.00.050.790 I llm_load_print_meta: n_vocab          = 50304
0.00.050.790 I llm_load_print_meta: n_merges         = 50009
0.00.050.791 I llm_load_print_meta: vocab_only       = 0
0.00.050.792 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.793 I llm_load_print_meta: n_embd           = 2048
0.00.050.793 I llm_load_print_meta: n_layer          = 24
0.00.050.796 I llm_load_print_meta: n_head           = 16
0.00.050.797 I llm_load_print_meta: n_head_kv        = 16
0.00.050.797 I llm_load_print_meta: n_rot            = 32
0.00.050.797 I llm_load_print_meta: n_swa            = 0
0.00.050.797 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.797 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.798 I llm_load_print_meta: n_gqa            = 1
0.00.050.801 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.801 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.802 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.802 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.802 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.803 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.803 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.804 I llm_load_print_meta: n_ff             = 8192
0.00.050.804 I llm_load_print_meta: n_expert         = 0
0.00.050.804 I llm_load_print_meta: n_expert_used    = 0
0.00.050.804 I llm_load_print_meta: causal attn      = 1
0.00.050.804 I llm_load_print_meta: pooling type     = 0
0.00.050.804 I llm_load_print_meta: rope type        = 2
0.00.050.805 I llm_load_print_meta: rope scaling     = linear
0.00.050.805 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.805 I llm_load_print_meta: freq_scale_train = 1
0.00.050.806 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.806 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.806 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.806 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.806 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.807 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.807 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.807 I llm_load_print_meta: model type       = 1.4B
0.00.050.807 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.808 I llm_load_print_meta: model params     = 1.41 B
0.00.050.808 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.808 I llm_load_print_meta: general.name     = 1.4B
0.00.050.809 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.809 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.809 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.809 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.810 I llm_load_print_meta: LF token         = 128 ''
0.00.050.810 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.810 I llm_load_print_meta: max token length = 1024
0.00.052.951 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.952 I llm_load_tensors: offloading output layer to GPU
0.00.052.952 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.963 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.965 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.881 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.882 I llama_new_context_with_model: n_ctx         = 128
0.00.053.882 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.882 I llama_new_context_with_model: n_batch       = 128
0.00.053.883 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.883 I llama_new_context_with_model: flash_attn    = 0
0.00.053.883 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.883 I llama_new_context_with_model: freq_scale    = 1
0.00.053.884 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.884 I ggml_metal_init: allocating
0.00.053.889 I ggml_metal_init: found device: Apple M4
0.00.053.891 I ggml_metal_init: picking default device: Apple M4
0.00.054.476 I ggml_metal_init: using embedded metal library
0.00.056.982 I ggml_metal_init: GPU name:   Apple M4
0.00.056.984 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.985 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.985 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.985 I ggml_metal_init: simdgroup reduction   = true
0.00.056.985 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.985 I ggml_metal_init: has bfloat            = true
0.00.056.986 I ggml_metal_init: use bfloat            = true
0.00.056.986 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.987 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.193 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.492 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.496 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.513 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.489 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.490 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.490 I llama_new_context_with_model: graph nodes  = 967
0.00.069.490 I llama_new_context_with_model: graph splits = 2
0.00.069.503 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.504 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.735.679 I 
0.00.735.718 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.735.729 I perplexity: tokenizing the input ..
0.00.743.556 I perplexity: tokenization took 7.825 ms
0.00.743.562 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.877.935 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.879.145 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.879.159 I llama_perf_context_print:        load time =     727.13 ms
0.00.879.161 I llama_perf_context_print: prompt eval time =     134.15 ms /   128 tokens (    1.05 ms per token,   954.18 tokens per second)
0.00.879.162 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.879.162 I llama_perf_context_print:       total time =     143.48 ms /   129 tokens
0.00.879.544 I ggml_metal_free: deallocating

real	0m0.894s
user	0m0.080s
sys	0m0.113s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.034 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.064 I main: llama backend init
0.00.000.066 I main: load the model and apply lora adapter, if any
0.00.009.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.587 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.592 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.594 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.594 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.594 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.595 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.595 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.598 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.598 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.599 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.599 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.603 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.603 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.605 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.605 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.606 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.564 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.622 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.512 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.513 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.513 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.514 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.514 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.514 I llama_model_loader: - type  f32:  194 tensors
0.00.024.515 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.515 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.515 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.822 I llm_load_vocab: special tokens cache size = 25
0.00.051.729 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.731 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.732 I llm_load_print_meta: arch             = gptneox
0.00.051.732 I llm_load_print_meta: vocab type       = BPE
0.00.051.732 I llm_load_print_meta: n_vocab          = 50304
0.00.051.732 I llm_load_print_meta: n_merges         = 50009
0.00.051.733 I llm_load_print_meta: vocab_only       = 0
0.00.051.733 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.733 I llm_load_print_meta: n_embd           = 2048
0.00.051.733 I llm_load_print_meta: n_layer          = 24
0.00.051.736 I llm_load_print_meta: n_head           = 16
0.00.051.737 I llm_load_print_meta: n_head_kv        = 16
0.00.051.737 I llm_load_print_meta: n_rot            = 32
0.00.051.737 I llm_load_print_meta: n_swa            = 0
0.00.051.737 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.737 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.738 I llm_load_print_meta: n_gqa            = 1
0.00.051.739 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.740 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.740 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.742 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.742 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.743 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.743 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.744 I llm_load_print_meta: n_ff             = 8192
0.00.051.744 I llm_load_print_meta: n_expert         = 0
0.00.051.745 I llm_load_print_meta: n_expert_used    = 0
0.00.051.745 I llm_load_print_meta: causal attn      = 1
0.00.051.745 I llm_load_print_meta: pooling type     = 0
0.00.051.745 I llm_load_print_meta: rope type        = 2
0.00.051.745 I llm_load_print_meta: rope scaling     = linear
0.00.051.748 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.748 I llm_load_print_meta: freq_scale_train = 1
0.00.051.748 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.748 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.749 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.749 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.749 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.749 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.749 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.749 I llm_load_print_meta: model type       = 1.4B
0.00.051.750 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.051.751 I llm_load_print_meta: model params     = 1.41 B
0.00.051.751 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.051.751 I llm_load_print_meta: general.name     = 1.4B
0.00.051.752 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.752 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.752 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.752 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.753 I llm_load_print_meta: LF token         = 128 ''
0.00.051.753 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.754 I llm_load_print_meta: max token length = 1024
0.00.053.674 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.674 I llm_load_tensors: offloading output layer to GPU
0.00.053.675 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.685 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.053.686 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.054.611 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.612 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.612 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.612 I llama_new_context_with_model: n_batch       = 2048
0.00.054.613 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.613 I llama_new_context_with_model: flash_attn    = 0
0.00.054.613 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.614 I llama_new_context_with_model: freq_scale    = 1
0.00.054.614 I ggml_metal_init: allocating
0.00.054.621 I ggml_metal_init: found device: Apple M4
0.00.054.623 I ggml_metal_init: picking default device: Apple M4
0.00.055.236 I ggml_metal_init: using embedded metal library
0.00.057.635 I ggml_metal_init: GPU name:   Apple M4
0.00.057.636 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.637 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.637 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.637 I ggml_metal_init: simdgroup reduction   = true
0.00.057.637 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.637 I ggml_metal_init: has bfloat            = true
0.00.057.638 I ggml_metal_init: use bfloat            = true
0.00.057.638 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.639 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.640 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.801 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.810 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.840 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.924 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.925 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.926 I llama_new_context_with_model: graph nodes  = 967
0.00.087.926 I llama_new_context_with_model: graph splits = 2
0.00.087.942 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.091 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.472.854 I main: llama threadpool init, n_threads = 4
0.00.472.898 I 
0.00.472.929 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.472.929 I 
0.00.473.177 I sampler seed: 1234
0.00.473.186 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.473.232 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.473.233 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.473.234 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.153.512 I llama_perf_sampler_print:    sampling time =       1.17 ms /    71 runs   (    0.02 ms per token, 60683.76 tokens per second)
0.01.153.512 I llama_perf_context_print:        load time =     463.09 ms
0.01.153.513 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.81 tokens per second)
0.01.153.514 I llama_perf_context_print:        eval time =     641.50 ms /    63 runs   (   10.18 ms per token,    98.21 tokens per second)
0.01.153.514 I llama_perf_context_print:       total time =     680.66 ms /    70 tokens
0.01.153.723 I ggml_metal_free: deallocating

real	0m1.171s
user	0m0.111s
sys	0m0.114s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.727 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.302 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.306 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.308 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.308 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.309 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.309 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.309 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.310 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.310 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.311 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.311 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.311 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.312 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.312 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.314 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.314 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.314 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.303 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.361 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.180 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.181 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.181 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.182 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.182 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.182 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.183 I llama_model_loader: - type  f32:  194 tensors
0.00.024.183 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.183 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.184 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.875 I llm_load_vocab: special tokens cache size = 25
0.00.050.601 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.604 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.604 I llm_load_print_meta: arch             = gptneox
0.00.050.604 I llm_load_print_meta: vocab type       = BPE
0.00.050.605 I llm_load_print_meta: n_vocab          = 50304
0.00.050.605 I llm_load_print_meta: n_merges         = 50009
0.00.050.605 I llm_load_print_meta: vocab_only       = 0
0.00.050.605 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.605 I llm_load_print_meta: n_embd           = 2048
0.00.050.606 I llm_load_print_meta: n_layer          = 24
0.00.050.608 I llm_load_print_meta: n_head           = 16
0.00.050.609 I llm_load_print_meta: n_head_kv        = 16
0.00.050.609 I llm_load_print_meta: n_rot            = 32
0.00.050.609 I llm_load_print_meta: n_swa            = 0
0.00.050.612 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.612 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.613 I llm_load_print_meta: n_gqa            = 1
0.00.050.614 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.614 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.615 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.617 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.617 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.617 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.617 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.618 I llm_load_print_meta: n_ff             = 8192
0.00.050.618 I llm_load_print_meta: n_expert         = 0
0.00.050.618 I llm_load_print_meta: n_expert_used    = 0
0.00.050.618 I llm_load_print_meta: causal attn      = 1
0.00.050.618 I llm_load_print_meta: pooling type     = 0
0.00.050.618 I llm_load_print_meta: rope type        = 2
0.00.050.619 I llm_load_print_meta: rope scaling     = linear
0.00.050.619 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.619 I llm_load_print_meta: freq_scale_train = 1
0.00.050.619 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.620 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.620 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.620 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.620 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.620 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.620 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.620 I llm_load_print_meta: model type       = 1.4B
0.00.050.625 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.626 I llm_load_print_meta: model params     = 1.41 B
0.00.050.626 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.626 I llm_load_print_meta: general.name     = 1.4B
0.00.050.626 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.627 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.632 I llm_load_print_meta: LF token         = 128 ''
0.00.050.632 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.633 I llm_load_print_meta: max token length = 1024
0.00.052.305 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.305 I llm_load_tensors: offloading output layer to GPU
0.00.052.305 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.315 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.316 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.152 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.153 I llama_new_context_with_model: n_ctx         = 128
0.00.053.154 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.154 I llama_new_context_with_model: n_batch       = 128
0.00.053.154 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.154 I llama_new_context_with_model: flash_attn    = 0
0.00.053.154 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.155 I llama_new_context_with_model: freq_scale    = 1
0.00.053.155 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.156 I ggml_metal_init: allocating
0.00.053.159 I ggml_metal_init: found device: Apple M4
0.00.053.161 I ggml_metal_init: picking default device: Apple M4
0.00.053.727 I ggml_metal_init: using embedded metal library
0.00.056.071 I ggml_metal_init: GPU name:   Apple M4
0.00.056.073 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.073 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.073 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.074 I ggml_metal_init: simdgroup reduction   = true
0.00.056.074 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.074 I ggml_metal_init: has bfloat            = true
0.00.056.074 I ggml_metal_init: use bfloat            = true
0.00.056.075 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.075 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.835 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.130 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.135 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.149 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.025 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.026 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.027 I llama_new_context_with_model: graph nodes  = 967
0.00.068.027 I llama_new_context_with_model: graph splits = 2
0.00.068.039 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.040 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.422.868 I 
0.00.422.916 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.422.927 I perplexity: tokenizing the input ..
0.00.430.762 I perplexity: tokenization took 7.833 ms
0.00.430.765 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.563.189 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.564.356 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.564.375 I llama_perf_context_print:        load time =     413.14 ms
0.00.564.376 I llama_perf_context_print: prompt eval time =     132.19 ms /   128 tokens (    1.03 ms per token,   968.29 tokens per second)
0.00.564.379 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.564.380 I llama_perf_context_print:       total time =     141.51 ms /   129 tokens
0.00.564.847 I ggml_metal_free: deallocating

real	0m0.580s
user	0m0.079s
sys	0m0.080s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.066 I main: llama backend init
0.00.000.068 I main: load the model and apply lora adapter, if any
0.00.008.731 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.150 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.155 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.162 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.162 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.162 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.163 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.163 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.164 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.164 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.164 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.165 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.165 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.166 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.167 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.167 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.167 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.109 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.130 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.099 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.100 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.101 I llama_model_loader: - type  f32:  194 tensors
0.00.024.101 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.101 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.101 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.102 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.395 I llm_load_vocab: special tokens cache size = 25
0.00.051.208 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.211 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.212 I llm_load_print_meta: arch             = gptneox
0.00.051.212 I llm_load_print_meta: vocab type       = BPE
0.00.051.212 I llm_load_print_meta: n_vocab          = 50304
0.00.051.212 I llm_load_print_meta: n_merges         = 50009
0.00.051.213 I llm_load_print_meta: vocab_only       = 0
0.00.051.213 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.213 I llm_load_print_meta: n_embd           = 2048
0.00.051.213 I llm_load_print_meta: n_layer          = 24
0.00.051.216 I llm_load_print_meta: n_head           = 16
0.00.051.217 I llm_load_print_meta: n_head_kv        = 16
0.00.051.217 I llm_load_print_meta: n_rot            = 32
0.00.051.217 I llm_load_print_meta: n_swa            = 0
0.00.051.218 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.218 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.218 I llm_load_print_meta: n_gqa            = 1
0.00.051.219 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.220 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.221 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.225 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.227 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.227 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.227 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.228 I llm_load_print_meta: n_ff             = 8192
0.00.051.229 I llm_load_print_meta: n_expert         = 0
0.00.051.229 I llm_load_print_meta: n_expert_used    = 0
0.00.051.229 I llm_load_print_meta: causal attn      = 1
0.00.051.229 I llm_load_print_meta: pooling type     = 0
0.00.051.229 I llm_load_print_meta: rope type        = 2
0.00.051.229 I llm_load_print_meta: rope scaling     = linear
0.00.051.230 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.233 I llm_load_print_meta: freq_scale_train = 1
0.00.051.233 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.233 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.234 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.234 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.234 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.235 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.236 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.236 I llm_load_print_meta: model type       = 1.4B
0.00.051.236 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.051.237 I llm_load_print_meta: model params     = 1.41 B
0.00.051.237 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.051.237 I llm_load_print_meta: general.name     = 1.4B
0.00.051.238 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.238 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.238 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.238 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.238 I llm_load_print_meta: LF token         = 128 ''
0.00.051.238 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.239 I llm_load_print_meta: max token length = 1024
0.00.053.212 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.213 I llm_load_tensors: offloading output layer to GPU
0.00.053.213 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.224 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.053.225 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.054.148 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.149 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.149 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.149 I llama_new_context_with_model: n_batch       = 2048
0.00.054.149 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.149 I llama_new_context_with_model: flash_attn    = 0
0.00.054.150 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.150 I llama_new_context_with_model: freq_scale    = 1
0.00.054.151 I ggml_metal_init: allocating
0.00.054.158 I ggml_metal_init: found device: Apple M4
0.00.054.160 I ggml_metal_init: picking default device: Apple M4
0.00.054.717 I ggml_metal_init: using embedded metal library
0.00.057.089 I ggml_metal_init: GPU name:   Apple M4
0.00.057.091 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.091 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.092 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.092 I ggml_metal_init: simdgroup reduction   = true
0.00.057.092 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.092 I ggml_metal_init: has bfloat            = true
0.00.057.092 I ggml_metal_init: use bfloat            = true
0.00.057.093 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.093 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.832 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.086.803 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.808 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.828 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.817 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.818 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.819 I llama_new_context_with_model: graph nodes  = 967
0.00.087.819 I llama_new_context_with_model: graph splits = 2
0.00.087.833 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.963 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.964 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.545.529 I main: llama threadpool init, n_threads = 4
0.00.545.569 I 
0.00.545.600 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.545.600 I 
0.00.545.833 I sampler seed: 1234
0.00.545.838 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.545.880 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.545.898 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.545.898 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.292.046 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59663.87 tokens per second)
0.01.292.046 I llama_perf_context_print:        load time =     536.79 ms
0.01.292.047 I llama_perf_context_print: prompt eval time =      44.37 ms /     7 tokens (    6.34 ms per token,   157.76 tokens per second)
0.01.292.048 I llama_perf_context_print:        eval time =     698.90 ms /    63 runs   (   11.09 ms per token,    90.14 tokens per second)
0.01.292.048 I llama_perf_context_print:       total time =     746.52 ms /    70 tokens
0.01.292.232 I ggml_metal_free: deallocating

real	0m1.309s
user	0m0.111s
sys	0m0.128s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.758 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.683 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.014.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.690 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.691 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.691 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.692 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.692 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.693 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.693 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.694 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.694 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.694 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.695 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.695 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.697 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.697 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.698 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.646 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.761 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.635 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.636 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.637 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.637 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.637 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.638 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.023.638 I llama_model_loader: - type  f32:  194 tensors
0.00.023.638 I llama_model_loader: - type q3_K:   25 tensors
0.00.023.639 I llama_model_loader: - type q4_K:   71 tensors
0.00.023.639 I llama_model_loader: - type q5_K:    1 tensors
0.00.023.639 I llama_model_loader: - type q6_K:    1 tensors
0.00.045.042 I llm_load_vocab: special tokens cache size = 25
0.00.050.879 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.881 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.882 I llm_load_print_meta: arch             = gptneox
0.00.050.882 I llm_load_print_meta: vocab type       = BPE
0.00.050.882 I llm_load_print_meta: n_vocab          = 50304
0.00.050.882 I llm_load_print_meta: n_merges         = 50009
0.00.050.883 I llm_load_print_meta: vocab_only       = 0
0.00.050.883 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.883 I llm_load_print_meta: n_embd           = 2048
0.00.050.883 I llm_load_print_meta: n_layer          = 24
0.00.050.886 I llm_load_print_meta: n_head           = 16
0.00.050.887 I llm_load_print_meta: n_head_kv        = 16
0.00.050.887 I llm_load_print_meta: n_rot            = 32
0.00.050.887 I llm_load_print_meta: n_swa            = 0
0.00.050.887 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.888 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.888 I llm_load_print_meta: n_gqa            = 1
0.00.050.889 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.890 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.890 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.891 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.891 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.892 I llm_load_print_meta: n_ff             = 8192
0.00.050.892 I llm_load_print_meta: n_expert         = 0
0.00.050.892 I llm_load_print_meta: n_expert_used    = 0
0.00.050.892 I llm_load_print_meta: causal attn      = 1
0.00.050.893 I llm_load_print_meta: pooling type     = 0
0.00.050.893 I llm_load_print_meta: rope type        = 2
0.00.050.893 I llm_load_print_meta: rope scaling     = linear
0.00.050.893 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.894 I llm_load_print_meta: freq_scale_train = 1
0.00.050.894 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.897 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.897 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.897 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.897 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.897 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.897 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.898 I llm_load_print_meta: model type       = 1.4B
0.00.050.898 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.898 I llm_load_print_meta: model params     = 1.41 B
0.00.050.899 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.899 I llm_load_print_meta: general.name     = 1.4B
0.00.050.903 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.904 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.904 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.904 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.904 I llm_load_print_meta: LF token         = 128 ''
0.00.050.904 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.905 I llm_load_print_meta: max token length = 1024
0.00.052.907 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.908 I llm_load_tensors: offloading output layer to GPU
0.00.052.908 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.919 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.920 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.832 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.833 I llama_new_context_with_model: n_ctx         = 128
0.00.053.833 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.833 I llama_new_context_with_model: n_batch       = 128
0.00.053.833 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.833 I llama_new_context_with_model: flash_attn    = 0
0.00.053.834 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.834 I llama_new_context_with_model: freq_scale    = 1
0.00.053.834 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.835 I ggml_metal_init: allocating
0.00.053.839 I ggml_metal_init: found device: Apple M4
0.00.053.841 I ggml_metal_init: picking default device: Apple M4
0.00.054.419 I ggml_metal_init: using embedded metal library
0.00.056.866 I ggml_metal_init: GPU name:   Apple M4
0.00.056.868 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.868 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.869 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.869 I ggml_metal_init: simdgroup reduction   = true
0.00.056.869 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.869 I ggml_metal_init: has bfloat            = true
0.00.056.869 I ggml_metal_init: use bfloat            = true
0.00.056.870 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.871 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.876 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.156 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.159 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.175 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.083 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.084 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.085 I llama_new_context_with_model: graph nodes  = 967
0.00.069.085 I llama_new_context_with_model: graph splits = 2
0.00.069.098 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.098 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.289 I 
0.00.474.342 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.354 I perplexity: tokenizing the input ..
0.00.482.074 I perplexity: tokenization took 7.719 ms
0.00.482.077 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.762 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.615.006 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.615.024 I llama_perf_context_print:        load time =     465.52 ms
0.00.615.025 I llama_perf_context_print: prompt eval time =     131.46 ms /   128 tokens (    1.03 ms per token,   973.69 tokens per second)
0.00.615.026 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.615.027 I llama_perf_context_print:       total time =     140.74 ms /   129 tokens
0.00.615.490 I ggml_metal_free: deallocating

real	0m0.630s
user	0m0.080s
sys	0m0.081s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.038 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.068 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.010.465 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.897 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.902 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.907 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.908 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.908 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.909 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.910 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.910 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.910 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.911 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.911 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.911 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.913 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.914 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.916 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.917 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.951 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.066 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.015 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.016 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.016 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.017 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.017 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.017 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.026.018 I llama_model_loader: - type  f32:  194 tensors
0.00.026.018 I llama_model_loader: - type q4_K:   61 tensors
0.00.026.018 I llama_model_loader: - type q5_K:   24 tensors
0.00.026.019 I llama_model_loader: - type q6_K:   13 tensors
0.00.047.499 I llm_load_vocab: special tokens cache size = 25
0.00.053.322 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.053.324 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.053.325 I llm_load_print_meta: arch             = gptneox
0.00.053.325 I llm_load_print_meta: vocab type       = BPE
0.00.053.325 I llm_load_print_meta: n_vocab          = 50304
0.00.053.326 I llm_load_print_meta: n_merges         = 50009
0.00.053.326 I llm_load_print_meta: vocab_only       = 0
0.00.053.326 I llm_load_print_meta: n_ctx_train      = 2048
0.00.053.326 I llm_load_print_meta: n_embd           = 2048
0.00.053.326 I llm_load_print_meta: n_layer          = 24
0.00.053.329 I llm_load_print_meta: n_head           = 16
0.00.053.332 I llm_load_print_meta: n_head_kv        = 16
0.00.053.332 I llm_load_print_meta: n_rot            = 32
0.00.053.332 I llm_load_print_meta: n_swa            = 0
0.00.053.332 I llm_load_print_meta: n_embd_head_k    = 128
0.00.053.332 I llm_load_print_meta: n_embd_head_v    = 128
0.00.053.333 I llm_load_print_meta: n_gqa            = 1
0.00.053.338 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.053.339 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.053.339 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.053.340 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.053.340 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.053.340 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.053.340 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.053.341 I llm_load_print_meta: n_ff             = 8192
0.00.053.341 I llm_load_print_meta: n_expert         = 0
0.00.053.341 I llm_load_print_meta: n_expert_used    = 0
0.00.053.341 I llm_load_print_meta: causal attn      = 1
0.00.053.342 I llm_load_print_meta: pooling type     = 0
0.00.053.342 I llm_load_print_meta: rope type        = 2
0.00.053.342 I llm_load_print_meta: rope scaling     = linear
0.00.053.342 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.053.343 I llm_load_print_meta: freq_scale_train = 1
0.00.053.343 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.053.343 I llm_load_print_meta: rope_finetuned   = unknown
0.00.053.343 I llm_load_print_meta: ssm_d_conv       = 0
0.00.053.343 I llm_load_print_meta: ssm_d_inner      = 0
0.00.053.343 I llm_load_print_meta: ssm_d_state      = 0
0.00.053.344 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.053.344 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.053.344 I llm_load_print_meta: model type       = 1.4B
0.00.053.345 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.053.345 I llm_load_print_meta: model params     = 1.41 B
0.00.053.347 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.053.347 I llm_load_print_meta: general.name     = 1.4B
0.00.053.347 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.053.347 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.053.347 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.053.348 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.053.348 I llm_load_print_meta: LF token         = 128 ''
0.00.053.348 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.053.348 I llm_load_print_meta: max token length = 1024
0.00.055.431 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.055.431 I llm_load_tensors: offloading output layer to GPU
0.00.055.432 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.055.442 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.055.443 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.056.378 I llama_new_context_with_model: n_seq_max     = 1
0.00.056.379 I llama_new_context_with_model: n_ctx         = 2048
0.00.056.379 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.056.380 I llama_new_context_with_model: n_batch       = 2048
0.00.056.380 I llama_new_context_with_model: n_ubatch      = 512
0.00.056.380 I llama_new_context_with_model: flash_attn    = 0
0.00.056.381 I llama_new_context_with_model: freq_base     = 10000.0
0.00.056.381 I llama_new_context_with_model: freq_scale    = 1
0.00.056.382 I ggml_metal_init: allocating
0.00.056.389 I ggml_metal_init: found device: Apple M4
0.00.056.391 I ggml_metal_init: picking default device: Apple M4
0.00.056.985 I ggml_metal_init: using embedded metal library
0.00.059.331 I ggml_metal_init: GPU name:   Apple M4
0.00.059.333 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.059.333 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.059.334 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.059.334 I ggml_metal_init: simdgroup reduction   = true
0.00.059.334 I ggml_metal_init: simdgroup matrix mul. = true
0.00.059.334 I ggml_metal_init: has bfloat            = true
0.00.059.334 I ggml_metal_init: use bfloat            = true
0.00.059.335 I ggml_metal_init: hasUnifiedMemory      = true
0.00.059.335 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.069.228 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.561 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.089.567 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.089.583 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.090.576 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.090.577 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.090.578 I llama_new_context_with_model: graph nodes  = 967
0.00.090.578 I llama_new_context_with_model: graph splits = 2
0.00.090.593 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.090.734 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.090.735 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.769.179 I main: llama threadpool init, n_threads = 4
0.00.769.264 I 
0.00.769.340 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.769.342 I 
0.00.769.841 I sampler seed: 1234
0.00.769.847 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.769.890 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.769.893 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.769.893 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.547.573 I llama_perf_sampler_print:    sampling time =       1.40 ms /    71 runs   (    0.02 ms per token, 50896.06 tokens per second)
0.01.547.575 I llama_perf_context_print:        load time =     758.71 ms
0.01.547.575 I llama_perf_context_print: prompt eval time =      59.16 ms /     7 tokens (    8.45 ms per token,   118.33 tokens per second)
0.01.547.576 I llama_perf_context_print:        eval time =     715.26 ms /    63 runs   (   11.35 ms per token,    88.08 tokens per second)
0.01.547.577 I llama_perf_context_print:       total time =     778.40 ms /    70 tokens
0.01.547.773 I ggml_metal_free: deallocating

real	0m1.566s
user	0m0.124s
sys	0m0.173s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.085 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.282 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.183 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.188 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.189 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.190 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.190 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.190 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.191 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.192 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.192 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.192 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.193 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.194 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.196 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.196 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.200 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.200 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.201 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.124 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.233 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.206 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.207 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.207 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.208 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.208 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.024.209 I llama_model_loader: - type  f32:  194 tensors
0.00.024.209 I llama_model_loader: - type q4_K:   61 tensors
0.00.024.210 I llama_model_loader: - type q5_K:   24 tensors
0.00.024.210 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.915 I llm_load_vocab: special tokens cache size = 25
0.00.050.704 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.706 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.707 I llm_load_print_meta: arch             = gptneox
0.00.050.707 I llm_load_print_meta: vocab type       = BPE
0.00.050.707 I llm_load_print_meta: n_vocab          = 50304
0.00.050.707 I llm_load_print_meta: n_merges         = 50009
0.00.050.708 I llm_load_print_meta: vocab_only       = 0
0.00.050.708 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.708 I llm_load_print_meta: n_embd           = 2048
0.00.050.708 I llm_load_print_meta: n_layer          = 24
0.00.050.711 I llm_load_print_meta: n_head           = 16
0.00.050.712 I llm_load_print_meta: n_head_kv        = 16
0.00.050.712 I llm_load_print_meta: n_rot            = 32
0.00.050.712 I llm_load_print_meta: n_swa            = 0
0.00.050.712 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.712 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.713 I llm_load_print_meta: n_gqa            = 1
0.00.050.714 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.715 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.715 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.715 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.716 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.716 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.716 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.717 I llm_load_print_meta: n_ff             = 8192
0.00.050.717 I llm_load_print_meta: n_expert         = 0
0.00.050.717 I llm_load_print_meta: n_expert_used    = 0
0.00.050.717 I llm_load_print_meta: causal attn      = 1
0.00.050.717 I llm_load_print_meta: pooling type     = 0
0.00.050.717 I llm_load_print_meta: rope type        = 2
0.00.050.719 I llm_load_print_meta: rope scaling     = linear
0.00.050.721 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.721 I llm_load_print_meta: freq_scale_train = 1
0.00.050.722 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.722 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.722 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.722 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.722 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.722 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.723 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.723 I llm_load_print_meta: model type       = 1.4B
0.00.050.723 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.724 I llm_load_print_meta: model params     = 1.41 B
0.00.050.729 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.729 I llm_load_print_meta: general.name     = 1.4B
0.00.050.730 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.730 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.730 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.730 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.731 I llm_load_print_meta: LF token         = 128 ''
0.00.050.731 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.731 I llm_load_print_meta: max token length = 1024
0.00.052.788 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.788 I llm_load_tensors: offloading output layer to GPU
0.00.052.789 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.800 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.801 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.755 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.756 I llama_new_context_with_model: n_ctx         = 128
0.00.053.756 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.756 I llama_new_context_with_model: n_batch       = 128
0.00.053.756 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.756 I llama_new_context_with_model: flash_attn    = 0
0.00.053.757 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.757 I llama_new_context_with_model: freq_scale    = 1
0.00.053.757 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.758 I ggml_metal_init: allocating
0.00.053.765 I ggml_metal_init: found device: Apple M4
0.00.053.767 I ggml_metal_init: picking default device: Apple M4
0.00.054.339 I ggml_metal_init: using embedded metal library
0.00.056.686 I ggml_metal_init: GPU name:   Apple M4
0.00.056.687 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.687 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.688 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.688 I ggml_metal_init: simdgroup reduction   = true
0.00.056.688 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.688 I ggml_metal_init: has bfloat            = true
0.00.056.690 I ggml_metal_init: use bfloat            = true
0.00.056.690 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.691 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.507 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.831 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.833 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.847 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.740 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.741 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.741 I llama_new_context_with_model: graph nodes  = 967
0.00.068.742 I llama_new_context_with_model: graph splits = 2
0.00.068.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.557.757 I 
0.00.557.801 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.557.814 I perplexity: tokenizing the input ..
0.00.565.499 I perplexity: tokenization took 7.683 ms
0.00.565.503 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.699.758 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.700.935 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.700.960 I llama_perf_context_print:        load time =     548.47 ms
0.00.700.961 I llama_perf_context_print: prompt eval time =     134.03 ms /   128 tokens (    1.05 ms per token,   955.02 tokens per second)
0.00.700.962 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.700.962 I llama_perf_context_print:       total time =     143.21 ms /   129 tokens
0.00.701.318 I ggml_metal_free: deallocating

real	0m0.716s
user	0m0.079s
sys	0m0.100s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.035 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.070 I main: llama backend init
0.00.000.072 I main: load the model and apply lora adapter, if any
0.00.009.086 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.002 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.007 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.008 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.009 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.009 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.009 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.010 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.011 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.011 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.012 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.012 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.012 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.013 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.014 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.015 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.017 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.920 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.002 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.897 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.898 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.898 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.899 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.899 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.900 I llama_model_loader: - type  f32:  194 tensors
0.00.024.900 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.900 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.714 I llm_load_vocab: special tokens cache size = 25
0.00.051.517 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.520 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.520 I llm_load_print_meta: arch             = gptneox
0.00.051.520 I llm_load_print_meta: vocab type       = BPE
0.00.051.521 I llm_load_print_meta: n_vocab          = 50304
0.00.051.521 I llm_load_print_meta: n_merges         = 50009
0.00.051.521 I llm_load_print_meta: vocab_only       = 0
0.00.051.521 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.521 I llm_load_print_meta: n_embd           = 2048
0.00.051.522 I llm_load_print_meta: n_layer          = 24
0.00.051.524 I llm_load_print_meta: n_head           = 16
0.00.051.525 I llm_load_print_meta: n_head_kv        = 16
0.00.051.525 I llm_load_print_meta: n_rot            = 32
0.00.051.526 I llm_load_print_meta: n_swa            = 0
0.00.051.526 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.526 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.527 I llm_load_print_meta: n_gqa            = 1
0.00.051.528 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.529 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.529 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.529 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.530 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.531 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.531 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.533 I llm_load_print_meta: n_ff             = 8192
0.00.051.534 I llm_load_print_meta: n_expert         = 0
0.00.051.534 I llm_load_print_meta: n_expert_used    = 0
0.00.051.534 I llm_load_print_meta: causal attn      = 1
0.00.051.534 I llm_load_print_meta: pooling type     = 0
0.00.051.534 I llm_load_print_meta: rope type        = 2
0.00.051.534 I llm_load_print_meta: rope scaling     = linear
0.00.051.535 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.535 I llm_load_print_meta: freq_scale_train = 1
0.00.051.535 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.536 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.536 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.536 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.536 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.536 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.536 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.537 I llm_load_print_meta: model type       = 1.4B
0.00.051.537 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.537 I llm_load_print_meta: model params     = 1.41 B
0.00.051.539 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.539 I llm_load_print_meta: general.name     = 1.4B
0.00.051.540 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.540 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.540 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.540 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.540 I llm_load_print_meta: LF token         = 128 ''
0.00.051.541 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.541 I llm_load_print_meta: max token length = 1024
0.00.053.628 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.628 I llm_load_tensors: offloading output layer to GPU
0.00.053.629 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.639 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.640 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.566 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.568 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.568 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.568 I llama_new_context_with_model: n_batch       = 2048
0.00.054.568 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.568 I llama_new_context_with_model: flash_attn    = 0
0.00.054.569 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.569 I llama_new_context_with_model: freq_scale    = 1
0.00.054.570 I ggml_metal_init: allocating
0.00.054.578 I ggml_metal_init: found device: Apple M4
0.00.054.581 I ggml_metal_init: picking default device: Apple M4
0.00.055.201 I ggml_metal_init: using embedded metal library
0.00.057.612 I ggml_metal_init: GPU name:   Apple M4
0.00.057.614 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.614 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.614 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.615 I ggml_metal_init: simdgroup reduction   = true
0.00.057.615 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.615 I ggml_metal_init: has bfloat            = true
0.00.057.615 I ggml_metal_init: use bfloat            = true
0.00.057.615 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.622 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.687 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.087.770 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.087.778 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.087.797 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.088.767 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.088.768 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.088.769 I llama_new_context_with_model: graph nodes  = 967
0.00.088.769 I llama_new_context_with_model: graph splits = 2
0.00.088.784 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.925 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.926 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.697.809 I main: llama threadpool init, n_threads = 4
0.00.697.854 I 
0.00.697.887 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.697.889 I 
0.00.698.137 I sampler seed: 1234
0.00.698.141 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.698.176 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.698.179 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.698.180 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.546.188 I llama_perf_sampler_print:    sampling time =       1.47 ms /    71 runs   (    0.02 ms per token, 48431.11 tokens per second)
0.01.546.189 I llama_perf_context_print:        load time =     688.71 ms
0.01.546.189 I llama_perf_context_print: prompt eval time =      51.34 ms /     7 tokens (    7.33 ms per token,   136.34 tokens per second)
0.01.546.190 I llama_perf_context_print:        eval time =     794.16 ms /    63 runs   (   12.61 ms per token,    79.33 tokens per second)
0.01.546.190 I llama_perf_context_print:       total time =     848.38 ms /    70 tokens
0.01.546.396 I ggml_metal_free: deallocating

real	0m1.563s
user	0m0.111s
sys	0m0.156s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.806 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.684 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.014.689 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.695 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.695 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.697 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.697 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.698 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.699 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.699 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.699 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.700 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.700 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.703 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.703 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.705 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.705 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.705 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.614 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.686 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.699 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.701 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.701 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.701 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.702 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.702 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.023.702 I llama_model_loader: - type  f32:  194 tensors
0.00.023.703 I llama_model_loader: - type q5_K:   61 tensors
0.00.023.703 I llama_model_loader: - type q6_K:   37 tensors
0.00.044.559 I llm_load_vocab: special tokens cache size = 25
0.00.050.454 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.456 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.457 I llm_load_print_meta: arch             = gptneox
0.00.050.457 I llm_load_print_meta: vocab type       = BPE
0.00.050.457 I llm_load_print_meta: n_vocab          = 50304
0.00.050.457 I llm_load_print_meta: n_merges         = 50009
0.00.050.457 I llm_load_print_meta: vocab_only       = 0
0.00.050.458 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.458 I llm_load_print_meta: n_embd           = 2048
0.00.050.458 I llm_load_print_meta: n_layer          = 24
0.00.050.461 I llm_load_print_meta: n_head           = 16
0.00.050.462 I llm_load_print_meta: n_head_kv        = 16
0.00.050.462 I llm_load_print_meta: n_rot            = 32
0.00.050.462 I llm_load_print_meta: n_swa            = 0
0.00.050.462 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.462 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.463 I llm_load_print_meta: n_gqa            = 1
0.00.050.464 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.465 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.466 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.466 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.467 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.467 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.467 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.467 I llm_load_print_meta: n_ff             = 8192
0.00.050.469 I llm_load_print_meta: n_expert         = 0
0.00.050.469 I llm_load_print_meta: n_expert_used    = 0
0.00.050.469 I llm_load_print_meta: causal attn      = 1
0.00.050.469 I llm_load_print_meta: pooling type     = 0
0.00.050.470 I llm_load_print_meta: rope type        = 2
0.00.050.470 I llm_load_print_meta: rope scaling     = linear
0.00.050.470 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.471 I llm_load_print_meta: freq_scale_train = 1
0.00.050.471 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.471 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.471 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.471 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.472 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.472 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.472 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.472 I llm_load_print_meta: model type       = 1.4B
0.00.050.472 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.050.473 I llm_load_print_meta: model params     = 1.41 B
0.00.050.473 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.050.473 I llm_load_print_meta: general.name     = 1.4B
0.00.050.474 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.474 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.474 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.474 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.475 I llm_load_print_meta: LF token         = 128 ''
0.00.050.475 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.475 I llm_load_print_meta: max token length = 1024
0.00.052.521 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.522 I llm_load_tensors: offloading output layer to GPU
0.00.052.522 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.532 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.052.533 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.053.429 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.430 I llama_new_context_with_model: n_ctx         = 128
0.00.053.430 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.430 I llama_new_context_with_model: n_batch       = 128
0.00.053.431 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.431 I llama_new_context_with_model: flash_attn    = 0
0.00.053.431 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.431 I llama_new_context_with_model: freq_scale    = 1
0.00.053.432 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.432 I ggml_metal_init: allocating
0.00.053.438 I ggml_metal_init: found device: Apple M4
0.00.053.440 I ggml_metal_init: picking default device: Apple M4
0.00.053.990 I ggml_metal_init: using embedded metal library
0.00.056.366 I ggml_metal_init: GPU name:   Apple M4
0.00.056.367 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.368 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.368 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.368 I ggml_metal_init: simdgroup reduction   = true
0.00.056.369 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.369 I ggml_metal_init: has bfloat            = true
0.00.056.369 I ggml_metal_init: use bfloat            = true
0.00.056.369 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.370 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.201 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.067.468 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.472 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.496 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.410 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.411 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.412 I llama_new_context_with_model: graph nodes  = 967
0.00.068.412 I llama_new_context_with_model: graph splits = 2
0.00.068.424 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.425 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.646.677 I 
0.00.646.726 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.646.737 I perplexity: tokenizing the input ..
0.00.654.472 I perplexity: tokenization took 7.734 ms
0.00.654.476 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.794.962 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.136 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.155 I llama_perf_context_print:        load time =     637.87 ms
0.00.796.156 I llama_perf_context_print: prompt eval time =     140.26 ms /   128 tokens (    1.10 ms per token,   912.58 tokens per second)
0.00.796.157 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.158 I llama_perf_context_print:       total time =     149.48 ms /   129 tokens
0.00.796.591 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.079s
sys	0m0.116s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.037 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.067 I main: llama backend init
0.00.000.070 I main: load the model and apply lora adapter, if any
0.00.009.876 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.676 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.016.680 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.682 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.682 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.683 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.684 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.685 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.686 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.686 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.687 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.687 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.687 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.687 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.690 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.691 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.691 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.708 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.788 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.683 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.684 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.684 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.685 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.685 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.685 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.025.686 I llama_model_loader: - type  f32:  194 tensors
0.00.025.686 I llama_model_loader: - type q6_K:   98 tensors
0.00.046.569 I llm_load_vocab: special tokens cache size = 25
0.00.052.586 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.588 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.589 I llm_load_print_meta: arch             = gptneox
0.00.052.589 I llm_load_print_meta: vocab type       = BPE
0.00.052.589 I llm_load_print_meta: n_vocab          = 50304
0.00.052.590 I llm_load_print_meta: n_merges         = 50009
0.00.052.590 I llm_load_print_meta: vocab_only       = 0
0.00.052.590 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.590 I llm_load_print_meta: n_embd           = 2048
0.00.052.590 I llm_load_print_meta: n_layer          = 24
0.00.052.593 I llm_load_print_meta: n_head           = 16
0.00.052.596 I llm_load_print_meta: n_head_kv        = 16
0.00.052.596 I llm_load_print_meta: n_rot            = 32
0.00.052.596 I llm_load_print_meta: n_swa            = 0
0.00.052.596 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.597 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.597 I llm_load_print_meta: n_gqa            = 1
0.00.052.598 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.599 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.599 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.600 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.600 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.600 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.600 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.601 I llm_load_print_meta: n_ff             = 8192
0.00.052.601 I llm_load_print_meta: n_expert         = 0
0.00.052.601 I llm_load_print_meta: n_expert_used    = 0
0.00.052.601 I llm_load_print_meta: causal attn      = 1
0.00.052.608 I llm_load_print_meta: pooling type     = 0
0.00.052.611 I llm_load_print_meta: rope type        = 2
0.00.052.611 I llm_load_print_meta: rope scaling     = linear
0.00.052.612 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.612 I llm_load_print_meta: freq_scale_train = 1
0.00.052.612 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.612 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.613 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.613 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.613 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.613 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.613 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.613 I llm_load_print_meta: model type       = 1.4B
0.00.052.614 I llm_load_print_meta: model ftype      = Q6_K
0.00.052.614 I llm_load_print_meta: model params     = 1.41 B
0.00.052.615 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.052.615 I llm_load_print_meta: general.name     = 1.4B
0.00.052.620 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.621 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.621 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.621 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.622 I llm_load_print_meta: LF token         = 128 ''
0.00.052.622 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.624 I llm_load_print_meta: max token length = 1024
0.00.054.693 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.693 I llm_load_tensors: offloading output layer to GPU
0.00.054.694 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.704 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.054.705 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.055.609 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.610 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.610 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.610 I llama_new_context_with_model: n_batch       = 2048
0.00.055.610 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.610 I llama_new_context_with_model: flash_attn    = 0
0.00.055.611 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.611 I llama_new_context_with_model: freq_scale    = 1
0.00.055.612 I ggml_metal_init: allocating
0.00.055.615 I ggml_metal_init: found device: Apple M4
0.00.055.617 I ggml_metal_init: picking default device: Apple M4
0.00.056.224 I ggml_metal_init: using embedded metal library
0.00.058.625 I ggml_metal_init: GPU name:   Apple M4
0.00.058.626 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.626 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.627 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.627 I ggml_metal_init: simdgroup reduction   = true
0.00.058.629 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.629 I ggml_metal_init: has bfloat            = true
0.00.058.629 I ggml_metal_init: use bfloat            = true
0.00.058.629 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.630 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.681 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.089.999 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.090.007 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.090.032 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.091.082 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.091.083 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.091.084 I llama_new_context_with_model: graph nodes  = 967
0.00.091.084 I llama_new_context_with_model: graph splits = 2
0.00.091.100 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.091.248 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.091.249 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.739.261 I main: llama threadpool init, n_threads = 4
0.00.739.298 I 
0.00.739.329 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.739.329 I 
0.00.739.547 I sampler seed: 1234
0.00.739.553 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.739.567 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.739.568 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.739.568 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.622.554 I llama_perf_sampler_print:    sampling time =       1.20 ms /    71 runs   (    0.02 ms per token, 59414.23 tokens per second)
0.01.622.555 I llama_perf_context_print:        load time =     729.38 ms
0.01.622.556 I llama_perf_context_print: prompt eval time =      54.44 ms /     7 tokens (    7.78 ms per token,   128.58 tokens per second)
0.01.622.557 I llama_perf_context_print:        eval time =     825.53 ms /    63 runs   (   13.10 ms per token,    76.31 tokens per second)
0.01.622.557 I llama_perf_context_print:       total time =     883.30 ms /    70 tokens
0.01.622.774 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.111s
sys	0m0.154s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4366 (5cab3e4a) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.860 I llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.585 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.589 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.590 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.591 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.591 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.592 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.592 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.593 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.593 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.594 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.596 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.597 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.598 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.598 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.508 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.591 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.522 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.524 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.524 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.525 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.525 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.526 I llama_model_loader: - type  f32:  194 tensors
0.00.024.526 I llama_model_loader: - type q6_K:   98 tensors
0.00.045.183 I llm_load_vocab: special tokens cache size = 25
0.00.051.021 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.024 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.024 I llm_load_print_meta: arch             = gptneox
0.00.051.024 I llm_load_print_meta: vocab type       = BPE
0.00.051.024 I llm_load_print_meta: n_vocab          = 50304
0.00.051.025 I llm_load_print_meta: n_merges         = 50009
0.00.051.025 I llm_load_print_meta: vocab_only       = 0
0.00.051.025 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.025 I llm_load_print_meta: n_embd           = 2048
0.00.051.025 I llm_load_print_meta: n_layer          = 24
0.00.051.029 I llm_load_print_meta: n_head           = 16
0.00.051.029 I llm_load_print_meta: n_head_kv        = 16
0.00.051.030 I llm_load_print_meta: n_rot            = 32
0.00.051.030 I llm_load_print_meta: n_swa            = 0
0.00.051.030 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.030 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.031 I llm_load_print_meta: n_gqa            = 1
0.00.051.032 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.033 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.033 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.034 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.035 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.035 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.036 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.036 I llm_load_print_meta: n_ff             = 8192
0.00.051.036 I llm_load_print_meta: n_expert         = 0
0.00.051.037 I llm_load_print_meta: n_expert_used    = 0
0.00.051.037 I llm_load_print_meta: causal attn      = 1
0.00.051.037 I llm_load_print_meta: pooling type     = 0
0.00.051.037 I llm_load_print_meta: rope type        = 2
0.00.051.037 I llm_load_print_meta: rope scaling     = linear
0.00.051.038 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.040 I llm_load_print_meta: freq_scale_train = 1
0.00.051.040 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.040 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.040 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.041 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.041 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.041 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.041 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.041 I llm_load_print_meta: model type       = 1.4B
0.00.051.042 I llm_load_print_meta: model ftype      = Q6_K
0.00.051.042 I llm_load_print_meta: model params     = 1.41 B
0.00.051.046 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.051.047 I llm_load_print_meta: general.name     = 1.4B
0.00.051.047 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.047 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.049 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.049 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.049 I llm_load_print_meta: LF token         = 128 ''
0.00.051.049 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.050 I llm_load_print_meta: max token length = 1024
0.00.053.086 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.087 I llm_load_tensors: offloading output layer to GPU
0.00.053.087 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.098 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.053.099 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.054.011 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.012 I llama_new_context_with_model: n_ctx         = 128
0.00.054.012 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.012 I llama_new_context_with_model: n_batch       = 128
0.00.054.012 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.013 I llama_new_context_with_model: flash_attn    = 0
0.00.054.013 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.013 I llama_new_context_with_model: freq_scale    = 1
0.00.054.014 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.014 I ggml_metal_init: allocating
0.00.054.023 I ggml_metal_init: found device: Apple M4
0.00.054.025 I ggml_metal_init: picking default device: Apple M4
0.00.054.629 I ggml_metal_init: using embedded metal library
0.00.056.959 I ggml_metal_init: GPU name:   Apple M4
0.00.056.961 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.961 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.961 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.962 I ggml_metal_init: simdgroup reduction   = true
0.00.056.962 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.962 I ggml_metal_init: has bfloat            = true
0.00.056.962 I ggml_metal_init: use bfloat            = true
0.00.056.962 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.963 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.792 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
0.00.068.023 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.026 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.038 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.887 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.888 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.888 I llama_new_context_with_model: graph nodes  = 967
0.00.068.889 I llama_new_context_with_model: graph splits = 2
0.00.068.901 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.902 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.322.012 I 
0.00.322.050 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.322.060 I perplexity: tokenizing the input ..
0.00.329.857 I perplexity: tokenization took 7.796 ms
0.00.329.862 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.470.085 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.471.309 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.471.319 I llama_perf_context_print:        load time =     312.15 ms
0.00.471.320 I llama_perf_context_print: prompt eval time =     140.00 ms /   128 tokens (    1.09 ms per token,   914.31 tokens per second)
0.00.471.321 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.471.321 I llama_perf_context_print:       total time =     149.31 ms /   129 tokens
0.00.471.653 I ggml_metal_free: deallocating

real	0m0.486s
user	0m0.078s
sys	0m0.065s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4366 (5cab3e4a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x13560a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x13560a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x13560af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x13560b520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x13560bad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x13560c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x13560c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x13560cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x13560d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x13560d690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x13560db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x13560e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x13560ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x13560f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x13560fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x135610290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x1356109b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x1356110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x1356117f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x135611fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x1356126e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x135612e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x135613520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x135613dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x1356144e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x1356147a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x135614db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x135615a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x135615f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x135616220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x1356166c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x135616980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x135617210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x135617750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x135617a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x135617eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x135618350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x1356187f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x135618c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x135619130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x1356195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x135619a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x135619f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x13561a3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x13561a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x13561ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x13561b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x13561bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x13561c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x13561c7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x13561cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x13561d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x13561da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x13561e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x13561e800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x13561eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x13561f140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x13561f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x13561fa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x135620200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1356204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x135620960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x135620e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x1356212a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x135621740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x135621be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x135622080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x135622520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1356229c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x135622e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x135623300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x1356237a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x135623c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x135624190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x1356246e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x135624c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x135625180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1356256d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x135625c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x135626170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1356266c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x135626c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x135627160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x1356276b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x135627c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x135628150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x1356286a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x135628bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x135629140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x135629690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x135629be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x13562a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x13562a680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x13562abd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x13562b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x13562b670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x13562bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x13561b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x13562c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x13562c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x13562cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x13562d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x13562d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x13562dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x13562e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x13562e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x13562ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x13562f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x13562f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x13562fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x135630250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1356307a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x135630cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x135631190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x135631630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x135631ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x135631f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x135632410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1356328b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x135632d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1356331f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x135633690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x135633b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x135633fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x135634470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x135634910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x135634db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x135635250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x1356356f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x135635b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x135636030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x1356364d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x135636970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x135636e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1356372b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x135637750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x135637bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x135638090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x135638530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1356389d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x135638e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x135639310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1356397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x135639c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13563a0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13563a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13563aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13563aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x13563b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x13563b810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x13563bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x13563c150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x13563c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x13563ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x13563cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x13563d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x13563d870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x13563dd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x13563e1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x13563e650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x13563eaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x13563ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x13563f430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x13563f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x13563fd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x135640210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1356406b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x135640b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x135640ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x135641490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x135641930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x135641dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x135642270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x135642710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x135642bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x135643050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x1356434f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x135643990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x135643e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x1356442d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x135644770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x135644c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1356450b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x135645550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x1356459f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x135645e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x135646330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x1356467d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x135646c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x135647110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x1356475b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x135647a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x135647ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x135648440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x135648990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x135648ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x135649430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x1356496f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x135649d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13564a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13564a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13564b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13564b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13564b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13564be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13564c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x13564cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x13564d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x13564d5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x13564da60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x13564e210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x13564e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x13564ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x13564f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x13564f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x13564fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1356501f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x135650740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x135650c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1356511e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x135651730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x135651c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x1356521d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x135652720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x135652c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x1356531c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x135653710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x135653c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1356541b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x135654700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x135654c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1356551a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1356556f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x135655c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x135656190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1356566e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x135656c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x135657180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1356576d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x135657c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x135658170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1356586c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x135658c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x135659160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x1356596b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x135659c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13565a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13565a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13565abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13565b140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13565b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13565bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13565c130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13565c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13565cbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13565d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13565d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13565dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13565e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13565e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13565ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13565f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13565f650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x13565fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1356600f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x135660640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x135660b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x135661030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x1356614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x135661970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x135661e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x1356622b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x135662750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x135662bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x135663090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x135663530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1356639d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x135663e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x135664310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1356647b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x135664c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1356650f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x135665640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x135665d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x135666480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x135666ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1356672c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x135667580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x135667d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x135668030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x135668640 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.141.212 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.141.215 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x125704d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x125705190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x125705600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x125705a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x125705ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x125706350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x1257067c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x125706c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x1257070a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x125707510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x125707980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x125708070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x125708b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x125709340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x125709b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12570a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12570a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12570b0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12570b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12570bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12570c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12570cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12570d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12570db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12570e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12570e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12570e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12570ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12570f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12570f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12570f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12570ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x125710380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x125710640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x125710ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x125710f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x125711390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x125711800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x125711c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1257120e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x125712550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1257129c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x125712e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1257132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x125713710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x125713b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x125713ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x125714460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1257148d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x125714d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1257151b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x125715620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x125715a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x125715f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x125716370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x1257167e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x125716d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x125717250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1257176c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x125717b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x125717fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x125718410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x125718880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x125718cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x125719160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x1257195d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x125719a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x125719eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12571a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12571a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12571ac00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12571b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12571b4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12571b950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12571bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12571c230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12571c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12571cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12571cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12571d3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12571d860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12571dcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12571e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12571e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12571ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12571ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12571f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12571f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12571fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x125720050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1257204c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x125720930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x125720da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x125721210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x125721680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x125721af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x125721f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1257223d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x125722840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x125722cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x125723120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x125723590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x125723a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x125723e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1257242e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x125724750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x125724bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x125725030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1257254a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x125725910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x125725d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1257261f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x125726660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x125726ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x125726f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1257273b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x125727820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x125727c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x125728100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x125728570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1257289e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x125728e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1257292c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x125729730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x125729ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12572a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12572a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12572a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12572ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12572b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12572b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12572bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12572bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12572c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12572c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12572cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12572d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12572d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12572d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12572de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12572e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12572e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12572eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12572eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12572f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12572f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12572fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1257301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x125730620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x125730a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x125730f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x125731370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x1257317e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x125731c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1257320c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x125732530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1257329a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x125732e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x125733280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1257336f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x125733b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x125733fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x125734440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1257348b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x125734d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x125735190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x125735600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x125735a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x125735ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x125736350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x1257367c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x125736c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x1257370a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x125737510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x125737980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x125737df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x125738260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x1257386d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x125738b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x125738fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x125739420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x125739890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x125739d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12573a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12573a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12573aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12573aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12573b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12573b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12573bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12573c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12573c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12573c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12573cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12573d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12573d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12573db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12573df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12573e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12573e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12573ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12573f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12573f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12573fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12573fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x125740310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x125740780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x125740d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x125741180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1257415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x125742140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x125742400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1257426c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x125742b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x125742fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x125743410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x125743880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x125743cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x125744160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x1257445d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x125744a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x125744eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x125745320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x125745790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x125745c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x125746070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1257464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x125746950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x125746dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x125747230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1257476a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x125747b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x125747f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x1257483f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x125748860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x125748cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x125749140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x1257495b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x125749a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x125749e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12574a300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12574a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12574abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12574b050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12574b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12574b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12574bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12574c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12574c680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12574caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12574cf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12574d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12574d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12574dcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12574e120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12574e590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12574ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12574ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12574f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12574f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12574fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x125750030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x1257504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x125750910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x125750d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1257511f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x125751660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x125751ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x125751f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x1257523b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x125752820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x125752c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x125753100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x125753570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1257539e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x125753e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1257542c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x125754730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x125754ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x125755010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x125755480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1257558f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x125755d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x1257567d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x125756ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x125757610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x125757d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x125757ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x125758460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x125758a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x125759070 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x124d05810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x124d06100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x124d063c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x124d06830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x124d06ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x124d07110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x124d07580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x124d079f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x124d07e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x124d04230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x124d046a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x124d08120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x124d08af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x124d092a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x124d09ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x124d0a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x124d0a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x124d0b010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x124d0b730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x124d0bf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x124d0c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x124d0cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x124d0d460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x124d0db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x124d0e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x124d0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x124d0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x124d0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x124d0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x124d0f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x124d0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x124d10160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x124d105d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x124d10a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x124d10d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x124d11210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x124d116e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x124d11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x124d12080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x124d12550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x124d12a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x124d12ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x124d133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x124d13890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x124d13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x124d141d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x124d14640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x124d14ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x124d14f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x124d15390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x124d15800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x124d15c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x124d160e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x124d16550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x124d16bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x124d17060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x124d17500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x124d177c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x124d17c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x124d180a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x124d185f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x124d18b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x124d19010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x124d19520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x124d19a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x124d19f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x124d1a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x124d1a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x124d1ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x124d1b360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x124d1b870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x124d1bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x124d1c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x124d1c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x124d1cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x124d1d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x124d1d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x124d1dea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x124d1e460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x124d1ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x124d1efe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x124d1f5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x124d1fb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x124d20120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x124d206e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x124d20ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x124d21260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x124d21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x124d21de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x124d223a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x124d22960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x124d22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x124d234e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x124d23aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x124d24060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x124d24620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x124d24be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x124d251a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x124d25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x124d25d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x124d262e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x124d268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x124d26e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x124d27420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x124d279e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x124d27fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x124d28560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x124d28b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x124d290e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x124d296a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x124d29c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x124d2a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x124d2a7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x124d2ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x124d2b2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x124d2b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x124d2bcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x124d2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x124d2c6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x124d2cc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x124d2d110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x124d2d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x124d2db30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x124d2e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x124d2e550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x124d2ea60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x124d2ef70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x124d2f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x124d2f990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x124d2fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x124d303b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x124d308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x124d30dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x124d312e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x124d317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x124d31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x124d32210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x124d32720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x124d32c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x124d33140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x124d33650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x124d33b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x124d34070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x124d34580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x124d34a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x124d34fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x124d354b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x124d359c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x124d35ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x124d363e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x124d368e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x124d36de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x124d372f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x124d37800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x124d37d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x124d38220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x124d38730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x124d38c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x124d39150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x124d39660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x124d39b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x124d3a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x124d3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x124d3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x124d3afb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x124d3b4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x124d3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x124d3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x124d3c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x124d3c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x124d3ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x124d3d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x124d3d830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x124d3dd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x124d3e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x124d3e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x124d3ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x124d3f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x124d3f690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x124d3fba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x124d400b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x124d405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x124d40ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x124d40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x124d414f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x124d41a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x124d41f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x124d42420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x124d42930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x124d42e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x124d43350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x124d43860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x124d43d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x124d44280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x124d44830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x124d44de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x124d45390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x124d45940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x124d45f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x124d46560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x124d46b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x124d47360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x124d47800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x124d47ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x124d480d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x124d486e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x124d48ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x124d49370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x124d49810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x124d49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x124d4a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x124d4a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x124d4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x124d4b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x124d4b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x124d4bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x124d4c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x124d4c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x124d4cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x124d4d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x124d4d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x124d4ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x124d4e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x124d4e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x124d4eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x124d4f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x124d4f960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x124d4feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x124d50400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x124d50950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x124d50ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x124d513f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x124d51940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x124d51e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x124d523e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x124d52930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x124d52e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x124d533d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x124d53920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x124d53e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x124d543c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x124d54910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x124d54e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x124d553b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x124d55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x124d55e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x124d563a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x124d568f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x124d56e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x124d57390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x124d578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x124d57e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x124d58380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x124d588d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x124d58e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x124d59370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x124d598c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x124d59e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x124d5a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x124d5a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x124d5ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x124d5b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x124d5b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x124d5bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x124d5c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x124d5c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x124d5cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x124d5d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x124d5d720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x124d5dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x124d5e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x124d5e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x124d5e9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x124d5ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x124d5f2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x124d5f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x124d5fc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x124d600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x124d60560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x124d60a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x124d60ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x124d61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x124d61890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x124d61fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x124d626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x124d62df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x124d63510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x124d637d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x124d63fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x124d64280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x124d64890 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.771s
user	0m0.292s
sys	0m0.305s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4366 (5cab3e4a)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_load_model_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d80ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d80b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d80b9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d80bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d80c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d80caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d80d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d80d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d80dc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d80e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d80e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d80eb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d80f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d80fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d8105e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d810d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d811420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d811b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d812260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d812a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d813150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d813870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d813f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d814830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d814f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d815210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d815820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d816490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d8169d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d816c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d817130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d8173f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d817c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d8181c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d818480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d818920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d818dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d819260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d819700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d819ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d81a040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d81a4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d81a980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d81ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d81b0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d81b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d81bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d81c620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d81cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d81d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d81d850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d81de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d81e470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d81ea80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d81f270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d81f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d81fbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d81fe70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d820480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d820c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d820f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d8213d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d821870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d821d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d8221b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d822650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d822af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d822f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d823430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d8238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d823d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d824210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d8246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d824c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d825150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d8256a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d825bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d826140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d826690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d826be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d827130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d827680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d827bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d828120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d828670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d828bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d829110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d829660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d829bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d82a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d82a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d82aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d82b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d82b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d82bb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d82c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d82c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d81c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d82caa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d82d250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d82d7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d82dcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d82e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d82e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d82ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d82f230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d82f780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d82fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d830220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d830770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d830cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d831210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d831760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d831c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d8320a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d832540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d8329e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d832e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d833320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d8337c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d833c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d834100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d8345a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d834a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d834ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d835380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d835820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d835cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d836160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d836600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d836aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d836f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d8373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d837880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d837d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d8381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d838660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d838b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d838fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d839440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d8398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d839d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d83a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d83a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d83ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d83b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d83b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d83b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d83bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d83c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d83c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d83cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d83d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d83d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d83d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d83de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d83e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d83e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d83ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d83f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d83f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d83fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d83fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d840340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d8407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d840c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d841120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d8415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d841a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d841f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d8423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d842840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d842ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d843180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d843620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d843ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d843f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d844400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d8448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d844d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d8451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d845680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d845b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d845fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d846460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d846900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d846da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d847240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d8476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d847b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d848020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d8484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d848960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d848eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d849400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d849950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d849ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d84a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d84a770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d84ad80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d84b390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d84bb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d84c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d84c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d84c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d84cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d84d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d84db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d84e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d84e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d84ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d84f1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d84f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d84fc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d8501c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d850710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d850c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d8511b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d851700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d851c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d8521a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d8526f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d852c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d853190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d8536e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d853c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d854180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d8546d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d854c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d855170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d8556c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d855c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d856160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d8566b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d856c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d857150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d8576a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d857bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d858140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d858690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d858be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d859130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d859680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d859bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d85a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d85a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d85abc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d85b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d85b660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d85bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d85c100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d85c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d85cba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d85d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d85d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d85db90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d85e0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d85e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d85eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d85f0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d85f620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d85fb70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d8600c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d860610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d860b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d8610b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d861600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d861aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d861f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d8623e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d862880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d862d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d8631c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d863660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d863b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d863fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d864440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d8648e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d864d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d865220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d8656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d865b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d8660b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d8667d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d866ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d867610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d867d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d867ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d8687e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d868aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d8690b0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.096.416 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.096.419 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11d80c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11d80c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11d80cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11d80d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11d80d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11d80d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11d80a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11d80aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11d8257c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11d825c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11d8260a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11d826510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11d826c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11d827410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11d827bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11d8282e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11d8289d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11d8290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11d8297b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11d82a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11d82a820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11d82af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11d82b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11d82bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11d82c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11d82c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11d82ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11d82d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11d82d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11d82da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11d82de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11d82e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11d82e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11d82ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11d82ee90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11d82f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11d82f770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11d82fbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11d830050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11d8304c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11d830930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11d830da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11d831210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11d831680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11d831af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11d831f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11d8323d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11d832840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11d832cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11d833120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11d833590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11d833a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11d833e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11d8342e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11d834750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11d834bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11d835030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11d8354a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11d835910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11d835d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11d8361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11d836660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11d836ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11d836f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11d8373b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11d837820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11d837c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11d838100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11d838570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11d8389e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11d838e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11d8392c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11d839730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11d839ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11d83a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11d83a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11d83a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11d83ad60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11d83b1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11d83b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11d83bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11d83bf20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11d83c390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11d83c800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11d83cc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11d83d0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11d83d550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11d83d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11d83de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11d83e2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11d83e710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11d83eb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11d83eff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11d83f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11d83f8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11d83fd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11d8401b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11d840620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11d840a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11d840f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11d841370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11d8417e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11d841c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11d8420c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11d842530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11d8429a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11d842e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11d843280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11d8436f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11d843b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11d843fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11d844440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11d8448b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11d844d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11d845190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11d845600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11d845a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11d845ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11d846350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11d8467c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11d846c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11d8470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11d847510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11d847980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11d847df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11d848260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11d8486d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11d848b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11d848fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11d849420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11d849890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11d849d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11d84a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11d84a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11d84aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11d84aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11d84b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11d84b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11d84bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11d84c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11d84c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11d84c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11d84cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11d84d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11d84d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11d84db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11d84df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11d84e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11d84e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11d84ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11d84f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11d84f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11d84fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11d84fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11d850310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11d850780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11d850bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11d851060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11d8514d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11d851940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11d851db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11d852220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11d852690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11d852b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11d852f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11d8533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11d853850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11d853cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11d854130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11d8545a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11d854a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11d854e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11d8552f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11d855760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11d855bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11d856040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11d8564b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11d856920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11d856d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11d857200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11d857670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11d857ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11d857f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11d8583c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11d858830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11d858ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11d859110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11d859580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11d8599f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11d859e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11d85a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11d85a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11d85abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11d85b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11d85b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11d85b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11d85bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11d85c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11d85c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11d85cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11d85cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11d85d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11d85d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11d85dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11d85e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11d85e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11d85e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11d85ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11d85f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11d85f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11d85fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11d860310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11d860780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11d860bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11d861060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11d8614d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11d861940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11d861db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11d862220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11d862690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11d862b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11d862f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11d8633e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11d863850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11d863cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11d864130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11d8645a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11d864a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11d864e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11d8652f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11d865760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11d865bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11d866040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11d8664b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11d866920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11d866d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11d867200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11d867670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11d867ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11d867f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11d8683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11d868830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11d868ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11d869110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11d818360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11d8187d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11d818c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11d8190b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11d819520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11d819990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11d819e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11d81a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11d81a6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11d81ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11d81afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11d81b430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11d81b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11d81bd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11d81c180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11d81c5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11d81ca60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11d81ced0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11d81d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11d81d7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11d81dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11d81e090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11d81e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11d81e970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11d81ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11d81f250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11d81f6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11d81fb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11d81ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11d820410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11d820880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11d820cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11d821160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11d8215d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11d821a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11d821eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11d822320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11d822790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11d822c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11d8232f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11d8239e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11d8240d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11d8247c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11d824c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11d8250a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11d816b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11d816fe0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
ggml_metal_free: deallocating
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11cf08f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11cf09400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11cf09870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11cf09ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11cf0a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11cf0a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11cf0aa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11cf0aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11cf0b310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11cf0b840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11cf0bcb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11cf0c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11cf0ce50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11cf0d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11cf0de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11cf0e530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11cf0ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11cf0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11cf0fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11cf10260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11cf10980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11cf110a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11cf117c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11cf11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11cf12600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11cf128c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11cf12b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11cf12ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11cf13460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11cf138d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11cf13dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11cf142e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11cf14750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11cf14a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11cf14e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11cf152f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11cf15850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11cf15d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11cf16250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11cf16750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11cf16c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11cf17150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11cf17650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11cf17b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11cf18050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11cf184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11cf18930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11cf18da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11cf19210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11cf19680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11cf19af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11cf19f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11cf1a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11cf1a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11cf1acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11cf1b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11cf1b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11cf1bbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11cf1c1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11cf1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11cf1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11cf1d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11cf1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11cf1dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11cf1e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11cf1e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11cf1ea40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11cf1eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11cf1f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11cf1f820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11cf1fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11cf20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11cf20600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11cf20b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11cf210a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11cf215f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11cf21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11cf22090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11cf225e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11cf22b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11cf23080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11cf235d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11cf23b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11cf24070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11cf245c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11cf24b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11cf25060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11cf255b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11cf25b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11cf26050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11cf265a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11cf26af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11cf27040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11cf27590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11cf27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11cf28030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11cf28580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11cf28ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11cf29020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11cf29570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11cf29ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11cf2a010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11cf2a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11cf2aab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11cf2b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11cf2b550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11cf2baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11cf2bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11cf2c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11cf2ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11cf2cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11cf2d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11cf2da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11cf2df20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11cf2e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11cf2e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11cf2ed00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11cf2f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11cf2f640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11cf2fae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11cf2ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11cf30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11cf308c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11cf30d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11cf31200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11cf316a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11cf31b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11cf31fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11cf32480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11cf32920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11cf32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11cf33260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11cf33700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11cf33ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11cf34040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11cf344e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11cf34980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11cf34e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11cf352c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11cf35760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11cf35c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11cf360a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11cf36540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11cf369e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11cf36e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11cf37320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11cf377c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11cf37c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11cf38100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11cf385a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11cf38a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11cf38ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11cf39380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11cf39820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11cf39cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11cf3a160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11cf3a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11cf3aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11cf3af40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11cf3b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11cf3b880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11cf3bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11cf3c1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11cf3c660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11cf3cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11cf3cfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11cf3d440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11cf3d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11cf3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11cf3e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11cf3e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11cf3eb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11cf3f000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11cf3f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11cf3f940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11cf3fde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11cf40280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11cf40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11cf40bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11cf41060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11cf41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11cf419a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11cf41e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11cf422e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11cf42780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11cf42c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11cf430c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11cf43560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11cf43a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11cf43ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11cf44340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11cf447e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11cf44c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11cf451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11cf45720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11cf45c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11cf461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11cf46480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11cf46a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11cf470a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11cf476b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11cf47ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11cf48340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11cf48600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11cf48c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11cf49220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11cf49a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11cf49eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11cf4a350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11cf4a7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11cf4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11cf4b4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11cf4ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11cf4bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11cf4c4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11cf4ca30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11cf4cf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11cf4d4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11cf4da20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11cf4df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11cf4e4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11cf4ea10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11cf4ef60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11cf4f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11cf4fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11cf4ff50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11cf504a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11cf509f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11cf50f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11cf51490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11cf519e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11cf51f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11cf52480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11cf529d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11cf52f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11cf53470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11cf539c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11cf53f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11cf54460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11cf549b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11cf54f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11cf55450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11cf559a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11cf55ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11cf56440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11cf56990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11cf56ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11cf57430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11cf57980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11cf57ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11cf58420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11cf58970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11cf58ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11cf59410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11cf59960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11cf59eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11cf5a400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11cf5a950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11cf5aea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11cf5b3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11cf5b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11cf5be90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11cf5c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11cf5c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11cf5ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11cf5d3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11cf5d920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11cf5ddc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11cf5e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11cf5e700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11cf5eba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11cf5f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11cf5f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11cf5f980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11cf5fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11cf602c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11cf60760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11cf60c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11cf610a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11cf61540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11cf619e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11cf61e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11cf623d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11cf62af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11cf63210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11cf63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11cf64050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11cf64310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11cf64b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11cf64dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11cf653d0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes
ggml_metal_free: deallocating

main : success

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.939s
user	0m0.246s
sys	0m0.147s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.59 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.13 sec*proc (2 tests)

Total Test time (real) =   1.14 sec
        1.16 real         0.74 user         0.05 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.26 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.28 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.53 sec*proc (2 tests)

Total Test time (real) =   0.54 sec
        0.54 real         0.15 user         0.04 sys
```
