### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.27 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    1.77 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.23 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.68 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.42 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.33 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    1.43 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.07 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.33 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.07 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.98 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.32 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.32 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    2.16 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.23 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.18 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.21 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    1.03 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.23 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.27 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.20 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.47 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed  178.35 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.94 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   26.03 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.33 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.23 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    = 221.30 sec*proc (28 tests)

Total Test time (real) = 221.31 sec

real	3m41.389s
user	7m39.137s
sys	0m6.312s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/28 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.46 sec
      Start  2: test-tokenizer-0-command-r
 2/28 Test  #2: test-tokenizer-0-command-r ........   Passed    0.30 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/28 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/28 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.10 sec
      Start  5: test-tokenizer-0-falcon
 5/28 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.07 sec
      Start  6: test-tokenizer-0-gpt-2
 6/28 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.06 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/28 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.22 sec
      Start  8: test-tokenizer-0-llama-spm
 8/28 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/28 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.06 sec
      Start 10: test-tokenizer-0-phi-3
10/28 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/28 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.15 sec
      Start 12: test-tokenizer-0-refact
12/28 Test #12: test-tokenizer-0-refact ...........   Passed    0.06 sec
      Start 13: test-tokenizer-0-starcoder
13/28 Test #13: test-tokenizer-0-starcoder ........   Passed    0.06 sec
      Start 14: test-sampling
14/28 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/28 Test #15: test-grammar-parser ...............   Passed    0.18 sec
      Start 16: test-grammar-integration
16/28 Test #16: test-grammar-integration ..........   Passed    0.18 sec
      Start 17: test-llama-grammar
17/28 Test #17: test-llama-grammar ................   Passed    0.27 sec
      Start 18: test-json-schema-to-grammar
18/28 Test #18: test-json-schema-to-grammar .......   Passed    2.13 sec
      Start 19: test-tokenizer-1-llama-spm
19/28 Test #19: test-tokenizer-1-llama-spm ........   Passed    0.33 sec
      Start 20: test-log
20/28 Test #20: test-log ..........................   Passed    0.18 sec
      Start 21: test-arg-parser
21/28 Test #21: test-arg-parser ...................   Passed    0.21 sec
      Start 22: test-chat-template
22/28 Test #22: test-chat-template ................   Passed    0.17 sec
      Start 23: test-gguf
23/28 Test #23: test-gguf .........................   Passed    0.30 sec
      Start 24: test-backend-ops
24/28 Test #24: test-backend-ops ..................   Passed   29.29 sec
      Start 27: test-barrier
25/28 Test #27: test-barrier ......................   Passed    0.37 sec
      Start 28: test-quantize-fns
26/28 Test #28: test-quantize-fns .................   Passed   14.10 sec
      Start 29: test-quantize-perf
27/28 Test #29: test-quantize-perf ................   Passed    0.21 sec
      Start 30: test-rope
28/28 Test #30: test-rope .........................   Passed    0.21 sec

100% tests passed, 0 tests failed out of 28

Label Time Summary:
main    =  51.66 sec*proc (28 tests)

Total Test time (real) =  51.67 sec

real	0m51.686s
user	1m11.895s
sys	0m5.576s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.079 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.015.995 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.020.150 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.157 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.160 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.161 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.161 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.162 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.163 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.164 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.165 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.166 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.166 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.167 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.171 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.171 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.172 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.173 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.173 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.177 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.178 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.024.643 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.025.830 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.832 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.025.833 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.025.834 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.025.834 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.025.835 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.025.835 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.025.836 I llama_model_loader: - type  f32:  124 tensors
0.00.025.836 I llama_model_loader: - type  f16:   73 tensors
0.00.030.270 I llm_load_vocab: special tokens cache size = 5
0.00.032.455 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.032.459 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.032.459 I llm_load_print_meta: arch             = bert
0.00.032.460 I llm_load_print_meta: vocab type       = WPM
0.00.032.460 I llm_load_print_meta: n_vocab          = 30522
0.00.032.461 I llm_load_print_meta: n_merges         = 0
0.00.032.461 I llm_load_print_meta: vocab_only       = 0
0.00.032.461 I llm_load_print_meta: n_ctx_train      = 512
0.00.032.461 I llm_load_print_meta: n_embd           = 384
0.00.032.462 I llm_load_print_meta: n_layer          = 12
0.00.032.464 I llm_load_print_meta: n_head           = 12
0.00.032.465 I llm_load_print_meta: n_head_kv        = 12
0.00.032.468 I llm_load_print_meta: n_rot            = 32
0.00.032.468 I llm_load_print_meta: n_swa            = 0
0.00.032.468 I llm_load_print_meta: n_embd_head_k    = 32
0.00.032.468 I llm_load_print_meta: n_embd_head_v    = 32
0.00.032.469 I llm_load_print_meta: n_gqa            = 1
0.00.032.470 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.032.471 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.032.472 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.032.472 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.032.472 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.032.473 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.032.473 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.032.474 I llm_load_print_meta: n_ff             = 1536
0.00.032.474 I llm_load_print_meta: n_expert         = 0
0.00.032.474 I llm_load_print_meta: n_expert_used    = 0
0.00.032.474 I llm_load_print_meta: causal attn      = 0
0.00.032.475 I llm_load_print_meta: pooling type     = 2
0.00.032.475 I llm_load_print_meta: rope type        = 2
0.00.032.475 I llm_load_print_meta: rope scaling     = linear
0.00.032.476 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.032.477 I llm_load_print_meta: freq_scale_train = 1
0.00.032.477 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.032.478 I llm_load_print_meta: rope_finetuned   = unknown
0.00.032.478 I llm_load_print_meta: ssm_d_conv       = 0
0.00.032.478 I llm_load_print_meta: ssm_d_inner      = 0
0.00.032.478 I llm_load_print_meta: ssm_d_state      = 0
0.00.032.479 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.032.479 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.032.479 I llm_load_print_meta: model type       = 33M
0.00.032.480 I llm_load_print_meta: model ftype      = F16
0.00.032.481 I llm_load_print_meta: model params     = 33.21 M
0.00.032.481 I llm_load_print_meta: model size       = 63.84 MiB (16.12 BPW) 
0.00.032.482 I llm_load_print_meta: general.name     = Bge Small
0.00.032.482 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.032.482 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.032.483 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.032.483 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.032.483 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.032.485 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.032.485 I llm_load_print_meta: max token length = 21
0.00.034.563 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.034.564 I llm_load_tensors: offloading output layer to GPU
0.00.034.569 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.034.596 I llm_load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.034.597 I llm_load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
................................................
0.00.035.195 I llama_new_context_with_model: n_seq_max     = 1
0.00.035.196 I llama_new_context_with_model: n_ctx         = 512
0.00.035.196 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.035.197 I llama_new_context_with_model: n_batch       = 2048
0.00.035.197 I llama_new_context_with_model: n_ubatch      = 2048
0.00.035.197 I llama_new_context_with_model: flash_attn    = 0
0.00.035.198 I llama_new_context_with_model: freq_base     = 10000.0
0.00.035.198 I llama_new_context_with_model: freq_scale    = 1
0.00.035.199 I ggml_metal_init: allocating
0.00.035.212 I ggml_metal_init: found device: Apple M4
0.00.035.218 I ggml_metal_init: picking default device: Apple M4
0.00.036.100 I ggml_metal_init: using embedded metal library
0.00.040.147 I ggml_metal_init: GPU name:   Apple M4
0.00.040.150 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.040.150 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.040.151 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.040.151 I ggml_metal_init: simdgroup reduction   = true
0.00.040.151 I ggml_metal_init: simdgroup matrix mul. = true
0.00.040.151 I ggml_metal_init: has bfloat            = true
0.00.040.152 I ggml_metal_init: use bfloat            = true
0.00.040.152 I ggml_metal_init: hasUnifiedMemory      = true
0.00.040.153 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.052.329 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.052.958 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.052.960 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.052.962 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.053.749 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.053.750 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.053.751 I llama_new_context_with_model: graph nodes  = 429
0.00.053.751 I llama_new_context_with_model: graph splits = 2
0.00.053.752 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.053.753 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.060.158 I 
0.00.060.176 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.060.835 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.065.526 I llama_perf_context_print:        load time =      44.15 ms
0.00.065.527 I llama_perf_context_print: prompt eval time =       4.55 ms /     9 tokens (    0.51 ms per token,  1975.85 tokens per second)
0.00.065.528 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.065.528 I llama_perf_context_print:       total time =       5.37 ms /    10 tokens
0.00.065.686 I ggml_metal_free: deallocating

real	0m0.252s
user	0m0.048s
sys	0m0.030s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.036 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.108 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.011.164 I llama_model_loader: loaded meta data with 25 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.167 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.168 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.169 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.169 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.170 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.170 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.170 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.171 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.171 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.171 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.172 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.174 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.176 I llama_model_loader: - kv  11:                          general.file_type u32              = 7
0.00.011.176 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.011.177 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.011.177 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.177 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.011.179 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.636 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.337 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.338 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.339 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.339 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.339 I llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
0.00.014.340 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.340 I llama_model_loader: - kv  24:               general.quantization_version u32              = 2
0.00.014.340 I llama_model_loader: - type  f32:  124 tensors
0.00.014.341 I llama_model_loader: - type q8_0:   73 tensors
0.00.016.895 I llm_load_vocab: special tokens cache size = 5
0.00.018.185 I llm_load_vocab: token to piece cache size = 0.2032 MB
0.00.018.188 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.018.188 I llm_load_print_meta: arch             = bert
0.00.018.188 I llm_load_print_meta: vocab type       = WPM
0.00.018.189 I llm_load_print_meta: n_vocab          = 30522
0.00.018.189 I llm_load_print_meta: n_merges         = 0
0.00.018.189 I llm_load_print_meta: vocab_only       = 0
0.00.018.189 I llm_load_print_meta: n_ctx_train      = 512
0.00.018.189 I llm_load_print_meta: n_embd           = 384
0.00.018.189 I llm_load_print_meta: n_layer          = 12
0.00.018.192 I llm_load_print_meta: n_head           = 12
0.00.018.194 I llm_load_print_meta: n_head_kv        = 12
0.00.018.194 I llm_load_print_meta: n_rot            = 32
0.00.018.194 I llm_load_print_meta: n_swa            = 0
0.00.018.194 I llm_load_print_meta: n_embd_head_k    = 32
0.00.018.194 I llm_load_print_meta: n_embd_head_v    = 32
0.00.018.195 I llm_load_print_meta: n_gqa            = 1
0.00.018.195 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.018.196 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.018.196 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.018.197 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.018.197 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.018.197 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.018.197 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.018.198 I llm_load_print_meta: n_ff             = 1536
0.00.018.198 I llm_load_print_meta: n_expert         = 0
0.00.018.198 I llm_load_print_meta: n_expert_used    = 0
0.00.018.198 I llm_load_print_meta: causal attn      = 0
0.00.018.199 I llm_load_print_meta: pooling type     = 2
0.00.018.199 I llm_load_print_meta: rope type        = 2
0.00.018.199 I llm_load_print_meta: rope scaling     = linear
0.00.018.200 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.018.200 I llm_load_print_meta: freq_scale_train = 1
0.00.018.200 I llm_load_print_meta: n_ctx_orig_yarn  = 512
0.00.018.200 I llm_load_print_meta: rope_finetuned   = unknown
0.00.018.200 I llm_load_print_meta: ssm_d_conv       = 0
0.00.018.201 I llm_load_print_meta: ssm_d_inner      = 0
0.00.018.201 I llm_load_print_meta: ssm_d_state      = 0
0.00.018.201 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.018.201 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.018.201 I llm_load_print_meta: model type       = 33M
0.00.018.202 I llm_load_print_meta: model ftype      = Q8_0
0.00.018.202 I llm_load_print_meta: model params     = 33.21 M
0.00.018.202 I llm_load_print_meta: model size       = 34.38 MiB (8.68 BPW) 
0.00.018.203 I llm_load_print_meta: general.name     = Bge Small
0.00.018.203 I llm_load_print_meta: UNK token        = 100 '[UNK]'
0.00.018.203 I llm_load_print_meta: SEP token        = 102 '[SEP]'
0.00.018.203 I llm_load_print_meta: PAD token        = 0 '[PAD]'
0.00.018.203 I llm_load_print_meta: CLS token        = 101 '[CLS]'
0.00.018.204 I llm_load_print_meta: MASK token       = 103 '[MASK]'
0.00.018.204 I llm_load_print_meta: LF token         = 0 '[PAD]'
0.00.018.204 I llm_load_print_meta: max token length = 21
0.00.019.508 I llm_load_tensors: offloading 12 repeating layers to GPU
0.00.019.508 I llm_load_tensors: offloading output layer to GPU
0.00.019.509 I llm_load_tensors: offloaded 13/13 layers to GPU
0.00.019.516 I llm_load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.518 I llm_load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
................................................
0.00.019.876 I llama_new_context_with_model: n_seq_max     = 1
0.00.019.877 I llama_new_context_with_model: n_ctx         = 512
0.00.019.878 I llama_new_context_with_model: n_ctx_per_seq = 512
0.00.019.878 I llama_new_context_with_model: n_batch       = 2048
0.00.019.878 I llama_new_context_with_model: n_ubatch      = 2048
0.00.019.878 I llama_new_context_with_model: flash_attn    = 0
0.00.019.878 I llama_new_context_with_model: freq_base     = 10000.0
0.00.019.879 I llama_new_context_with_model: freq_scale    = 1
0.00.019.879 I ggml_metal_init: allocating
0.00.019.882 I ggml_metal_init: found device: Apple M4
0.00.019.884 I ggml_metal_init: picking default device: Apple M4
0.00.020.524 I ggml_metal_init: using embedded metal library
0.00.023.050 I ggml_metal_init: GPU name:   Apple M4
0.00.023.052 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.023.052 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.023.053 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.023.053 I ggml_metal_init: simdgroup reduction   = true
0.00.023.053 I ggml_metal_init: simdgroup matrix mul. = true
0.00.023.053 I ggml_metal_init: has bfloat            = true
0.00.023.054 I ggml_metal_init: use bfloat            = true
0.00.023.054 I ggml_metal_init: hasUnifiedMemory      = true
0.00.023.055 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.330 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.859 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.862 I llama_new_context_with_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.863 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.034.442 I llama_new_context_with_model:      Metal compute buffer size =    16.00 MiB
0.00.034.443 I llama_new_context_with_model:        CPU compute buffer size =     2.51 MiB
0.00.034.444 I llama_new_context_with_model: graph nodes  = 429
0.00.034.444 I llama_new_context_with_model: graph splits = 2
0.00.034.445 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.446 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.898 I 
0.00.038.918 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.446 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.861 I llama_perf_context_print:        load time =      29.78 ms
0.00.043.862 I llama_perf_context_print: prompt eval time =       4.28 ms /     9 tokens (    0.48 ms per token,  2100.84 tokens per second)
0.00.043.863 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.863 I llama_perf_context_print:       total time =       4.96 ms /    10 tokens
0.00.044.068 I ggml_metal_free: deallocating

real	0m0.056s
user	0m0.030s
sys	0m0.015s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.194 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.021.028 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.030.940 I llama_model_loader: loaded meta data with 29 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.030.945 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.030.947 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.030.948 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.030.948 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.030.949 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.030.950 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.030.951 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.030.952 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.030.952 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.030.953 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.030.953 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.030.957 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.030.957 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.030.958 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.030.958 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.030.959 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.038.658 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.040.781 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.045.208 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.045.210 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.045.210 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.045.211 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.045.211 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.045.212 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.045.212 I llama_model_loader: - kv  23:                tokenizer.ggml.cls_token_id u32              = 0
0.00.045.212 I llama_model_loader: - kv  24:               tokenizer.ggml.mask_token_id u32              = 4
0.00.045.213 I llama_model_loader: - kv  25:            tokenizer.ggml.token_type_count u32              = 2
0.00.045.213 I llama_model_loader: - kv  26:               tokenizer.ggml.add_bos_token bool             = true
0.00.045.213 I llama_model_loader: - kv  27:               tokenizer.ggml.add_eos_token bool             = true
0.00.045.214 I llama_model_loader: - kv  28:               general.quantization_version u32              = 2
0.00.045.214 I llama_model_loader: - type  f32:   40 tensors
0.00.045.214 I llama_model_loader: - type  f16:   30 tensors
0.00.063.145 W llm_load_vocab: empty token at index 5
0.00.067.689 W llm_load_vocab: model vocab missing newline token, using special_pad_id instead
0.00.069.033 W llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.069.064 I llm_load_vocab: special tokens cache size = 5
0.00.325.421 I llm_load_vocab: token to piece cache size = 1.5060 MB
0.00.325.427 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.325.428 I llm_load_print_meta: arch             = jina-bert-v2
0.00.325.428 I llm_load_print_meta: vocab type       = BPE
0.00.325.429 I llm_load_print_meta: n_vocab          = 61056
0.00.325.432 I llm_load_print_meta: n_merges         = 39382
0.00.325.432 I llm_load_print_meta: vocab_only       = 0
0.00.325.432 I llm_load_print_meta: n_ctx_train      = 8192
0.00.325.432 I llm_load_print_meta: n_embd           = 384
0.00.325.432 I llm_load_print_meta: n_layer          = 4
0.00.325.441 I llm_load_print_meta: n_head           = 12
0.00.325.442 I llm_load_print_meta: n_head_kv        = 12
0.00.325.442 I llm_load_print_meta: n_rot            = 32
0.00.325.442 I llm_load_print_meta: n_swa            = 0
0.00.325.442 I llm_load_print_meta: n_embd_head_k    = 32
0.00.325.442 I llm_load_print_meta: n_embd_head_v    = 32
0.00.325.443 I llm_load_print_meta: n_gqa            = 1
0.00.325.444 I llm_load_print_meta: n_embd_k_gqa     = 384
0.00.325.444 I llm_load_print_meta: n_embd_v_gqa     = 384
0.00.325.447 I llm_load_print_meta: f_norm_eps       = 1.0e-12
0.00.325.447 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.325.447 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.325.448 I llm_load_print_meta: f_max_alibi_bias = 8.0e+00
0.00.325.448 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.325.449 I llm_load_print_meta: n_ff             = 1536
0.00.325.449 I llm_load_print_meta: n_expert         = 0
0.00.325.449 I llm_load_print_meta: n_expert_used    = 0
0.00.325.450 I llm_load_print_meta: causal attn      = 0
0.00.325.451 I llm_load_print_meta: pooling type     = -1
0.00.325.451 I llm_load_print_meta: rope type        = -1
0.00.325.451 I llm_load_print_meta: rope scaling     = linear
0.00.325.451 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.325.452 I llm_load_print_meta: freq_scale_train = 1
0.00.325.452 I llm_load_print_meta: n_ctx_orig_yarn  = 8192
0.00.325.452 I llm_load_print_meta: rope_finetuned   = unknown
0.00.325.452 I llm_load_print_meta: ssm_d_conv       = 0
0.00.325.452 I llm_load_print_meta: ssm_d_inner      = 0
0.00.325.453 I llm_load_print_meta: ssm_d_state      = 0
0.00.325.453 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.325.453 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.325.453 I llm_load_print_meta: model type       = 33M
0.00.325.454 I llm_load_print_meta: model ftype      = F16
0.00.325.459 I llm_load_print_meta: model params     = 32.90 M
0.00.325.460 I llm_load_print_meta: model size       = 62.78 MiB (16.01 BPW) 
0.00.325.461 I llm_load_print_meta: general.name     = Jina Bert Implementation
0.00.325.461 I llm_load_print_meta: BOS token        = 0 '<s>'
0.00.325.461 I llm_load_print_meta: EOS token        = 2 '</s>'
0.00.325.463 I llm_load_print_meta: UNK token        = 3 '<unk>'
0.00.325.463 I llm_load_print_meta: SEP token        = 2 '</s>'
0.00.325.463 I llm_load_print_meta: PAD token        = 1 '<pad>'
0.00.325.463 I llm_load_print_meta: CLS token        = 0 '<s>'
0.00.325.464 I llm_load_print_meta: MASK token       = 4 '<mask>'
0.00.325.464 I llm_load_print_meta: EOG token        = 2 '</s>'
0.00.325.464 I llm_load_print_meta: max token length = 45
0.00.326.621 I llm_load_tensors: offloading 4 repeating layers to GPU
0.00.326.621 I llm_load_tensors: offloading output layer to GPU
0.00.326.622 I llm_load_tensors: offloaded 5/5 layers to GPU
0.00.326.648 I llm_load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.326.649 I llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
....................
0.00.327.508 I llama_new_context_with_model: n_seq_max     = 1
0.00.327.509 I llama_new_context_with_model: n_ctx         = 8192
0.00.327.509 I llama_new_context_with_model: n_ctx_per_seq = 8192
0.00.327.510 I llama_new_context_with_model: n_batch       = 2048
0.00.327.510 I llama_new_context_with_model: n_ubatch      = 2048
0.00.327.510 I llama_new_context_with_model: flash_attn    = 0
0.00.327.511 I llama_new_context_with_model: freq_base     = 10000.0
0.00.327.511 I llama_new_context_with_model: freq_scale    = 1
0.00.327.512 I ggml_metal_init: allocating
0.00.327.515 I ggml_metal_init: found device: Apple M4
0.00.327.516 I ggml_metal_init: picking default device: Apple M4
0.00.328.502 I ggml_metal_init: using embedded metal library
0.00.331.446 I ggml_metal_init: GPU name:   Apple M4
0.00.331.448 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.331.448 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.331.448 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.331.449 I ggml_metal_init: simdgroup reduction   = true
0.00.331.449 I ggml_metal_init: simdgroup matrix mul. = true
0.00.331.449 I ggml_metal_init: has bfloat            = true
0.00.331.449 I ggml_metal_init: use bfloat            = true
0.00.331.449 I ggml_metal_init: hasUnifiedMemory      = true
0.00.331.450 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.340.911 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.343.404 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.343.406 I llama_new_context_with_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.343.407 I llama_new_context_with_model:        CPU  output buffer size =     0.00 MiB
0.00.344.025 I llama_new_context_with_model:      Metal compute buffer size =   220.01 MiB
0.00.344.026 I llama_new_context_with_model:        CPU compute buffer size =    22.02 MiB
0.00.344.026 I llama_new_context_with_model: graph nodes  = 154
0.00.344.026 I llama_new_context_with_model: graph splits = 2
0.00.344.027 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.344.027 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.356.382 I 
0.00.356.401 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.356.556 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.356.556 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.356.559 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.356.559 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.356.563 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.356.563 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.357.069 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.360.663 I llama_perf_context_print:        load time =     335.35 ms
0.00.360.664 I llama_perf_context_print: prompt eval time =       3.58 ms /    62 tokens (    0.06 ms per token, 17294.28 tokens per second)
0.00.360.665 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.360.666 I llama_perf_context_print:       total time =       4.28 ms /    63 tokens
0.00.360.908 I ggml_metal_free: deallocating

real	0m1.072s
user	0m0.334s
sys	0m0.044s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.168 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.281 I main: llama backend init
0.00.000.287 I main: load the model and apply lora adapter, if any
0.00.057.652 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.068.541 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.068.552 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.068.556 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.068.556 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.068.557 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.068.558 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.068.558 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.068.560 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.068.561 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.068.561 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.068.562 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.068.563 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.068.563 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.068.564 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.068.569 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.068.570 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.068.571 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.075.422 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.077.631 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.084.628 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.084.633 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.084.634 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.084.635 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.084.636 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.084.637 I llama_model_loader: - type  f32:  194 tensors
0.00.084.637 I llama_model_loader: - type  f16:   98 tensors
0.00.122.762 I llm_load_vocab: special tokens cache size = 25
0.00.130.646 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.130.650 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.130.651 I llm_load_print_meta: arch             = gptneox
0.00.130.651 I llm_load_print_meta: vocab type       = BPE
0.00.130.651 I llm_load_print_meta: n_vocab          = 50304
0.00.130.651 I llm_load_print_meta: n_merges         = 50009
0.00.130.651 I llm_load_print_meta: vocab_only       = 0
0.00.130.652 I llm_load_print_meta: n_ctx_train      = 2048
0.00.130.652 I llm_load_print_meta: n_embd           = 2048
0.00.130.654 I llm_load_print_meta: n_layer          = 24
0.00.130.658 I llm_load_print_meta: n_head           = 16
0.00.130.659 I llm_load_print_meta: n_head_kv        = 16
0.00.130.659 I llm_load_print_meta: n_rot            = 32
0.00.130.659 I llm_load_print_meta: n_swa            = 0
0.00.130.659 I llm_load_print_meta: n_embd_head_k    = 128
0.00.130.659 I llm_load_print_meta: n_embd_head_v    = 128
0.00.130.660 I llm_load_print_meta: n_gqa            = 1
0.00.130.661 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.130.661 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.130.662 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.130.662 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.130.663 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.130.663 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.130.663 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.130.664 I llm_load_print_meta: n_ff             = 8192
0.00.130.664 I llm_load_print_meta: n_expert         = 0
0.00.130.664 I llm_load_print_meta: n_expert_used    = 0
0.00.130.664 I llm_load_print_meta: causal attn      = 1
0.00.130.664 I llm_load_print_meta: pooling type     = 0
0.00.130.666 I llm_load_print_meta: rope type        = 2
0.00.130.666 I llm_load_print_meta: rope scaling     = linear
0.00.130.667 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.130.667 I llm_load_print_meta: freq_scale_train = 1
0.00.130.667 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.130.667 I llm_load_print_meta: rope_finetuned   = unknown
0.00.130.668 I llm_load_print_meta: ssm_d_conv       = 0
0.00.130.669 I llm_load_print_meta: ssm_d_inner      = 0
0.00.130.669 I llm_load_print_meta: ssm_d_state      = 0
0.00.130.669 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.130.670 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.130.670 I llm_load_print_meta: model type       = 1.4B
0.00.130.670 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.130.671 I llm_load_print_meta: model params     = 1.41 B
0.00.130.671 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.130.672 I llm_load_print_meta: general.name     = 1.4B
0.00.130.672 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.130.672 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.130.672 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.130.672 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.130.673 I llm_load_print_meta: LF token         = 128 ''
0.00.130.673 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.130.673 I llm_load_print_meta: max token length = 1024
0.00.133.461 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.133.461 I llm_load_tensors: offloading output layer to GPU
0.00.133.461 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.133.481 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.133.482 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.134.550 I llama_new_context_with_model: n_seq_max     = 1
0.00.134.551 I llama_new_context_with_model: n_ctx         = 2048
0.00.134.551 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.134.551 I llama_new_context_with_model: n_batch       = 2048
0.00.134.551 I llama_new_context_with_model: n_ubatch      = 512
0.00.134.551 I llama_new_context_with_model: flash_attn    = 0
0.00.134.552 I llama_new_context_with_model: freq_base     = 10000.0
0.00.134.552 I llama_new_context_with_model: freq_scale    = 1
0.00.134.553 I ggml_metal_init: allocating
0.00.134.563 I ggml_metal_init: found device: Apple M4
0.00.134.565 I ggml_metal_init: picking default device: Apple M4
0.00.135.305 I ggml_metal_init: using embedded metal library
0.00.144.932 I ggml_metal_init: GPU name:   Apple M4
0.00.144.934 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.144.934 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.144.935 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.144.935 I ggml_metal_init: simdgroup reduction   = true
0.00.144.935 I ggml_metal_init: simdgroup matrix mul. = true
0.00.144.935 I ggml_metal_init: has bfloat            = true
0.00.144.935 I ggml_metal_init: use bfloat            = true
0.00.144.936 I ggml_metal_init: hasUnifiedMemory      = true
0.00.144.936 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.170.022 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.192.393 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.192.401 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.192.422 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.193.520 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.193.524 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.193.524 I llama_new_context_with_model: graph nodes  = 967
0.00.193.524 I llama_new_context_with_model: graph splits = 2
0.00.193.528 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.193.669 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.193.670 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.276.682 I main: llama threadpool init, n_threads = 4
0.00.276.730 I 
0.00.276.755 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.276.756 I 
0.00.276.826 I sampler seed: 1234
0.00.276.831 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.276.867 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.276.868 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.276.868 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.119.087 I llama_perf_sampler_print:    sampling time =       1.25 ms /    71 runs   (    0.02 ms per token, 56845.48 tokens per second)
0.02.119.088 I llama_perf_context_print:        load time =     219.02 ms
0.02.119.089 I llama_perf_context_print: prompt eval time =      43.61 ms /     7 tokens (    6.23 ms per token,   160.51 tokens per second)
0.02.119.090 I llama_perf_context_print:        eval time =    1795.62 ms /    63 runs   (   28.50 ms per token,    35.09 tokens per second)
0.02.119.090 I llama_perf_context_print:       total time =    1842.41 ms /    70 tokens
0.02.119.370 I ggml_metal_free: deallocating

real	0m2.423s
user	0m0.150s
sys	0m0.108s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.578 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.023.433 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.035.602 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.612 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.615 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.035.616 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.616 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.035.617 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.035.618 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.035.619 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.035.620 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.035.620 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.035.621 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.035.622 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.035.622 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.035.623 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.035.626 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.035.627 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.628 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.043.467 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.045.653 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.052.668 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.052.670 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.052.670 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.052.671 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.052.671 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.052.672 I llama_model_loader: - type  f32:  194 tensors
0.00.052.672 I llama_model_loader: - type  f16:   98 tensors
0.00.082.222 I llm_load_vocab: special tokens cache size = 25
0.00.088.661 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.088.664 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.088.665 I llm_load_print_meta: arch             = gptneox
0.00.088.665 I llm_load_print_meta: vocab type       = BPE
0.00.088.665 I llm_load_print_meta: n_vocab          = 50304
0.00.088.665 I llm_load_print_meta: n_merges         = 50009
0.00.088.665 I llm_load_print_meta: vocab_only       = 0
0.00.088.666 I llm_load_print_meta: n_ctx_train      = 2048
0.00.088.666 I llm_load_print_meta: n_embd           = 2048
0.00.088.666 I llm_load_print_meta: n_layer          = 24
0.00.088.668 I llm_load_print_meta: n_head           = 16
0.00.088.669 I llm_load_print_meta: n_head_kv        = 16
0.00.088.669 I llm_load_print_meta: n_rot            = 32
0.00.088.669 I llm_load_print_meta: n_swa            = 0
0.00.088.669 I llm_load_print_meta: n_embd_head_k    = 128
0.00.088.670 I llm_load_print_meta: n_embd_head_v    = 128
0.00.088.672 I llm_load_print_meta: n_gqa            = 1
0.00.088.673 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.088.674 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.088.674 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.088.674 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.088.676 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.088.676 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.088.676 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.088.677 I llm_load_print_meta: n_ff             = 8192
0.00.088.677 I llm_load_print_meta: n_expert         = 0
0.00.088.677 I llm_load_print_meta: n_expert_used    = 0
0.00.088.677 I llm_load_print_meta: causal attn      = 1
0.00.088.677 I llm_load_print_meta: pooling type     = 0
0.00.088.677 I llm_load_print_meta: rope type        = 2
0.00.088.677 I llm_load_print_meta: rope scaling     = linear
0.00.088.678 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.088.678 I llm_load_print_meta: freq_scale_train = 1
0.00.088.678 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.088.678 I llm_load_print_meta: rope_finetuned   = unknown
0.00.088.678 I llm_load_print_meta: ssm_d_conv       = 0
0.00.088.678 I llm_load_print_meta: ssm_d_inner      = 0
0.00.088.679 I llm_load_print_meta: ssm_d_state      = 0
0.00.088.679 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.088.679 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.088.679 I llm_load_print_meta: model type       = 1.4B
0.00.088.680 I llm_load_print_meta: model ftype      = all F32 (guessed)
0.00.088.680 I llm_load_print_meta: model params     = 1.41 B
0.00.088.681 I llm_load_print_meta: model size       = 2.64 GiB (16.01 BPW) 
0.00.088.681 I llm_load_print_meta: general.name     = 1.4B
0.00.088.681 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.088.681 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.088.682 I llm_load_print_meta: LF token         = 128 ''
0.00.088.683 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.088.683 I llm_load_print_meta: max token length = 1024
0.00.091.250 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.091.250 I llm_load_tensors: offloading output layer to GPU
0.00.091.250 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.091.261 I llm_load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.091.262 I llm_load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
...............................................................................
0.00.092.152 I llama_new_context_with_model: n_seq_max     = 1
0.00.092.153 I llama_new_context_with_model: n_ctx         = 128
0.00.092.153 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.092.153 I llama_new_context_with_model: n_batch       = 128
0.00.092.153 I llama_new_context_with_model: n_ubatch      = 128
0.00.092.153 I llama_new_context_with_model: flash_attn    = 0
0.00.092.154 I llama_new_context_with_model: freq_base     = 10000.0
0.00.092.154 I llama_new_context_with_model: freq_scale    = 1
0.00.092.154 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.092.155 I ggml_metal_init: allocating
0.00.092.158 I ggml_metal_init: found device: Apple M4
0.00.092.160 I ggml_metal_init: picking default device: Apple M4
0.00.092.771 I ggml_metal_init: using embedded metal library
0.00.095.418 I ggml_metal_init: GPU name:   Apple M4
0.00.095.420 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.095.420 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.095.421 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.095.421 I ggml_metal_init: simdgroup reduction   = true
0.00.095.421 I ggml_metal_init: simdgroup matrix mul. = true
0.00.095.421 I ggml_metal_init: has bfloat            = true
0.00.095.421 I ggml_metal_init: use bfloat            = true
0.00.095.422 I ggml_metal_init: hasUnifiedMemory      = true
0.00.095.422 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.104.448 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.106.114 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.106.116 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.106.138 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.107.045 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.107.046 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.107.047 I llama_new_context_with_model: graph nodes  = 967
0.00.107.047 I llama_new_context_with_model: graph splits = 2
0.00.107.048 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.107.048 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.211.117 I 
0.01.211.169 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.211.211 I perplexity: tokenizing the input ..
0.01.224.629 I perplexity: tokenization took 13.413 ms
0.01.224.640 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.359.451 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.361.106 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.361.132 I llama_perf_context_print:        load time =    1187.67 ms
0.01.361.133 I llama_perf_context_print: prompt eval time =     133.86 ms /   128 tokens (    1.05 ms per token,   956.22 tokens per second)
0.01.361.134 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.361.134 I llama_perf_context_print:       total time =     150.02 ms /   129 tokens
0.01.361.868 I ggml_metal_free: deallocating

real	0m1.552s
user	0m0.122s
sys	0m0.222s
```
- q8_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.080 I main: llama backend init
0.00.000.082 I main: load the model and apply lora adapter, if any
0.00.009.801 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.022.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.022.775 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.022.777 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.022.779 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.022.780 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.022.780 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.022.781 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.022.781 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.022.782 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.022.782 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.022.782 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.022.783 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.022.783 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.022.784 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.022.786 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.022.786 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.022.787 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.026.531 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.027.572 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.359 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.360 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.361 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.361 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.361 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.362 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.362 I llama_model_loader: - type  f32:  194 tensors
0.00.031.362 I llama_model_loader: - type q8_0:   98 tensors
0.00.052.391 I llm_load_vocab: special tokens cache size = 25
0.00.058.417 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.058.421 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.058.422 I llm_load_print_meta: arch             = gptneox
0.00.058.422 I llm_load_print_meta: vocab type       = BPE
0.00.058.426 I llm_load_print_meta: n_vocab          = 50304
0.00.058.426 I llm_load_print_meta: n_merges         = 50009
0.00.058.426 I llm_load_print_meta: vocab_only       = 0
0.00.058.427 I llm_load_print_meta: n_ctx_train      = 2048
0.00.058.427 I llm_load_print_meta: n_embd           = 2048
0.00.058.431 I llm_load_print_meta: n_layer          = 24
0.00.058.437 I llm_load_print_meta: n_head           = 16
0.00.058.439 I llm_load_print_meta: n_head_kv        = 16
0.00.058.439 I llm_load_print_meta: n_rot            = 32
0.00.058.439 I llm_load_print_meta: n_swa            = 0
0.00.058.439 I llm_load_print_meta: n_embd_head_k    = 128
0.00.058.439 I llm_load_print_meta: n_embd_head_v    = 128
0.00.058.440 I llm_load_print_meta: n_gqa            = 1
0.00.058.441 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.058.441 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.058.442 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.058.442 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.058.443 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.058.443 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.058.443 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.058.444 I llm_load_print_meta: n_ff             = 8192
0.00.058.444 I llm_load_print_meta: n_expert         = 0
0.00.058.444 I llm_load_print_meta: n_expert_used    = 0
0.00.058.444 I llm_load_print_meta: causal attn      = 1
0.00.058.445 I llm_load_print_meta: pooling type     = 0
0.00.058.445 I llm_load_print_meta: rope type        = 2
0.00.058.445 I llm_load_print_meta: rope scaling     = linear
0.00.058.446 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.058.446 I llm_load_print_meta: freq_scale_train = 1
0.00.058.448 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.058.448 I llm_load_print_meta: rope_finetuned   = unknown
0.00.058.448 I llm_load_print_meta: ssm_d_conv       = 0
0.00.058.448 I llm_load_print_meta: ssm_d_inner      = 0
0.00.058.448 I llm_load_print_meta: ssm_d_state      = 0
0.00.058.448 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.058.449 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.058.449 I llm_load_print_meta: model type       = 1.4B
0.00.058.449 I llm_load_print_meta: model ftype      = Q8_0
0.00.058.450 I llm_load_print_meta: model params     = 1.41 B
0.00.058.450 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.058.450 I llm_load_print_meta: general.name     = 1.4B
0.00.058.453 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.058.453 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.058.453 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.058.454 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.058.454 I llm_load_print_meta: LF token         = 128 ''
0.00.058.454 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.058.454 I llm_load_print_meta: max token length = 1024
0.00.060.840 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.060.840 I llm_load_tensors: offloading output layer to GPU
0.00.060.841 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.060.852 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.060.853 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.061.787 I llama_new_context_with_model: n_seq_max     = 1
0.00.061.787 I llama_new_context_with_model: n_ctx         = 2048
0.00.061.787 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.061.788 I llama_new_context_with_model: n_batch       = 2048
0.00.061.788 I llama_new_context_with_model: n_ubatch      = 512
0.00.061.788 I llama_new_context_with_model: flash_attn    = 0
0.00.061.788 I llama_new_context_with_model: freq_base     = 10000.0
0.00.061.789 I llama_new_context_with_model: freq_scale    = 1
0.00.061.789 I ggml_metal_init: allocating
0.00.061.793 I ggml_metal_init: found device: Apple M4
0.00.061.795 I ggml_metal_init: picking default device: Apple M4
0.00.062.537 I ggml_metal_init: using embedded metal library
0.00.065.041 I ggml_metal_init: GPU name:   Apple M4
0.00.065.043 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.043 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.044 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.044 I ggml_metal_init: simdgroup reduction   = true
0.00.065.044 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.044 I ggml_metal_init: has bfloat            = true
0.00.065.044 I ggml_metal_init: use bfloat            = true
0.00.065.045 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.046 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.075.413 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.101.704 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.101.717 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.101.746 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.102.872 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.102.875 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.102.875 I llama_new_context_with_model: graph nodes  = 967
0.00.102.876 I llama_new_context_with_model: graph splits = 2
0.00.102.880 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.103.021 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.103.021 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.407.506 I main: llama threadpool init, n_threads = 4
0.01.407.584 I 
0.01.407.646 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.407.648 I 
0.01.408.169 I sampler seed: 1234
0.01.408.176 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.408.223 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.408.226 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.408.226 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.501.452 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55729.98 tokens per second)
0.02.501.452 I llama_perf_context_print:        load time =    1397.69 ms
0.02.501.453 I llama_perf_context_print: prompt eval time =      50.53 ms /     7 tokens (    7.22 ms per token,   138.52 tokens per second)
0.02.501.454 I llama_perf_context_print:        eval time =    1039.78 ms /    63 runs   (   16.50 ms per token,    60.59 tokens per second)
0.02.501.454 I llama_perf_context_print:       total time =    1093.95 ms /    70 tokens
0.02.501.659 I ggml_metal_free: deallocating

real	0m2.520s
user	0m0.121s
sys	0m0.244s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.125 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.122 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.019.226 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.019.231 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.233 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.233 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.234 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.234 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.234 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.236 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.236 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.236 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.237 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.237 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.239 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.239 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.241 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.242 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.242 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.024.716 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.026.234 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.031.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.031.766 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.031.766 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.031.766 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.031.767 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.031.767 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.031.768 I llama_model_loader: - type  f32:  194 tensors
0.00.031.768 I llama_model_loader: - type q8_0:   98 tensors
0.00.056.746 I llm_load_vocab: special tokens cache size = 25
0.00.062.784 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.062.787 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.062.788 I llm_load_print_meta: arch             = gptneox
0.00.062.788 I llm_load_print_meta: vocab type       = BPE
0.00.062.788 I llm_load_print_meta: n_vocab          = 50304
0.00.062.788 I llm_load_print_meta: n_merges         = 50009
0.00.062.789 I llm_load_print_meta: vocab_only       = 0
0.00.062.789 I llm_load_print_meta: n_ctx_train      = 2048
0.00.062.789 I llm_load_print_meta: n_embd           = 2048
0.00.062.789 I llm_load_print_meta: n_layer          = 24
0.00.062.793 I llm_load_print_meta: n_head           = 16
0.00.062.794 I llm_load_print_meta: n_head_kv        = 16
0.00.062.794 I llm_load_print_meta: n_rot            = 32
0.00.062.795 I llm_load_print_meta: n_swa            = 0
0.00.062.795 I llm_load_print_meta: n_embd_head_k    = 128
0.00.062.795 I llm_load_print_meta: n_embd_head_v    = 128
0.00.062.796 I llm_load_print_meta: n_gqa            = 1
0.00.062.796 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.062.797 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.062.798 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.062.798 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.062.798 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.062.798 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.062.799 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.062.799 I llm_load_print_meta: n_ff             = 8192
0.00.062.800 I llm_load_print_meta: n_expert         = 0
0.00.062.800 I llm_load_print_meta: n_expert_used    = 0
0.00.062.800 I llm_load_print_meta: causal attn      = 1
0.00.062.800 I llm_load_print_meta: pooling type     = 0
0.00.062.800 I llm_load_print_meta: rope type        = 2
0.00.062.800 I llm_load_print_meta: rope scaling     = linear
0.00.062.801 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.062.801 I llm_load_print_meta: freq_scale_train = 1
0.00.062.801 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.062.802 I llm_load_print_meta: rope_finetuned   = unknown
0.00.062.802 I llm_load_print_meta: ssm_d_conv       = 0
0.00.062.802 I llm_load_print_meta: ssm_d_inner      = 0
0.00.062.804 I llm_load_print_meta: ssm_d_state      = 0
0.00.062.805 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.062.805 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.062.805 I llm_load_print_meta: model type       = 1.4B
0.00.062.806 I llm_load_print_meta: model ftype      = Q8_0
0.00.062.806 I llm_load_print_meta: model params     = 1.41 B
0.00.062.806 I llm_load_print_meta: model size       = 1.40 GiB (8.51 BPW) 
0.00.062.807 I llm_load_print_meta: general.name     = 1.4B
0.00.062.807 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.062.807 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.062.807 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.062.807 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.062.812 I llm_load_print_meta: LF token         = 128 ''
0.00.062.812 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.062.813 I llm_load_print_meta: max token length = 1024
0.00.065.150 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.065.151 I llm_load_tensors: offloading output layer to GPU
0.00.065.151 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.065.162 I llm_load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.065.163 I llm_load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
...............................................................................
0.00.066.085 I llama_new_context_with_model: n_seq_max     = 1
0.00.066.086 I llama_new_context_with_model: n_ctx         = 128
0.00.066.086 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.066.086 I llama_new_context_with_model: n_batch       = 128
0.00.066.086 I llama_new_context_with_model: n_ubatch      = 128
0.00.066.086 I llama_new_context_with_model: flash_attn    = 0
0.00.066.087 I llama_new_context_with_model: freq_base     = 10000.0
0.00.066.087 I llama_new_context_with_model: freq_scale    = 1
0.00.066.087 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.066.088 I ggml_metal_init: allocating
0.00.066.094 I ggml_metal_init: found device: Apple M4
0.00.066.097 I ggml_metal_init: picking default device: Apple M4
0.00.066.742 I ggml_metal_init: using embedded metal library
0.00.069.400 I ggml_metal_init: GPU name:   Apple M4
0.00.069.402 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.069.403 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.069.403 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.069.403 I ggml_metal_init: simdgroup reduction   = true
0.00.069.403 I ggml_metal_init: simdgroup matrix mul. = true
0.00.069.404 I ggml_metal_init: has bfloat            = true
0.00.069.404 I ggml_metal_init: use bfloat            = true
0.00.069.404 I ggml_metal_init: hasUnifiedMemory      = true
0.00.069.406 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.079.579 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.081.026 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.081.030 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.081.044 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.082.110 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.082.111 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.082.111 I llama_new_context_with_model: graph nodes  = 967
0.00.082.111 I llama_new_context_with_model: graph splits = 2
0.00.082.113 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.082.113 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.905.298 I 
0.00.905.320 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.905.335 I perplexity: tokenizing the input ..
0.00.913.319 I perplexity: tokenization took 7.983 ms
0.00.913.326 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.037.986 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.039.233 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.039.253 I llama_perf_context_print:        load time =     894.17 ms
0.01.039.254 I llama_perf_context_print: prompt eval time =     124.43 ms /   128 tokens (    0.97 ms per token,  1028.68 tokens per second)
0.01.039.255 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.039.255 I llama_perf_context_print:       total time =     133.96 ms /   129 tokens
0.01.039.790 I ggml_metal_free: deallocating

real	0m1.056s
user	0m0.091s
sys	0m0.145s
```
- q4_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.011.975 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.018.991 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.018.996 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.998 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.998 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.999 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.999 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.999 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.000 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.001 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.001 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.001 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.002 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.002 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.002 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.004 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.005 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.005 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.790 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.837 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.853 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.027.855 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.855 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.855 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.855 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.856 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.027.861 I llama_model_loader: - type  f32:  194 tensors
0.00.027.861 I llama_model_loader: - type q4_0:   97 tensors
0.00.027.861 I llama_model_loader: - type q6_K:    1 tensors
0.00.049.642 I llm_load_vocab: special tokens cache size = 25
0.00.055.832 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.055.838 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.055.838 I llm_load_print_meta: arch             = gptneox
0.00.055.840 I llm_load_print_meta: vocab type       = BPE
0.00.055.841 I llm_load_print_meta: n_vocab          = 50304
0.00.055.841 I llm_load_print_meta: n_merges         = 50009
0.00.055.841 I llm_load_print_meta: vocab_only       = 0
0.00.055.841 I llm_load_print_meta: n_ctx_train      = 2048
0.00.055.841 I llm_load_print_meta: n_embd           = 2048
0.00.055.842 I llm_load_print_meta: n_layer          = 24
0.00.055.847 I llm_load_print_meta: n_head           = 16
0.00.055.847 I llm_load_print_meta: n_head_kv        = 16
0.00.055.848 I llm_load_print_meta: n_rot            = 32
0.00.055.848 I llm_load_print_meta: n_swa            = 0
0.00.055.848 I llm_load_print_meta: n_embd_head_k    = 128
0.00.055.848 I llm_load_print_meta: n_embd_head_v    = 128
0.00.055.849 I llm_load_print_meta: n_gqa            = 1
0.00.055.849 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.055.850 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.055.850 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.055.851 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.055.851 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.055.851 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.055.852 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.055.852 I llm_load_print_meta: n_ff             = 8192
0.00.055.853 I llm_load_print_meta: n_expert         = 0
0.00.055.853 I llm_load_print_meta: n_expert_used    = 0
0.00.055.853 I llm_load_print_meta: causal attn      = 1
0.00.055.853 I llm_load_print_meta: pooling type     = 0
0.00.055.853 I llm_load_print_meta: rope type        = 2
0.00.055.854 I llm_load_print_meta: rope scaling     = linear
0.00.055.854 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.055.854 I llm_load_print_meta: freq_scale_train = 1
0.00.055.854 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.055.855 I llm_load_print_meta: rope_finetuned   = unknown
0.00.055.855 I llm_load_print_meta: ssm_d_conv       = 0
0.00.055.855 I llm_load_print_meta: ssm_d_inner      = 0
0.00.055.856 I llm_load_print_meta: ssm_d_state      = 0
0.00.055.856 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.055.856 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.055.856 I llm_load_print_meta: model type       = 1.4B
0.00.055.857 I llm_load_print_meta: model ftype      = Q4_0
0.00.055.857 I llm_load_print_meta: model params     = 1.41 B
0.00.055.857 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.055.858 I llm_load_print_meta: general.name     = 1.4B
0.00.055.858 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.055.858 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.055.858 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.055.858 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.055.858 I llm_load_print_meta: LF token         = 128 ''
0.00.055.859 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.055.859 I llm_load_print_meta: max token length = 1024
0.00.058.125 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.058.125 I llm_load_tensors: offloading output layer to GPU
0.00.058.125 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.058.138 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.058.139 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.059.190 I llama_new_context_with_model: n_seq_max     = 1
0.00.059.191 I llama_new_context_with_model: n_ctx         = 2048
0.00.059.191 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.059.191 I llama_new_context_with_model: n_batch       = 2048
0.00.059.191 I llama_new_context_with_model: n_ubatch      = 512
0.00.059.191 I llama_new_context_with_model: flash_attn    = 0
0.00.059.192 I llama_new_context_with_model: freq_base     = 10000.0
0.00.059.192 I llama_new_context_with_model: freq_scale    = 1
0.00.059.193 I ggml_metal_init: allocating
0.00.059.200 I ggml_metal_init: found device: Apple M4
0.00.059.203 I ggml_metal_init: picking default device: Apple M4
0.00.059.998 I ggml_metal_init: using embedded metal library
0.00.062.616 I ggml_metal_init: GPU name:   Apple M4
0.00.062.618 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.062.618 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.062.619 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.062.619 I ggml_metal_init: simdgroup reduction   = true
0.00.062.619 I ggml_metal_init: simdgroup matrix mul. = true
0.00.062.619 I ggml_metal_init: has bfloat            = true
0.00.062.619 I ggml_metal_init: use bfloat            = true
0.00.062.620 I ggml_metal_init: hasUnifiedMemory      = true
0.00.062.620 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.073.449 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.096.607 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.096.614 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.096.640 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.097.786 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.097.788 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.097.788 I llama_new_context_with_model: graph nodes  = 967
0.00.097.788 I llama_new_context_with_model: graph splits = 2
0.00.097.792 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.097.933 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.097.934 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.673.425 I main: llama threadpool init, n_threads = 4
0.00.673.462 I 
0.00.673.491 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.673.491 I 
0.00.673.712 I sampler seed: 1234
0.00.673.716 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.673.760 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.673.761 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.673.761 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.354.963 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48102.98 tokens per second)
0.01.354.965 I llama_perf_context_print:        load time =     661.45 ms
0.01.354.966 I llama_perf_context_print: prompt eval time =      43.20 ms /     7 tokens (    6.17 ms per token,   162.03 tokens per second)
0.01.354.966 I llama_perf_context_print:        eval time =     635.56 ms /    63 runs   (   10.09 ms per token,    99.13 tokens per second)
0.01.354.967 I llama_perf_context_print:       total time =     681.54 ms /    70 tokens
0.01.355.246 I ggml_metal_free: deallocating

real	0m1.373s
user	0m0.112s
sys	0m0.131s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.087 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.483 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.251 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.015.255 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.260 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.261 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.263 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.263 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.263 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.264 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.264 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.265 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.265 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.266 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.269 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.270 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.271 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.271 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.272 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.977 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.986 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.686 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.687 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.688 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.688 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.688 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.688 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.023.689 I llama_model_loader: - type  f32:  194 tensors
0.00.023.689 I llama_model_loader: - type q4_0:   97 tensors
0.00.023.690 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.560 I llm_load_vocab: special tokens cache size = 25
0.00.049.405 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.408 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.408 I llm_load_print_meta: arch             = gptneox
0.00.049.409 I llm_load_print_meta: vocab type       = BPE
0.00.049.409 I llm_load_print_meta: n_vocab          = 50304
0.00.049.409 I llm_load_print_meta: n_merges         = 50009
0.00.049.409 I llm_load_print_meta: vocab_only       = 0
0.00.049.409 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.409 I llm_load_print_meta: n_embd           = 2048
0.00.049.410 I llm_load_print_meta: n_layer          = 24
0.00.049.412 I llm_load_print_meta: n_head           = 16
0.00.049.413 I llm_load_print_meta: n_head_kv        = 16
0.00.049.414 I llm_load_print_meta: n_rot            = 32
0.00.049.414 I llm_load_print_meta: n_swa            = 0
0.00.049.414 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.414 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.415 I llm_load_print_meta: n_gqa            = 1
0.00.049.416 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.416 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.417 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.417 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.417 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.418 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.418 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.418 I llm_load_print_meta: n_ff             = 8192
0.00.049.418 I llm_load_print_meta: n_expert         = 0
0.00.049.419 I llm_load_print_meta: n_expert_used    = 0
0.00.049.419 I llm_load_print_meta: causal attn      = 1
0.00.049.421 I llm_load_print_meta: pooling type     = 0
0.00.049.421 I llm_load_print_meta: rope type        = 2
0.00.049.421 I llm_load_print_meta: rope scaling     = linear
0.00.049.421 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.422 I llm_load_print_meta: freq_scale_train = 1
0.00.049.422 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.422 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.422 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.422 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.423 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.423 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.423 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.423 I llm_load_print_meta: model type       = 1.4B
0.00.049.424 I llm_load_print_meta: model ftype      = Q4_0
0.00.049.424 I llm_load_print_meta: model params     = 1.41 B
0.00.049.425 I llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
0.00.049.425 I llm_load_print_meta: general.name     = 1.4B
0.00.049.425 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.425 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.425 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.425 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.426 I llm_load_print_meta: LF token         = 128 ''
0.00.049.426 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.426 I llm_load_print_meta: max token length = 1024
0.00.051.374 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.374 I llm_load_tensors: offloading output layer to GPU
0.00.051.374 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.385 I llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.051.386 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
0.00.052.369 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.370 I llama_new_context_with_model: n_ctx         = 128
0.00.052.370 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.370 I llama_new_context_with_model: n_batch       = 128
0.00.052.370 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.370 I llama_new_context_with_model: flash_attn    = 0
0.00.052.371 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.371 I llama_new_context_with_model: freq_scale    = 1
0.00.052.371 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.372 I ggml_metal_init: allocating
0.00.052.375 I ggml_metal_init: found device: Apple M4
0.00.052.377 I ggml_metal_init: picking default device: Apple M4
0.00.052.949 I ggml_metal_init: using embedded metal library
0.00.055.276 I ggml_metal_init: GPU name:   Apple M4
0.00.055.278 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.278 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.279 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.279 I ggml_metal_init: simdgroup reduction   = true
0.00.055.279 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.279 I ggml_metal_init: has bfloat            = true
0.00.055.279 I ggml_metal_init: use bfloat            = true
0.00.055.280 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.280 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.006 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.372 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.374 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.390 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.258 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.259 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.260 I llama_new_context_with_model: graph nodes  = 967
0.00.067.260 I llama_new_context_with_model: graph splits = 2
0.00.067.261 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.261 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.602.554 I 
0.00.602.595 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.602.611 I perplexity: tokenizing the input ..
0.00.610.007 I perplexity: tokenization took 7.394 ms
0.00.610.013 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.818 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.733.485 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.733.498 I llama_perf_context_print:        load time =     593.06 ms
0.00.733.498 I llama_perf_context_print: prompt eval time =     121.58 ms /   128 tokens (    0.95 ms per token,  1052.81 tokens per second)
0.00.733.499 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.499 I llama_perf_context_print:       total time =     130.95 ms /   129 tokens
0.00.733.842 I ggml_metal_free: deallocating

real	0m0.749s
user	0m0.078s
sys	0m0.084s
```
- q4_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.048 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.847 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.023.650 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.023.657 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.023.658 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.023.659 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.023.659 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.023.665 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.023.665 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.023.666 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.023.666 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.023.666 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.023.667 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.023.667 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.023.667 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.023.668 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.023.670 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.023.670 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.023.670 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.027.465 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.028.536 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.032.343 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.032.345 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.032.345 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.032.345 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.032.346 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.032.346 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.032.347 I llama_model_loader: - type  f32:  194 tensors
0.00.032.347 I llama_model_loader: - type q4_1:   97 tensors
0.00.032.347 I llama_model_loader: - type q6_K:    1 tensors
0.00.053.783 I llm_load_vocab: special tokens cache size = 25
0.00.059.703 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.059.706 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.059.707 I llm_load_print_meta: arch             = gptneox
0.00.059.707 I llm_load_print_meta: vocab type       = BPE
0.00.059.707 I llm_load_print_meta: n_vocab          = 50304
0.00.059.707 I llm_load_print_meta: n_merges         = 50009
0.00.059.707 I llm_load_print_meta: vocab_only       = 0
0.00.059.708 I llm_load_print_meta: n_ctx_train      = 2048
0.00.059.708 I llm_load_print_meta: n_embd           = 2048
0.00.059.708 I llm_load_print_meta: n_layer          = 24
0.00.059.711 I llm_load_print_meta: n_head           = 16
0.00.059.712 I llm_load_print_meta: n_head_kv        = 16
0.00.059.712 I llm_load_print_meta: n_rot            = 32
0.00.059.712 I llm_load_print_meta: n_swa            = 0
0.00.059.712 I llm_load_print_meta: n_embd_head_k    = 128
0.00.059.713 I llm_load_print_meta: n_embd_head_v    = 128
0.00.059.713 I llm_load_print_meta: n_gqa            = 1
0.00.059.714 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.059.715 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.059.715 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.059.715 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.059.716 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.059.716 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.059.716 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.059.717 I llm_load_print_meta: n_ff             = 8192
0.00.059.717 I llm_load_print_meta: n_expert         = 0
0.00.059.717 I llm_load_print_meta: n_expert_used    = 0
0.00.059.717 I llm_load_print_meta: causal attn      = 1
0.00.059.717 I llm_load_print_meta: pooling type     = 0
0.00.059.717 I llm_load_print_meta: rope type        = 2
0.00.059.717 I llm_load_print_meta: rope scaling     = linear
0.00.059.718 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.059.718 I llm_load_print_meta: freq_scale_train = 1
0.00.059.718 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.059.719 I llm_load_print_meta: rope_finetuned   = unknown
0.00.059.719 I llm_load_print_meta: ssm_d_conv       = 0
0.00.059.721 I llm_load_print_meta: ssm_d_inner      = 0
0.00.059.721 I llm_load_print_meta: ssm_d_state      = 0
0.00.059.721 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.059.722 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.059.722 I llm_load_print_meta: model type       = 1.4B
0.00.059.722 I llm_load_print_meta: model ftype      = Q4_1
0.00.059.723 I llm_load_print_meta: model params     = 1.41 B
0.00.059.723 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.059.723 I llm_load_print_meta: general.name     = 1.4B
0.00.059.724 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.059.724 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.059.724 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.059.724 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.059.724 I llm_load_print_meta: LF token         = 128 ''
0.00.059.725 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.059.725 I llm_load_print_meta: max token length = 1024
0.00.061.721 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.061.722 I llm_load_tensors: offloading output layer to GPU
0.00.061.722 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.061.732 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.061.734 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.062.656 I llama_new_context_with_model: n_seq_max     = 1
0.00.062.657 I llama_new_context_with_model: n_ctx         = 2048
0.00.062.657 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.062.657 I llama_new_context_with_model: n_batch       = 2048
0.00.062.657 I llama_new_context_with_model: n_ubatch      = 512
0.00.062.658 I llama_new_context_with_model: flash_attn    = 0
0.00.062.658 I llama_new_context_with_model: freq_base     = 10000.0
0.00.062.658 I llama_new_context_with_model: freq_scale    = 1
0.00.062.659 I ggml_metal_init: allocating
0.00.062.662 I ggml_metal_init: found device: Apple M4
0.00.062.664 I ggml_metal_init: picking default device: Apple M4
0.00.063.287 I ggml_metal_init: using embedded metal library
0.00.065.689 I ggml_metal_init: GPU name:   Apple M4
0.00.065.691 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.065.691 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.065.692 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.065.692 I ggml_metal_init: simdgroup reduction   = true
0.00.065.692 I ggml_metal_init: simdgroup matrix mul. = true
0.00.065.692 I ggml_metal_init: has bfloat            = true
0.00.065.693 I ggml_metal_init: use bfloat            = true
0.00.065.693 I ggml_metal_init: hasUnifiedMemory      = true
0.00.065.694 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.076.194 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.097.937 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.097.949 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.097.971 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.098.954 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.098.956 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.098.956 I llama_new_context_with_model: graph nodes  = 967
0.00.098.957 I llama_new_context_with_model: graph splits = 2
0.00.098.962 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.099.100 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.099.101 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.048.932 I main: llama threadpool init, n_threads = 4
0.01.049.034 I 
0.01.049.102 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.049.105 I 
0.01.049.694 I sampler seed: 1234
0.01.049.708 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.049.790 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.049.796 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.049.796 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.769.113 I llama_perf_sampler_print:    sampling time =       1.37 ms /    71 runs   (    0.02 ms per token, 51787.02 tokens per second)
0.01.769.114 I llama_perf_context_print:        load time =    1040.06 ms
0.01.769.115 I llama_perf_context_print: prompt eval time =      40.32 ms /     7 tokens (    5.76 ms per token,   173.59 tokens per second)
0.01.769.115 I llama_perf_context_print:        eval time =     675.91 ms /    63 runs   (   10.73 ms per token,    93.21 tokens per second)
0.01.769.116 I llama_perf_context_print:       total time =     720.20 ms /    70 tokens
0.01.769.383 I ggml_metal_free: deallocating

real	0m1.788s
user	0m0.124s
sys	0m0.175s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.365 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.111 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.015.116 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.117 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.118 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.119 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.120 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.120 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.121 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.121 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.121 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.122 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.122 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.122 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.123 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.125 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.125 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.125 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.773 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.853 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.674 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.676 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.676 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.676 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.677 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.677 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.023.678 I llama_model_loader: - type  f32:  194 tensors
0.00.023.678 I llama_model_loader: - type q4_1:   97 tensors
0.00.023.678 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.861 I llm_load_vocab: special tokens cache size = 25
0.00.049.866 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.871 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.871 I llm_load_print_meta: arch             = gptneox
0.00.049.872 I llm_load_print_meta: vocab type       = BPE
0.00.049.874 I llm_load_print_meta: n_vocab          = 50304
0.00.049.874 I llm_load_print_meta: n_merges         = 50009
0.00.049.874 I llm_load_print_meta: vocab_only       = 0
0.00.049.875 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.875 I llm_load_print_meta: n_embd           = 2048
0.00.049.875 I llm_load_print_meta: n_layer          = 24
0.00.049.878 I llm_load_print_meta: n_head           = 16
0.00.049.879 I llm_load_print_meta: n_head_kv        = 16
0.00.049.879 I llm_load_print_meta: n_rot            = 32
0.00.049.880 I llm_load_print_meta: n_swa            = 0
0.00.049.880 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.882 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.883 I llm_load_print_meta: n_gqa            = 1
0.00.049.887 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.887 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.888 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.888 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.888 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.888 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.888 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.889 I llm_load_print_meta: n_ff             = 8192
0.00.049.889 I llm_load_print_meta: n_expert         = 0
0.00.049.889 I llm_load_print_meta: n_expert_used    = 0
0.00.049.889 I llm_load_print_meta: causal attn      = 1
0.00.049.890 I llm_load_print_meta: pooling type     = 0
0.00.049.891 I llm_load_print_meta: rope type        = 2
0.00.049.891 I llm_load_print_meta: rope scaling     = linear
0.00.049.891 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.892 I llm_load_print_meta: freq_scale_train = 1
0.00.049.892 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.892 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.892 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.892 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.892 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.893 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.893 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.893 I llm_load_print_meta: model type       = 1.4B
0.00.049.893 I llm_load_print_meta: model ftype      = Q4_1
0.00.049.894 I llm_load_print_meta: model params     = 1.41 B
0.00.049.894 I llm_load_print_meta: model size       = 864.46 MiB (5.13 BPW) 
0.00.049.894 I llm_load_print_meta: general.name     = 1.4B
0.00.049.894 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: LF token         = 128 ''
0.00.049.895 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.895 I llm_load_print_meta: max token length = 1024
0.00.051.907 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.907 I llm_load_tensors: offloading output layer to GPU
0.00.051.907 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.918 I llm_load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.051.920 I llm_load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
............................................................................
0.00.052.786 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.787 I llama_new_context_with_model: n_ctx         = 128
0.00.052.787 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.787 I llama_new_context_with_model: n_batch       = 128
0.00.052.787 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.787 I llama_new_context_with_model: flash_attn    = 0
0.00.052.788 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.788 I llama_new_context_with_model: freq_scale    = 1
0.00.052.788 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.789 I ggml_metal_init: allocating
0.00.052.795 I ggml_metal_init: found device: Apple M4
0.00.052.800 I ggml_metal_init: picking default device: Apple M4
0.00.053.392 I ggml_metal_init: using embedded metal library
0.00.055.767 I ggml_metal_init: GPU name:   Apple M4
0.00.055.768 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.769 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.769 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.769 I ggml_metal_init: simdgroup reduction   = true
0.00.055.770 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.770 I ggml_metal_init: has bfloat            = true
0.00.055.770 I ggml_metal_init: use bfloat            = true
0.00.055.770 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.771 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.153 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.491 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.500 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.517 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.348 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.349 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.350 I llama_new_context_with_model: graph nodes  = 967
0.00.067.350 I llama_new_context_with_model: graph splits = 2
0.00.067.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.684.043 I 
0.00.684.098 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.684.112 I perplexity: tokenizing the input ..
0.00.691.762 I perplexity: tokenization took 7.647 ms
0.00.691.767 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.813.642 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.815.048 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.815.062 I llama_perf_context_print:        load time =     674.67 ms
0.00.815.064 I llama_perf_context_print: prompt eval time =     121.63 ms /   128 tokens (    0.95 ms per token,  1052.35 tokens per second)
0.00.815.066 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.815.066 I llama_perf_context_print:       total time =     131.02 ms /   129 tokens
0.00.815.482 I ggml_metal_free: deallocating

real	0m0.831s
user	0m0.078s
sys	0m0.085s
```
- q5_0:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.046 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.075 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.011.377 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.671 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.017.676 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.677 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.678 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.678 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.679 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.679 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.680 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.680 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.681 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.681 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.681 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.682 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.682 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.684 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.685 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.685 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.501 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.597 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.409 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.026.410 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.410 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.411 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.411 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.411 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.026.412 I llama_model_loader: - type  f32:  194 tensors
0.00.026.412 I llama_model_loader: - type q5_0:   97 tensors
0.00.026.412 I llama_model_loader: - type q6_K:    1 tensors
0.00.047.016 I llm_load_vocab: special tokens cache size = 25
0.00.052.798 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.801 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.801 I llm_load_print_meta: arch             = gptneox
0.00.052.801 I llm_load_print_meta: vocab type       = BPE
0.00.052.802 I llm_load_print_meta: n_vocab          = 50304
0.00.052.802 I llm_load_print_meta: n_merges         = 50009
0.00.052.802 I llm_load_print_meta: vocab_only       = 0
0.00.052.802 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.802 I llm_load_print_meta: n_embd           = 2048
0.00.052.803 I llm_load_print_meta: n_layer          = 24
0.00.052.805 I llm_load_print_meta: n_head           = 16
0.00.052.806 I llm_load_print_meta: n_head_kv        = 16
0.00.052.806 I llm_load_print_meta: n_rot            = 32
0.00.052.806 I llm_load_print_meta: n_swa            = 0
0.00.052.806 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.806 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.807 I llm_load_print_meta: n_gqa            = 1
0.00.052.808 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.808 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.809 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.809 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.811 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.811 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.811 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.812 I llm_load_print_meta: n_ff             = 8192
0.00.052.812 I llm_load_print_meta: n_expert         = 0
0.00.052.812 I llm_load_print_meta: n_expert_used    = 0
0.00.052.812 I llm_load_print_meta: causal attn      = 1
0.00.052.815 I llm_load_print_meta: pooling type     = 0
0.00.052.815 I llm_load_print_meta: rope type        = 2
0.00.052.815 I llm_load_print_meta: rope scaling     = linear
0.00.052.815 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.816 I llm_load_print_meta: freq_scale_train = 1
0.00.052.816 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.816 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.816 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.816 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.816 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.817 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.817 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.817 I llm_load_print_meta: model type       = 1.4B
0.00.052.817 I llm_load_print_meta: model ftype      = Q5_0
0.00.052.818 I llm_load_print_meta: model params     = 1.41 B
0.00.052.818 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.052.819 I llm_load_print_meta: general.name     = 1.4B
0.00.052.823 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.823 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.823 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.823 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.823 I llm_load_print_meta: LF token         = 128 ''
0.00.052.824 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.824 I llm_load_print_meta: max token length = 1024
0.00.054.875 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.875 I llm_load_tensors: offloading output layer to GPU
0.00.054.875 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.886 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.054.887 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.055.802 I llama_new_context_with_model: n_seq_max     = 1
0.00.055.802 I llama_new_context_with_model: n_ctx         = 2048
0.00.055.803 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.055.803 I llama_new_context_with_model: n_batch       = 2048
0.00.055.803 I llama_new_context_with_model: n_ubatch      = 512
0.00.055.803 I llama_new_context_with_model: flash_attn    = 0
0.00.055.804 I llama_new_context_with_model: freq_base     = 10000.0
0.00.055.804 I llama_new_context_with_model: freq_scale    = 1
0.00.055.805 I ggml_metal_init: allocating
0.00.055.808 I ggml_metal_init: found device: Apple M4
0.00.055.810 I ggml_metal_init: picking default device: Apple M4
0.00.056.421 I ggml_metal_init: using embedded metal library
0.00.058.746 I ggml_metal_init: GPU name:   Apple M4
0.00.058.747 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.058.748 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.058.748 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.058.748 I ggml_metal_init: simdgroup reduction   = true
0.00.058.748 I ggml_metal_init: simdgroup matrix mul. = true
0.00.058.749 I ggml_metal_init: has bfloat            = true
0.00.058.749 I ggml_metal_init: use bfloat            = true
0.00.058.749 I ggml_metal_init: hasUnifiedMemory      = true
0.00.058.750 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.068.465 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.088.476 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.088.494 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.088.515 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.089.591 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.089.592 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.089.592 I llama_new_context_with_model: graph nodes  = 967
0.00.089.593 I llama_new_context_with_model: graph splits = 2
0.00.089.595 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.089.736 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.736 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.788.717 I main: llama threadpool init, n_threads = 4
0.00.788.750 I 
0.00.788.769 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.788.771 I 
0.00.789.005 I sampler seed: 1234
0.00.789.010 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.789.025 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.789.025 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.789.025 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.572.494 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59814.66 tokens per second)
0.01.572.495 I llama_perf_context_print:        load time =     777.34 ms
0.01.572.495 I llama_perf_context_print: prompt eval time =      43.16 ms /     7 tokens (    6.17 ms per token,   162.19 tokens per second)
0.01.572.496 I llama_perf_context_print:        eval time =     737.36 ms /    63 runs   (   11.70 ms per token,    85.44 tokens per second)
0.01.572.496 I llama_perf_context_print:       total time =     783.78 ms /    70 tokens
0.01.572.701 I ggml_metal_free: deallocating

real	0m1.590s
user	0m0.109s
sys	0m0.159s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.106 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.704 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.174 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.015.178 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.180 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.186 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.187 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.187 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.187 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.188 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.188 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.189 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.189 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.191 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.192 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.192 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.194 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.195 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.195 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.921 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.927 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.760 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.761 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.761 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.762 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.762 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.762 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.023.763 I llama_model_loader: - type  f32:  194 tensors
0.00.023.763 I llama_model_loader: - type q5_0:   97 tensors
0.00.023.764 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.407 I llm_load_vocab: special tokens cache size = 25
0.00.050.703 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.706 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.707 I llm_load_print_meta: arch             = gptneox
0.00.050.707 I llm_load_print_meta: vocab type       = BPE
0.00.050.707 I llm_load_print_meta: n_vocab          = 50304
0.00.050.708 I llm_load_print_meta: n_merges         = 50009
0.00.050.708 I llm_load_print_meta: vocab_only       = 0
0.00.050.708 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.708 I llm_load_print_meta: n_embd           = 2048
0.00.050.708 I llm_load_print_meta: n_layer          = 24
0.00.050.713 I llm_load_print_meta: n_head           = 16
0.00.050.713 I llm_load_print_meta: n_head_kv        = 16
0.00.050.714 I llm_load_print_meta: n_rot            = 32
0.00.050.714 I llm_load_print_meta: n_swa            = 0
0.00.050.714 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.714 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.715 I llm_load_print_meta: n_gqa            = 1
0.00.050.716 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.716 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.717 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.717 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.718 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.718 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.721 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.722 I llm_load_print_meta: n_ff             = 8192
0.00.050.723 I llm_load_print_meta: n_expert         = 0
0.00.050.723 I llm_load_print_meta: n_expert_used    = 0
0.00.050.723 I llm_load_print_meta: causal attn      = 1
0.00.050.724 I llm_load_print_meta: pooling type     = 0
0.00.050.724 I llm_load_print_meta: rope type        = 2
0.00.050.724 I llm_load_print_meta: rope scaling     = linear
0.00.050.724 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.725 I llm_load_print_meta: freq_scale_train = 1
0.00.050.725 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.725 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.725 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.725 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.725 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.725 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.726 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.726 I llm_load_print_meta: model type       = 1.4B
0.00.050.726 I llm_load_print_meta: model ftype      = Q5_0
0.00.050.727 I llm_load_print_meta: model params     = 1.41 B
0.00.050.727 I llm_load_print_meta: model size       = 942.60 MiB (5.59 BPW) 
0.00.050.727 I llm_load_print_meta: general.name     = 1.4B
0.00.050.731 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.731 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.731 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.731 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.731 I llm_load_print_meta: LF token         = 128 ''
0.00.050.732 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.732 I llm_load_print_meta: max token length = 1024
0.00.052.732 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.733 I llm_load_tensors: offloading output layer to GPU
0.00.052.733 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.744 I llm_load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.052.745 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
..........................................................................
0.00.053.650 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.651 I llama_new_context_with_model: n_ctx         = 128
0.00.053.651 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.651 I llama_new_context_with_model: n_batch       = 128
0.00.053.651 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.652 I llama_new_context_with_model: flash_attn    = 0
0.00.053.652 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.652 I llama_new_context_with_model: freq_scale    = 1
0.00.053.653 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.653 I ggml_metal_init: allocating
0.00.053.662 I ggml_metal_init: found device: Apple M4
0.00.053.664 I ggml_metal_init: picking default device: Apple M4
0.00.054.270 I ggml_metal_init: using embedded metal library
0.00.056.629 I ggml_metal_init: GPU name:   Apple M4
0.00.056.631 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.631 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.632 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.632 I ggml_metal_init: simdgroup reduction   = true
0.00.056.632 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.632 I ggml_metal_init: has bfloat            = true
0.00.056.633 I ggml_metal_init: use bfloat            = true
0.00.056.633 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.634 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.878 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.068.138 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.068.145 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.068.162 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.069.086 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.069.087 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.069.088 I llama_new_context_with_model: graph nodes  = 967
0.00.069.088 I llama_new_context_with_model: graph splits = 2
0.00.069.090 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.069.090 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.709.930 I 
0.00.709.964 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.709.990 I perplexity: tokenizing the input ..
0.00.718.115 I perplexity: tokenization took 8.123 ms
0.00.718.123 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.853.052 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.854.314 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.854.328 I llama_perf_context_print:        load time =     700.22 ms
0.00.854.332 I llama_perf_context_print: prompt eval time =     134.71 ms /   128 tokens (    1.05 ms per token,   950.22 tokens per second)
0.00.854.334 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.854.334 I llama_perf_context_print:       total time =     144.40 ms /   129 tokens
0.00.854.816 I ggml_metal_free: deallocating

real	0m0.871s
user	0m0.079s
sys	0m0.111s
```
- q5_1:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.008.821 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.016.142 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.146 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.148 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.148 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.148 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.149 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.150 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.150 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.151 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.151 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.151 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.152 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.152 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.156 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.156 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.157 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.948 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.967 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.742 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.743 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.743 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.744 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.744 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.024.744 I llama_model_loader: - type  f32:  194 tensors
0.00.024.745 I llama_model_loader: - type q5_1:   97 tensors
0.00.024.745 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.771 I llm_load_vocab: special tokens cache size = 25
0.00.050.619 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.622 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.622 I llm_load_print_meta: arch             = gptneox
0.00.050.623 I llm_load_print_meta: vocab type       = BPE
0.00.050.623 I llm_load_print_meta: n_vocab          = 50304
0.00.050.623 I llm_load_print_meta: n_merges         = 50009
0.00.050.623 I llm_load_print_meta: vocab_only       = 0
0.00.050.624 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.624 I llm_load_print_meta: n_embd           = 2048
0.00.050.624 I llm_load_print_meta: n_layer          = 24
0.00.050.627 I llm_load_print_meta: n_head           = 16
0.00.050.628 I llm_load_print_meta: n_head_kv        = 16
0.00.050.628 I llm_load_print_meta: n_rot            = 32
0.00.050.628 I llm_load_print_meta: n_swa            = 0
0.00.050.628 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.628 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.629 I llm_load_print_meta: n_gqa            = 1
0.00.050.630 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.631 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.631 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.634 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.634 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.634 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.634 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.635 I llm_load_print_meta: n_ff             = 8192
0.00.050.635 I llm_load_print_meta: n_expert         = 0
0.00.050.635 I llm_load_print_meta: n_expert_used    = 0
0.00.050.637 I llm_load_print_meta: causal attn      = 1
0.00.050.638 I llm_load_print_meta: pooling type     = 0
0.00.050.638 I llm_load_print_meta: rope type        = 2
0.00.050.638 I llm_load_print_meta: rope scaling     = linear
0.00.050.638 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.639 I llm_load_print_meta: freq_scale_train = 1
0.00.050.639 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.639 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.639 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.639 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.640 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.640 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.640 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.640 I llm_load_print_meta: model type       = 1.4B
0.00.050.640 I llm_load_print_meta: model ftype      = Q5_1
0.00.050.641 I llm_load_print_meta: model params     = 1.41 B
0.00.050.641 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.050.642 I llm_load_print_meta: general.name     = 1.4B
0.00.050.643 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.646 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.646 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.646 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.647 I llm_load_print_meta: LF token         = 128 ''
0.00.050.648 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.648 I llm_load_print_meta: max token length = 1024
0.00.052.650 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.650 I llm_load_tensors: offloading output layer to GPU
0.00.052.651 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.662 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.052.663 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.053.576 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.577 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.578 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.578 I llama_new_context_with_model: n_batch       = 2048
0.00.053.578 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.578 I llama_new_context_with_model: flash_attn    = 0
0.00.053.579 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.579 I llama_new_context_with_model: freq_scale    = 1
0.00.053.579 I ggml_metal_init: allocating
0.00.053.582 I ggml_metal_init: found device: Apple M4
0.00.053.585 I ggml_metal_init: picking default device: Apple M4
0.00.054.178 I ggml_metal_init: using embedded metal library
0.00.056.474 I ggml_metal_init: GPU name:   Apple M4
0.00.056.476 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.476 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.476 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.477 I ggml_metal_init: simdgroup reduction   = true
0.00.056.478 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.478 I ggml_metal_init: has bfloat            = true
0.00.056.478 I ggml_metal_init: use bfloat            = true
0.00.056.479 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.479 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.024 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.146 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.151 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.169 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.151 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.152 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.153 I llama_new_context_with_model: graph nodes  = 967
0.00.086.153 I llama_new_context_with_model: graph splits = 2
0.00.086.155 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.296 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.297 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.700.596 I main: llama threadpool init, n_threads = 4
0.00.700.632 I 
0.00.700.668 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.700.670 I 
0.00.700.911 I sampler seed: 1234
0.00.700.916 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.700.932 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.700.932 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.700.934 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.539.474 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61154.18 tokens per second)
0.01.539.475 I llama_perf_context_print:        load time =     691.77 ms
0.01.539.476 I llama_perf_context_print: prompt eval time =      42.26 ms /     7 tokens (    6.04 ms per token,   165.66 tokens per second)
0.01.539.477 I llama_perf_context_print:        eval time =     793.43 ms /    63 runs   (   12.59 ms per token,    79.40 tokens per second)
0.01.539.477 I llama_perf_context_print:       total time =     838.88 ms /    70 tokens
0.01.539.737 I ggml_metal_free: deallocating

real	0m1.556s
user	0m0.108s
sys	0m0.153s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.479 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.156 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.015.159 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.161 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.168 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.168 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.169 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.169 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.170 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.170 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.170 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.171 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.173 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.173 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.173 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.175 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.175 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.176 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.796 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.475 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.476 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.477 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.477 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.477 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.478 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.023.478 I llama_model_loader: - type  f32:  194 tensors
0.00.023.479 I llama_model_loader: - type q5_1:   97 tensors
0.00.023.479 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.375 I llm_load_vocab: special tokens cache size = 25
0.00.049.177 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.180 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.180 I llm_load_print_meta: arch             = gptneox
0.00.049.181 I llm_load_print_meta: vocab type       = BPE
0.00.049.181 I llm_load_print_meta: n_vocab          = 50304
0.00.049.181 I llm_load_print_meta: n_merges         = 50009
0.00.049.181 I llm_load_print_meta: vocab_only       = 0
0.00.049.181 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.182 I llm_load_print_meta: n_embd           = 2048
0.00.049.182 I llm_load_print_meta: n_layer          = 24
0.00.049.185 I llm_load_print_meta: n_head           = 16
0.00.049.185 I llm_load_print_meta: n_head_kv        = 16
0.00.049.186 I llm_load_print_meta: n_rot            = 32
0.00.049.188 I llm_load_print_meta: n_swa            = 0
0.00.049.188 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.188 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.189 I llm_load_print_meta: n_gqa            = 1
0.00.049.190 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.190 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.191 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.191 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.191 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.191 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.192 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.192 I llm_load_print_meta: n_ff             = 8192
0.00.049.194 I llm_load_print_meta: n_expert         = 0
0.00.049.194 I llm_load_print_meta: n_expert_used    = 0
0.00.049.194 I llm_load_print_meta: causal attn      = 1
0.00.049.194 I llm_load_print_meta: pooling type     = 0
0.00.049.194 I llm_load_print_meta: rope type        = 2
0.00.049.195 I llm_load_print_meta: rope scaling     = linear
0.00.049.195 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.195 I llm_load_print_meta: freq_scale_train = 1
0.00.049.195 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.196 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.196 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.196 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.196 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.196 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.196 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.197 I llm_load_print_meta: model type       = 1.4B
0.00.049.202 I llm_load_print_meta: model ftype      = Q5_1
0.00.049.202 I llm_load_print_meta: model params     = 1.41 B
0.00.049.204 I llm_load_print_meta: model size       = 1020.74 MiB (6.05 BPW) 
0.00.049.204 I llm_load_print_meta: general.name     = 1.4B
0.00.049.204 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.205 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.205 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.205 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.205 I llm_load_print_meta: LF token         = 128 ''
0.00.049.205 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.206 I llm_load_print_meta: max token length = 1024
0.00.051.214 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.214 I llm_load_tensors: offloading output layer to GPU
0.00.051.214 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.225 I llm_load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.051.226 I llm_load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
..............................................................................
0.00.052.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.148 I llama_new_context_with_model: n_ctx         = 128
0.00.052.148 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.148 I llama_new_context_with_model: n_batch       = 128
0.00.052.148 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.149 I llama_new_context_with_model: flash_attn    = 0
0.00.052.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.149 I llama_new_context_with_model: freq_scale    = 1
0.00.052.150 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.150 I ggml_metal_init: allocating
0.00.052.153 I ggml_metal_init: found device: Apple M4
0.00.052.155 I ggml_metal_init: picking default device: Apple M4
0.00.052.701 I ggml_metal_init: using embedded metal library
0.00.055.033 I ggml_metal_init: GPU name:   Apple M4
0.00.055.035 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.035 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.035 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.036 I ggml_metal_init: simdgroup reduction   = true
0.00.055.036 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.036 I ggml_metal_init: has bfloat            = true
0.00.055.036 I ggml_metal_init: use bfloat            = true
0.00.055.036 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.037 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.482 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.781 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.783 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.808 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.702 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.703 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.704 I llama_new_context_with_model: graph nodes  = 967
0.00.066.704 I llama_new_context_with_model: graph splits = 2
0.00.066.705 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.705 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.635.722 I 
0.00.635.764 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.635.788 I perplexity: tokenizing the input ..
0.00.643.801 I perplexity: tokenization took 8.01 ms
0.00.643.804 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.778.441 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.779.589 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.779.611 I llama_perf_context_print:        load time =     626.23 ms
0.00.779.612 I llama_perf_context_print: prompt eval time =     134.41 ms /   128 tokens (    1.05 ms per token,   952.30 tokens per second)
0.00.779.613 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.779.614 I llama_perf_context_print:       total time =     143.90 ms /   129 tokens
0.00.780.092 I ggml_metal_free: deallocating

real	0m0.794s
user	0m0.077s
sys	0m0.105s
```
- q2_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.045 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.076 I main: llama backend init
0.00.000.078 I main: load the model and apply lora adapter, if any
0.00.009.831 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.334 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.338 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.339 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.340 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.340 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.340 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.341 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.342 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.342 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.342 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.342 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.343 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.343 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.344 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.345 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.345 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.345 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.259 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.337 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.186 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.187 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.188 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.188 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.188 I llama_model_loader: - type  f32:  194 tensors
0.00.024.189 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.189 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.189 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.902 I llm_load_vocab: special tokens cache size = 25
0.00.050.935 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.937 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.938 I llm_load_print_meta: arch             = gptneox
0.00.050.938 I llm_load_print_meta: vocab type       = BPE
0.00.050.939 I llm_load_print_meta: n_vocab          = 50304
0.00.050.939 I llm_load_print_meta: n_merges         = 50009
0.00.050.939 I llm_load_print_meta: vocab_only       = 0
0.00.050.939 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.939 I llm_load_print_meta: n_embd           = 2048
0.00.050.939 I llm_load_print_meta: n_layer          = 24
0.00.050.942 I llm_load_print_meta: n_head           = 16
0.00.050.943 I llm_load_print_meta: n_head_kv        = 16
0.00.050.943 I llm_load_print_meta: n_rot            = 32
0.00.050.943 I llm_load_print_meta: n_swa            = 0
0.00.050.945 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.945 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.945 I llm_load_print_meta: n_gqa            = 1
0.00.050.946 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.947 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.947 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.948 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.948 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.948 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.948 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.949 I llm_load_print_meta: n_ff             = 8192
0.00.050.949 I llm_load_print_meta: n_expert         = 0
0.00.050.949 I llm_load_print_meta: n_expert_used    = 0
0.00.050.950 I llm_load_print_meta: causal attn      = 1
0.00.050.950 I llm_load_print_meta: pooling type     = 0
0.00.050.950 I llm_load_print_meta: rope type        = 2
0.00.050.950 I llm_load_print_meta: rope scaling     = linear
0.00.050.950 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.951 I llm_load_print_meta: freq_scale_train = 1
0.00.050.951 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.951 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.951 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.951 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.952 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.952 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.954 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.954 I llm_load_print_meta: model type       = 1.4B
0.00.050.955 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.050.955 I llm_load_print_meta: model params     = 1.41 B
0.00.050.956 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.050.956 I llm_load_print_meta: general.name     = 1.4B
0.00.050.956 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.956 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.956 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.961 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.961 I llm_load_print_meta: LF token         = 128 ''
0.00.050.961 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.961 I llm_load_print_meta: max token length = 1024
0.00.052.889 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.889 I llm_load_tensors: offloading output layer to GPU
0.00.052.889 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.900 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.052.901 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.053.837 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.838 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.838 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.838 I llama_new_context_with_model: n_batch       = 2048
0.00.053.838 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.839 I llama_new_context_with_model: flash_attn    = 0
0.00.053.839 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.839 I llama_new_context_with_model: freq_scale    = 1
0.00.053.840 I ggml_metal_init: allocating
0.00.053.843 I ggml_metal_init: found device: Apple M4
0.00.053.845 I ggml_metal_init: picking default device: Apple M4
0.00.054.447 I ggml_metal_init: using embedded metal library
0.00.056.740 I ggml_metal_init: GPU name:   Apple M4
0.00.056.741 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.742 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.742 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.742 I ggml_metal_init: simdgroup reduction   = true
0.00.056.743 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.743 I ggml_metal_init: has bfloat            = true
0.00.056.743 I ggml_metal_init: use bfloat            = true
0.00.056.743 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.744 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.537 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.085.535 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.085.540 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.085.558 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.086.621 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.086.623 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.086.624 I llama_new_context_with_model: graph nodes  = 967
0.00.086.624 I llama_new_context_with_model: graph splits = 2
0.00.086.627 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.086.774 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.086.775 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.443.888 I main: llama threadpool init, n_threads = 4
0.00.443.934 I 
0.00.443.960 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.443.960 I 
0.00.444.204 I sampler seed: 1234
0.00.444.210 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.444.226 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.444.227 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.444.227 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.123.888 I llama_perf_sampler_print:    sampling time =       1.19 ms /    71 runs   (    0.02 ms per token, 59915.61 tokens per second)
0.01.123.888 I llama_perf_context_print:        load time =     434.05 ms
0.01.123.889 I llama_perf_context_print: prompt eval time =      35.80 ms /     7 tokens (    5.11 ms per token,   195.52 tokens per second)
0.01.123.893 I llama_perf_context_print:        eval time =     640.89 ms /    63 runs   (   10.17 ms per token,    98.30 tokens per second)
0.01.123.894 I llama_perf_context_print:       total time =     680.01 ms /    70 tokens
0.01.124.101 I ggml_metal_free: deallocating

real	0m1.141s
user	0m0.110s
sys	0m0.109s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.090 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.380 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.840 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.014.844 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.846 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.846 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.847 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.847 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.847 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.848 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.850 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.851 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.851 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.551 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.612 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.419 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.420 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.421 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.421 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.421 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.422 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.023.422 I llama_model_loader: - type  f32:  194 tensors
0.00.023.422 I llama_model_loader: - type q2_K:   49 tensors
0.00.023.423 I llama_model_loader: - type q3_K:   48 tensors
0.00.023.423 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.170 I llm_load_vocab: special tokens cache size = 25
0.00.049.074 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.076 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.077 I llm_load_print_meta: arch             = gptneox
0.00.049.077 I llm_load_print_meta: vocab type       = BPE
0.00.049.078 I llm_load_print_meta: n_vocab          = 50304
0.00.049.078 I llm_load_print_meta: n_merges         = 50009
0.00.049.078 I llm_load_print_meta: vocab_only       = 0
0.00.049.078 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.078 I llm_load_print_meta: n_embd           = 2048
0.00.049.079 I llm_load_print_meta: n_layer          = 24
0.00.049.081 I llm_load_print_meta: n_head           = 16
0.00.049.082 I llm_load_print_meta: n_head_kv        = 16
0.00.049.083 I llm_load_print_meta: n_rot            = 32
0.00.049.083 I llm_load_print_meta: n_swa            = 0
0.00.049.083 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.083 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.084 I llm_load_print_meta: n_gqa            = 1
0.00.049.085 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.085 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.086 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.086 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.086 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.086 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.086 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.087 I llm_load_print_meta: n_ff             = 8192
0.00.049.087 I llm_load_print_meta: n_expert         = 0
0.00.049.087 I llm_load_print_meta: n_expert_used    = 0
0.00.049.088 I llm_load_print_meta: causal attn      = 1
0.00.049.088 I llm_load_print_meta: pooling type     = 0
0.00.049.089 I llm_load_print_meta: rope type        = 2
0.00.049.089 I llm_load_print_meta: rope scaling     = linear
0.00.049.090 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.090 I llm_load_print_meta: freq_scale_train = 1
0.00.049.091 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.093 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.093 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.093 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.093 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.093 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.093 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.094 I llm_load_print_meta: model type       = 1.4B
0.00.049.094 I llm_load_print_meta: model ftype      = Q2_K - Medium
0.00.049.094 I llm_load_print_meta: model params     = 1.41 B
0.00.049.095 I llm_load_print_meta: model size       = 542.04 MiB (3.21 BPW) 
0.00.049.099 I llm_load_print_meta: general.name     = 1.4B
0.00.049.099 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.099 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.100 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.102 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.103 I llm_load_print_meta: LF token         = 128 ''
0.00.049.103 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.103 I llm_load_print_meta: max token length = 1024
0.00.051.029 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.030 I llm_load_tensors: offloading output layer to GPU
0.00.051.030 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.041 I llm_load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.051.042 I llm_load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
............................................................................
0.00.051.931 I llama_new_context_with_model: n_seq_max     = 1
0.00.051.932 I llama_new_context_with_model: n_ctx         = 128
0.00.051.932 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.051.932 I llama_new_context_with_model: n_batch       = 128
0.00.051.932 I llama_new_context_with_model: n_ubatch      = 128
0.00.051.932 I llama_new_context_with_model: flash_attn    = 0
0.00.051.933 I llama_new_context_with_model: freq_base     = 10000.0
0.00.051.933 I llama_new_context_with_model: freq_scale    = 1
0.00.051.933 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.051.934 I ggml_metal_init: allocating
0.00.051.939 I ggml_metal_init: found device: Apple M4
0.00.051.941 I ggml_metal_init: picking default device: Apple M4
0.00.052.478 I ggml_metal_init: using embedded metal library
0.00.054.792 I ggml_metal_init: GPU name:   Apple M4
0.00.054.794 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.054.794 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.054.794 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.054.795 I ggml_metal_init: simdgroup reduction   = true
0.00.054.795 I ggml_metal_init: simdgroup matrix mul. = true
0.00.054.795 I ggml_metal_init: has bfloat            = true
0.00.054.795 I ggml_metal_init: use bfloat            = true
0.00.054.796 I ggml_metal_init: hasUnifiedMemory      = true
0.00.054.796 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.201 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.065.460 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.065.464 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.065.480 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.066.348 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.066.349 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.066.349 I llama_new_context_with_model: graph nodes  = 967
0.00.066.350 I llama_new_context_with_model: graph splits = 2
0.00.066.351 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.066.351 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.375.877 I 
0.00.375.906 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.375.921 I perplexity: tokenizing the input ..
0.00.383.516 I perplexity: tokenization took 7.593 ms
0.00.383.520 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.516.267 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.517.490 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.517.511 I llama_perf_context_print:        load time =     366.49 ms
0.00.517.513 I llama_perf_context_print: prompt eval time =     132.52 ms /   128 tokens (    1.04 ms per token,   965.93 tokens per second)
0.00.517.513 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.517.514 I llama_perf_context_print:       total time =     141.64 ms /   129 tokens
0.00.517.883 I ggml_metal_free: deallocating

real	0m0.533s
user	0m0.076s
sys	0m0.068s
```
- q3_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.072 I main: llama backend init
0.00.000.074 I main: load the model and apply lora adapter, if any
0.00.009.024 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.313 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.318 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.319 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.320 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.320 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.320 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.321 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.321 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.322 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.322 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.322 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.323 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.323 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.323 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.326 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.326 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.327 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.219 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.259 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.117 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.118 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.118 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.119 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.119 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.119 I llama_model_loader: - type  f32:  194 tensors
0.00.024.120 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.120 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.120 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.121 I llama_model_loader: - type q6_K:    1 tensors
0.00.043.990 I llm_load_vocab: special tokens cache size = 25
0.00.049.888 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.890 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.891 I llm_load_print_meta: arch             = gptneox
0.00.049.891 I llm_load_print_meta: vocab type       = BPE
0.00.049.891 I llm_load_print_meta: n_vocab          = 50304
0.00.049.891 I llm_load_print_meta: n_merges         = 50009
0.00.049.892 I llm_load_print_meta: vocab_only       = 0
0.00.049.892 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.892 I llm_load_print_meta: n_embd           = 2048
0.00.049.892 I llm_load_print_meta: n_layer          = 24
0.00.049.895 I llm_load_print_meta: n_head           = 16
0.00.049.896 I llm_load_print_meta: n_head_kv        = 16
0.00.049.896 I llm_load_print_meta: n_rot            = 32
0.00.049.896 I llm_load_print_meta: n_swa            = 0
0.00.049.896 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.899 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.900 I llm_load_print_meta: n_gqa            = 1
0.00.049.900 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.901 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.902 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.902 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.902 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.903 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.903 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.904 I llm_load_print_meta: n_ff             = 8192
0.00.049.905 I llm_load_print_meta: n_expert         = 0
0.00.049.906 I llm_load_print_meta: n_expert_used    = 0
0.00.049.906 I llm_load_print_meta: causal attn      = 1
0.00.049.906 I llm_load_print_meta: pooling type     = 0
0.00.049.907 I llm_load_print_meta: rope type        = 2
0.00.049.907 I llm_load_print_meta: rope scaling     = linear
0.00.049.907 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.908 I llm_load_print_meta: freq_scale_train = 1
0.00.049.908 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.908 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.908 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.908 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.908 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.909 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.909 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.909 I llm_load_print_meta: model type       = 1.4B
0.00.049.910 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.049.910 I llm_load_print_meta: model params     = 1.41 B
0.00.049.911 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.049.911 I llm_load_print_meta: general.name     = 1.4B
0.00.049.911 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.911 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.911 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.911 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.912 I llm_load_print_meta: LF token         = 128 ''
0.00.049.915 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.915 I llm_load_print_meta: max token length = 1024
0.00.051.866 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.867 I llm_load_tensors: offloading output layer to GPU
0.00.051.867 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.877 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.051.879 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.052.733 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.734 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.734 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.734 I llama_new_context_with_model: n_batch       = 2048
0.00.052.734 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.735 I llama_new_context_with_model: flash_attn    = 0
0.00.052.735 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.735 I llama_new_context_with_model: freq_scale    = 1
0.00.052.736 I ggml_metal_init: allocating
0.00.052.744 I ggml_metal_init: found device: Apple M4
0.00.052.746 I ggml_metal_init: picking default device: Apple M4
0.00.053.354 I ggml_metal_init: using embedded metal library
0.00.055.679 I ggml_metal_init: GPU name:   Apple M4
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.682 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.683 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.683 I ggml_metal_init: simdgroup reduction   = true
0.00.055.683 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.683 I ggml_metal_init: has bfloat            = true
0.00.055.683 I ggml_metal_init: use bfloat            = true
0.00.055.684 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.684 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.064.986 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.564 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.573 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.594 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.563 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.565 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.565 I llama_new_context_with_model: graph nodes  = 967
0.00.085.565 I llama_new_context_with_model: graph splits = 2
0.00.085.568 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.684 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.684 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.528.619 I main: llama threadpool init, n_threads = 4
0.00.528.658 I 
0.00.528.684 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.528.684 I 
0.00.528.917 I sampler seed: 1234
0.00.528.923 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.528.938 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.528.938 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.528.938 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.273.536 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52282.77 tokens per second)
0.01.273.537 I llama_perf_context_print:        load time =     519.59 ms
0.01.273.538 I llama_perf_context_print: prompt eval time =      40.45 ms /     7 tokens (    5.78 ms per token,   173.07 tokens per second)
0.01.273.538 I llama_perf_context_print:        eval time =     701.29 ms /    63 runs   (   11.13 ms per token,    89.83 tokens per second)
0.01.273.539 I llama_perf_context_print:       total time =     744.92 ms /    70 tokens
0.01.273.781 I ggml_metal_free: deallocating

real	0m1.290s
user	0m0.109s
sys	0m0.123s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.093 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.244 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.974 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.015.978 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.984 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.984 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.987 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.987 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.988 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.989 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.989 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.989 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.990 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.993 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.993 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.994 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.995 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.995 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.996 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.674 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.735 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.554 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.555 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.555 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.556 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.556 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.556 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.024.557 I llama_model_loader: - type  f32:  194 tensors
0.00.024.557 I llama_model_loader: - type q3_K:   25 tensors
0.00.024.557 I llama_model_loader: - type q4_K:   71 tensors
0.00.024.558 I llama_model_loader: - type q5_K:    1 tensors
0.00.024.558 I llama_model_loader: - type q6_K:    1 tensors
0.00.044.232 I llm_load_vocab: special tokens cache size = 25
0.00.050.391 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.394 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.394 I llm_load_print_meta: arch             = gptneox
0.00.050.394 I llm_load_print_meta: vocab type       = BPE
0.00.050.394 I llm_load_print_meta: n_vocab          = 50304
0.00.050.395 I llm_load_print_meta: n_merges         = 50009
0.00.050.395 I llm_load_print_meta: vocab_only       = 0
0.00.050.395 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.395 I llm_load_print_meta: n_embd           = 2048
0.00.050.395 I llm_load_print_meta: n_layer          = 24
0.00.050.398 I llm_load_print_meta: n_head           = 16
0.00.050.399 I llm_load_print_meta: n_head_kv        = 16
0.00.050.399 I llm_load_print_meta: n_rot            = 32
0.00.050.399 I llm_load_print_meta: n_swa            = 0
0.00.050.400 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.402 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.403 I llm_load_print_meta: n_gqa            = 1
0.00.050.403 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.405 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.406 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.407 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.407 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.408 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.408 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.408 I llm_load_print_meta: n_ff             = 8192
0.00.050.409 I llm_load_print_meta: n_expert         = 0
0.00.050.410 I llm_load_print_meta: n_expert_used    = 0
0.00.050.410 I llm_load_print_meta: causal attn      = 1
0.00.050.410 I llm_load_print_meta: pooling type     = 0
0.00.050.410 I llm_load_print_meta: rope type        = 2
0.00.050.411 I llm_load_print_meta: rope scaling     = linear
0.00.050.411 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.411 I llm_load_print_meta: freq_scale_train = 1
0.00.050.411 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.412 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.412 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.412 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.412 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.412 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.412 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.413 I llm_load_print_meta: model type       = 1.4B
0.00.050.413 I llm_load_print_meta: model ftype      = Q3_K - Medium
0.00.050.414 I llm_load_print_meta: model params     = 1.41 B
0.00.050.414 I llm_load_print_meta: model size       = 724.27 MiB (4.29 BPW) 
0.00.050.415 I llm_load_print_meta: general.name     = 1.4B
0.00.050.415 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.415 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.415 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.415 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.416 I llm_load_print_meta: LF token         = 128 ''
0.00.050.416 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.416 I llm_load_print_meta: max token length = 1024
0.00.052.386 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.386 I llm_load_tensors: offloading output layer to GPU
0.00.052.386 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.397 I llm_load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.052.398 I llm_load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
................................................................................
0.00.053.306 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.307 I llama_new_context_with_model: n_ctx         = 128
0.00.053.307 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.307 I llama_new_context_with_model: n_batch       = 128
0.00.053.307 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.307 I llama_new_context_with_model: flash_attn    = 0
0.00.053.308 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.308 I llama_new_context_with_model: freq_scale    = 1
0.00.053.308 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.309 I ggml_metal_init: allocating
0.00.053.315 I ggml_metal_init: found device: Apple M4
0.00.053.317 I ggml_metal_init: picking default device: Apple M4
0.00.053.913 I ggml_metal_init: using embedded metal library
0.00.056.264 I ggml_metal_init: GPU name:   Apple M4
0.00.056.265 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.266 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.266 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.266 I ggml_metal_init: simdgroup reduction   = true
0.00.056.266 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.267 I ggml_metal_init: has bfloat            = true
0.00.056.267 I ggml_metal_init: use bfloat            = true
0.00.056.267 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.268 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.665 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.295 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.299 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.314 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.224 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.225 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.225 I llama_new_context_with_model: graph nodes  = 967
0.00.068.225 I llama_new_context_with_model: graph splits = 2
0.00.068.226 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.227 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.474.004 I 
0.00.474.036 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.474.052 I perplexity: tokenizing the input ..
0.00.481.992 I perplexity: tokenization took 7.938 ms
0.00.481.996 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.613.526 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.614.740 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.614.757 I llama_perf_context_print:        load time =     463.75 ms
0.00.614.758 I llama_perf_context_print: prompt eval time =     131.30 ms /   128 tokens (    1.03 ms per token,   974.85 tokens per second)
0.00.614.759 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.614.760 I llama_perf_context_print:       total time =     140.76 ms /   129 tokens
0.00.615.320 I ggml_metal_free: deallocating

real	0m0.629s
user	0m0.077s
sys	0m0.083s
```
- q4_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.049 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.077 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.008.758 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.062 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.015.069 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.070 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.071 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.071 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.072 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.072 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.073 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.073 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.074 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.074 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.074 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.076 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.079 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.081 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.081 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.081 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.853 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.905 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.786 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.788 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.788 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.789 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.789 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.790 I llama_model_loader: - type  f32:  194 tensors
0.00.023.790 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.791 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.791 I llama_model_loader: - type q6_K:   13 tensors
0.00.044.801 I llm_load_vocab: special tokens cache size = 25
0.00.050.911 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.916 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.916 I llm_load_print_meta: arch             = gptneox
0.00.050.917 I llm_load_print_meta: vocab type       = BPE
0.00.050.917 I llm_load_print_meta: n_vocab          = 50304
0.00.050.917 I llm_load_print_meta: n_merges         = 50009
0.00.050.917 I llm_load_print_meta: vocab_only       = 0
0.00.050.917 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.918 I llm_load_print_meta: n_embd           = 2048
0.00.050.918 I llm_load_print_meta: n_layer          = 24
0.00.050.922 I llm_load_print_meta: n_head           = 16
0.00.050.923 I llm_load_print_meta: n_head_kv        = 16
0.00.050.923 I llm_load_print_meta: n_rot            = 32
0.00.050.923 I llm_load_print_meta: n_swa            = 0
0.00.050.923 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.926 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.926 I llm_load_print_meta: n_gqa            = 1
0.00.050.927 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.928 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.928 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.929 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.929 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.930 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.930 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.930 I llm_load_print_meta: n_ff             = 8192
0.00.050.930 I llm_load_print_meta: n_expert         = 0
0.00.050.931 I llm_load_print_meta: n_expert_used    = 0
0.00.050.932 I llm_load_print_meta: causal attn      = 1
0.00.050.932 I llm_load_print_meta: pooling type     = 0
0.00.050.932 I llm_load_print_meta: rope type        = 2
0.00.050.932 I llm_load_print_meta: rope scaling     = linear
0.00.050.933 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.933 I llm_load_print_meta: freq_scale_train = 1
0.00.050.933 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.933 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.934 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.934 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.934 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.934 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.934 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.934 I llm_load_print_meta: model type       = 1.4B
0.00.050.935 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.050.935 I llm_load_print_meta: model params     = 1.41 B
0.00.050.936 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.050.936 I llm_load_print_meta: general.name     = 1.4B
0.00.050.937 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.937 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.937 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.937 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.938 I llm_load_print_meta: LF token         = 128 ''
0.00.050.938 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.938 I llm_load_print_meta: max token length = 1024
0.00.052.885 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.885 I llm_load_tensors: offloading output layer to GPU
0.00.052.885 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.896 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.052.898 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.053.766 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.767 I llama_new_context_with_model: n_ctx         = 2048
0.00.053.767 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.053.768 I llama_new_context_with_model: n_batch       = 2048
0.00.053.768 I llama_new_context_with_model: n_ubatch      = 512
0.00.053.768 I llama_new_context_with_model: flash_attn    = 0
0.00.053.769 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.769 I llama_new_context_with_model: freq_scale    = 1
0.00.053.770 I ggml_metal_init: allocating
0.00.053.774 I ggml_metal_init: found device: Apple M4
0.00.053.777 I ggml_metal_init: picking default device: Apple M4
0.00.054.412 I ggml_metal_init: using embedded metal library
0.00.056.805 I ggml_metal_init: GPU name:   Apple M4
0.00.056.807 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.807 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.807 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.808 I ggml_metal_init: simdgroup reduction   = true
0.00.056.808 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.808 I ggml_metal_init: has bfloat            = true
0.00.056.808 I ggml_metal_init: use bfloat            = true
0.00.056.809 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.810 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.037 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.911 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.918 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.939 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.931 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.933 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.933 I llama_new_context_with_model: graph nodes  = 967
0.00.087.933 I llama_new_context_with_model: graph splits = 2
0.00.087.936 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.088.065 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.088.065 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.615.922 I main: llama threadpool init, n_threads = 4
0.00.615.962 I 
0.00.615.983 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.615.983 I 
0.00.616.211 I sampler seed: 1234
0.00.616.216 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.616.269 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.616.271 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.616.271 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.373.653 I llama_perf_sampler_print:    sampling time =       1.29 ms /    71 runs   (    0.02 ms per token, 54996.13 tokens per second)
0.01.373.654 I llama_perf_context_print:        load time =     607.15 ms
0.01.373.655 I llama_perf_context_print: prompt eval time =      51.02 ms /     7 tokens (    7.29 ms per token,   137.21 tokens per second)
0.01.373.656 I llama_perf_context_print:        eval time =     703.34 ms /    63 runs   (   11.16 ms per token,    89.57 tokens per second)
0.01.373.656 I llama_perf_context_print:       total time =     757.74 ms /    70 tokens
0.01.373.878 I ggml_metal_free: deallocating

real	0m1.391s
user	0m0.111s
sys	0m0.140s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.092 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.160 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.014.833 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.014.838 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.014.839 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.014.840 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.014.840 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.014.840 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.014.841 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.014.841 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.014.842 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.014.842 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.014.842 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.014.843 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.014.843 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.014.844 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.014.845 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.014.845 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.014.846 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.537 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.597 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.340 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.341 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.342 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.342 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.342 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.343 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.023.343 I llama_model_loader: - type  f32:  194 tensors
0.00.023.343 I llama_model_loader: - type q4_K:   61 tensors
0.00.023.343 I llama_model_loader: - type q5_K:   24 tensors
0.00.023.344 I llama_model_loader: - type q6_K:   13 tensors
0.00.043.830 I llm_load_vocab: special tokens cache size = 25
0.00.049.629 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.633 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.634 I llm_load_print_meta: arch             = gptneox
0.00.049.634 I llm_load_print_meta: vocab type       = BPE
0.00.049.635 I llm_load_print_meta: n_vocab          = 50304
0.00.049.635 I llm_load_print_meta: n_merges         = 50009
0.00.049.635 I llm_load_print_meta: vocab_only       = 0
0.00.049.635 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.635 I llm_load_print_meta: n_embd           = 2048
0.00.049.635 I llm_load_print_meta: n_layer          = 24
0.00.049.639 I llm_load_print_meta: n_head           = 16
0.00.049.640 I llm_load_print_meta: n_head_kv        = 16
0.00.049.640 I llm_load_print_meta: n_rot            = 32
0.00.049.640 I llm_load_print_meta: n_swa            = 0
0.00.049.641 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.641 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.642 I llm_load_print_meta: n_gqa            = 1
0.00.049.642 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.643 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.644 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.644 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.644 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.644 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.644 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.645 I llm_load_print_meta: n_ff             = 8192
0.00.049.645 I llm_load_print_meta: n_expert         = 0
0.00.049.647 I llm_load_print_meta: n_expert_used    = 0
0.00.049.647 I llm_load_print_meta: causal attn      = 1
0.00.049.647 I llm_load_print_meta: pooling type     = 0
0.00.049.647 I llm_load_print_meta: rope type        = 2
0.00.049.648 I llm_load_print_meta: rope scaling     = linear
0.00.049.648 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.648 I llm_load_print_meta: freq_scale_train = 1
0.00.049.648 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.649 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.649 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.649 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.649 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.649 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.649 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.650 I llm_load_print_meta: model type       = 1.4B
0.00.049.652 I llm_load_print_meta: model ftype      = Q4_K - Medium
0.00.049.652 I llm_load_print_meta: model params     = 1.41 B
0.00.049.653 I llm_load_print_meta: model size       = 871.81 MiB (5.17 BPW) 
0.00.049.653 I llm_load_print_meta: general.name     = 1.4B
0.00.049.653 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.653 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.653 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.653 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.654 I llm_load_print_meta: LF token         = 128 ''
0.00.049.654 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.654 I llm_load_print_meta: max token length = 1024
0.00.051.577 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.578 I llm_load_tensors: offloading output layer to GPU
0.00.051.578 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.589 I llm_load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.051.591 I llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
..............................................................................
0.00.052.473 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.474 I llama_new_context_with_model: n_ctx         = 128
0.00.052.474 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.052.474 I llama_new_context_with_model: n_batch       = 128
0.00.052.474 I llama_new_context_with_model: n_ubatch      = 128
0.00.052.475 I llama_new_context_with_model: flash_attn    = 0
0.00.052.475 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.475 I llama_new_context_with_model: freq_scale    = 1
0.00.052.476 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.052.477 I ggml_metal_init: allocating
0.00.052.480 I ggml_metal_init: found device: Apple M4
0.00.052.482 I ggml_metal_init: picking default device: Apple M4
0.00.053.082 I ggml_metal_init: using embedded metal library
0.00.055.480 I ggml_metal_init: GPU name:   Apple M4
0.00.055.481 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.482 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.482 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.482 I ggml_metal_init: simdgroup reduction   = true
0.00.055.482 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.483 I ggml_metal_init: has bfloat            = true
0.00.055.483 I ggml_metal_init: use bfloat            = true
0.00.055.483 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.484 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.568 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.853 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.855 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.870 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.770 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.771 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.771 I llama_new_context_with_model: graph nodes  = 967
0.00.067.771 I llama_new_context_with_model: graph splits = 2
0.00.067.773 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.773 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.553.380 I 
0.00.553.415 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.553.426 I perplexity: tokenizing the input ..
0.00.561.235 I perplexity: tokenization took 7.808 ms
0.00.561.239 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.695.963 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.697.312 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.697.333 I llama_perf_context_print:        load time =     544.22 ms
0.00.697.340 I llama_perf_context_print: prompt eval time =     134.49 ms /   128 tokens (    1.05 ms per token,   951.77 tokens per second)
0.00.697.342 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.697.342 I llama_perf_context_print:       total time =     143.95 ms /   129 tokens
0.00.697.900 I ggml_metal_free: deallocating

real	0m0.712s
user	0m0.078s
sys	0m0.096s
```
- q5_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.047 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.078 I main: llama backend init
0.00.000.080 I main: load the model and apply lora adapter, if any
0.00.011.469 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.487 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.491 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.493 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.493 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.494 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.494 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.494 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.495 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.496 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.496 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.496 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.497 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.497 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.497 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.500 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.500 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.501 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.202 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.256 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.964 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.965 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.965 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.966 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.966 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.966 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.967 I llama_model_loader: - type  f32:  194 tensors
0.00.025.967 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.968 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.982 I llm_load_vocab: special tokens cache size = 25
0.00.052.005 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.052.008 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.052.008 I llm_load_print_meta: arch             = gptneox
0.00.052.009 I llm_load_print_meta: vocab type       = BPE
0.00.052.009 I llm_load_print_meta: n_vocab          = 50304
0.00.052.009 I llm_load_print_meta: n_merges         = 50009
0.00.052.009 I llm_load_print_meta: vocab_only       = 0
0.00.052.009 I llm_load_print_meta: n_ctx_train      = 2048
0.00.052.010 I llm_load_print_meta: n_embd           = 2048
0.00.052.010 I llm_load_print_meta: n_layer          = 24
0.00.052.013 I llm_load_print_meta: n_head           = 16
0.00.052.014 I llm_load_print_meta: n_head_kv        = 16
0.00.052.014 I llm_load_print_meta: n_rot            = 32
0.00.052.014 I llm_load_print_meta: n_swa            = 0
0.00.052.014 I llm_load_print_meta: n_embd_head_k    = 128
0.00.052.014 I llm_load_print_meta: n_embd_head_v    = 128
0.00.052.015 I llm_load_print_meta: n_gqa            = 1
0.00.052.016 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.052.016 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.052.017 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.052.017 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.052.018 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.052.018 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.052.018 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.052.019 I llm_load_print_meta: n_ff             = 8192
0.00.052.019 I llm_load_print_meta: n_expert         = 0
0.00.052.019 I llm_load_print_meta: n_expert_used    = 0
0.00.052.020 I llm_load_print_meta: causal attn      = 1
0.00.052.023 I llm_load_print_meta: pooling type     = 0
0.00.052.023 I llm_load_print_meta: rope type        = 2
0.00.052.023 I llm_load_print_meta: rope scaling     = linear
0.00.052.023 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.052.024 I llm_load_print_meta: freq_scale_train = 1
0.00.052.024 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.052.026 I llm_load_print_meta: rope_finetuned   = unknown
0.00.052.026 I llm_load_print_meta: ssm_d_conv       = 0
0.00.052.026 I llm_load_print_meta: ssm_d_inner      = 0
0.00.052.026 I llm_load_print_meta: ssm_d_state      = 0
0.00.052.026 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.052.026 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.052.027 I llm_load_print_meta: model type       = 1.4B
0.00.052.027 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.052.027 I llm_load_print_meta: model params     = 1.41 B
0.00.052.028 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.052.028 I llm_load_print_meta: general.name     = 1.4B
0.00.052.029 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.052.029 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.052.029 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.052.029 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.052.033 I llm_load_print_meta: LF token         = 128 ''
0.00.052.033 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.052.033 I llm_load_print_meta: max token length = 1024
0.00.054.031 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.054.031 I llm_load_tensors: offloading output layer to GPU
0.00.054.031 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.054.041 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.054.043 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.902 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.903 I llama_new_context_with_model: n_ctx         = 2048
0.00.054.903 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.054.903 I llama_new_context_with_model: n_batch       = 2048
0.00.054.903 I llama_new_context_with_model: n_ubatch      = 512
0.00.054.904 I llama_new_context_with_model: flash_attn    = 0
0.00.054.904 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.904 I llama_new_context_with_model: freq_scale    = 1
0.00.054.905 I ggml_metal_init: allocating
0.00.054.912 I ggml_metal_init: found device: Apple M4
0.00.054.914 I ggml_metal_init: picking default device: Apple M4
0.00.055.529 I ggml_metal_init: using embedded metal library
0.00.057.844 I ggml_metal_init: GPU name:   Apple M4
0.00.057.845 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.847 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.847 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.847 I ggml_metal_init: simdgroup reduction   = true
0.00.057.847 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.848 I ggml_metal_init: has bfloat            = true
0.00.057.848 I ggml_metal_init: use bfloat            = true
0.00.057.848 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.849 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.067.371 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.086.463 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.086.470 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.086.498 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.087.575 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.087.577 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.087.577 I llama_new_context_with_model: graph nodes  = 967
0.00.087.578 I llama_new_context_with_model: graph splits = 2
0.00.087.580 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.087.728 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.087.729 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.710.440 I main: llama threadpool init, n_threads = 4
0.00.710.479 I 
0.00.710.510 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.710.510 I 
0.00.710.749 I sampler seed: 1234
0.00.710.753 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.710.768 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.710.768 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.710.770 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.556.601 I llama_perf_sampler_print:    sampling time =       1.16 ms /    71 runs   (    0.02 ms per token, 61418.69 tokens per second)
0.01.556.602 I llama_perf_context_print:        load time =     698.97 ms
0.01.556.603 I llama_perf_context_print: prompt eval time =      51.56 ms /     7 tokens (    7.37 ms per token,   135.77 tokens per second)
0.01.556.603 I llama_perf_context_print:        eval time =     791.34 ms /    63 runs   (   12.56 ms per token,    79.61 tokens per second)
0.01.556.604 I llama_perf_context_print:       total time =     846.17 ms /    70 tokens
0.01.556.799 I ggml_metal_free: deallocating

real	0m1.574s
user	0m0.108s
sys	0m0.170s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.089 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.011.746 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.017.263 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.017.267 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.268 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.273 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.273 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.273 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.274 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.275 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.275 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.276 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.278 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.278 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.278 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.281 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.281 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.282 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.955 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.006 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.731 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.025.733 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.733 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.733 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.733 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.734 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.025.734 I llama_model_loader: - type  f32:  194 tensors
0.00.025.735 I llama_model_loader: - type q5_K:   61 tensors
0.00.025.735 I llama_model_loader: - type q6_K:   37 tensors
0.00.045.270 I llm_load_vocab: special tokens cache size = 25
0.00.051.212 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.051.214 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.051.215 I llm_load_print_meta: arch             = gptneox
0.00.051.215 I llm_load_print_meta: vocab type       = BPE
0.00.051.215 I llm_load_print_meta: n_vocab          = 50304
0.00.051.216 I llm_load_print_meta: n_merges         = 50009
0.00.051.216 I llm_load_print_meta: vocab_only       = 0
0.00.051.216 I llm_load_print_meta: n_ctx_train      = 2048
0.00.051.216 I llm_load_print_meta: n_embd           = 2048
0.00.051.216 I llm_load_print_meta: n_layer          = 24
0.00.051.219 I llm_load_print_meta: n_head           = 16
0.00.051.219 I llm_load_print_meta: n_head_kv        = 16
0.00.051.220 I llm_load_print_meta: n_rot            = 32
0.00.051.220 I llm_load_print_meta: n_swa            = 0
0.00.051.220 I llm_load_print_meta: n_embd_head_k    = 128
0.00.051.220 I llm_load_print_meta: n_embd_head_v    = 128
0.00.051.221 I llm_load_print_meta: n_gqa            = 1
0.00.051.222 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.051.222 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.051.223 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.051.223 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.051.223 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.051.224 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.051.224 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.051.224 I llm_load_print_meta: n_ff             = 8192
0.00.051.225 I llm_load_print_meta: n_expert         = 0
0.00.051.225 I llm_load_print_meta: n_expert_used    = 0
0.00.051.225 I llm_load_print_meta: causal attn      = 1
0.00.051.225 I llm_load_print_meta: pooling type     = 0
0.00.051.225 I llm_load_print_meta: rope type        = 2
0.00.051.225 I llm_load_print_meta: rope scaling     = linear
0.00.051.226 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.051.226 I llm_load_print_meta: freq_scale_train = 1
0.00.051.226 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.051.227 I llm_load_print_meta: rope_finetuned   = unknown
0.00.051.227 I llm_load_print_meta: ssm_d_conv       = 0
0.00.051.227 I llm_load_print_meta: ssm_d_inner      = 0
0.00.051.227 I llm_load_print_meta: ssm_d_state      = 0
0.00.051.227 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.051.227 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.051.227 I llm_load_print_meta: model type       = 1.4B
0.00.051.230 I llm_load_print_meta: model ftype      = Q5_K - Medium
0.00.051.231 I llm_load_print_meta: model params     = 1.41 B
0.00.051.231 I llm_load_print_meta: model size       = 1006.35 MiB (5.97 BPW) 
0.00.051.231 I llm_load_print_meta: general.name     = 1.4B
0.00.051.232 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.051.232 I llm_load_print_meta: LF token         = 128 ''
0.00.051.233 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.051.233 I llm_load_print_meta: max token length = 1024
0.00.053.273 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.053.273 I llm_load_tensors: offloading output layer to GPU
0.00.053.273 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.053.284 I llm_load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.053.285 I llm_load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
.................................................................................
0.00.054.225 I llama_new_context_with_model: n_seq_max     = 1
0.00.054.225 I llama_new_context_with_model: n_ctx         = 128
0.00.054.226 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.054.226 I llama_new_context_with_model: n_batch       = 128
0.00.054.226 I llama_new_context_with_model: n_ubatch      = 128
0.00.054.226 I llama_new_context_with_model: flash_attn    = 0
0.00.054.227 I llama_new_context_with_model: freq_base     = 10000.0
0.00.054.227 I llama_new_context_with_model: freq_scale    = 1
0.00.054.227 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.054.228 I ggml_metal_init: allocating
0.00.054.231 I ggml_metal_init: found device: Apple M4
0.00.054.233 I ggml_metal_init: picking default device: Apple M4
0.00.054.809 I ggml_metal_init: using embedded metal library
0.00.057.113 I ggml_metal_init: GPU name:   Apple M4
0.00.057.114 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.057.114 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.057.115 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.057.115 I ggml_metal_init: simdgroup reduction   = true
0.00.057.115 I ggml_metal_init: simdgroup matrix mul. = true
0.00.057.115 I ggml_metal_init: has bfloat            = true
0.00.057.115 I ggml_metal_init: use bfloat            = true
0.00.057.116 I ggml_metal_init: hasUnifiedMemory      = true
0.00.057.116 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.066.566 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.067.919 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.067.921 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.067.938 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.068.836 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.068.837 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.068.837 I llama_new_context_with_model: graph nodes  = 967
0.00.068.837 I llama_new_context_with_model: graph splits = 2
0.00.068.838 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.068.838 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.638.792 I 
0.00.638.823 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.638.838 I perplexity: tokenizing the input ..
0.00.646.680 I perplexity: tokenization took 7.841 ms
0.00.646.684 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.787.510 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.788.691 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.788.707 I llama_perf_context_print:        load time =     627.04 ms
0.00.788.708 I llama_perf_context_print: prompt eval time =     140.60 ms /   128 tokens (    1.10 ms per token,   910.39 tokens per second)
0.00.788.709 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.788.709 I llama_perf_context_print:       total time =     149.92 ms /   129 tokens
0.00.789.214 I ggml_metal_free: deallocating

real	0m0.803s
user	0m0.076s
sys	0m0.122s
```
- q6_k:
```
+ ./bin/llama-cli --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.044 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.073 I main: llama backend init
0.00.000.075 I main: load the model and apply lora adapter, if any
0.00.008.717 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.702 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.706 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.708 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.708 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.712 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.713 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.713 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.714 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.714 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.714 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.715 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.715 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.715 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.717 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.719 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.719 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.719 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.390 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.363 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.037 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.024.038 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.038 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.039 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.039 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.039 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.040 I llama_model_loader: - type  f32:  194 tensors
0.00.024.040 I llama_model_loader: - type q6_K:   98 tensors
0.00.043.889 I llm_load_vocab: special tokens cache size = 25
0.00.049.827 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.049.830 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.049.830 I llm_load_print_meta: arch             = gptneox
0.00.049.830 I llm_load_print_meta: vocab type       = BPE
0.00.049.831 I llm_load_print_meta: n_vocab          = 50304
0.00.049.831 I llm_load_print_meta: n_merges         = 50009
0.00.049.831 I llm_load_print_meta: vocab_only       = 0
0.00.049.831 I llm_load_print_meta: n_ctx_train      = 2048
0.00.049.831 I llm_load_print_meta: n_embd           = 2048
0.00.049.832 I llm_load_print_meta: n_layer          = 24
0.00.049.835 I llm_load_print_meta: n_head           = 16
0.00.049.835 I llm_load_print_meta: n_head_kv        = 16
0.00.049.836 I llm_load_print_meta: n_rot            = 32
0.00.049.836 I llm_load_print_meta: n_swa            = 0
0.00.049.836 I llm_load_print_meta: n_embd_head_k    = 128
0.00.049.836 I llm_load_print_meta: n_embd_head_v    = 128
0.00.049.837 I llm_load_print_meta: n_gqa            = 1
0.00.049.838 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.049.838 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.049.839 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.049.839 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.049.839 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.049.839 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.049.840 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.049.840 I llm_load_print_meta: n_ff             = 8192
0.00.049.840 I llm_load_print_meta: n_expert         = 0
0.00.049.841 I llm_load_print_meta: n_expert_used    = 0
0.00.049.841 I llm_load_print_meta: causal attn      = 1
0.00.049.841 I llm_load_print_meta: pooling type     = 0
0.00.049.841 I llm_load_print_meta: rope type        = 2
0.00.049.841 I llm_load_print_meta: rope scaling     = linear
0.00.049.843 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.049.844 I llm_load_print_meta: freq_scale_train = 1
0.00.049.844 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.049.844 I llm_load_print_meta: rope_finetuned   = unknown
0.00.049.844 I llm_load_print_meta: ssm_d_conv       = 0
0.00.049.844 I llm_load_print_meta: ssm_d_inner      = 0
0.00.049.845 I llm_load_print_meta: ssm_d_state      = 0
0.00.049.846 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.049.846 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.049.846 I llm_load_print_meta: model type       = 1.4B
0.00.049.847 I llm_load_print_meta: model ftype      = Q6_K
0.00.049.847 I llm_load_print_meta: model params     = 1.41 B
0.00.049.848 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.049.848 I llm_load_print_meta: general.name     = 1.4B
0.00.049.848 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.049.848 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.049.848 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.049.849 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.049.849 I llm_load_print_meta: LF token         = 128 ''
0.00.049.849 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.049.849 I llm_load_print_meta: max token length = 1024
0.00.051.848 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.051.848 I llm_load_tensors: offloading output layer to GPU
0.00.051.848 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.051.859 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.051.860 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.052.794 I llama_new_context_with_model: n_seq_max     = 1
0.00.052.795 I llama_new_context_with_model: n_ctx         = 2048
0.00.052.795 I llama_new_context_with_model: n_ctx_per_seq = 2048
0.00.052.796 I llama_new_context_with_model: n_batch       = 2048
0.00.052.796 I llama_new_context_with_model: n_ubatch      = 512
0.00.052.796 I llama_new_context_with_model: flash_attn    = 0
0.00.052.797 I llama_new_context_with_model: freq_base     = 10000.0
0.00.052.797 I llama_new_context_with_model: freq_scale    = 1
0.00.052.797 I ggml_metal_init: allocating
0.00.052.803 I ggml_metal_init: found device: Apple M4
0.00.052.805 I ggml_metal_init: picking default device: Apple M4
0.00.053.413 I ggml_metal_init: using embedded metal library
0.00.055.739 I ggml_metal_init: GPU name:   Apple M4
0.00.055.740 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.055.741 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.055.741 I ggml_metal_init: simdgroup reduction   = true
0.00.055.741 I ggml_metal_init: simdgroup matrix mul. = true
0.00.055.742 I ggml_metal_init: has bfloat            = true
0.00.055.742 I ggml_metal_init: use bfloat            = true
0.00.055.742 I ggml_metal_init: hasUnifiedMemory      = true
0.00.055.743 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.246 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.084.449 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.084.456 I llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.084.489 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.085.461 I llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
0.00.085.462 I llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
0.00.085.463 I llama_new_context_with_model: graph nodes  = 967
0.00.085.463 I llama_new_context_with_model: graph splits = 2
0.00.085.466 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.085.597 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.085.597 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.741.357 I main: llama threadpool init, n_threads = 4
0.00.741.403 I 
0.00.741.449 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.741.450 I 
0.00.741.686 I sampler seed: 1234
0.00.741.691 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.741.714 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.741.715 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.741.715 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.

He added: Ive

0.01.622.701 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58921.16 tokens per second)
0.01.622.701 I llama_perf_context_print:        load time =     732.63 ms
0.01.622.702 I llama_perf_context_print: prompt eval time =      54.38 ms /     7 tokens (    7.77 ms per token,   128.72 tokens per second)
0.01.622.705 I llama_perf_context_print:        eval time =     823.61 ms /    63 runs   (   13.07 ms per token,    76.49 tokens per second)
0.01.622.707 I llama_perf_context_print:       total time =     881.35 ms /    70 tokens
0.01.622.907 I ggml_metal_free: deallocating

real	0m1.641s
user	0m0.109s
sys	0m0.161s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.088 I build: 4435 (017cc5f4) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.308 I llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
0.00.015.036 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.040 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.041 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.042 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.042 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.042 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.043 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.044 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.044 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.044 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.045 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.045 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.045 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.046 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.047 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.047 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.048 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.018.870 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.019.922 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.023.708 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
0.00.023.709 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.023.710 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.023.710 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.023.710 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.023.711 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.023.711 I llama_model_loader: - type  f32:  194 tensors
0.00.023.711 I llama_model_loader: - type q6_K:   98 tensors
0.00.044.062 I llm_load_vocab: special tokens cache size = 25
0.00.050.169 I llm_load_vocab: token to piece cache size = 0.2984 MB
0.00.050.172 I llm_load_print_meta: format           = GGUF V3 (latest)
0.00.050.172 I llm_load_print_meta: arch             = gptneox
0.00.050.172 I llm_load_print_meta: vocab type       = BPE
0.00.050.173 I llm_load_print_meta: n_vocab          = 50304
0.00.050.173 I llm_load_print_meta: n_merges         = 50009
0.00.050.173 I llm_load_print_meta: vocab_only       = 0
0.00.050.173 I llm_load_print_meta: n_ctx_train      = 2048
0.00.050.173 I llm_load_print_meta: n_embd           = 2048
0.00.050.173 I llm_load_print_meta: n_layer          = 24
0.00.050.176 I llm_load_print_meta: n_head           = 16
0.00.050.179 I llm_load_print_meta: n_head_kv        = 16
0.00.050.179 I llm_load_print_meta: n_rot            = 32
0.00.050.179 I llm_load_print_meta: n_swa            = 0
0.00.050.180 I llm_load_print_meta: n_embd_head_k    = 128
0.00.050.180 I llm_load_print_meta: n_embd_head_v    = 128
0.00.050.180 I llm_load_print_meta: n_gqa            = 1
0.00.050.181 I llm_load_print_meta: n_embd_k_gqa     = 2048
0.00.050.182 I llm_load_print_meta: n_embd_v_gqa     = 2048
0.00.050.182 I llm_load_print_meta: f_norm_eps       = 1.0e-05
0.00.050.183 I llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
0.00.050.183 I llm_load_print_meta: f_clamp_kqv      = 0.0e+00
0.00.050.183 I llm_load_print_meta: f_max_alibi_bias = 0.0e+00
0.00.050.183 I llm_load_print_meta: f_logit_scale    = 0.0e+00
0.00.050.188 I llm_load_print_meta: n_ff             = 8192
0.00.050.188 I llm_load_print_meta: n_expert         = 0
0.00.050.190 I llm_load_print_meta: n_expert_used    = 0
0.00.050.190 I llm_load_print_meta: causal attn      = 1
0.00.050.190 I llm_load_print_meta: pooling type     = 0
0.00.050.190 I llm_load_print_meta: rope type        = 2
0.00.050.190 I llm_load_print_meta: rope scaling     = linear
0.00.050.191 I llm_load_print_meta: freq_base_train  = 10000.0
0.00.050.191 I llm_load_print_meta: freq_scale_train = 1
0.00.050.191 I llm_load_print_meta: n_ctx_orig_yarn  = 2048
0.00.050.191 I llm_load_print_meta: rope_finetuned   = unknown
0.00.050.192 I llm_load_print_meta: ssm_d_conv       = 0
0.00.050.192 I llm_load_print_meta: ssm_d_inner      = 0
0.00.050.192 I llm_load_print_meta: ssm_d_state      = 0
0.00.050.192 I llm_load_print_meta: ssm_dt_rank      = 0
0.00.050.193 I llm_load_print_meta: ssm_dt_b_c_rms   = 0
0.00.050.200 I llm_load_print_meta: model type       = 1.4B
0.00.050.202 I llm_load_print_meta: model ftype      = Q6_K
0.00.050.204 I llm_load_print_meta: model params     = 1.41 B
0.00.050.204 I llm_load_print_meta: model size       = 1.08 GiB (6.57 BPW) 
0.00.050.204 I llm_load_print_meta: general.name     = 1.4B
0.00.050.205 I llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
0.00.050.205 I llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
0.00.050.205 I llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
0.00.050.205 I llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
0.00.050.205 I llm_load_print_meta: LF token         = 128 ''
0.00.050.206 I llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
0.00.050.206 I llm_load_print_meta: max token length = 1024
0.00.052.261 I llm_load_tensors: offloading 24 repeating layers to GPU
0.00.052.261 I llm_load_tensors: offloading output layer to GPU
0.00.052.262 I llm_load_tensors: offloaded 25/25 layers to GPU
0.00.052.272 I llm_load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.052.274 I llm_load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
...............................................................................
0.00.053.147 I llama_new_context_with_model: n_seq_max     = 1
0.00.053.148 I llama_new_context_with_model: n_ctx         = 128
0.00.053.148 I llama_new_context_with_model: n_ctx_per_seq = 128
0.00.053.148 I llama_new_context_with_model: n_batch       = 128
0.00.053.148 I llama_new_context_with_model: n_ubatch      = 128
0.00.053.148 I llama_new_context_with_model: flash_attn    = 0
0.00.053.149 I llama_new_context_with_model: freq_base     = 10000.0
0.00.053.149 I llama_new_context_with_model: freq_scale    = 1
0.00.053.149 W llama_new_context_with_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.053.150 I ggml_metal_init: allocating
0.00.053.153 I ggml_metal_init: found device: Apple M4
0.00.053.155 I ggml_metal_init: picking default device: Apple M4
0.00.053.729 I ggml_metal_init: using embedded metal library
0.00.056.053 I ggml_metal_init: GPU name:   Apple M4
0.00.056.055 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.056.055 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.056.055 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.056.056 I ggml_metal_init: simdgroup reduction   = true
0.00.056.056 I ggml_metal_init: simdgroup matrix mul. = true
0.00.056.056 I ggml_metal_init: has bfloat            = true
0.00.056.056 I ggml_metal_init: use bfloat            = true
0.00.056.056 I ggml_metal_init: hasUnifiedMemory      = true
0.00.056.057 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.065.488 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.066.731 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.066.734 I llama_new_context_with_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.066.748 I llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
0.00.067.576 I llama_new_context_with_model:      Metal compute buffer size =    25.56 MiB
0.00.067.577 I llama_new_context_with_model:        CPU compute buffer size =     1.06 MiB
0.00.067.577 I llama_new_context_with_model: graph nodes  = 967
0.00.067.577 I llama_new_context_with_model: graph splits = 2
0.00.067.578 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.067.579 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.427.989 I 
0.00.428.016 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.428.033 I perplexity: tokenizing the input ..
0.00.435.650 I perplexity: tokenization took 7.616 ms
0.00.435.653 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.576.174 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.577.418 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.577.441 I llama_perf_context_print:        load time =     418.68 ms
0.00.577.443 I llama_perf_context_print: prompt eval time =     140.28 ms /   128 tokens (    1.10 ms per token,   912.47 tokens per second)
0.00.577.444 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.577.447 I llama_perf_context_print:       total time =     149.45 ms /   129 tokens
0.00.577.925 I ggml_metal_free: deallocating

real	0m0.591s
user	0m0.077s
sys	0m0.085s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4435 (017cc5f4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x12ae0a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x12ae0a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x12ae0aed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x12ae0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x12ae0ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x12ae0bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x12ae0c590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x12ae0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x12ae0d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x12ae0d5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x12ae0daf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x12ae0dff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x12ae0eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x12ae0f2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x12ae0fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x12ae101f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x12ae10910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x12ae11030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x12ae11750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x12ae11f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x12ae12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x12ae12d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x12ae13480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x12ae13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x12ae14440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x12ae14700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x12ae14d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x12ae15980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x12ae15ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x12ae16180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x12ae16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x12ae168e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x12ae17170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x12ae176b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x12ae17970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x12ae17e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x12ae182b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x12ae18750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x12ae18bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x12ae19090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x12ae19530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x12ae199d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x12ae19e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x12ae1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x12ae1a5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x12ae1abe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x12ae1b1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x12ae1bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x12ae1c120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x12ae1c730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x12ae1cd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x12ae1d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x12ae1d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x12ae1df70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x12ae1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x12ae1ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x12ae1f0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x12ae1f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x12ae1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x12ae20160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x12ae20420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x12ae208c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x12ae20d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x12ae21200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x12ae216a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x12ae21b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x12ae21fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x12ae22480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x12ae22920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x12ae22dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x12ae23260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x12ae23700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x12ae23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12ae240f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12ae24640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x12ae24b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x12ae250e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x12ae25630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x12ae25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x12ae260d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x12ae26620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x12ae26b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x12ae270c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12ae27610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12ae27b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x12ae280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x12ae28600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x12ae28b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12ae290a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x12ae295f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x12ae29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12ae2a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x12ae2a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x12ae2ab30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x12ae2b080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12ae2b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x12ae2bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x12ae1b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x12ae2bf90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x12ae2c740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x12ae2cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12ae2d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12ae2d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12ae2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x12ae2e1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x12ae2e720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x12ae2ec70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x12ae2f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x12ae2f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12ae2fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x12ae301b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x12ae30700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x12ae30c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12ae310f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x12ae31590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x12ae31a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x12ae31ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x12ae32370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x12ae32810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x12ae32cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x12ae33150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x12ae335f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x12ae33a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x12ae33f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x12ae343d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x12ae34870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x12ae34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12ae351b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x12ae35650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x12ae35af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12ae35f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x12ae36430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12ae368d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x12ae36d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x12ae37210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x12ae376b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x12ae37b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x12ae37ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x12ae38490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x12ae38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x12ae38dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x12ae39270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x12ae39710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x12ae39bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x12ae3a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x12ae3a4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x12ae3a990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x12ae3ae30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x12ae3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x12ae3b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x12ae3bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x12ae3c0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12ae3c550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x12ae3c9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x12ae3ce90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x12ae3d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x12ae3d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x12ae3dc70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12ae3e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x12ae3e5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x12ae3ea50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x12ae3eef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12ae3f390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x12ae3f830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x12ae3fcd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12ae40170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x12ae40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x12ae40ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x12ae40f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12ae413f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x12ae41890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x12ae41d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x12ae421d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x12ae42670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x12ae42b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x12ae42fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x12ae43450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x12ae438f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x12ae43d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x12ae44230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x12ae446d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x12ae44b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x12ae45010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x12ae454b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x12ae45950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x12ae45df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x12ae46290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x12ae46730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x12ae46bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x12ae47070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x12ae47510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x12ae479b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x12ae47e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x12ae483a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x12ae488f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x12ae48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x12ae49390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x12ae49650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x12ae49c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x12ae4a270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x12ae4a880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x12ae4b070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x12ae4b510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x12ae4b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x12ae4bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x12ae4c3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x12ae4cbe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x12ae4d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x12ae4d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x12ae4d9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x12ae4e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x12ae4e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x12ae4ec10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x12ae4f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x12ae4f6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x12ae4fc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x12ae50150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x12ae506a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x12ae50bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x12ae51140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x12ae51690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x12ae51be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x12ae52130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x12ae52680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x12ae52bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x12ae53120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x12ae53670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x12ae53bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x12ae54110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x12ae54660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x12ae54bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x12ae55100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x12ae55650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x12ae55ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x12ae560f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x12ae56640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x12ae56b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12ae570e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x12ae57630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12ae57b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x12ae580d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x12ae58620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x12ae58b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x12ae590c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12ae59610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x12ae59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12ae5a0b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x12ae5a600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12ae5ab50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x12ae5b0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12ae5b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x12ae5bb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12ae5c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x12ae5c5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x12ae5cb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x12ae5d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x12ae5d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x12ae5db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x12ae5e070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x12ae5e5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x12ae5eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x12ae5f060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x12ae5f5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x12ae5fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x12ae60050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x12ae605a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x12ae60af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x12ae60f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x12ae61430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x12ae618d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x12ae61d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x12ae62210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x12ae626b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x12ae62b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x12ae62ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x12ae63490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x12ae63930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x12ae63dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x12ae64270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x12ae64710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x12ae64bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x12ae65050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x12ae655a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x12ae65cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x12ae663e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x12ae66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x12ae67220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x12ae674e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x12ae67cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12ae67f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12ae685a0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
0.00.142.864 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.142.867 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x11e308320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x11e308790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x11e308c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x11e309070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x11e3094e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x11e309950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x11e309dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x11e30a230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x11e30a6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x11e30ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x11e30af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x11e30b640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x11e30c160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x11e30c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x11e30d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x11e30d840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x11e30df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x11e30e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x11e30eda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x11e30f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x11e30fc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x11e3103b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x11e310ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x11e3111f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x11e311910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x11e311bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x11e311e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x11e312300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x11e312770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x11e312be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x11e3130e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x11e3135f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x11e313a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x11e313d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x11e314190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x11e314600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x11e314b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x11e315060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x11e315560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x11e315a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x11e315f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x11e316460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x11e316960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x11e316e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x11e317360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x11e3177d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x11e317c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x11e3180b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x11e318520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x11e318990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x11e318e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x11e319270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x11e3196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x11e319b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x11e319fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x11e31a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x11e31ac30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x11e31aef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x11e31b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x11e31bcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x11e31c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x11e31c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x11e31cad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x11e31cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x11e31d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x11e31d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x11e31dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x11e31e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x11e31e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x11e31eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x11e31efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x11e31f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x11e31f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x11e31fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x11e3203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x11e320900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x11e320e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x11e3213a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x11e3218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x11e321e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x11e322390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x11e3228e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x11e322e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x11e323380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x11e3238d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x11e323e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x11e324370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x11e3248c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x11e324e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x11e325360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x11e3258b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x11e325e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x11e326350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x11e3268a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x11e326df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x11e327340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x11e327890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x11e327de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x11e328330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x11e328880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x11e328dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x11e329320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x11e329870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x11e329dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x11e32a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x11e32a860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x11e32adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x11e32b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x11e32b850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x11e32bda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x11e32c2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x11e32c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x11e32cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x11e32d230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x11e32d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x11e32db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x11e32e010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x11e32e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x11e32e950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x11e32edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x11e32f290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x11e32f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x11e32fbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x11e330070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x11e330510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x11e3309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x11e330e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x11e3312f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x11e331790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x11e331c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x11e3320d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x11e332570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x11e332a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x11e332eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x11e333350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x11e3337f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x11e333c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x11e334130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x11e3345d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x11e334a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x11e334f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x11e3353b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x11e335850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x11e335cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x11e336190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x11e336630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x11e336ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x11e336f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x11e337410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x11e3378b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x11e337d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x11e3381f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x11e338690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x11e338b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x11e338fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x11e339470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x11e339910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x11e339db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x11e33a250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x11e33a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x11e33ab90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x11e33b030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x11e33b4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x11e33b970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x11e33be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x11e33c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x11e33c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x11e33cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x11e33d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x11e33d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x11e33d9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x11e33de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x11e33e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x11e33e7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x11e33ec50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x11e33f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x11e33f590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x11e33fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x11e33fed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x11e340370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x11e340810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x11e340cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x11e341150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x11e3415f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x11e341a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x11e341f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x11e3423d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x11e342870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x11e342d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x11e3431b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x11e343650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x11e343af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x11e343f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x11e3444e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x11e344a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x11e344f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x11e3454d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x11e345790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x11e345da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x11e3463b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x11e3469c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x11e3471b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x11e347650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x11e347910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x11e347f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x11e348530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x11e348d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x11e3491c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x11e349660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x11e349b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x11e34a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x11e34a800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x11e34ad50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x11e34b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x11e34b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x11e34bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x11e34c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x11e34c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x11e34cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x11e34d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x11e34d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x11e34dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x11e34e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x11e34e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x11e34ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x11e34f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x11e34f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x11e34fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x11e350250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x11e3507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x11e350cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x11e351240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x11e351790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x11e351ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x11e352230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x11e352780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x11e352cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x11e353220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x11e353770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x11e353cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x11e354210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x11e354760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x11e354cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x11e355200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x11e355750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x11e355ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x11e3561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x11e356740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x11e356c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x11e3571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x11e357730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x11e357c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x11e3581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x11e358720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x11e358c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x11e3591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x11e359710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x11e359c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x11e35a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x11e35a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x11e35ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x11e35b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x11e35b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x11e35bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x11e35c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x11e35c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x11e35cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x11e35d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x11e35d570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x11e35da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x11e35deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x11e35e350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x11e35e7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x11e35ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x11e35f130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x11e35f5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x11e35fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x11e35ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x11e3603b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x11e360850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x11e360cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x11e361190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x11e3616e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x11e361e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x11e362520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x11e362c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x11e363360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x11e363620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x11e363e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x11e3640d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x11e3646e0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x117f044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x117f04950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x117f04dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x117f05230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x117f056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x117f05b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x117f05f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x117f063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x117f06860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x117f06db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x117f07220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x117f078a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x117f083c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x117f08b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x117f09380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x117f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x117f0a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x117f0a8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x117f0b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x117f0b7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x117f0bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x117f0c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x117f0cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x117f0d450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x117f0db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x117f0de30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x117f0e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x117f0e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x117f0e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x117f0ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117f0f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x117f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x117f0fc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x117f0ff10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x117f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x117f107f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x117f10c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x117f110d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x117f11540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x117f119b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x117f11e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x117f12290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x117f12700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x117f12b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x117f12fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x117f13450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x117f138c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x117f13d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x117f141a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x117f14610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x117f14a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x117f14ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x117f15360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x117f157d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x117f15c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x117f160b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x117f16620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x117f16b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x117f16f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x117f17400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x117f17870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x117f17ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x117f18150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x117f185c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x117f18a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x117f18ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x117f19310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x117f19780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x117f19bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x117f1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x117f1a4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x117f1a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x117f1adb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x117f1b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x117f1b690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x117f1bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x117f1bf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x117f1c3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x117f1c850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x117f1ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x117f1d130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x117f1d5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x117f1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x117f1de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x117f1e2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x117f1e760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x117f1ebd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x117f1f040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x117f1f4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x117f1f920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x117f1fd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x117f20200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x117f20670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x117f20ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x117f20f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x117f213c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x117f21830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x117f21ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x117f22110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x117f22580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x117f229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x117f22e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x117f232d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x117f23b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x117f23e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x117f24290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x117f24700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x117f24b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x117f24fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x117f25450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x117f258c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x117f25d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x117f261a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x117f26610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x117f26a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x117f26ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x117f27360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x117f277d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x117f27c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x117f280b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x117f28520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x117f28990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x117f28e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x117f29270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x117f296e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x117f29b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x117f29fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x117f2a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x117f2a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x117f2ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x117f2b180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x117f2b5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x117f2ba60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x117f2bed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x117f2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x117f2c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x117f2cc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x117f2d090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x117f2d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x117f2d970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x117f2dde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x117f2e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x117f2e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x117f2eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x117f2efa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x117f2f410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x117f2f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x117f2fcf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x117f30160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x117f305d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x117f30a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x117f30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x117f31320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x117f31790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x117f31c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x117f32070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x117f324e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x117f32950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x117f32dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x117f33230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x117f336a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x117f33b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x117f33f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x117f343f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x117f34860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x117f34cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x117f35140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x117f355b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x117f35a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x117f35e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x117f36300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x117f36770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x117f36be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x117f37050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x117f374c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x117f37930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x117f37da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x117f38210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x117f38680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x117f38af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x117f38f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x117f393d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x117f39840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x117f39cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x117f3a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x117f3a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x117f3aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x117f3ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x117f3b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x117f3b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x117f3bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x117f3c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x117f3c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x117f3c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x117f3cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x117f3d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x117f3d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x117f3dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x117f3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x117f3e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x117f3e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x117f3ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x117f3f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x117f3f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x117f3f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x117f3fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x117f402c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x117f40730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x117f40ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x117f41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x117f41b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x117f41e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x117f42110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x117f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x117f429f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x117f42e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x117f432d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x117f43740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x117f43bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x117f44020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x117f44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x117f44900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x117f44d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x117f451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x117f45650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x117f45ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x117f45f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x117f463a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x117f46810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x117f46c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x117f470f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x117f47560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x117f479d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x117f47e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x117f482b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x117f48720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x117f48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x117f49000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x117f49470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x117f498e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x117f49d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x117f4a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x117f4a630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x117f4aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x117f4af10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x117f4b380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x117f4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x117f4bc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x117f4c0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x117f4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x117f4c9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x117f4ce20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x117f4d290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x117f4d700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x117f4db70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x117f4dfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x117f4e450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x117f4e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x117f4ed30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x117f4f1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x117f4f610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x117f4fa80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x117f4fef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x117f50360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x117f507d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x117f50c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x117f510b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x117f51520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x117f51990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x117f51e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x117f52270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x117f526e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x117f52b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x117f52fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x117f53430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x117f538a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x117f53d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x117f54180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x117f545f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x117f54a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x117f54ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x117f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x117f557b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x117f56220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x117f56940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x117f57060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x117f57780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x117f57a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x117f57eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x117f584b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x117f58ac0 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 967
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.828s
user	0m0.300s
sys	0m0.321s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4435 (017cc5f4)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = [" ", " t", " a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
llm_load_vocab: control token:      1 '<|padding|>' is not marked as EOG
llm_load_vocab: special tokens cache size = 25
llm_load_vocab: token to piece cache size = 0.2984 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = gptneox
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 50304
llm_load_print_meta: n_merges         = 50009
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 16
llm_load_print_meta: n_head_kv        = 16
llm_load_print_meta: n_rot            = 32
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 1.0e-05
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 1.4B
llm_load_print_meta: model ftype      = Q4_0
llm_load_print_meta: model params     = 1.41 B
llm_load_print_meta: model size       = 786.31 MiB (4.66 BPW) 
llm_load_print_meta: general.name     = 1.4B
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 128 ''
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: max token length = 1024
llm_load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors: Metal_Mapped model buffer size =   786.33 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
............................................................................
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145e069a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145e07140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145e075b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145e07a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145e07e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145e08300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145e08770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145e08be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145e09050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145e094c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145e09930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145e09fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145e0aaf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145e0b2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145e0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145e0c1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145e0c8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145e0d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145e0d730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145e0df00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145e0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145e0ed40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145e0f460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145e0fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145e10420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145e106e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145e109a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145e10e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145e114c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145e11930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145e11da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145e12330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145e127a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145e12a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145e12ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145e13650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145e13910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145e13d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145e141f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145e14660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145e14ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145e14f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145e153b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145e15820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145e15c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145e16100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145e16570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145e16fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145e17260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145e176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145e17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145e17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145e18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145e18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145e18d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145e193b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145e19850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145e19b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145e19f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145e1a650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145e1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145e1ad10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145e1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145e1b710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145e1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145e1c110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145e1c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145e1cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145e1d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145e1d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145e1da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145e1df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145e1e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145e1e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145e1eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145e1f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145e1fa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145e1ffd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145e20580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145e20b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145e210e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145e21690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145e21c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145e221f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145e227a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145e22d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145e23300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145e238b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145e23e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145e24410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145e249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145e24f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145e25520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145e25ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145e26080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145e26630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145e26be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145e16b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145e27340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145e277b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145e27c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145e281d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145e28780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145e28d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145e292e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145e29890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145e29e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145e2a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145e2a9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145e2af50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145e2b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145e2bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145e2c060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145e2c610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145e2cb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145e2d010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145e2d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145e2da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145e2df10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145e2e410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145e2e910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145e2ee10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145e2f810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145e2fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145e30210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145e30710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145e30c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145e31110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145e31610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145e31b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145e32010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145e32510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145e32a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145e32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145e33410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145e33910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145e33e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145e34310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145e34810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145e34d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145e35210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145e35710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145e35c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145e36110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145e36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145e36b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145e37010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145e37510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145e37a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145e37f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145e38410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145e38910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145e38e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145e39310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145e39810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145e39d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145e3a210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145e3a710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145e3ac10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145e3b110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145e3b610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145e3bb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145e3c010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145e3c510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145e3ca10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145e3cf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145e3d410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145e3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145e3de10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145e3e310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145e3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145e3ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145e3f210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145e3f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145e3fc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145e40110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145e40610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145e40b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145e41010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145e41510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145e41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145e41f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145e42410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145e42910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145e42e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145e43310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145e43810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145e43d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145e44210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145e44710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145e44c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145e45110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145e45610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145e45bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145e46170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145e46720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145e46cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145e472e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145e478f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145e47f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145e486f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145e48b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145e48e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145e49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145e49a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145e4a260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145e4a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145e4aba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145e4b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145e4b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145e4bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145e4c290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145e4c7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145e4cd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145e4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145e4d7d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145e4dd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145e4e270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145e4e7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145e4ed10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145e4f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145e4f7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145e4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145e50250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145e507a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145e50cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145e51240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145e51790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145e51ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145e52230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145e52780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145e52cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145e53220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145e53770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145e53cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145e54210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145e54760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145e54cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145e55200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145e55750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145e55ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145e561f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145e56740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145e56c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145e571e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145e57730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145e57c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145e581d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145e58720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145e58c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145e591c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145e59710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145e59c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145e5a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145e5a700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145e5ac50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145e5b1a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145e5b6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145e5bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145e5c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145e5c6e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145e5cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145e5d180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145e5d6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145e5dc20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145e5e170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145e5e610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145e5eab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145e5ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145e5f3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145e5f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145e5fd30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145e601d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145e60670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145e60b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145e60fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145e61450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145e618f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145e61d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145e62230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145e626d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145e62c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145e63340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145e63a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145e64180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145e648a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145e64b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145e65350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145e65610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145e65c20 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
0.00.089.274 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.089.279 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145f07dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145f08240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145f086b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145f08c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145f09230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145f097e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145f09d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145f0a340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145f0a8f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145f0adf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145f0b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145f0b7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145f0c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145f0cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145f0d2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145f0d9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145f0e110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145f0e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145f0ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145f0f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145f0fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145f10560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145f10c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145f113a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145f11ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145f11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145f12390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145f129a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145f12fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145f137a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145f13c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145f13f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145f14790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145f14cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145f14f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145f15430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145f158d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145f15d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145f16210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145f166b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145f16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145f17490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145f17bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145f18200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145f18810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145f18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145f19430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145f19a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145f1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145f1a660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145f1ac70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145f1b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145f1ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145f1bf10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145f1c3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145f1c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145f1cc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145f1d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145f1d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145f1ddb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145f1e250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145f1e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145f1eb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145f1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145f1f4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145f1f970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145f1fe10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145f202b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145f20750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145f20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145f21090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145f215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145f21b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145f22080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145f225d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145f22b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145f23070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145f235c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145f23b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145f24060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145f245b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145f24b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145f25050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145f255a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145f25af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145f26040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145f26590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145f26ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145f27030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145f27580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145f27ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145f28020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145f28570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145f28ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145f29010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145f29560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145f29ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145f2a000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145f2a550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145f2aaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145f2aff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145f2b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145f2ba90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145f2bfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145f2c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145f2ca80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145f2cfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145f2d520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145f2da70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145f2dfc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145f2e9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145f2ee50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145f2f2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145f2f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145f2fc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145f300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145f30570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145f30a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145f30eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145f31350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145f317f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145f31c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145f32130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145f325d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145f32a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145f32f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145f333b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145f33850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145f33cf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145f34190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145f34630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145f34ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145f34f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145f35410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145f358b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145f35d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145f361f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145f36690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145f36b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145f36fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145f37470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145f37910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145f37db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145f38250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145f386f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145f38b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145f39030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145f394d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145f39970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145f39e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145f3a2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145f3a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145f3abf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145f3b090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145f3b530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145f3b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145f3c310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145f3c7b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145f3cc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145f3d0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145f3d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145f3da30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145f3ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145f3e370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145f3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145f3ecb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145f3f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145f3f5f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145f3fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145f3ff30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145f403d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145f40870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145f40d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145f411b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145f41650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145f41af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145f41f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145f42430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145f428d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145f42d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145f43210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145f436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145f43b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145f43ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145f44490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145f44930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145f44dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145f45270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145f45710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145f45c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145f461b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145f46700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145f46c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145f46f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145f47520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145f47b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145f48140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145f48930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145f48dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145f49090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145f496a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145f49cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145f4a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145f4a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145f4ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145f4b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145f4ba30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145f4bf80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145f4c4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145f4ca20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145f4cf70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145f4da10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145f4df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145f4e4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145f4ea00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145f4ef50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145f4f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145f4f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145f4ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145f50490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145f509e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145f50f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145f51480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145f519d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145f51f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145f52470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145f529c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145f52f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145f53460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145f539b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145f53f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145f54450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145f549a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145f54ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145f55440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145f55990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145f55ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145f56430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145f56980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145f56ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145f57420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145f57970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145f57ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145f58410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145f58960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145f58eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145f59400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145f59950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145f59ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145f5a3f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145f5a940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145f5ae90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145f5b3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145f5b930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145f5be80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145f5c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145f5c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145f5ce70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145f5d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145f5d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145f5de60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145f5e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145f5e850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145f5ecf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145f5f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145f5f630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145f5fad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145f5ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145f60410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145f608b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145f60d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145f611f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145f61690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145f61b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145f61fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145f62470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145f62910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145f62e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145f63580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145f63ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145f643c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145f64ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145f64da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145f65590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145f65850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145f65e60 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 2048
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 1
llama_new_context_with_model: freq_base     = 10000.0
llama_new_context_with_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x145f09aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x145f08f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x145f08970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x145f49350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x145f12650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x145f477e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x145f65b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x145f471d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x145f47df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x145f1a920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x145f1a310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x145f1c930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x145f12040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x145f190e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x145f196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x145f184c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x145f12c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x145f17eb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x145f1af30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x145f19d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x145f07970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x145f1b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x145f1cf40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x145f65060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x145f141c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x145f14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x145f13270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x145f49f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x145f48400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x145f662c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x145f66580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x145f66840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x145f66b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x145f66dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x145f67080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x145f67340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x145f67600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x145f678c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x145f67b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x145f67e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x145f68100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x145f683c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x145f68680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x145f68940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x145f68c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x145f68ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x145f69180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x145f69440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x145f69700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x145f699c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x145f69c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x145f69f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x145f6a200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x145f6a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x145f6a780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x145f6aa40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x145f6ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x145f6afc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x145f6b280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x145f6b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x145f6b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x145f6bac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x145f6bd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x145f6c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x145f6c300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x145f6c5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x145f6c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x145f6cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x145f6ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x145f6d0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x145f6d380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x145f6d640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x145f6d900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x145f6dbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x145f6de80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x145f6e140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x145f6e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x145f6e6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x145f6e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x145f6ec40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x145f6ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x145f6f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x145f6f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x145f6f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x145f6fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x145f6fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x145f6ff80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x145f70240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x145f70500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x145f707c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x145f70a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x145f70d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x145f71000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x145f712c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x145f71580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x145f71840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x145f71b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x145f71dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x145f72080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x145f72340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x145f72600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x145f728c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x145f72b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x145f72e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x145f73100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x145f733c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x145f73680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x145f73940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x145f73c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x145f73ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x145f74180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x145f74440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x145f74700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x145f749c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x145f74c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x145f74f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x145f75200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x145f754c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x145f75780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x145f75a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x145f75d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x145f75fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x145f76280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x145f76540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x145f76800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x145f76ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x145f76d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x145f77040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x145f77300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x145f775c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x145f77880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x145f77b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x145f77e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x145f780c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x145f78380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x145f78640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x145f78900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x145f78bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x145f78e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x145f79140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x145f79400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x145f796c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x145f79980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x145f79c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x145f79f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x145f7a1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x145f7a480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x145f7a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x145f7aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x145f7acc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x145f7af80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x145f7b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x145f7b500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x145f7b7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x145f7ba80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x145f7bd40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x145f7c000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x145f7c2c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x145f7c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x145f7c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x145f7cb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x145f7cdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x145f7d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x145f7d340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x145f7d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x145f7d8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x145f7db80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x145f7de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x145f7e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x145f7e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x145f7e680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x145f7e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x145f7ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x145f7eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x145f7f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x145f7f440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x145f7f700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x145f7f9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x145f7fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x145f7ff40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x145f80200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x145f804c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x145f80780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x145f80a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x145f80d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x145f80fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x145f81280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x145f81540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x145f81800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x145f81ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x145f81d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x145f82040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x145f82300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x145f825c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x145f82880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x145f82b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x145f82e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x145f830c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x145f83380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x145f83640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x145f83900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x145f83bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x145f83e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x145f84140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x145f84400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x145f846c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x145f84980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x145f84c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x145f84f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x145f851c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x145f85790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x145f85a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x145f85d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x145f85fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x145f86290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x145f86550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x145f86810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x145f86ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x145f86d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x145f87050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x145f87310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x145f875d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x145f87890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x145f87b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x145f87e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x145f880d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x145f88390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x145f88650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x145f88910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x145f88bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x145f88e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x145f89150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x145f89410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x145f896d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x145f89990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x145f89c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x145f89f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x145f8a1d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x145f8a490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x145f8a750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x145f8aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x145f8acd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x145f8b220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x145f8b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x145f8bcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x145f8c210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x145f8c760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x145f8ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x145f8d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x145f8d750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x145f8dca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x145f8e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x145f8e740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x145f8ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x145f8f1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x145f8f730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x145f8fc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x145f901d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x145f90720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x145f90c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x145f911c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x145f91710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x145f91c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x145f921b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x145f92700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x145f92c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x145f931a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x145f93460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x145f93720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x145f93c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x145f94120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x145f94620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x145f94b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x145f95020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x145f95520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x145f95a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x145f95f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x145f96420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x145f96920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x145f96e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x145f97320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x145f97820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x145f97d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x145f98730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x145f98e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x145f99570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x145f99c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x145f99f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x145f9a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x145f9aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x145f9b010 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_new_context_with_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB
llama_new_context_with_model:      Metal compute buffer size =   102.25 MiB
llama_new_context_with_model:        CPU compute buffer size =     8.01 MiB
llama_new_context_with_model: graph nodes  = 872
llama_new_context_with_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.919s
user	0m0.243s
sys	0m0.136s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.54 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.64 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.18 sec*proc (2 tests)

Total Test time (real) =   1.19 sec
        1.21 real         0.74 user         0.06 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 25: test-model-load-cancel
1/2 Test #25: test-model-load-cancel ...........   Passed    0.26 sec
    Start 26: test-autorelease
2/2 Test #26: test-autorelease .................   Passed    0.26 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.52 sec*proc (2 tests)

Total Test time (real) =   0.53 sec
        0.53 real         0.15 user         0.04 sys
```
