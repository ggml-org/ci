### ctest_debug

Runs ctest in debug mode
- status: 0
```
+ ctest --output-on-failure -L main -E test-opt
Test project /Users/ggml/work/llama.cpp/build-ci-debug
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.51 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    1.15 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.17 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.46 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.29 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.23 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.70 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.09 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.23 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.09 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.64 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.23 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.23 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    2.24 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.19 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.31 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.20 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed   18.14 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.38 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    1.20 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.23 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.36 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    3.11 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    1.30 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed  194.32 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.90 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   26.42 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.35 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.22 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    = 257.89 sec*proc (29 tests)

Total Test time (real) = 257.90 sec

real	4m17.970s
user	8m37.202s
sys	0m7.266s
```

### ctest_release

Runs ctest in release mode
- status: 0
```
+ ctest --output-on-failure -L main
Test project /Users/ggml/work/llama.cpp/build-ci-release
      Start  1: test-tokenizer-0-bert-bge
 1/29 Test  #1: test-tokenizer-0-bert-bge .........   Passed    1.31 sec
      Start  2: test-tokenizer-0-command-r
 2/29 Test  #2: test-tokenizer-0-command-r ........   Passed    0.23 sec
      Start  3: test-tokenizer-0-deepseek-coder
 3/29 Test  #3: test-tokenizer-0-deepseek-coder ...   Passed    0.04 sec
      Start  4: test-tokenizer-0-deepseek-llm
 4/29 Test  #4: test-tokenizer-0-deepseek-llm .....   Passed    0.08 sec
      Start  5: test-tokenizer-0-falcon
 5/29 Test  #5: test-tokenizer-0-falcon ...........   Passed    0.06 sec
      Start  6: test-tokenizer-0-gpt-2
 6/29 Test  #6: test-tokenizer-0-gpt-2 ............   Passed    0.05 sec
      Start  7: test-tokenizer-0-llama-bpe
 7/29 Test  #7: test-tokenizer-0-llama-bpe ........   Passed    0.15 sec
      Start  8: test-tokenizer-0-llama-spm
 8/29 Test  #8: test-tokenizer-0-llama-spm ........   Passed    0.03 sec
      Start  9: test-tokenizer-0-mpt
 9/29 Test  #9: test-tokenizer-0-mpt ..............   Passed    0.05 sec
      Start 10: test-tokenizer-0-phi-3
10/29 Test #10: test-tokenizer-0-phi-3 ............   Passed    0.03 sec
      Start 11: test-tokenizer-0-qwen2
11/29 Test #11: test-tokenizer-0-qwen2 ............   Passed    0.12 sec
      Start 12: test-tokenizer-0-refact
12/29 Test #12: test-tokenizer-0-refact ...........   Passed    0.05 sec
      Start 13: test-tokenizer-0-starcoder
13/29 Test #13: test-tokenizer-0-starcoder ........   Passed    0.05 sec
      Start 14: test-sampling
14/29 Test #14: test-sampling .....................   Passed    0.91 sec
      Start 15: test-grammar-parser
15/29 Test #15: test-grammar-parser ...............   Passed    0.17 sec
      Start 16: test-grammar-integration
16/29 Test #16: test-grammar-integration ..........   Passed    0.22 sec
      Start 17: test-llama-grammar
17/29 Test #17: test-llama-grammar ................   Passed    0.16 sec
      Start 18: test-chat
18/29 Test #18: test-chat .........................   Passed    1.74 sec
      Start 19: test-json-schema-to-grammar
19/29 Test #19: test-json-schema-to-grammar .......   Passed    2.23 sec
      Start 20: test-tokenizer-1-llama-spm
20/29 Test #20: test-tokenizer-1-llama-spm ........   Passed    0.34 sec
      Start 21: test-log
21/29 Test #21: test-log ..........................   Passed    0.21 sec
      Start 22: test-arg-parser
22/29 Test #22: test-arg-parser ...................   Passed    0.24 sec
      Start 23: test-chat-template
23/29 Test #23: test-chat-template ................   Passed    0.47 sec
      Start 24: test-gguf
24/29 Test #24: test-gguf .........................   Passed    0.46 sec
      Start 25: test-backend-ops
25/29 Test #25: test-backend-ops ..................   Passed   30.90 sec
      Start 28: test-barrier
26/29 Test #28: test-barrier ......................   Passed    0.41 sec
      Start 29: test-quantize-fns
27/29 Test #29: test-quantize-fns .................   Passed   14.17 sec
      Start 30: test-quantize-perf
28/29 Test #30: test-quantize-perf ................   Passed    0.21 sec
      Start 31: test-rope
29/29 Test #31: test-rope .........................   Passed    0.20 sec

100% tests passed, 0 tests failed out of 29

Label Time Summary:
main    =  55.30 sec*proc (29 tests)

Total Test time (real) =  55.31 sec

real	0m55.325s
user	1m16.934s
sys	0m6.417s
```
### embd_bge_small

BGE Small (BERT):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-f16.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.122 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.016.153 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.020.091 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.020.097 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.020.099 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.020.100 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.020.100 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.020.101 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.020.101 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.020.102 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.020.103 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.020.103 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.020.104 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.020.104 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.020.107 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.020.110 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.020.111 I llama_model_loader: - kv  12:                      bert.attention.causal bool             = false
0.00.020.111 I llama_model_loader: - kv  13:                          bert.pooling_type u32              = 2
0.00.020.112 I llama_model_loader: - kv  14:            tokenizer.ggml.token_type_count u32              = 2
0.00.020.112 I llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
0.00.020.112 I llama_model_loader: - kv  16:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.023.470 I llama_model_loader: - kv  17:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.024.393 I llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.394 I llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.024.395 I llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.024.395 I llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
0.00.024.396 I llama_model_loader: - kv  22:               tokenizer.ggml.mask_token_id u32              = 103
0.00.024.396 I llama_model_loader: - kv  23:               general.quantization_version u32              = 2
0.00.024.397 I llama_model_loader: - type  f32:  124 tensors
0.00.024.397 I llama_model_loader: - type  f16:   73 tensors
0.00.024.397 I print_info: file format = GGUF V3 (latest)
0.00.024.398 I print_info: file type   = F16
0.00.024.399 I print_info: file size   = 63.84 MiB (16.12 BPW) 
0.00.027.726 I load: special tokens cache size = 5
0.00.029.394 I load: token to piece cache size = 0.2032 MB
0.00.029.398 I print_info: arch             = bert
0.00.029.398 I print_info: vocab_only       = 0
0.00.029.398 I print_info: n_ctx_train      = 512
0.00.029.398 I print_info: n_embd           = 384
0.00.029.399 I print_info: n_layer          = 12
0.00.029.402 I print_info: n_head           = 12
0.00.029.405 I print_info: n_head_kv        = 12
0.00.029.405 I print_info: n_rot            = 32
0.00.029.405 I print_info: n_swa            = 0
0.00.029.405 I print_info: n_embd_head_k    = 32
0.00.029.406 I print_info: n_embd_head_v    = 32
0.00.029.406 I print_info: n_gqa            = 1
0.00.029.407 I print_info: n_embd_k_gqa     = 384
0.00.029.407 I print_info: n_embd_v_gqa     = 384
0.00.029.414 I print_info: f_norm_eps       = 1.0e-12
0.00.029.415 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.029.415 I print_info: f_clamp_kqv      = 0.0e+00
0.00.029.415 I print_info: f_max_alibi_bias = 0.0e+00
0.00.029.415 I print_info: f_logit_scale    = 0.0e+00
0.00.029.416 I print_info: n_ff             = 1536
0.00.029.416 I print_info: n_expert         = 0
0.00.029.417 I print_info: n_expert_used    = 0
0.00.029.417 I print_info: causal attn      = 0
0.00.029.417 I print_info: pooling type     = 2
0.00.029.418 I print_info: rope type        = 2
0.00.029.418 I print_info: rope scaling     = linear
0.00.029.420 I print_info: freq_base_train  = 10000.0
0.00.029.421 I print_info: freq_scale_train = 1
0.00.029.421 I print_info: n_ctx_orig_yarn  = 512
0.00.029.421 I print_info: rope_finetuned   = unknown
0.00.029.421 I print_info: ssm_d_conv       = 0
0.00.029.421 I print_info: ssm_d_inner      = 0
0.00.029.422 I print_info: ssm_d_state      = 0
0.00.029.422 I print_info: ssm_dt_rank      = 0
0.00.029.422 I print_info: ssm_dt_b_c_rms   = 0
0.00.029.424 I print_info: model type       = 33M
0.00.029.425 I print_info: model params     = 33.21 M
0.00.029.425 I print_info: general.name     = Bge Small
0.00.029.426 I print_info: vocab type       = WPM
0.00.029.426 I print_info: n_vocab          = 30522
0.00.029.426 I print_info: n_merges         = 0
0.00.029.426 I print_info: BOS token        = 101 '[CLS]'
0.00.029.427 I print_info: UNK token        = 100 '[UNK]'
0.00.029.432 I print_info: SEP token        = 102 '[SEP]'
0.00.029.433 I print_info: PAD token        = 0 '[PAD]'
0.00.029.433 I print_info: MASK token       = 103 '[MASK]'
0.00.029.433 I print_info: LF token         = 0 '[PAD]'
0.00.029.433 I print_info: max token length = 21
0.00.031.799 I load_tensors: offloading 12 repeating layers to GPU
0.00.031.801 I load_tensors: offloading output layer to GPU
0.00.031.801 I load_tensors: offloaded 13/13 layers to GPU
0.00.031.820 I load_tensors: Metal_Mapped model buffer size =    40.73 MiB
0.00.031.822 I load_tensors:   CPU_Mapped model buffer size =    23.11 MiB
0.00.032.019 I llama_init_from_model: n_seq_max     = 1
0.00.032.020 I llama_init_from_model: n_ctx         = 512
0.00.032.021 I llama_init_from_model: n_ctx_per_seq = 512
0.00.032.021 I llama_init_from_model: n_batch       = 2048
0.00.032.021 I llama_init_from_model: n_ubatch      = 2048
0.00.032.021 I llama_init_from_model: flash_attn    = 0
0.00.032.022 I llama_init_from_model: freq_base     = 10000.0
0.00.032.022 I llama_init_from_model: freq_scale    = 1
0.00.032.023 I ggml_metal_init: allocating
0.00.032.026 I ggml_metal_init: found device: Apple M4
0.00.032.032 I ggml_metal_init: picking default device: Apple M4
0.00.032.640 I ggml_metal_init: using embedded metal library
0.00.035.813 I ggml_metal_init: GPU name:   Apple M4
0.00.035.816 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.035.816 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.035.817 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.035.817 I ggml_metal_init: simdgroup reduction   = true
0.00.035.817 I ggml_metal_init: simdgroup matrix mul. = true
0.00.035.817 I ggml_metal_init: has residency sets    = true
0.00.035.817 I ggml_metal_init: has bfloat            = true
0.00.035.818 I ggml_metal_init: use bfloat            = true
0.00.035.818 I ggml_metal_init: hasUnifiedMemory      = true
0.00.035.820 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.045.834 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.046.475 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.046.477 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.046.496 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.047.646 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.047.648 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.047.648 I llama_init_from_model: graph nodes  = 429
0.00.047.648 I llama_init_from_model: graph splits = 2
0.00.047.650 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.047.650 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.052.158 I 
0.00.052.187 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.052.769 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.043984 -0.019895  0.007670 -0.000838  0.001372 -0.037036  0.109436  0.042589  0.092046 -0.015918  0.006773 -0.035692 -0.017889  0.015040  0.018126  0.015860 -0.011308  0.010431 -0.085223 -0.008454  0.091368 -0.017067 -0.060328 -0.024484  0.027524  0.076059  0.027984 -0.014564  0.017645 -0.033268 -0.037874 -0.019012  0.068670 -0.009845 -0.025030  0.072346 -0.046570  0.011026 -0.050252  0.047693  0.032387 -0.011761  0.022055  0.049645  0.010465  0.005795 -0.028860  0.008933 -0.018507 -0.051466 -0.046036  0.030497 -0.035418  0.054204 -0.069650  0.044238  0.029786  0.046305  0.073410 -0.042580  0.076109  0.038851 -0.181182  0.082503  0.042250 -0.064549 -0.060097 -0.017853  0.006472  0.005873  0.017196 -0.026615  0.064578  0.112612  0.035159 -0.067416  0.027073 -0.067295 -0.033493 -0.033219  0.033252  0.013523 -0.003346 -0.037473 -0.052055  0.055155 -0.001982 -0.038276  0.064447  0.028824 -0.043334 -0.029236 -0.039462  0.036315  0.008379 -0.015481 -0.036573  0.018130  0.028600  0.342831 -0.044484  0.056114  0.017624 -0.020867 -0.066808  0.000146 -0.037885 -0.030063 -0.008541 -0.021595  0.000548 -0.003219  0.004020  0.018908 -0.008548  0.025835  0.049433  0.000091  0.050920 -0.042479 -0.031884  0.023606  0.030691 -0.023148 -0.046271 -0.079278  0.115198  0.046757  0.027829 -0.040710  0.067777 -0.022955  0.010332 -0.032937 -0.018305  0.043844  0.024268  0.052413  0.007469  0.008905  0.011244 -0.074655 -0.065561 -0.026750 -0.041192 -0.023884  0.026710  0.006912  0.027741  0.052873 -0.036673  0.057707 -0.000170  0.031764 -0.019760 -0.022083  0.041053 -0.058903  0.019619  0.043144  0.043582  0.041593 -0.022527  0.027037 -0.021833  0.005430 -0.041317 -0.001239  0.024452  0.002088  0.044333 -0.022745  0.043662  0.064763  0.055430  0.037060 -0.000919  0.046111  0.045790 -0.008487  0.063057 -0.073235 -0.011935  0.032107  0.023957  0.014725 -0.033688  0.001083 -0.015827 -0.019002  0.047872  0.110842  0.028428  0.031366 -0.013288 -0.057499  0.006644  0.005140 -0.012262 -0.051443 -0.000965 -0.017642 -0.019440 -0.040937  0.009209 -0.057957  0.050961  0.052340 -0.009617 -0.040259 -0.014075 -0.024877 -0.017251  0.006302  0.006588 -0.026919  0.015600  0.030760  0.002578  0.023206 -0.022199 -0.098557 -0.051105 -0.278031 -0.014994 -0.061553 -0.027218  0.017666 -0.010939 -0.017077  0.035055  0.046994 -0.015432  0.015232 -0.025477  0.047844 -0.005949 -0.000749 -0.061008 -0.068946 -0.060377 -0.035942  0.043319 -0.055047  0.015075  0.000537 -0.058175 -0.010439  0.012643  0.151495  0.127111 -0.013622  0.041993 -0.025672  0.014026 -0.001037 -0.150469  0.044857  0.005320 -0.036274 -0.029801 -0.020192 -0.034891  0.010235  0.033548 -0.048178 -0.051795 -0.017463 -0.023492  0.047349  0.052076 -0.016783 -0.055452  0.025836 -0.005707  0.010710  0.038705  0.008191 -0.009764 -0.105779 -0.027430 -0.096101  0.025068 -0.011260  0.092355  0.056101  0.003758  0.027791  0.002092 -0.051085 -0.039890 -0.013542 -0.044956 -0.015335  0.002910 -0.043498 -0.077951  0.065217 -0.006840 -0.001630 -0.014642  0.071560  0.023714 -0.037180  0.009175  0.001562 -0.032268  0.015468  0.037880  0.000337 -0.053207  0.021321 -0.039829  0.000031  0.013400  0.019798 -0.057879  0.006473 -0.049531 -0.267858  0.039156 -0.067972  0.038248 -0.012333  0.041485 -0.016117  0.052383 -0.071363  0.011369  0.024719 -0.007232  0.082087  0.028535 -0.021504  0.040494 -0.004570 -0.074594 -0.014749  0.020037  0.002304  0.023148  0.197212 -0.043222 -0.025983 -0.004959 -0.019278  0.074258  0.001716 -0.031979 -0.036599 -0.045082  0.000547 -0.011560  0.018126 -0.029461 -0.008463  0.006419  0.050799 -0.014959  0.006176  0.026096 -0.030809  0.048055  0.114093 -0.040826 -0.011457  0.005419 -0.003599  0.025154 -0.059142  0.013753 -0.010399  0.038709  0.051449  0.035413  0.035031 -0.017027  0.026360 -0.014490 -0.050032  0.003219  0.054121  0.039723 -0.039127 

0.00.057.094 I llama_perf_context_print:        load time =      36.00 ms
0.00.057.095 I llama_perf_context_print: prompt eval time =       4.19 ms /     9 tokens (    0.47 ms per token,  2146.43 tokens per second)
0.00.057.096 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.057.096 I llama_perf_context_print:       total time =       4.94 ms /    10 tokens
0.00.057.302 I ggml_metal_free: deallocating

real	0m0.283s
user	0m0.042s
sys	0m0.032s
```
- q8_0:
```
+ ./bin/llama-embedding --model ../models-mnt/bge-small/ggml-model-q8_0.gguf -p 'I believe the meaning of life is' -ngl 99 -c 0
0.00.000.044 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.933 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.011.371 I llama_model_loader: loaded meta data with 24 key-value pairs and 197 tensors from ../models-mnt/bge-small/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.011.375 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.011.376 I llama_model_loader: - kv   0:                       general.architecture str              = bert
0.00.011.377 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.011.379 I llama_model_loader: - kv   2:                               general.name str              = Bge Small
0.00.011.379 I llama_model_loader: - kv   3:                           general.basename str              = bge
0.00.011.380 I llama_model_loader: - kv   4:                         general.size_label str              = small
0.00.011.381 I llama_model_loader: - kv   5:                           bert.block_count u32              = 12
0.00.011.381 I llama_model_loader: - kv   6:                        bert.context_length u32              = 512
0.00.011.381 I llama_model_loader: - kv   7:                      bert.embedding_length u32              = 384
0.00.011.382 I llama_model_loader: - kv   8:                   bert.feed_forward_length u32              = 1536
0.00.011.382 I llama_model_loader: - kv   9:                  bert.attention.head_count u32              = 12
0.00.011.384 I llama_model_loader: - kv  10:          bert.attention.layer_norm_epsilon f32              = 0.000000
0.00.011.385 I llama_model_loader: - kv  11:                      bert.attention.causal bool             = false
0.00.011.385 I llama_model_loader: - kv  12:                          bert.pooling_type u32              = 2
0.00.011.385 I llama_model_loader: - kv  13:            tokenizer.ggml.token_type_count u32              = 2
0.00.011.388 I llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = bert
0.00.011.388 I llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = jina-v2-en
0.00.013.586 I llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
0.00.014.208 I llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.014.209 I llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 100
0.00.014.209 I llama_model_loader: - kv  19:          tokenizer.ggml.seperator_token_id u32              = 102
0.00.014.210 I llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0
0.00.014.210 I llama_model_loader: - kv  21:               tokenizer.ggml.mask_token_id u32              = 103
0.00.014.210 I llama_model_loader: - kv  22:               general.quantization_version u32              = 2
0.00.014.210 I llama_model_loader: - kv  23:                          general.file_type u32              = 7
0.00.014.211 I llama_model_loader: - type  f32:  124 tensors
0.00.014.211 I llama_model_loader: - type q8_0:   73 tensors
0.00.014.212 I print_info: file format = GGUF V3 (latest)
0.00.014.212 I print_info: file type   = Q8_0
0.00.014.213 I print_info: file size   = 34.38 MiB (8.68 BPW) 
0.00.016.508 I load: special tokens cache size = 5
0.00.017.735 I load: token to piece cache size = 0.2032 MB
0.00.017.738 I print_info: arch             = bert
0.00.017.738 I print_info: vocab_only       = 0
0.00.017.738 I print_info: n_ctx_train      = 512
0.00.017.739 I print_info: n_embd           = 384
0.00.017.739 I print_info: n_layer          = 12
0.00.017.743 I print_info: n_head           = 12
0.00.017.743 I print_info: n_head_kv        = 12
0.00.017.744 I print_info: n_rot            = 32
0.00.017.744 I print_info: n_swa            = 0
0.00.017.744 I print_info: n_embd_head_k    = 32
0.00.017.744 I print_info: n_embd_head_v    = 32
0.00.017.745 I print_info: n_gqa            = 1
0.00.017.746 I print_info: n_embd_k_gqa     = 384
0.00.017.746 I print_info: n_embd_v_gqa     = 384
0.00.017.747 I print_info: f_norm_eps       = 1.0e-12
0.00.017.747 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.017.747 I print_info: f_clamp_kqv      = 0.0e+00
0.00.017.748 I print_info: f_max_alibi_bias = 0.0e+00
0.00.017.748 I print_info: f_logit_scale    = 0.0e+00
0.00.017.750 I print_info: n_ff             = 1536
0.00.017.750 I print_info: n_expert         = 0
0.00.017.750 I print_info: n_expert_used    = 0
0.00.017.750 I print_info: causal attn      = 0
0.00.017.751 I print_info: pooling type     = 2
0.00.017.751 I print_info: rope type        = 2
0.00.017.751 I print_info: rope scaling     = linear
0.00.017.751 I print_info: freq_base_train  = 10000.0
0.00.017.752 I print_info: freq_scale_train = 1
0.00.017.752 I print_info: n_ctx_orig_yarn  = 512
0.00.017.752 I print_info: rope_finetuned   = unknown
0.00.017.752 I print_info: ssm_d_conv       = 0
0.00.017.752 I print_info: ssm_d_inner      = 0
0.00.017.752 I print_info: ssm_d_state      = 0
0.00.017.752 I print_info: ssm_dt_rank      = 0
0.00.017.753 I print_info: ssm_dt_b_c_rms   = 0
0.00.017.753 I print_info: model type       = 33M
0.00.017.753 I print_info: model params     = 33.21 M
0.00.017.753 I print_info: general.name     = Bge Small
0.00.017.754 I print_info: vocab type       = WPM
0.00.017.754 I print_info: n_vocab          = 30522
0.00.017.757 I print_info: n_merges         = 0
0.00.017.757 I print_info: BOS token        = 101 '[CLS]'
0.00.017.757 I print_info: UNK token        = 100 '[UNK]'
0.00.017.757 I print_info: SEP token        = 102 '[SEP]'
0.00.017.757 I print_info: PAD token        = 0 '[PAD]'
0.00.017.758 I print_info: MASK token       = 103 '[MASK]'
0.00.017.758 I print_info: LF token         = 0 '[PAD]'
0.00.017.758 I print_info: max token length = 21
0.00.019.613 I load_tensors: offloading 12 repeating layers to GPU
0.00.019.614 I load_tensors: offloading output layer to GPU
0.00.019.615 I load_tensors: offloaded 13/13 layers to GPU
0.00.019.623 I load_tensors: Metal_Mapped model buffer size =    21.75 MiB
0.00.019.624 I load_tensors:   CPU_Mapped model buffer size =    12.63 MiB
0.00.019.837 I llama_init_from_model: n_seq_max     = 1
0.00.019.838 I llama_init_from_model: n_ctx         = 512
0.00.019.838 I llama_init_from_model: n_ctx_per_seq = 512
0.00.019.838 I llama_init_from_model: n_batch       = 2048
0.00.019.838 I llama_init_from_model: n_ubatch      = 2048
0.00.019.838 I llama_init_from_model: flash_attn    = 0
0.00.019.839 I llama_init_from_model: freq_base     = 10000.0
0.00.019.839 I llama_init_from_model: freq_scale    = 1
0.00.019.840 I ggml_metal_init: allocating
0.00.019.864 I ggml_metal_init: found device: Apple M4
0.00.019.867 I ggml_metal_init: picking default device: Apple M4
0.00.020.421 I ggml_metal_init: using embedded metal library
0.00.022.825 I ggml_metal_init: GPU name:   Apple M4
0.00.022.826 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.022.827 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.022.827 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.022.828 I ggml_metal_init: simdgroup reduction   = true
0.00.022.828 I ggml_metal_init: simdgroup matrix mul. = true
0.00.022.828 I ggml_metal_init: has residency sets    = true
0.00.022.828 I ggml_metal_init: has bfloat            = true
0.00.022.828 I ggml_metal_init: use bfloat            = true
0.00.022.829 I ggml_metal_init: hasUnifiedMemory      = true
0.00.022.830 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.033.138 I llama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
0.00.033.733 I llama_kv_cache_init:      Metal KV buffer size =     9.00 MiB
0.00.033.737 I llama_init_from_model: KV self size  =    9.00 MiB, K (f16):    4.50 MiB, V (f16):    4.50 MiB
0.00.033.750 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.034.783 I llama_init_from_model:      Metal compute buffer size =    16.00 MiB
0.00.034.785 I llama_init_from_model:        CPU compute buffer size =     2.51 MiB
0.00.034.785 I llama_init_from_model: graph nodes  = 429
0.00.034.785 I llama_init_from_model: graph splits = 2
0.00.034.786 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 512
0.00.034.787 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.038.885 I 
0.00.038.910 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.039.452 I batch_decode: n_tokens = 9, n_seq = 1

embedding 0: -0.044994 -0.019858  0.007986 -0.001410  0.001531 -0.036276  0.109404  0.043235  0.092836 -0.015201  0.005971 -0.036330 -0.018159  0.015015  0.017667  0.014510 -0.011792  0.011812 -0.084764 -0.008081  0.091650 -0.017638 -0.061084 -0.025066  0.026879  0.076180  0.028256 -0.014273  0.017420 -0.033553 -0.037520 -0.018280  0.068843 -0.010129 -0.024896  0.071968 -0.046406  0.011471 -0.050333  0.048931  0.032317 -0.012107  0.021812  0.049796  0.010366  0.005264 -0.028665  0.008581 -0.018508 -0.052358 -0.046410  0.029426 -0.034989  0.053289 -0.069849  0.043767  0.029291  0.046759  0.073664 -0.043090  0.075213  0.038641 -0.180496  0.081648  0.043389 -0.065615 -0.059878 -0.017387  0.006913  0.005155  0.017165 -0.026679  0.064347  0.112564  0.034871 -0.067783  0.027124 -0.066989 -0.034296 -0.033344  0.033189  0.014325 -0.004253 -0.036945 -0.052008  0.054151 -0.001715 -0.038112  0.063016  0.028668 -0.041665 -0.028879 -0.039263  0.036879  0.007718 -0.015657 -0.036270  0.018699  0.029792  0.344948 -0.043880  0.056487  0.018022 -0.021448 -0.065281  0.000253 -0.037910 -0.029663 -0.008828 -0.020643  0.001162 -0.003510  0.003091  0.018576 -0.009637  0.025316  0.049298 -0.000937  0.051568 -0.042363 -0.031550  0.023315  0.030044 -0.023267 -0.045483 -0.079644  0.114489  0.047503  0.027208 -0.041477  0.067717 -0.022845  0.010232 -0.033486 -0.017199  0.044319  0.022956  0.052194  0.008122  0.008112  0.009951 -0.074643 -0.065035 -0.025938 -0.040806 -0.024533  0.027409  0.006078  0.027693  0.052243 -0.036587  0.058682  0.001467  0.032191 -0.019964 -0.021840  0.041805 -0.058801  0.019348  0.042626  0.043999  0.040911 -0.021909  0.028025 -0.022801  0.005520 -0.041266 -0.000085  0.024325  0.001532  0.044802 -0.022888  0.043056  0.065030  0.056292  0.037666 -0.001004  0.047521  0.045624 -0.008614  0.062270 -0.073313 -0.011718  0.032708  0.023348  0.014650 -0.033924  0.001158 -0.015872 -0.018974  0.047982  0.110740  0.029177  0.030881 -0.012325 -0.057836  0.006939  0.004378 -0.011864 -0.051754 -0.003018 -0.017766 -0.020051 -0.041018  0.009118 -0.058568  0.050632  0.051795 -0.008968 -0.040668 -0.014765 -0.024320 -0.015980  0.005823  0.007147 -0.027335  0.016275  0.030734  0.002342  0.023167 -0.022169 -0.098059 -0.050901 -0.277240 -0.015100 -0.061566 -0.026219  0.017141 -0.010285 -0.017522  0.034725  0.047897 -0.016594  0.015489 -0.024132  0.048613 -0.004993 -0.000001 -0.060860 -0.068346 -0.059622 -0.036157  0.043605 -0.056353  0.014713  0.000292 -0.058627 -0.010995  0.012274  0.151597  0.127025 -0.012582  0.042765 -0.025227  0.013629 -0.000381 -0.150686  0.043974  0.004714 -0.036955 -0.029586 -0.019853 -0.034063  0.009522  0.034004 -0.048909 -0.051765 -0.017243 -0.025080  0.047588  0.050981 -0.017553 -0.056746  0.024054 -0.006285  0.011003  0.038719  0.008083 -0.008595 -0.106361 -0.027365 -0.096966  0.025294 -0.011096  0.091579  0.056180  0.004494  0.028066  0.001727 -0.051388 -0.039497 -0.013817 -0.045907 -0.015238  0.002920 -0.043938 -0.077716  0.066102 -0.006195 -0.000885 -0.013768  0.071562  0.023826 -0.036363  0.007848  0.001487 -0.032916  0.016336  0.037378  0.000625 -0.052389  0.021549 -0.039695 -0.000026  0.013496  0.020436 -0.057513  0.005900 -0.049906 -0.267546  0.038686 -0.067509  0.037450 -0.011471  0.041547 -0.015904  0.051347 -0.072313  0.012571  0.024808 -0.007420  0.082069  0.028175 -0.021956  0.040736 -0.003945 -0.074192 -0.014923  0.020417  0.002644  0.023207  0.197144 -0.043068 -0.025876 -0.004886 -0.018582  0.073936  0.001341 -0.031663 -0.037140 -0.044262  0.000049 -0.010741  0.018225 -0.028045 -0.008828  0.006047  0.050183 -0.015017  0.007037  0.025894 -0.030628  0.048075  0.112795 -0.039982 -0.011695  0.005039 -0.002800  0.025072 -0.059720  0.014182 -0.010019  0.038100  0.051088  0.034726  0.035947 -0.017035  0.027243 -0.015391 -0.051195  0.003744  0.053928  0.040014 -0.039140 

0.00.043.919 I llama_perf_context_print:        load time =      29.95 ms
0.00.043.920 I llama_perf_context_print: prompt eval time =       4.34 ms /     9 tokens (    0.48 ms per token,  2073.26 tokens per second)
0.00.043.921 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.043.921 I llama_perf_context_print:       total time =       5.03 ms /    10 tokens
0.00.044.127 I ggml_metal_free: deallocating

real	0m0.055s
user	0m0.029s
sys	0m0.016s
```
### rerank_tiny

Rerank Tiny (Jina):
- status: 0
- f16: 
```
+ ./bin/llama-embedding --model ../models-mnt/rerank-tiny/ggml-model-f16.gguf -p 'what is panda?</s></s>hi\nwhat is panda?</s></s>it'\''s a bear\nwhat is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.' -ngl 99 -c 0 --pooling rank --embd-normalize -1 --verbose-prompt
0.00.000.260 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.022.738 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.035.014 I llama_model_loader: loaded meta data with 28 key-value pairs and 70 tensors from ../models-mnt/rerank-tiny/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.035.019 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.035.021 I llama_model_loader: - kv   0:                       general.architecture str              = jina-bert-v2
0.00.035.022 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.035.023 I llama_model_loader: - kv   2:                               general.name str              = Jina Bert Implementation
0.00.035.023 I llama_model_loader: - kv   3:                       general.organization str              = Jinaai
0.00.035.024 I llama_model_loader: - kv   4:                         general.size_label str              = 33M
0.00.035.025 I llama_model_loader: - kv   5:                   jina-bert-v2.block_count u32              = 4
0.00.035.026 I llama_model_loader: - kv   6:                jina-bert-v2.context_length u32              = 8192
0.00.035.026 I llama_model_loader: - kv   7:              jina-bert-v2.embedding_length u32              = 384
0.00.035.027 I llama_model_loader: - kv   8:           jina-bert-v2.feed_forward_length u32              = 1536
0.00.035.031 I llama_model_loader: - kv   9:          jina-bert-v2.attention.head_count u32              = 12
0.00.035.034 I llama_model_loader: - kv  10:  jina-bert-v2.attention.layer_norm_epsilon f32              = 0.000000
0.00.035.035 I llama_model_loader: - kv  11:                          general.file_type u32              = 1
0.00.035.037 I llama_model_loader: - kv  12:              jina-bert-v2.attention.causal bool             = false
0.00.035.038 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.035.038 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = jina-v1-en
0.00.042.309 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,61056]   = ["<s>", "<pad>", "</s>", "<unk>", "<m...
0.00.044.425 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,61056]   = [3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, ...
0.00.049.185 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,39382]   = ["t h", "i n", "a n", "e r", "th e", ...
0.00.049.186 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.049.187 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2
0.00.049.187 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 3
0.00.049.188 I llama_model_loader: - kv  21:          tokenizer.ggml.seperator_token_id u32              = 2
0.00.049.188 I llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 1
0.00.049.189 I llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 4
0.00.049.189 I llama_model_loader: - kv  24:            tokenizer.ggml.token_type_count u32              = 2
0.00.049.189 I llama_model_loader: - kv  25:               tokenizer.ggml.add_bos_token bool             = true
0.00.049.190 I llama_model_loader: - kv  26:               tokenizer.ggml.add_eos_token bool             = true
0.00.049.190 I llama_model_loader: - kv  27:               general.quantization_version u32              = 2
0.00.049.191 I llama_model_loader: - type  f32:   40 tensors
0.00.049.191 I llama_model_loader: - type  f16:   30 tensors
0.00.049.192 I print_info: file format = GGUF V3 (latest)
0.00.049.193 I print_info: file type   = F16
0.00.049.194 I print_info: file size   = 62.78 MiB (16.01 BPW) 
0.00.053.564 W load: empty token at index 5
0.00.058.642 W load: model vocab missing newline token, using special_pad_id instead
0.00.060.130 W load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
0.00.060.170 I load: special tokens cache size = 5
0.00.328.115 I load: token to piece cache size = 1.5060 MB
0.00.328.124 I print_info: arch             = jina-bert-v2
0.00.328.124 I print_info: vocab_only       = 0
0.00.328.129 I print_info: n_ctx_train      = 8192
0.00.328.130 I print_info: n_embd           = 384
0.00.328.130 I print_info: n_layer          = 4
0.00.328.134 I print_info: n_head           = 12
0.00.328.135 I print_info: n_head_kv        = 12
0.00.328.135 I print_info: n_rot            = 32
0.00.328.135 I print_info: n_swa            = 0
0.00.328.135 I print_info: n_embd_head_k    = 32
0.00.328.135 I print_info: n_embd_head_v    = 32
0.00.328.136 I print_info: n_gqa            = 1
0.00.328.136 I print_info: n_embd_k_gqa     = 384
0.00.328.136 I print_info: n_embd_v_gqa     = 384
0.00.328.137 I print_info: f_norm_eps       = 1.0e-12
0.00.328.137 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.328.137 I print_info: f_clamp_kqv      = 0.0e+00
0.00.328.138 I print_info: f_max_alibi_bias = 8.0e+00
0.00.328.138 I print_info: f_logit_scale    = 0.0e+00
0.00.328.139 I print_info: n_ff             = 1536
0.00.328.139 I print_info: n_expert         = 0
0.00.328.139 I print_info: n_expert_used    = 0
0.00.328.139 I print_info: causal attn      = 0
0.00.328.139 I print_info: pooling type     = -1
0.00.328.140 I print_info: rope type        = -1
0.00.328.141 I print_info: rope scaling     = linear
0.00.328.141 I print_info: freq_base_train  = 10000.0
0.00.328.142 I print_info: freq_scale_train = 1
0.00.328.142 I print_info: n_ctx_orig_yarn  = 8192
0.00.328.142 I print_info: rope_finetuned   = unknown
0.00.328.142 I print_info: ssm_d_conv       = 0
0.00.328.142 I print_info: ssm_d_inner      = 0
0.00.328.142 I print_info: ssm_d_state      = 0
0.00.328.142 I print_info: ssm_dt_rank      = 0
0.00.328.143 I print_info: ssm_dt_b_c_rms   = 0
0.00.328.143 I print_info: model type       = 33M
0.00.328.143 I print_info: model params     = 32.90 M
0.00.328.145 I print_info: general.name     = Jina Bert Implementation
0.00.328.145 I print_info: vocab type       = BPE
0.00.328.145 I print_info: n_vocab          = 61056
0.00.328.145 I print_info: n_merges         = 39382
0.00.328.146 I print_info: BOS token        = 0 '<s>'
0.00.328.146 I print_info: EOS token        = 2 '</s>'
0.00.328.146 I print_info: UNK token        = 3 '<unk>'
0.00.328.146 I print_info: SEP token        = 2 '</s>'
0.00.328.147 I print_info: PAD token        = 1 '<pad>'
0.00.328.147 I print_info: MASK token       = 4 '<mask>'
0.00.328.147 I print_info: EOG token        = 2 '</s>'
0.00.328.147 I print_info: max token length = 45
0.00.329.273 I load_tensors: offloading 4 repeating layers to GPU
0.00.329.274 I load_tensors: offloading output layer to GPU
0.00.329.274 I load_tensors: offloaded 5/5 layers to GPU
0.00.329.289 I load_tensors: Metal_Mapped model buffer size =    18.06 MiB
0.00.329.291 I load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
0.00.329.442 I llama_init_from_model: n_seq_max     = 1
0.00.329.443 I llama_init_from_model: n_ctx         = 8192
0.00.329.444 I llama_init_from_model: n_ctx_per_seq = 8192
0.00.329.444 I llama_init_from_model: n_batch       = 2048
0.00.329.444 I llama_init_from_model: n_ubatch      = 2048
0.00.329.444 I llama_init_from_model: flash_attn    = 0
0.00.329.444 I llama_init_from_model: freq_base     = 10000.0
0.00.329.445 I llama_init_from_model: freq_scale    = 1
0.00.329.445 I ggml_metal_init: allocating
0.00.329.450 I ggml_metal_init: found device: Apple M4
0.00.329.453 I ggml_metal_init: picking default device: Apple M4
0.00.329.968 I ggml_metal_init: using embedded metal library
0.00.332.468 I ggml_metal_init: GPU name:   Apple M4
0.00.332.470 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.332.470 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.332.470 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.332.471 I ggml_metal_init: simdgroup reduction   = true
0.00.332.471 I ggml_metal_init: simdgroup matrix mul. = true
0.00.332.471 I ggml_metal_init: has residency sets    = true
0.00.332.471 I ggml_metal_init: has bfloat            = true
0.00.332.471 I ggml_metal_init: use bfloat            = true
0.00.332.472 I ggml_metal_init: hasUnifiedMemory      = true
0.00.332.473 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.342.676 I llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 4, can_shift = 1
0.00.345.676 I llama_kv_cache_init:      Metal KV buffer size =    48.00 MiB
0.00.345.677 I llama_init_from_model: KV self size  =   48.00 MiB, K (f16):   24.00 MiB, V (f16):   24.00 MiB
0.00.345.699 I llama_init_from_model:        CPU  output buffer size =     0.00 MiB
0.00.351.985 I llama_init_from_model:      Metal compute buffer size =   220.01 MiB
0.00.351.987 I llama_init_from_model:        CPU compute buffer size =    22.02 MiB
0.00.351.987 I llama_init_from_model: graph nodes  = 154
0.00.351.987 I llama_init_from_model: graph splits = 2
0.00.351.989 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 8192
0.00.351.989 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.361.114 I 
0.00.361.152 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.361.245 I main: prompt 0: 'what is panda?</s></s>hi'
0.00.361.246 I main: number of tokens in prompt = 9
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 23233 -> 'hi'
     2 -> '</s>'


0.00.361.249 I main: prompt 1: 'what is panda?</s></s>it's a bear'
0.00.361.249 I main: number of tokens in prompt = 13
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21153 -> 'it'
    37 -> '''
    87 -> 's'
    69 -> 'a'
 25706 -> 'bear'
     2 -> '</s>'


0.00.361.254 I main: prompt 2: 'what is panda?</s></s>The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.'
0.00.361.254 I main: number of tokens in prompt = 40
     0 -> '<s>'
 21381 -> 'what'
 21152 -> 'is'
 49990 -> 'panda'
    61 -> '?'
     2 -> '</s>'
     2 -> '</s>'
 21215 -> 'he'
 28390 -> 'giant'
 49990 -> 'panda'
    38 -> '('
 21163 -> 'il'
 26237 -> 'uro'
 21223 -> 'po'
 23179 -> 'da'
 36906 -> 'melan'
 26791 -> 'ole'
    89 -> 'u'
 21402 -> 'ca'
 21686 -> '),'
 23314 -> 'sometimes'
 22517 -> 'called'
    69 -> 'a'
 49990 -> 'panda'
 25706 -> 'bear'
 21142 -> 'or'
 22810 -> 'simply'
 49990 -> 'panda'
    42 -> ','
 21152 -> 'is'
    69 -> 'a'
 25706 -> 'bear'
 25677 -> 'species'
 28930 -> 'ende'
 22024 -> 'mic'
 21148 -> 'to'
    76 -> 'h'
 22344 -> 'ina'
    44 -> '.'
     2 -> '</s>'


0.00.361.735 I batch_decode: n_tokens = 62, n_seq = 3

rerank score 0:    0.023
rerank score 1:    0.024
rerank score 2:    0.199

0.00.365.262 I llama_perf_context_print:        load time =     338.37 ms
0.00.365.263 I llama_perf_context_print: prompt eval time =       3.52 ms /    62 tokens (    0.06 ms per token, 17618.64 tokens per second)
0.00.365.264 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.365.265 I llama_perf_context_print:       total time =       4.15 ms /    63 tokens
0.00.365.534 I ggml_metal_free: deallocating

real	0m1.189s
user	0m0.342s
sys	0m0.048s
  - rerank score 0 @ 0.023 OK
  - rerank score 1 @ 0.024 OK
  - rerank score 2 @ 0.199 OK
```
### pythia_1_4b

Pythia 1.4B:
- status: 0
- perplexity:
  - f16 @ 10.1498 OK
  - q8_0 @ 10.1362 OK
  - q4_0 @ 11.1740 OK
  - q4_1 @ 10.5507 OK
  - q5_0 @ 10.0972 OK
  - q5_1 @ 10.1971 OK
  - q3_k @ 12.0517 OK
  - q4_k @ 10.1031 OK
  - q5_k @ 10.2433 OK
  - q6_k @ 10.3179 OK
- imatrix:
```
Final estimate: PPL = 10.1498 +/- 3.22650
```
- f16: 
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.209 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.394 I main: llama backend init
0.00.000.402 I main: load the model and apply lora adapter, if any
0.00.048.405 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.060.568 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.060.581 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.060.585 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.060.586 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.060.586 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.060.587 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.060.588 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.060.592 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.060.592 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.060.593 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.060.594 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.060.595 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.060.595 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.060.596 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.060.602 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.060.602 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.060.603 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.067.433 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.069.567 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.076.533 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.076.541 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.076.542 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.076.542 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.076.543 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.076.544 I llama_model_loader: - type  f32:  194 tensors
0.00.076.544 I llama_model_loader: - type  f16:   98 tensors
0.00.076.546 I print_info: file format = GGUF V3 (latest)
0.00.076.547 I print_info: file type   = all F32 (guessed)
0.00.076.549 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.093.605 I load: special tokens cache size = 25
0.00.103.582 I load: token to piece cache size = 0.2984 MB
0.00.103.586 I print_info: arch             = gptneox
0.00.103.586 I print_info: vocab_only       = 0
0.00.103.586 I print_info: n_ctx_train      = 2048
0.00.103.586 I print_info: n_embd           = 2048
0.00.103.587 I print_info: n_layer          = 24
0.00.103.592 I print_info: n_head           = 16
0.00.103.593 I print_info: n_head_kv        = 16
0.00.103.593 I print_info: n_rot            = 32
0.00.103.593 I print_info: n_swa            = 0
0.00.103.594 I print_info: n_embd_head_k    = 128
0.00.103.594 I print_info: n_embd_head_v    = 128
0.00.103.595 I print_info: n_gqa            = 1
0.00.103.596 I print_info: n_embd_k_gqa     = 2048
0.00.103.596 I print_info: n_embd_v_gqa     = 2048
0.00.103.597 I print_info: f_norm_eps       = 1.0e-05
0.00.103.602 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.103.602 I print_info: f_clamp_kqv      = 0.0e+00
0.00.103.602 I print_info: f_max_alibi_bias = 0.0e+00
0.00.103.603 I print_info: f_logit_scale    = 0.0e+00
0.00.103.604 I print_info: n_ff             = 8192
0.00.103.604 I print_info: n_expert         = 0
0.00.103.604 I print_info: n_expert_used    = 0
0.00.103.604 I print_info: causal attn      = 1
0.00.103.604 I print_info: pooling type     = 0
0.00.103.605 I print_info: rope type        = 2
0.00.103.605 I print_info: rope scaling     = linear
0.00.103.607 I print_info: freq_base_train  = 10000.0
0.00.103.607 I print_info: freq_scale_train = 1
0.00.103.608 I print_info: n_ctx_orig_yarn  = 2048
0.00.103.608 I print_info: rope_finetuned   = unknown
0.00.103.608 I print_info: ssm_d_conv       = 0
0.00.103.608 I print_info: ssm_d_inner      = 0
0.00.103.608 I print_info: ssm_d_state      = 0
0.00.103.609 I print_info: ssm_dt_rank      = 0
0.00.103.609 I print_info: ssm_dt_b_c_rms   = 0
0.00.103.609 I print_info: model type       = 1.4B
0.00.103.610 I print_info: model params     = 1.41 B
0.00.103.610 I print_info: general.name     = 1.4B
0.00.103.610 I print_info: vocab type       = BPE
0.00.103.611 I print_info: n_vocab          = 50304
0.00.103.611 I print_info: n_merges         = 50009
0.00.103.611 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.103.612 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.103.612 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.103.612 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.103.612 I print_info: LF token         = 187 'Ċ'
0.00.103.613 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.103.613 I print_info: max token length = 1024
0.00.147.629 I load_tensors: offloading 24 repeating layers to GPU
0.00.147.632 I load_tensors: offloading output layer to GPU
0.00.147.633 I load_tensors: offloaded 25/25 layers to GPU
0.00.147.655 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.00.147.656 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.00.147.975 I llama_init_from_model: n_seq_max     = 1
0.00.147.977 I llama_init_from_model: n_ctx         = 2048
0.00.147.977 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.147.977 I llama_init_from_model: n_batch       = 2048
0.00.147.977 I llama_init_from_model: n_ubatch      = 512
0.00.147.977 I llama_init_from_model: flash_attn    = 0
0.00.147.978 I llama_init_from_model: freq_base     = 10000.0
0.00.147.978 I llama_init_from_model: freq_scale    = 1
0.00.147.979 I ggml_metal_init: allocating
0.00.147.996 I ggml_metal_init: found device: Apple M4
0.00.148.002 I ggml_metal_init: picking default device: Apple M4
0.00.148.590 I ggml_metal_init: using embedded metal library
0.00.158.063 I ggml_metal_init: GPU name:   Apple M4
0.00.158.065 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.158.066 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.158.066 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.158.066 I ggml_metal_init: simdgroup reduction   = true
0.00.158.066 I ggml_metal_init: simdgroup matrix mul. = true
0.00.158.066 I ggml_metal_init: has residency sets    = true
0.00.158.067 I ggml_metal_init: has bfloat            = true
0.00.158.067 I ggml_metal_init: use bfloat            = true
0.00.158.067 I ggml_metal_init: hasUnifiedMemory      = true
0.00.158.068 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.181.449 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.210.330 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.210.336 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.210.378 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.215.164 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.215.167 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.215.168 I llama_init_from_model: graph nodes  = 967
0.00.215.168 I llama_init_from_model: graph splits = 2
0.00.215.171 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.215.301 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.215.301 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.281.942 I main: llama threadpool init, n_threads = 4
0.00.281.981 I 
0.00.282.010 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.282.012 I 
0.00.282.057 I sampler seed: 1234
0.00.282.062 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.282.090 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.282.091 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.282.091 I 
I believe the meaning of life is to do the things we love, with the people we love, in the way we love, and in the time we love.

The last few months have been very hard for me. I don't like feeling guilty. I don't like feeling bad. I don't like feeling like I am a failure,

0.02.114.175 I llama_perf_sampler_print:    sampling time =       1.21 ms /    71 runs   (    0.02 ms per token, 58677.69 tokens per second)
0.02.114.176 I llama_perf_context_print:        load time =     232.67 ms
0.02.114.177 I llama_perf_context_print: prompt eval time =      43.63 ms /     7 tokens (    6.23 ms per token,   160.45 tokens per second)
0.02.114.177 I llama_perf_context_print:        eval time =    1785.58 ms /    63 runs   (   28.34 ms per token,    35.28 tokens per second)
0.02.114.178 I llama_perf_context_print:       total time =    1833.09 ms /    70 tokens
0.02.114.421 I ggml_metal_free: deallocating

real	0m2.441s
user	0m0.131s
sys	0m0.139s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-f16.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.564 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.032.073 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.048.770 I llama_model_loader: loaded meta data with 22 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-f16.gguf (version GGUF V3 (latest))
0.00.048.789 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.048.794 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.048.795 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.048.811 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.048.812 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.048.813 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.048.816 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.048.817 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.048.817 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.048.818 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.048.819 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.048.822 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.048.823 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.048.827 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.048.828 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.048.829 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.056.129 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.058.553 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.068.019 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.068.025 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.068.025 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.068.026 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.068.032 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.068.034 I llama_model_loader: - type  f32:  194 tensors
0.00.068.034 I llama_model_loader: - type  f16:   98 tensors
0.00.068.036 I print_info: file format = GGUF V3 (latest)
0.00.068.036 I print_info: file type   = all F32 (guessed)
0.00.068.039 I print_info: file size   = 2.64 GiB (16.01 BPW) 
0.00.083.462 I load: special tokens cache size = 25
0.00.092.553 I load: token to piece cache size = 0.2984 MB
0.00.092.556 I print_info: arch             = gptneox
0.00.092.557 I print_info: vocab_only       = 0
0.00.092.557 I print_info: n_ctx_train      = 2048
0.00.092.557 I print_info: n_embd           = 2048
0.00.092.557 I print_info: n_layer          = 24
0.00.092.561 I print_info: n_head           = 16
0.00.092.562 I print_info: n_head_kv        = 16
0.00.092.562 I print_info: n_rot            = 32
0.00.092.562 I print_info: n_swa            = 0
0.00.092.562 I print_info: n_embd_head_k    = 128
0.00.092.563 I print_info: n_embd_head_v    = 128
0.00.092.564 I print_info: n_gqa            = 1
0.00.092.565 I print_info: n_embd_k_gqa     = 2048
0.00.092.566 I print_info: n_embd_v_gqa     = 2048
0.00.092.566 I print_info: f_norm_eps       = 1.0e-05
0.00.092.567 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.092.567 I print_info: f_clamp_kqv      = 0.0e+00
0.00.092.567 I print_info: f_max_alibi_bias = 0.0e+00
0.00.092.567 I print_info: f_logit_scale    = 0.0e+00
0.00.092.568 I print_info: n_ff             = 8192
0.00.092.568 I print_info: n_expert         = 0
0.00.092.569 I print_info: n_expert_used    = 0
0.00.092.569 I print_info: causal attn      = 1
0.00.092.569 I print_info: pooling type     = 0
0.00.092.569 I print_info: rope type        = 2
0.00.092.570 I print_info: rope scaling     = linear
0.00.092.572 I print_info: freq_base_train  = 10000.0
0.00.092.572 I print_info: freq_scale_train = 1
0.00.092.573 I print_info: n_ctx_orig_yarn  = 2048
0.00.092.573 I print_info: rope_finetuned   = unknown
0.00.092.573 I print_info: ssm_d_conv       = 0
0.00.092.573 I print_info: ssm_d_inner      = 0
0.00.092.573 I print_info: ssm_d_state      = 0
0.00.092.574 I print_info: ssm_dt_rank      = 0
0.00.092.574 I print_info: ssm_dt_b_c_rms   = 0
0.00.092.574 I print_info: model type       = 1.4B
0.00.092.575 I print_info: model params     = 1.41 B
0.00.092.575 I print_info: general.name     = 1.4B
0.00.092.575 I print_info: vocab type       = BPE
0.00.092.575 I print_info: n_vocab          = 50304
0.00.092.576 I print_info: n_merges         = 50009
0.00.092.580 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.092.580 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.092.580 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.092.580 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.092.581 I print_info: LF token         = 187 'Ċ'
0.00.092.581 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.092.582 I print_info: max token length = 1024
0.01.472.227 I load_tensors: offloading 24 repeating layers to GPU
0.01.472.231 I load_tensors: offloading output layer to GPU
0.01.472.232 I load_tensors: offloaded 25/25 layers to GPU
0.01.472.257 I load_tensors: Metal_Mapped model buffer size =  2502.97 MiB
0.01.472.259 I load_tensors:   CPU_Mapped model buffer size =   196.50 MiB
0.01.473.147 I llama_init_from_model: n_seq_max     = 1
0.01.473.148 I llama_init_from_model: n_ctx         = 128
0.01.473.149 I llama_init_from_model: n_ctx_per_seq = 128
0.01.473.149 I llama_init_from_model: n_batch       = 128
0.01.473.149 I llama_init_from_model: n_ubatch      = 128
0.01.473.150 I llama_init_from_model: flash_attn    = 0
0.01.473.150 I llama_init_from_model: freq_base     = 10000.0
0.01.473.151 I llama_init_from_model: freq_scale    = 1
0.01.473.151 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.01.473.152 I ggml_metal_init: allocating
0.01.473.203 I ggml_metal_init: found device: Apple M4
0.01.473.212 I ggml_metal_init: picking default device: Apple M4
0.01.474.318 I ggml_metal_init: using embedded metal library
0.01.478.486 I ggml_metal_init: GPU name:   Apple M4
0.01.478.488 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.478.489 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.478.490 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.478.490 I ggml_metal_init: simdgroup reduction   = true
0.01.478.490 I ggml_metal_init: simdgroup matrix mul. = true
0.01.478.490 I ggml_metal_init: has residency sets    = true
0.01.478.491 I ggml_metal_init: has bfloat            = true
0.01.478.491 I ggml_metal_init: use bfloat            = true
0.01.478.491 I ggml_metal_init: hasUnifiedMemory      = true
0.01.478.492 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.490.346 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.492.129 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.01.492.132 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.01.492.159 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.493.857 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.01.493.859 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.01.493.859 I llama_init_from_model: graph nodes  = 967
0.01.493.859 I llama_init_from_model: graph splits = 2
0.01.493.860 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.01.493.861 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.529.604 I 
0.01.529.645 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.529.649 I perplexity: tokenizing the input ..
0.01.535.345 I perplexity: tokenization took 5.694 ms
0.01.535.352 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.653.886 I perplexity: 0.12 seconds per pass - ETA 0.00 minutes
[1]10.1498,
0.01.655.421 I Final estimate: PPL = 10.1498 +/- 3.22650

0.01.655.456 I llama_perf_context_print:        load time =    1497.52 ms
0.01.655.458 I llama_perf_context_print: prompt eval time =     118.20 ms /   128 tokens (    0.92 ms per token,  1082.86 tokens per second)
0.01.655.459 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.655.459 I llama_perf_context_print:       total time =     125.85 ms /   129 tokens
0.01.655.823 I ggml_metal_free: deallocating

real	0m1.869s
user	0m0.106s
sys	0m0.268s
```
- q8_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.097 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.010.313 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.028.335 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.028.341 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.028.344 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.028.345 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.028.346 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.028.346 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.028.347 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.028.348 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.028.348 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.028.348 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.028.349 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.028.349 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.028.349 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.028.350 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.028.351 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.028.352 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.028.353 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.032.282 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.033.339 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.037.227 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.037.229 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.037.229 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.037.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.037.230 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.037.230 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.037.231 I llama_model_loader: - type  f32:  194 tensors
0.00.037.231 I llama_model_loader: - type q8_0:   98 tensors
0.00.037.232 I print_info: file format = GGUF V3 (latest)
0.00.037.233 I print_info: file type   = Q8_0
0.00.037.233 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.046.583 I load: special tokens cache size = 25
0.00.053.686 I load: token to piece cache size = 0.2984 MB
0.00.053.690 I print_info: arch             = gptneox
0.00.053.691 I print_info: vocab_only       = 0
0.00.053.691 I print_info: n_ctx_train      = 2048
0.00.053.691 I print_info: n_embd           = 2048
0.00.053.692 I print_info: n_layer          = 24
0.00.053.701 I print_info: n_head           = 16
0.00.053.702 I print_info: n_head_kv        = 16
0.00.053.702 I print_info: n_rot            = 32
0.00.053.704 I print_info: n_swa            = 0
0.00.053.704 I print_info: n_embd_head_k    = 128
0.00.053.704 I print_info: n_embd_head_v    = 128
0.00.053.705 I print_info: n_gqa            = 1
0.00.053.706 I print_info: n_embd_k_gqa     = 2048
0.00.053.706 I print_info: n_embd_v_gqa     = 2048
0.00.053.708 I print_info: f_norm_eps       = 1.0e-05
0.00.053.708 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.053.709 I print_info: f_clamp_kqv      = 0.0e+00
0.00.053.709 I print_info: f_max_alibi_bias = 0.0e+00
0.00.053.710 I print_info: f_logit_scale    = 0.0e+00
0.00.053.711 I print_info: n_ff             = 8192
0.00.053.711 I print_info: n_expert         = 0
0.00.053.711 I print_info: n_expert_used    = 0
0.00.053.712 I print_info: causal attn      = 1
0.00.053.712 I print_info: pooling type     = 0
0.00.053.712 I print_info: rope type        = 2
0.00.053.712 I print_info: rope scaling     = linear
0.00.053.713 I print_info: freq_base_train  = 10000.0
0.00.053.713 I print_info: freq_scale_train = 1
0.00.053.713 I print_info: n_ctx_orig_yarn  = 2048
0.00.053.713 I print_info: rope_finetuned   = unknown
0.00.053.713 I print_info: ssm_d_conv       = 0
0.00.053.713 I print_info: ssm_d_inner      = 0
0.00.053.714 I print_info: ssm_d_state      = 0
0.00.053.714 I print_info: ssm_dt_rank      = 0
0.00.053.714 I print_info: ssm_dt_b_c_rms   = 0
0.00.053.714 I print_info: model type       = 1.4B
0.00.053.715 I print_info: model params     = 1.41 B
0.00.053.719 I print_info: general.name     = 1.4B
0.00.053.720 I print_info: vocab type       = BPE
0.00.053.720 I print_info: n_vocab          = 50304
0.00.053.720 I print_info: n_merges         = 50009
0.00.053.720 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.053.721 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.053.721 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.053.721 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.053.721 I print_info: LF token         = 187 'Ċ'
0.00.053.722 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.053.722 I print_info: max token length = 1024
0.01.077.504 I load_tensors: offloading 24 repeating layers to GPU
0.01.077.508 I load_tensors: offloading output layer to GPU
0.01.077.510 I load_tensors: offloaded 25/25 layers to GPU
0.01.077.534 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.01.077.535 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.01.078.275 I llama_init_from_model: n_seq_max     = 1
0.01.078.276 I llama_init_from_model: n_ctx         = 2048
0.01.078.277 I llama_init_from_model: n_ctx_per_seq = 2048
0.01.078.277 I llama_init_from_model: n_batch       = 2048
0.01.078.278 I llama_init_from_model: n_ubatch      = 512
0.01.078.278 I llama_init_from_model: flash_attn    = 0
0.01.078.278 I llama_init_from_model: freq_base     = 10000.0
0.01.078.279 I llama_init_from_model: freq_scale    = 1
0.01.078.280 I ggml_metal_init: allocating
0.01.078.287 I ggml_metal_init: found device: Apple M4
0.01.078.304 I ggml_metal_init: picking default device: Apple M4
0.01.079.491 I ggml_metal_init: using embedded metal library
0.01.084.798 I ggml_metal_init: GPU name:   Apple M4
0.01.084.801 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.01.084.802 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.01.084.802 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.01.084.803 I ggml_metal_init: simdgroup reduction   = true
0.01.084.803 I ggml_metal_init: simdgroup matrix mul. = true
0.01.084.803 I ggml_metal_init: has residency sets    = true
0.01.084.804 I ggml_metal_init: has bfloat            = true
0.01.084.804 I ggml_metal_init: use bfloat            = true
0.01.084.805 I ggml_metal_init: hasUnifiedMemory      = true
0.01.084.806 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.01.100.171 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.01.155.030 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.01.155.040 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.01.155.078 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.01.159.491 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.01.159.493 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.01.159.493 I llama_init_from_model: graph nodes  = 967
0.01.159.494 I llama_init_from_model: graph splits = 2
0.01.159.499 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.01.159.623 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.01.159.623 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.01.217.120 I main: llama threadpool init, n_threads = 4
0.01.217.166 I 
0.01.217.188 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.01.217.190 I 
0.01.217.343 I sampler seed: 1234
0.01.217.347 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.01.217.392 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.01.217.396 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.01.217.396 I 
I believe the meaning of life is to do the best one can with the tools one has, and not to seek to change the tools or to put them in a new configuration.

That is, the best tool in the box will not help the one who is not the best tool in the box.

The best tool in the box,

0.02.329.397 I llama_perf_sampler_print:    sampling time =       1.54 ms /    71 runs   (    0.02 ms per token, 46254.07 tokens per second)
0.02.329.398 I llama_perf_context_print:        load time =    1206.10 ms
0.02.329.400 I llama_perf_context_print: prompt eval time =      49.27 ms /     7 tokens (    7.04 ms per token,   142.07 tokens per second)
0.02.329.401 I llama_perf_context_print:        eval time =    1060.13 ms /    63 runs   (   16.83 ms per token,    59.43 tokens per second)
0.02.329.401 I llama_perf_context_print:       total time =    1112.99 ms /    70 tokens
0.02.329.689 I ggml_metal_free: deallocating

real	0m2.347s
user	0m0.110s
sys	0m0.265s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.192 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.213 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q8_0.gguf (version GGUF V3 (latest))
0.00.018.221 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.222 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.223 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.223 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.223 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.224 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.225 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.225 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.225 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.226 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.226 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.226 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.227 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.229 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.229 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.229 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.030 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.091 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.911 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.913 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.913 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.914 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.914 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.914 I llama_model_loader: - kv  22:                          general.file_type u32              = 7
0.00.026.915 I llama_model_loader: - type  f32:  194 tensors
0.00.026.915 I llama_model_loader: - type q8_0:   98 tensors
0.00.026.916 I print_info: file format = GGUF V3 (latest)
0.00.026.916 I print_info: file type   = Q8_0
0.00.026.919 I print_info: file size   = 1.40 GiB (8.51 BPW) 
0.00.035.188 I load: special tokens cache size = 25
0.00.041.000 I load: token to piece cache size = 0.2984 MB
0.00.041.004 I print_info: arch             = gptneox
0.00.041.005 I print_info: vocab_only       = 0
0.00.041.005 I print_info: n_ctx_train      = 2048
0.00.041.005 I print_info: n_embd           = 2048
0.00.041.005 I print_info: n_layer          = 24
0.00.041.010 I print_info: n_head           = 16
0.00.041.010 I print_info: n_head_kv        = 16
0.00.041.011 I print_info: n_rot            = 32
0.00.041.011 I print_info: n_swa            = 0
0.00.041.011 I print_info: n_embd_head_k    = 128
0.00.041.011 I print_info: n_embd_head_v    = 128
0.00.041.012 I print_info: n_gqa            = 1
0.00.041.013 I print_info: n_embd_k_gqa     = 2048
0.00.041.013 I print_info: n_embd_v_gqa     = 2048
0.00.041.014 I print_info: f_norm_eps       = 1.0e-05
0.00.041.014 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.014 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.014 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.014 I print_info: f_logit_scale    = 0.0e+00
0.00.041.015 I print_info: n_ff             = 8192
0.00.041.015 I print_info: n_expert         = 0
0.00.041.015 I print_info: n_expert_used    = 0
0.00.041.015 I print_info: causal attn      = 1
0.00.041.016 I print_info: pooling type     = 0
0.00.041.016 I print_info: rope type        = 2
0.00.041.016 I print_info: rope scaling     = linear
0.00.041.016 I print_info: freq_base_train  = 10000.0
0.00.041.018 I print_info: freq_scale_train = 1
0.00.041.020 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.020 I print_info: rope_finetuned   = unknown
0.00.041.020 I print_info: ssm_d_conv       = 0
0.00.041.021 I print_info: ssm_d_inner      = 0
0.00.041.021 I print_info: ssm_d_state      = 0
0.00.041.021 I print_info: ssm_dt_rank      = 0
0.00.041.021 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.021 I print_info: model type       = 1.4B
0.00.041.022 I print_info: model params     = 1.41 B
0.00.041.022 I print_info: general.name     = 1.4B
0.00.041.022 I print_info: vocab type       = BPE
0.00.041.022 I print_info: n_vocab          = 50304
0.00.041.023 I print_info: n_merges         = 50009
0.00.041.023 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.023 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.023 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.023 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.023 I print_info: LF token         = 187 'Ċ'
0.00.041.025 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.025 I print_info: max token length = 1024
0.00.832.177 I load_tensors: offloading 24 repeating layers to GPU
0.00.832.183 I load_tensors: offloading output layer to GPU
0.00.832.184 I load_tensors: offloaded 25/25 layers to GPU
0.00.832.212 I load_tensors: Metal_Mapped model buffer size =  1435.25 MiB
0.00.832.214 I load_tensors:   CPU_Mapped model buffer size =   104.39 MiB
0.00.833.560 I llama_init_from_model: n_seq_max     = 1
0.00.833.562 I llama_init_from_model: n_ctx         = 128
0.00.833.563 I llama_init_from_model: n_ctx_per_seq = 128
0.00.833.563 I llama_init_from_model: n_batch       = 128
0.00.833.563 I llama_init_from_model: n_ubatch      = 128
0.00.833.564 I llama_init_from_model: flash_attn    = 0
0.00.833.564 I llama_init_from_model: freq_base     = 10000.0
0.00.833.565 I llama_init_from_model: freq_scale    = 1
0.00.833.566 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.833.567 I ggml_metal_init: allocating
0.00.833.608 I ggml_metal_init: found device: Apple M4
0.00.833.621 I ggml_metal_init: picking default device: Apple M4
0.00.834.952 I ggml_metal_init: using embedded metal library
0.00.840.553 I ggml_metal_init: GPU name:   Apple M4
0.00.840.556 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.840.557 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.840.558 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.840.559 I ggml_metal_init: simdgroup reduction   = true
0.00.840.559 I ggml_metal_init: simdgroup matrix mul. = true
0.00.840.559 I ggml_metal_init: has residency sets    = true
0.00.840.560 I ggml_metal_init: has bfloat            = true
0.00.840.560 I ggml_metal_init: use bfloat            = true
0.00.840.561 I ggml_metal_init: hasUnifiedMemory      = true
0.00.840.564 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.855.826 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.859.090 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.859.094 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.859.139 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.862.295 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.862.296 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.862.296 I llama_init_from_model: graph nodes  = 967
0.00.862.297 I llama_init_from_model: graph splits = 2
0.00.862.300 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.862.300 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.891.550 I 
0.00.891.618 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.891.625 I perplexity: tokenizing the input ..
0.00.898.935 I perplexity: tokenization took 7.307 ms
0.00.898.943 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.01.038.982 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1362,
0.01.040.590 I Final estimate: PPL = 10.1362 +/- 3.22437

0.01.040.626 I llama_perf_context_print:        load time =     882.35 ms
0.01.040.627 I llama_perf_context_print: prompt eval time =     139.13 ms /   128 tokens (    1.09 ms per token,   920.03 tokens per second)
0.01.040.628 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.01.040.628 I llama_perf_context_print:       total time =     149.08 ms /   129 tokens
0.01.041.030 I ggml_metal_free: deallocating

real	0m1.055s
user	0m0.078s
sys	0m0.160s
```
- q4_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.060 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.096 I main: llama backend init
0.00.000.099 I main: load the model and apply lora adapter, if any
0.00.011.942 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.905 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.019.912 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.920 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.921 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.921 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.921 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.922 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.922 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.923 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.923 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.923 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.924 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.925 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.925 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.927 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.927 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.927 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.899 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.963 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.888 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.889 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.890 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.890 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.890 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.891 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.028.891 I llama_model_loader: - type  f32:  194 tensors
0.00.028.891 I llama_model_loader: - type q4_0:   97 tensors
0.00.028.892 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.893 I print_info: file format = GGUF V3 (latest)
0.00.028.897 I print_info: file type   = Q4_0
0.00.028.898 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.037.082 I load: special tokens cache size = 25
0.00.043.164 I load: token to piece cache size = 0.2984 MB
0.00.043.168 I print_info: arch             = gptneox
0.00.043.168 I print_info: vocab_only       = 0
0.00.043.168 I print_info: n_ctx_train      = 2048
0.00.043.169 I print_info: n_embd           = 2048
0.00.043.169 I print_info: n_layer          = 24
0.00.043.173 I print_info: n_head           = 16
0.00.043.173 I print_info: n_head_kv        = 16
0.00.043.174 I print_info: n_rot            = 32
0.00.043.174 I print_info: n_swa            = 0
0.00.043.174 I print_info: n_embd_head_k    = 128
0.00.043.175 I print_info: n_embd_head_v    = 128
0.00.043.176 I print_info: n_gqa            = 1
0.00.043.177 I print_info: n_embd_k_gqa     = 2048
0.00.043.178 I print_info: n_embd_v_gqa     = 2048
0.00.043.178 I print_info: f_norm_eps       = 1.0e-05
0.00.043.179 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.043.179 I print_info: f_clamp_kqv      = 0.0e+00
0.00.043.179 I print_info: f_max_alibi_bias = 0.0e+00
0.00.043.179 I print_info: f_logit_scale    = 0.0e+00
0.00.043.180 I print_info: n_ff             = 8192
0.00.043.180 I print_info: n_expert         = 0
0.00.043.180 I print_info: n_expert_used    = 0
0.00.043.180 I print_info: causal attn      = 1
0.00.043.180 I print_info: pooling type     = 0
0.00.043.180 I print_info: rope type        = 2
0.00.043.181 I print_info: rope scaling     = linear
0.00.043.181 I print_info: freq_base_train  = 10000.0
0.00.043.181 I print_info: freq_scale_train = 1
0.00.043.181 I print_info: n_ctx_orig_yarn  = 2048
0.00.043.182 I print_info: rope_finetuned   = unknown
0.00.043.182 I print_info: ssm_d_conv       = 0
0.00.043.182 I print_info: ssm_d_inner      = 0
0.00.043.182 I print_info: ssm_d_state      = 0
0.00.043.182 I print_info: ssm_dt_rank      = 0
0.00.043.182 I print_info: ssm_dt_b_c_rms   = 0
0.00.043.182 I print_info: model type       = 1.4B
0.00.043.184 I print_info: model params     = 1.41 B
0.00.043.184 I print_info: general.name     = 1.4B
0.00.043.185 I print_info: vocab type       = BPE
0.00.043.185 I print_info: n_vocab          = 50304
0.00.043.185 I print_info: n_merges         = 50009
0.00.043.186 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.043.186 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.043.186 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.043.186 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.043.186 I print_info: LF token         = 187 'Ċ'
0.00.043.186 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.043.187 I print_info: max token length = 1024
0.00.613.395 I load_tensors: offloading 24 repeating layers to GPU
0.00.613.406 I load_tensors: offloading output layer to GPU
0.00.613.407 I load_tensors: offloaded 25/25 layers to GPU
0.00.613.445 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.613.446 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.614.657 I llama_init_from_model: n_seq_max     = 1
0.00.614.671 I llama_init_from_model: n_ctx         = 2048
0.00.614.671 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.614.672 I llama_init_from_model: n_batch       = 2048
0.00.614.672 I llama_init_from_model: n_ubatch      = 512
0.00.614.673 I llama_init_from_model: flash_attn    = 0
0.00.614.674 I llama_init_from_model: freq_base     = 10000.0
0.00.614.675 I llama_init_from_model: freq_scale    = 1
0.00.614.677 I ggml_metal_init: allocating
0.00.614.754 I ggml_metal_init: found device: Apple M4
0.00.614.767 I ggml_metal_init: picking default device: Apple M4
0.00.616.458 I ggml_metal_init: using embedded metal library
0.00.622.702 I ggml_metal_init: GPU name:   Apple M4
0.00.622.711 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.622.711 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.622.712 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.622.713 I ggml_metal_init: simdgroup reduction   = true
0.00.622.713 I ggml_metal_init: simdgroup matrix mul. = true
0.00.622.714 I ggml_metal_init: has residency sets    = true
0.00.622.714 I ggml_metal_init: has bfloat            = true
0.00.622.714 I ggml_metal_init: use bfloat            = true
0.00.622.715 I ggml_metal_init: hasUnifiedMemory      = true
0.00.622.720 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.642.343 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.698.539 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.698.548 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.698.583 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.703.580 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.703.582 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.703.583 I llama_init_from_model: graph nodes  = 967
0.00.703.583 I llama_init_from_model: graph splits = 2
0.00.703.588 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.703.714 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.703.714 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.761.355 I main: llama threadpool init, n_threads = 4
0.00.761.397 I 
0.00.761.419 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.761.419 I 
0.00.761.569 I sampler seed: 1234
0.00.761.574 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.761.594 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.761.595 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.761.595 I 
I believe the meaning of life is to love. If that is not what you are looking for, I will be happy to explain it to you and you can go back to your life." - Unknown

"I think I am on the way to true happiness. I believe in the importance of family and friends. I believe that when you are truly

0.01.446.970 I llama_perf_sampler_print:    sampling time =       1.36 ms /    71 runs   (    0.02 ms per token, 52359.88 tokens per second)
0.01.446.970 I llama_perf_context_print:        load time =     748.70 ms
0.01.446.971 I llama_perf_context_print: prompt eval time =      48.76 ms /     7 tokens (    6.97 ms per token,   143.55 tokens per second)
0.01.446.971 I llama_perf_context_print:        eval time =     633.78 ms /    63 runs   (   10.06 ms per token,    99.40 tokens per second)
0.01.446.972 I llama_perf_context_print:       total time =     686.33 ms /    70 tokens
0.01.447.254 I ggml_metal_free: deallocating

real	0m1.467s
user	0m0.110s
sys	0m0.197s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.805 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.342 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
0.00.017.348 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.354 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.355 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.355 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.356 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.357 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.357 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.359 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.360 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.360 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.360 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.361 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.363 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.363 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.363 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.391 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.464 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.276 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.278 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.278 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.278 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.279 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.279 I llama_model_loader: - kv  22:                          general.file_type u32              = 2
0.00.026.280 I llama_model_loader: - type  f32:  194 tensors
0.00.026.280 I llama_model_loader: - type q4_0:   97 tensors
0.00.026.280 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.281 I print_info: file format = GGUF V3 (latest)
0.00.026.287 I print_info: file type   = Q4_0
0.00.026.288 I print_info: file size   = 786.31 MiB (4.66 BPW) 
0.00.034.231 I load: special tokens cache size = 25
0.00.040.245 I load: token to piece cache size = 0.2984 MB
0.00.040.249 I print_info: arch             = gptneox
0.00.040.249 I print_info: vocab_only       = 0
0.00.040.249 I print_info: n_ctx_train      = 2048
0.00.040.249 I print_info: n_embd           = 2048
0.00.040.249 I print_info: n_layer          = 24
0.00.040.254 I print_info: n_head           = 16
0.00.040.254 I print_info: n_head_kv        = 16
0.00.040.254 I print_info: n_rot            = 32
0.00.040.255 I print_info: n_swa            = 0
0.00.040.255 I print_info: n_embd_head_k    = 128
0.00.040.255 I print_info: n_embd_head_v    = 128
0.00.040.256 I print_info: n_gqa            = 1
0.00.040.257 I print_info: n_embd_k_gqa     = 2048
0.00.040.257 I print_info: n_embd_v_gqa     = 2048
0.00.040.260 I print_info: f_norm_eps       = 1.0e-05
0.00.040.260 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.261 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.261 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.261 I print_info: f_logit_scale    = 0.0e+00
0.00.040.262 I print_info: n_ff             = 8192
0.00.040.262 I print_info: n_expert         = 0
0.00.040.262 I print_info: n_expert_used    = 0
0.00.040.262 I print_info: causal attn      = 1
0.00.040.262 I print_info: pooling type     = 0
0.00.040.262 I print_info: rope type        = 2
0.00.040.263 I print_info: rope scaling     = linear
0.00.040.263 I print_info: freq_base_train  = 10000.0
0.00.040.264 I print_info: freq_scale_train = 1
0.00.040.264 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.264 I print_info: rope_finetuned   = unknown
0.00.040.264 I print_info: ssm_d_conv       = 0
0.00.040.264 I print_info: ssm_d_inner      = 0
0.00.040.264 I print_info: ssm_d_state      = 0
0.00.040.264 I print_info: ssm_dt_rank      = 0
0.00.040.265 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.265 I print_info: model type       = 1.4B
0.00.040.265 I print_info: model params     = 1.41 B
0.00.040.266 I print_info: general.name     = 1.4B
0.00.040.266 I print_info: vocab type       = BPE
0.00.040.266 I print_info: n_vocab          = 50304
0.00.040.266 I print_info: n_merges         = 50009
0.00.040.267 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.267 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.267 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.267 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.269 I print_info: LF token         = 187 'Ċ'
0.00.040.269 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.269 I print_info: max token length = 1024
0.00.597.117 I load_tensors: offloading 24 repeating layers to GPU
0.00.597.128 I load_tensors: offloading output layer to GPU
0.00.597.129 I load_tensors: offloaded 25/25 layers to GPU
0.00.597.164 I load_tensors: Metal_Mapped model buffer size =   786.33 MiB
0.00.597.165 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.598.115 I llama_init_from_model: n_seq_max     = 1
0.00.598.119 I llama_init_from_model: n_ctx         = 128
0.00.598.120 I llama_init_from_model: n_ctx_per_seq = 128
0.00.598.120 I llama_init_from_model: n_batch       = 128
0.00.598.121 I llama_init_from_model: n_ubatch      = 128
0.00.598.121 I llama_init_from_model: flash_attn    = 0
0.00.598.123 I llama_init_from_model: freq_base     = 10000.0
0.00.598.123 I llama_init_from_model: freq_scale    = 1
0.00.598.124 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.598.130 I ggml_metal_init: allocating
0.00.598.194 I ggml_metal_init: found device: Apple M4
0.00.598.208 I ggml_metal_init: picking default device: Apple M4
0.00.600.035 I ggml_metal_init: using embedded metal library
0.00.606.648 I ggml_metal_init: GPU name:   Apple M4
0.00.606.653 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.606.654 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.606.655 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.606.656 I ggml_metal_init: simdgroup reduction   = true
0.00.606.656 I ggml_metal_init: simdgroup matrix mul. = true
0.00.606.656 I ggml_metal_init: has residency sets    = true
0.00.606.657 I ggml_metal_init: has bfloat            = true
0.00.606.657 I ggml_metal_init: use bfloat            = true
0.00.606.658 I ggml_metal_init: hasUnifiedMemory      = true
0.00.606.667 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.625.099 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.628.646 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.628.650 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.628.692 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.631.955 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.631.957 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.631.957 I llama_init_from_model: graph nodes  = 967
0.00.631.958 I llama_init_from_model: graph splits = 2
0.00.631.961 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.631.961 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.658.383 I 
0.00.658.464 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.658.476 I perplexity: tokenizing the input ..
0.00.664.869 I perplexity: tokenization took 6.391 ms
0.00.664.874 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.797.598 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]11.1740,
0.00.799.137 I Final estimate: PPL = 11.1740 +/- 3.48446

0.00.799.160 I llama_perf_context_print:        load time =     648.57 ms
0.00.799.161 I llama_perf_context_print: prompt eval time =     132.43 ms /   128 tokens (    1.03 ms per token,   966.53 tokens per second)
0.00.799.162 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.799.162 I llama_perf_context_print:       total time =     140.78 ms /   129 tokens
0.00.799.567 I ggml_metal_free: deallocating

real	0m0.816s
user	0m0.080s
sys	0m0.129s
```
- q4_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.050 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.084 I main: llama backend init
0.00.000.086 I main: load the model and apply lora adapter, if any
0.00.009.122 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.406 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.017.411 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.412 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.413 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.413 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.418 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.418 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.419 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.419 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.421 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.422 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.422 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.422 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.423 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.426 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.427 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.427 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.221 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.221 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.965 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.966 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.967 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.968 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.968 I llama_model_loader: - type  f32:  194 tensors
0.00.025.968 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.969 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.969 I print_info: file format = GGUF V3 (latest)
0.00.025.970 I print_info: file type   = Q4_1
0.00.025.974 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.034.159 I load: special tokens cache size = 25
0.00.040.123 I load: token to piece cache size = 0.2984 MB
0.00.040.125 I print_info: arch             = gptneox
0.00.040.126 I print_info: vocab_only       = 0
0.00.040.126 I print_info: n_ctx_train      = 2048
0.00.040.126 I print_info: n_embd           = 2048
0.00.040.126 I print_info: n_layer          = 24
0.00.040.129 I print_info: n_head           = 16
0.00.040.129 I print_info: n_head_kv        = 16
0.00.040.130 I print_info: n_rot            = 32
0.00.040.130 I print_info: n_swa            = 0
0.00.040.130 I print_info: n_embd_head_k    = 128
0.00.040.130 I print_info: n_embd_head_v    = 128
0.00.040.132 I print_info: n_gqa            = 1
0.00.040.133 I print_info: n_embd_k_gqa     = 2048
0.00.040.134 I print_info: n_embd_v_gqa     = 2048
0.00.040.138 I print_info: f_norm_eps       = 1.0e-05
0.00.040.138 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.139 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.140 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.141 I print_info: f_logit_scale    = 0.0e+00
0.00.040.143 I print_info: n_ff             = 8192
0.00.040.143 I print_info: n_expert         = 0
0.00.040.143 I print_info: n_expert_used    = 0
0.00.040.144 I print_info: causal attn      = 1
0.00.040.144 I print_info: pooling type     = 0
0.00.040.145 I print_info: rope type        = 2
0.00.040.146 I print_info: rope scaling     = linear
0.00.040.146 I print_info: freq_base_train  = 10000.0
0.00.040.148 I print_info: freq_scale_train = 1
0.00.040.148 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.148 I print_info: rope_finetuned   = unknown
0.00.040.148 I print_info: ssm_d_conv       = 0
0.00.040.148 I print_info: ssm_d_inner      = 0
0.00.040.148 I print_info: ssm_d_state      = 0
0.00.040.148 I print_info: ssm_dt_rank      = 0
0.00.040.149 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.149 I print_info: model type       = 1.4B
0.00.040.150 I print_info: model params     = 1.41 B
0.00.040.150 I print_info: general.name     = 1.4B
0.00.040.151 I print_info: vocab type       = BPE
0.00.040.151 I print_info: n_vocab          = 50304
0.00.040.151 I print_info: n_merges         = 50009
0.00.040.151 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.151 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.151 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.152 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.152 I print_info: LF token         = 187 'Ċ'
0.00.040.152 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.152 I print_info: max token length = 1024
0.00.601.841 I load_tensors: offloading 24 repeating layers to GPU
0.00.601.854 I load_tensors: offloading output layer to GPU
0.00.601.854 I load_tensors: offloaded 25/25 layers to GPU
0.00.601.890 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.601.892 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.603.360 I llama_init_from_model: n_seq_max     = 1
0.00.603.365 I llama_init_from_model: n_ctx         = 2048
0.00.603.366 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.603.366 I llama_init_from_model: n_batch       = 2048
0.00.603.366 I llama_init_from_model: n_ubatch      = 512
0.00.603.367 I llama_init_from_model: flash_attn    = 0
0.00.603.369 I llama_init_from_model: freq_base     = 10000.0
0.00.603.369 I llama_init_from_model: freq_scale    = 1
0.00.603.375 I ggml_metal_init: allocating
0.00.603.466 I ggml_metal_init: found device: Apple M4
0.00.603.480 I ggml_metal_init: picking default device: Apple M4
0.00.605.367 I ggml_metal_init: using embedded metal library
0.00.612.243 I ggml_metal_init: GPU name:   Apple M4
0.00.612.247 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.612.248 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.612.249 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.612.250 I ggml_metal_init: simdgroup reduction   = true
0.00.612.250 I ggml_metal_init: simdgroup matrix mul. = true
0.00.612.250 I ggml_metal_init: has residency sets    = true
0.00.612.250 I ggml_metal_init: has bfloat            = true
0.00.612.251 I ggml_metal_init: use bfloat            = true
0.00.612.251 I ggml_metal_init: hasUnifiedMemory      = true
0.00.612.253 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.630.099 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.309 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.687.320 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.687.400 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.692.614 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.692.616 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.692.616 I llama_init_from_model: graph nodes  = 967
0.00.692.616 I llama_init_from_model: graph splits = 2
0.00.692.622 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.692.754 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.692.755 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.749.700 I main: llama threadpool init, n_threads = 4
0.00.749.743 I 
0.00.749.763 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.749.766 I 
0.00.749.931 I sampler seed: 1234
0.00.749.935 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.749.987 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.749.990 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.749.990 I 
I believe the meaning of life is to give
<em>a<br>
<em>s<br>
<em>n<br>
<em>a<br>
<em>c<br>
<em>i<br>
<em>s<br>
<em>e<

0.01.479.704 I llama_perf_sampler_print:    sampling time =       1.27 ms /    71 runs   (    0.02 ms per token, 55905.51 tokens per second)
0.01.479.705 I llama_perf_context_print:        load time =     739.89 ms
0.01.479.706 I llama_perf_context_print: prompt eval time =      49.01 ms /     7 tokens (    7.00 ms per token,   142.82 tokens per second)
0.01.479.706 I llama_perf_context_print:        eval time =     677.94 ms /    63 runs   (   10.76 ms per token,    92.93 tokens per second)
0.01.479.707 I llama_perf_context_print:       total time =     730.69 ms /    70 tokens
0.01.479.940 I ggml_metal_free: deallocating

real	0m1.497s
user	0m0.110s
sys	0m0.205s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.100 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.940 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.590 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_1.gguf (version GGUF V3 (latest))
0.00.016.596 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.603 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.604 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.604 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.605 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.605 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.607 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.607 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.608 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.608 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.608 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.610 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.610 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.612 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.612 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.612 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.527 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.542 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.469 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.471 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.472 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.473 I llama_model_loader: - kv  22:                          general.file_type u32              = 3
0.00.025.473 I llama_model_loader: - type  f32:  194 tensors
0.00.025.474 I llama_model_loader: - type q4_1:   97 tensors
0.00.025.474 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.475 I print_info: file format = GGUF V3 (latest)
0.00.025.479 I print_info: file type   = Q4_1
0.00.025.481 I print_info: file size   = 864.46 MiB (5.13 BPW) 
0.00.033.819 I load: special tokens cache size = 25
0.00.039.806 I load: token to piece cache size = 0.2984 MB
0.00.039.809 I print_info: arch             = gptneox
0.00.039.809 I print_info: vocab_only       = 0
0.00.039.810 I print_info: n_ctx_train      = 2048
0.00.039.810 I print_info: n_embd           = 2048
0.00.039.810 I print_info: n_layer          = 24
0.00.039.814 I print_info: n_head           = 16
0.00.039.815 I print_info: n_head_kv        = 16
0.00.039.815 I print_info: n_rot            = 32
0.00.039.815 I print_info: n_swa            = 0
0.00.039.816 I print_info: n_embd_head_k    = 128
0.00.039.816 I print_info: n_embd_head_v    = 128
0.00.039.817 I print_info: n_gqa            = 1
0.00.039.817 I print_info: n_embd_k_gqa     = 2048
0.00.039.818 I print_info: n_embd_v_gqa     = 2048
0.00.039.819 I print_info: f_norm_eps       = 1.0e-05
0.00.039.819 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.819 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.822 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.822 I print_info: f_logit_scale    = 0.0e+00
0.00.039.823 I print_info: n_ff             = 8192
0.00.039.823 I print_info: n_expert         = 0
0.00.039.824 I print_info: n_expert_used    = 0
0.00.039.824 I print_info: causal attn      = 1
0.00.039.824 I print_info: pooling type     = 0
0.00.039.824 I print_info: rope type        = 2
0.00.039.824 I print_info: rope scaling     = linear
0.00.039.825 I print_info: freq_base_train  = 10000.0
0.00.039.825 I print_info: freq_scale_train = 1
0.00.039.825 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.826 I print_info: rope_finetuned   = unknown
0.00.039.826 I print_info: ssm_d_conv       = 0
0.00.039.826 I print_info: ssm_d_inner      = 0
0.00.039.826 I print_info: ssm_d_state      = 0
0.00.039.826 I print_info: ssm_dt_rank      = 0
0.00.039.826 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.827 I print_info: model type       = 1.4B
0.00.039.827 I print_info: model params     = 1.41 B
0.00.039.827 I print_info: general.name     = 1.4B
0.00.039.828 I print_info: vocab type       = BPE
0.00.039.828 I print_info: n_vocab          = 50304
0.00.039.828 I print_info: n_merges         = 50009
0.00.039.828 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.829 I print_info: LF token         = 187 'Ċ'
0.00.039.830 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.830 I print_info: max token length = 1024
0.00.595.936 I load_tensors: offloading 24 repeating layers to GPU
0.00.595.944 I load_tensors: offloading output layer to GPU
0.00.595.945 I load_tensors: offloaded 25/25 layers to GPU
0.00.595.964 I load_tensors: Metal_Mapped model buffer size =   864.47 MiB
0.00.595.965 I load_tensors:   CPU_Mapped model buffer size =    61.41 MiB
0.00.596.795 I llama_init_from_model: n_seq_max     = 1
0.00.596.799 I llama_init_from_model: n_ctx         = 128
0.00.596.800 I llama_init_from_model: n_ctx_per_seq = 128
0.00.596.800 I llama_init_from_model: n_batch       = 128
0.00.596.800 I llama_init_from_model: n_ubatch      = 128
0.00.596.801 I llama_init_from_model: flash_attn    = 0
0.00.596.802 I llama_init_from_model: freq_base     = 10000.0
0.00.596.803 I llama_init_from_model: freq_scale    = 1
0.00.596.803 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.596.804 I ggml_metal_init: allocating
0.00.596.841 I ggml_metal_init: found device: Apple M4
0.00.596.853 I ggml_metal_init: picking default device: Apple M4
0.00.597.841 I ggml_metal_init: using embedded metal library
0.00.601.911 I ggml_metal_init: GPU name:   Apple M4
0.00.601.921 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.601.921 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.601.922 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.601.923 I ggml_metal_init: simdgroup reduction   = true
0.00.601.923 I ggml_metal_init: simdgroup matrix mul. = true
0.00.601.923 I ggml_metal_init: has residency sets    = true
0.00.601.924 I ggml_metal_init: has bfloat            = true
0.00.601.924 I ggml_metal_init: use bfloat            = true
0.00.601.925 I ggml_metal_init: hasUnifiedMemory      = true
0.00.601.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.617.800 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.619.422 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.619.426 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.619.453 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.621.027 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.621.028 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.621.029 I llama_init_from_model: graph nodes  = 967
0.00.621.029 I llama_init_from_model: graph splits = 2
0.00.621.031 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.621.031 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.642.856 I 
0.00.642.896 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.642.901 I perplexity: tokenizing the input ..
0.00.646.928 I perplexity: tokenization took 4.026 ms
0.00.646.934 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.782.219 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.5507,
0.00.783.565 I Final estimate: PPL = 10.5507 +/- 3.34263

0.00.783.592 I llama_perf_context_print:        load time =     633.91 ms
0.00.783.594 I llama_perf_context_print: prompt eval time =     135.06 ms /   128 tokens (    1.06 ms per token,   947.76 tokens per second)
0.00.783.595 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.783.595 I llama_perf_context_print:       total time =     140.74 ms /   129 tokens
0.00.783.997 I ggml_metal_free: deallocating

real	0m0.797s
user	0m0.072s
sys	0m0.109s
```
- q5_0:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.054 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.090 I main: llama backend init
0.00.000.092 I main: load the model and apply lora adapter, if any
0.00.011.941 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.019.467 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.019.472 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.019.474 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.019.474 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.019.475 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.019.475 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.019.477 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.019.478 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.019.479 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.019.479 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.019.479 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.019.480 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.019.480 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.019.480 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.019.483 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.019.483 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.019.484 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.023.354 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.024.393 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.028.228 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.028.230 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.028.230 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.028.230 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.028.231 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.028.231 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.028.231 I llama_model_loader: - type  f32:  194 tensors
0.00.028.232 I llama_model_loader: - type q5_0:   97 tensors
0.00.028.232 I llama_model_loader: - type q6_K:    1 tensors
0.00.028.233 I print_info: file format = GGUF V3 (latest)
0.00.028.233 I print_info: file type   = Q5_0
0.00.028.234 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.036.541 I load: special tokens cache size = 25
0.00.042.579 I load: token to piece cache size = 0.2984 MB
0.00.042.582 I print_info: arch             = gptneox
0.00.042.582 I print_info: vocab_only       = 0
0.00.042.582 I print_info: n_ctx_train      = 2048
0.00.042.583 I print_info: n_embd           = 2048
0.00.042.583 I print_info: n_layer          = 24
0.00.042.586 I print_info: n_head           = 16
0.00.042.587 I print_info: n_head_kv        = 16
0.00.042.587 I print_info: n_rot            = 32
0.00.042.587 I print_info: n_swa            = 0
0.00.042.587 I print_info: n_embd_head_k    = 128
0.00.042.588 I print_info: n_embd_head_v    = 128
0.00.042.588 I print_info: n_gqa            = 1
0.00.042.589 I print_info: n_embd_k_gqa     = 2048
0.00.042.590 I print_info: n_embd_v_gqa     = 2048
0.00.042.590 I print_info: f_norm_eps       = 1.0e-05
0.00.042.591 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.042.591 I print_info: f_clamp_kqv      = 0.0e+00
0.00.042.591 I print_info: f_max_alibi_bias = 0.0e+00
0.00.042.591 I print_info: f_logit_scale    = 0.0e+00
0.00.042.592 I print_info: n_ff             = 8192
0.00.042.592 I print_info: n_expert         = 0
0.00.042.593 I print_info: n_expert_used    = 0
0.00.042.593 I print_info: causal attn      = 1
0.00.042.593 I print_info: pooling type     = 0
0.00.042.595 I print_info: rope type        = 2
0.00.042.596 I print_info: rope scaling     = linear
0.00.042.597 I print_info: freq_base_train  = 10000.0
0.00.042.597 I print_info: freq_scale_train = 1
0.00.042.597 I print_info: n_ctx_orig_yarn  = 2048
0.00.042.598 I print_info: rope_finetuned   = unknown
0.00.042.598 I print_info: ssm_d_conv       = 0
0.00.042.598 I print_info: ssm_d_inner      = 0
0.00.042.598 I print_info: ssm_d_state      = 0
0.00.042.598 I print_info: ssm_dt_rank      = 0
0.00.042.598 I print_info: ssm_dt_b_c_rms   = 0
0.00.042.598 I print_info: model type       = 1.4B
0.00.042.599 I print_info: model params     = 1.41 B
0.00.042.599 I print_info: general.name     = 1.4B
0.00.042.599 I print_info: vocab type       = BPE
0.00.042.599 I print_info: n_vocab          = 50304
0.00.042.600 I print_info: n_merges         = 50009
0.00.042.600 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.042.600 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.042.600 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.042.600 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.042.601 I print_info: LF token         = 187 'Ċ'
0.00.042.605 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.042.605 I print_info: max token length = 1024
0.00.654.891 I load_tensors: offloading 24 repeating layers to GPU
0.00.654.903 I load_tensors: offloading output layer to GPU
0.00.654.903 I load_tensors: offloaded 25/25 layers to GPU
0.00.654.935 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.654.941 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.656.366 I llama_init_from_model: n_seq_max     = 1
0.00.656.371 I llama_init_from_model: n_ctx         = 2048
0.00.656.371 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.656.372 I llama_init_from_model: n_batch       = 2048
0.00.656.372 I llama_init_from_model: n_ubatch      = 512
0.00.656.372 I llama_init_from_model: flash_attn    = 0
0.00.656.374 I llama_init_from_model: freq_base     = 10000.0
0.00.656.374 I llama_init_from_model: freq_scale    = 1
0.00.656.380 I ggml_metal_init: allocating
0.00.656.448 I ggml_metal_init: found device: Apple M4
0.00.656.461 I ggml_metal_init: picking default device: Apple M4
0.00.658.217 I ggml_metal_init: using embedded metal library
0.00.664.695 I ggml_metal_init: GPU name:   Apple M4
0.00.664.700 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.664.701 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.664.702 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.664.703 I ggml_metal_init: simdgroup reduction   = true
0.00.664.703 I ggml_metal_init: simdgroup matrix mul. = true
0.00.664.704 I ggml_metal_init: has residency sets    = true
0.00.664.704 I ggml_metal_init: has bfloat            = true
0.00.664.704 I ggml_metal_init: use bfloat            = true
0.00.664.705 I ggml_metal_init: hasUnifiedMemory      = true
0.00.664.707 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.682.485 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.742.134 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.742.140 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.742.175 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.746.836 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.746.838 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.746.839 I llama_init_from_model: graph nodes  = 967
0.00.746.839 I llama_init_from_model: graph splits = 2
0.00.746.843 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.746.974 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.746.975 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.805.560 I main: llama threadpool init, n_threads = 4
0.00.805.599 I 
0.00.805.621 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.805.621 I 
0.00.805.788 I sampler seed: 1234
0.00.805.793 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.805.840 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.805.843 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.805.844 I 
I believe the meaning of life is to do what is best for your self, and to be happy, and to enjoy the present moment, and the future is a gift that you will never regret.

I believe the meaning of life is to enjoy every moment of your life, and to be happy, and to enjoy the present moment, and the

0.01.591.412 I llama_perf_sampler_print:    sampling time =       1.33 ms /    71 runs   (    0.02 ms per token, 53223.39 tokens per second)
0.01.591.413 I llama_perf_context_print:        load time =     792.89 ms
0.01.591.414 I llama_perf_context_print: prompt eval time =      50.56 ms /     7 tokens (    7.22 ms per token,   138.45 tokens per second)
0.01.591.414 I llama_perf_context_print:        eval time =     732.18 ms /    63 runs   (   11.62 ms per token,    86.04 tokens per second)
0.01.591.415 I llama_perf_context_print:       total time =     786.58 ms /    70 tokens
0.01.591.621 I ggml_metal_free: deallocating

real	0m1.607s
user	0m0.110s
sys	0m0.217s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.103 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.092 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.362 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_0.gguf (version GGUF V3 (latest))
0.00.016.367 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.374 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.375 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.375 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.375 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.376 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.377 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.378 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.378 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.379 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.380 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.381 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.131 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.108 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.841 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.842 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.842 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.843 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.843 I llama_model_loader: - kv  22:                          general.file_type u32              = 8
0.00.024.844 I llama_model_loader: - type  f32:  194 tensors
0.00.024.844 I llama_model_loader: - type q5_0:   97 tensors
0.00.024.844 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.845 I print_info: file format = GGUF V3 (latest)
0.00.024.845 I print_info: file type   = Q5_0
0.00.024.846 I print_info: file size   = 942.60 MiB (5.59 BPW) 
0.00.032.733 I load: special tokens cache size = 25
0.00.038.787 I load: token to piece cache size = 0.2984 MB
0.00.038.790 I print_info: arch             = gptneox
0.00.038.790 I print_info: vocab_only       = 0
0.00.038.791 I print_info: n_ctx_train      = 2048
0.00.038.791 I print_info: n_embd           = 2048
0.00.038.791 I print_info: n_layer          = 24
0.00.038.794 I print_info: n_head           = 16
0.00.038.795 I print_info: n_head_kv        = 16
0.00.038.795 I print_info: n_rot            = 32
0.00.038.795 I print_info: n_swa            = 0
0.00.038.797 I print_info: n_embd_head_k    = 128
0.00.038.797 I print_info: n_embd_head_v    = 128
0.00.038.798 I print_info: n_gqa            = 1
0.00.038.798 I print_info: n_embd_k_gqa     = 2048
0.00.038.799 I print_info: n_embd_v_gqa     = 2048
0.00.038.800 I print_info: f_norm_eps       = 1.0e-05
0.00.038.800 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.800 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.800 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.801 I print_info: f_logit_scale    = 0.0e+00
0.00.038.801 I print_info: n_ff             = 8192
0.00.038.801 I print_info: n_expert         = 0
0.00.038.802 I print_info: n_expert_used    = 0
0.00.038.802 I print_info: causal attn      = 1
0.00.038.802 I print_info: pooling type     = 0
0.00.038.802 I print_info: rope type        = 2
0.00.038.802 I print_info: rope scaling     = linear
0.00.038.803 I print_info: freq_base_train  = 10000.0
0.00.038.803 I print_info: freq_scale_train = 1
0.00.038.804 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.804 I print_info: rope_finetuned   = unknown
0.00.038.804 I print_info: ssm_d_conv       = 0
0.00.038.804 I print_info: ssm_d_inner      = 0
0.00.038.804 I print_info: ssm_d_state      = 0
0.00.038.804 I print_info: ssm_dt_rank      = 0
0.00.038.804 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.805 I print_info: model type       = 1.4B
0.00.038.805 I print_info: model params     = 1.41 B
0.00.038.805 I print_info: general.name     = 1.4B
0.00.038.806 I print_info: vocab type       = BPE
0.00.038.806 I print_info: n_vocab          = 50304
0.00.038.806 I print_info: n_merges         = 50009
0.00.038.808 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.808 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.808 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.808 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.809 I print_info: LF token         = 187 'Ċ'
0.00.038.809 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.809 I print_info: max token length = 1024
0.00.655.762 I load_tensors: offloading 24 repeating layers to GPU
0.00.655.776 I load_tensors: offloading output layer to GPU
0.00.655.777 I load_tensors: offloaded 25/25 layers to GPU
0.00.655.809 I load_tensors: Metal_Mapped model buffer size =   942.61 MiB
0.00.655.810 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.657.213 I llama_init_from_model: n_seq_max     = 1
0.00.657.223 I llama_init_from_model: n_ctx         = 128
0.00.657.224 I llama_init_from_model: n_ctx_per_seq = 128
0.00.657.228 I llama_init_from_model: n_batch       = 128
0.00.657.228 I llama_init_from_model: n_ubatch      = 128
0.00.657.229 I llama_init_from_model: flash_attn    = 0
0.00.657.230 I llama_init_from_model: freq_base     = 10000.0
0.00.657.230 I llama_init_from_model: freq_scale    = 1
0.00.657.231 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.657.239 I ggml_metal_init: allocating
0.00.657.341 I ggml_metal_init: found device: Apple M4
0.00.657.355 I ggml_metal_init: picking default device: Apple M4
0.00.659.268 I ggml_metal_init: using embedded metal library
0.00.666.071 I ggml_metal_init: GPU name:   Apple M4
0.00.666.076 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.077 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.078 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.079 I ggml_metal_init: simdgroup reduction   = true
0.00.666.079 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.079 I ggml_metal_init: has residency sets    = true
0.00.666.079 I ggml_metal_init: has bfloat            = true
0.00.666.080 I ggml_metal_init: use bfloat            = true
0.00.666.081 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.084 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.653 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.687.201 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.687.205 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.687.254 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.690.495 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.690.497 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.690.497 I llama_init_from_model: graph nodes  = 967
0.00.690.498 I llama_init_from_model: graph splits = 2
0.00.690.501 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.690.505 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.717.325 I 
0.00.717.407 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.717.415 I perplexity: tokenizing the input ..
0.00.723.311 I perplexity: tokenization took 5.894 ms
0.00.723.317 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.857.202 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]10.0972,
0.00.858.721 I Final estimate: PPL = 10.0972 +/- 3.20136

0.00.858.739 I llama_perf_context_print:        load time =     708.22 ms
0.00.858.740 I llama_perf_context_print: prompt eval time =     133.64 ms /   128 tokens (    1.04 ms per token,   957.78 tokens per second)
0.00.858.740 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.858.741 I llama_perf_context_print:       total time =     141.42 ms /   129 tokens
0.00.859.116 I ggml_metal_free: deallocating

real	0m0.873s
user	0m0.078s
sys	0m0.143s
```
- q5_1:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.008.594 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.620 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.016.624 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.626 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.627 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.627 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.627 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.628 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.628 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.629 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.630 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.631 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.633 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.634 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.635 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.635 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.636 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.371 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.385 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.098 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.099 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.100 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.101 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.025.101 I llama_model_loader: - type  f32:  194 tensors
0.00.025.101 I llama_model_loader: - type q5_1:   97 tensors
0.00.025.102 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.102 I print_info: file format = GGUF V3 (latest)
0.00.025.103 I print_info: file type   = Q5_1
0.00.025.103 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.032.998 I load: special tokens cache size = 25
0.00.038.975 I load: token to piece cache size = 0.2984 MB
0.00.038.977 I print_info: arch             = gptneox
0.00.038.977 I print_info: vocab_only       = 0
0.00.038.978 I print_info: n_ctx_train      = 2048
0.00.038.978 I print_info: n_embd           = 2048
0.00.038.978 I print_info: n_layer          = 24
0.00.038.980 I print_info: n_head           = 16
0.00.038.981 I print_info: n_head_kv        = 16
0.00.038.981 I print_info: n_rot            = 32
0.00.038.981 I print_info: n_swa            = 0
0.00.038.982 I print_info: n_embd_head_k    = 128
0.00.038.982 I print_info: n_embd_head_v    = 128
0.00.038.984 I print_info: n_gqa            = 1
0.00.038.985 I print_info: n_embd_k_gqa     = 2048
0.00.038.986 I print_info: n_embd_v_gqa     = 2048
0.00.038.991 I print_info: f_norm_eps       = 1.0e-05
0.00.038.991 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.991 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.993 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.993 I print_info: f_logit_scale    = 0.0e+00
0.00.038.994 I print_info: n_ff             = 8192
0.00.038.994 I print_info: n_expert         = 0
0.00.038.995 I print_info: n_expert_used    = 0
0.00.038.995 I print_info: causal attn      = 1
0.00.038.995 I print_info: pooling type     = 0
0.00.038.995 I print_info: rope type        = 2
0.00.038.995 I print_info: rope scaling     = linear
0.00.038.999 I print_info: freq_base_train  = 10000.0
0.00.038.999 I print_info: freq_scale_train = 1
0.00.038.999 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.999 I print_info: rope_finetuned   = unknown
0.00.038.999 I print_info: ssm_d_conv       = 0
0.00.039.000 I print_info: ssm_d_inner      = 0
0.00.039.000 I print_info: ssm_d_state      = 0
0.00.039.000 I print_info: ssm_dt_rank      = 0
0.00.039.004 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.005 I print_info: model type       = 1.4B
0.00.039.006 I print_info: model params     = 1.41 B
0.00.039.006 I print_info: general.name     = 1.4B
0.00.039.007 I print_info: vocab type       = BPE
0.00.039.007 I print_info: n_vocab          = 50304
0.00.039.007 I print_info: n_merges         = 50009
0.00.039.007 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.007 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: LF token         = 187 'Ċ'
0.00.039.008 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.008 I print_info: max token length = 1024
0.00.720.063 I load_tensors: offloading 24 repeating layers to GPU
0.00.720.073 I load_tensors: offloading output layer to GPU
0.00.720.074 I load_tensors: offloaded 25/25 layers to GPU
0.00.720.113 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.720.115 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.721.686 I llama_init_from_model: n_seq_max     = 1
0.00.721.690 I llama_init_from_model: n_ctx         = 2048
0.00.721.691 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.721.691 I llama_init_from_model: n_batch       = 2048
0.00.721.691 I llama_init_from_model: n_ubatch      = 512
0.00.721.692 I llama_init_from_model: flash_attn    = 0
0.00.721.694 I llama_init_from_model: freq_base     = 10000.0
0.00.721.694 I llama_init_from_model: freq_scale    = 1
0.00.721.697 I ggml_metal_init: allocating
0.00.721.810 I ggml_metal_init: found device: Apple M4
0.00.721.823 I ggml_metal_init: picking default device: Apple M4
0.00.723.416 I ggml_metal_init: using embedded metal library
0.00.729.818 I ggml_metal_init: GPU name:   Apple M4
0.00.729.822 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.729.823 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.729.823 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.729.824 I ggml_metal_init: simdgroup reduction   = true
0.00.729.824 I ggml_metal_init: simdgroup matrix mul. = true
0.00.729.824 I ggml_metal_init: has residency sets    = true
0.00.729.824 I ggml_metal_init: has bfloat            = true
0.00.729.825 I ggml_metal_init: use bfloat            = true
0.00.729.825 I ggml_metal_init: hasUnifiedMemory      = true
0.00.729.827 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.747.017 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.799.015 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.799.022 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.799.058 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.803.482 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.803.484 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.803.484 I llama_init_from_model: graph nodes  = 967
0.00.803.484 I llama_init_from_model: graph splits = 2
0.00.803.490 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.803.613 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.803.614 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.863.551 I main: llama threadpool init, n_threads = 4
0.00.863.594 I 
0.00.863.615 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.863.616 I 
0.00.863.794 I sampler seed: 1234
0.00.863.799 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.863.810 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.863.810 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.863.811 I 
I believe the meaning of life is to give all things an equal chance, and to be content with one's lot in life.

When one is in a good situation, one can think of no better way to express it than through music, art or literature.

If one's life is not good, one can be sure that one is

0.01.707.008 I llama_perf_sampler_print:    sampling time =       1.30 ms /    71 runs   (    0.02 ms per token, 54406.13 tokens per second)
0.01.707.009 I llama_perf_context_print:        load time =     854.25 ms
0.01.707.010 I llama_perf_context_print: prompt eval time =      52.18 ms /     7 tokens (    7.45 ms per token,   134.15 tokens per second)
0.01.707.010 I llama_perf_context_print:        eval time =     788.13 ms /    63 runs   (   12.51 ms per token,    79.94 tokens per second)
0.01.707.011 I llama_perf_context_print:       total time =     844.17 ms /    70 tokens
0.01.707.209 I ggml_metal_free: deallocating

real	0m1.722s
user	0m0.107s
sys	0m0.231s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.110 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.181 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.363 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_1.gguf (version GGUF V3 (latest))
0.00.017.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.370 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.376 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.376 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.377 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.378 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.378 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.379 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.379 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.379 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.380 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.380 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.381 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.382 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.382 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.213 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.274 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.100 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.102 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.102 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.103 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.103 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.103 I llama_model_loader: - kv  22:                          general.file_type u32              = 9
0.00.026.104 I llama_model_loader: - type  f32:  194 tensors
0.00.026.104 I llama_model_loader: - type q5_1:   97 tensors
0.00.026.104 I llama_model_loader: - type q6_K:    1 tensors
0.00.026.105 I print_info: file format = GGUF V3 (latest)
0.00.026.106 I print_info: file type   = Q5_1
0.00.026.107 I print_info: file size   = 1020.74 MiB (6.05 BPW) 
0.00.034.001 I load: special tokens cache size = 25
0.00.039.742 I load: token to piece cache size = 0.2984 MB
0.00.039.745 I print_info: arch             = gptneox
0.00.039.745 I print_info: vocab_only       = 0
0.00.039.746 I print_info: n_ctx_train      = 2048
0.00.039.746 I print_info: n_embd           = 2048
0.00.039.746 I print_info: n_layer          = 24
0.00.039.750 I print_info: n_head           = 16
0.00.039.751 I print_info: n_head_kv        = 16
0.00.039.751 I print_info: n_rot            = 32
0.00.039.751 I print_info: n_swa            = 0
0.00.039.751 I print_info: n_embd_head_k    = 128
0.00.039.751 I print_info: n_embd_head_v    = 128
0.00.039.752 I print_info: n_gqa            = 1
0.00.039.753 I print_info: n_embd_k_gqa     = 2048
0.00.039.753 I print_info: n_embd_v_gqa     = 2048
0.00.039.754 I print_info: f_norm_eps       = 1.0e-05
0.00.039.754 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.755 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.755 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.755 I print_info: f_logit_scale    = 0.0e+00
0.00.039.756 I print_info: n_ff             = 8192
0.00.039.758 I print_info: n_expert         = 0
0.00.039.758 I print_info: n_expert_used    = 0
0.00.039.758 I print_info: causal attn      = 1
0.00.039.758 I print_info: pooling type     = 0
0.00.039.758 I print_info: rope type        = 2
0.00.039.758 I print_info: rope scaling     = linear
0.00.039.759 I print_info: freq_base_train  = 10000.0
0.00.039.759 I print_info: freq_scale_train = 1
0.00.039.759 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.760 I print_info: rope_finetuned   = unknown
0.00.039.760 I print_info: ssm_d_conv       = 0
0.00.039.760 I print_info: ssm_d_inner      = 0
0.00.039.760 I print_info: ssm_d_state      = 0
0.00.039.760 I print_info: ssm_dt_rank      = 0
0.00.039.760 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.761 I print_info: model type       = 1.4B
0.00.039.761 I print_info: model params     = 1.41 B
0.00.039.761 I print_info: general.name     = 1.4B
0.00.039.762 I print_info: vocab type       = BPE
0.00.039.763 I print_info: n_vocab          = 50304
0.00.039.763 I print_info: n_merges         = 50009
0.00.039.763 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.763 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.764 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.766 I print_info: LF token         = 187 'Ċ'
0.00.039.766 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.766 I print_info: max token length = 1024
0.00.690.933 I load_tensors: offloading 24 repeating layers to GPU
0.00.690.948 I load_tensors: offloading output layer to GPU
0.00.690.948 I load_tensors: offloaded 25/25 layers to GPU
0.00.690.981 I load_tensors: Metal_Mapped model buffer size =  1020.75 MiB
0.00.690.999 I load_tensors:   CPU_Mapped model buffer size =    73.69 MiB
0.00.692.574 I llama_init_from_model: n_seq_max     = 1
0.00.692.579 I llama_init_from_model: n_ctx         = 128
0.00.692.580 I llama_init_from_model: n_ctx_per_seq = 128
0.00.692.581 I llama_init_from_model: n_batch       = 128
0.00.692.581 I llama_init_from_model: n_ubatch      = 128
0.00.692.582 I llama_init_from_model: flash_attn    = 0
0.00.692.584 I llama_init_from_model: freq_base     = 10000.0
0.00.692.585 I llama_init_from_model: freq_scale    = 1
0.00.692.585 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.692.588 I ggml_metal_init: allocating
0.00.692.661 I ggml_metal_init: found device: Apple M4
0.00.692.675 I ggml_metal_init: picking default device: Apple M4
0.00.694.397 I ggml_metal_init: using embedded metal library
0.00.700.768 I ggml_metal_init: GPU name:   Apple M4
0.00.700.772 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.700.773 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.700.774 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.700.775 I ggml_metal_init: simdgroup reduction   = true
0.00.700.775 I ggml_metal_init: simdgroup matrix mul. = true
0.00.700.775 I ggml_metal_init: has residency sets    = true
0.00.700.776 I ggml_metal_init: has bfloat            = true
0.00.700.776 I ggml_metal_init: use bfloat            = true
0.00.700.777 I ggml_metal_init: hasUnifiedMemory      = true
0.00.700.778 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.717.651 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.721.092 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.721.099 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.721.150 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.724.336 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.724.338 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.724.338 I llama_init_from_model: graph nodes  = 967
0.00.724.339 I llama_init_from_model: graph splits = 2
0.00.724.342 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.724.345 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.753.650 I 
0.00.753.729 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.753.737 I perplexity: tokenizing the input ..
0.00.760.416 I perplexity: tokenization took 6.675 ms
0.00.760.430 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.901.935 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1971,
0.00.903.480 I Final estimate: PPL = 10.1971 +/- 3.18866

0.00.903.506 I llama_perf_context_print:        load time =     743.46 ms
0.00.903.507 I llama_perf_context_print: prompt eval time =     140.96 ms /   128 tokens (    1.10 ms per token,   908.08 tokens per second)
0.00.903.508 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.903.508 I llama_perf_context_print:       total time =     149.86 ms /   129 tokens
0.00.903.865 I ggml_metal_free: deallocating

real	0m0.920s
user	0m0.079s
sys	0m0.144s
```
- q2_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.051 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.088 I main: llama backend init
0.00.000.090 I main: load the model and apply lora adapter, if any
0.00.010.149 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.875 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.016.880 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.885 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.886 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.886 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.887 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.887 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.890 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.890 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.891 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.892 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.893 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.893 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.894 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.895 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.896 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.896 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.724 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.770 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.563 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.565 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.565 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.565 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.566 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.566 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.025.567 I llama_model_loader: - type  f32:  194 tensors
0.00.025.567 I llama_model_loader: - type q2_K:   49 tensors
0.00.025.567 I llama_model_loader: - type q3_K:   48 tensors
0.00.025.567 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.568 I print_info: file format = GGUF V3 (latest)
0.00.025.568 I print_info: file type   = Q2_K - Medium
0.00.025.569 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.033.867 I load: special tokens cache size = 25
0.00.039.775 I load: token to piece cache size = 0.2984 MB
0.00.039.778 I print_info: arch             = gptneox
0.00.039.778 I print_info: vocab_only       = 0
0.00.039.778 I print_info: n_ctx_train      = 2048
0.00.039.778 I print_info: n_embd           = 2048
0.00.039.778 I print_info: n_layer          = 24
0.00.039.781 I print_info: n_head           = 16
0.00.039.782 I print_info: n_head_kv        = 16
0.00.039.782 I print_info: n_rot            = 32
0.00.039.782 I print_info: n_swa            = 0
0.00.039.782 I print_info: n_embd_head_k    = 128
0.00.039.784 I print_info: n_embd_head_v    = 128
0.00.039.785 I print_info: n_gqa            = 1
0.00.039.786 I print_info: n_embd_k_gqa     = 2048
0.00.039.786 I print_info: n_embd_v_gqa     = 2048
0.00.039.787 I print_info: f_norm_eps       = 1.0e-05
0.00.039.787 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.787 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.787 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.788 I print_info: f_logit_scale    = 0.0e+00
0.00.039.788 I print_info: n_ff             = 8192
0.00.039.788 I print_info: n_expert         = 0
0.00.039.789 I print_info: n_expert_used    = 0
0.00.039.789 I print_info: causal attn      = 1
0.00.039.789 I print_info: pooling type     = 0
0.00.039.789 I print_info: rope type        = 2
0.00.039.789 I print_info: rope scaling     = linear
0.00.039.790 I print_info: freq_base_train  = 10000.0
0.00.039.790 I print_info: freq_scale_train = 1
0.00.039.790 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.791 I print_info: rope_finetuned   = unknown
0.00.039.791 I print_info: ssm_d_conv       = 0
0.00.039.791 I print_info: ssm_d_inner      = 0
0.00.039.791 I print_info: ssm_d_state      = 0
0.00.039.793 I print_info: ssm_dt_rank      = 0
0.00.039.793 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.793 I print_info: model type       = 1.4B
0.00.039.793 I print_info: model params     = 1.41 B
0.00.039.793 I print_info: general.name     = 1.4B
0.00.039.794 I print_info: vocab type       = BPE
0.00.039.794 I print_info: n_vocab          = 50304
0.00.039.794 I print_info: n_merges         = 50009
0.00.039.795 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.795 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.795 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.795 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.795 I print_info: LF token         = 187 'Ċ'
0.00.039.796 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.797 I print_info: max token length = 1024
0.00.396.187 I load_tensors: offloading 24 repeating layers to GPU
0.00.396.196 I load_tensors: offloading output layer to GPU
0.00.396.197 I load_tensors: offloaded 25/25 layers to GPU
0.00.396.226 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.396.228 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.397.574 I llama_init_from_model: n_seq_max     = 1
0.00.397.579 I llama_init_from_model: n_ctx         = 2048
0.00.397.580 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.397.580 I llama_init_from_model: n_batch       = 2048
0.00.397.581 I llama_init_from_model: n_ubatch      = 512
0.00.397.581 I llama_init_from_model: flash_attn    = 0
0.00.397.584 I llama_init_from_model: freq_base     = 10000.0
0.00.397.584 I llama_init_from_model: freq_scale    = 1
0.00.397.595 I ggml_metal_init: allocating
0.00.397.645 I ggml_metal_init: found device: Apple M4
0.00.397.659 I ggml_metal_init: picking default device: Apple M4
0.00.399.333 I ggml_metal_init: using embedded metal library
0.00.405.080 I ggml_metal_init: GPU name:   Apple M4
0.00.405.094 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.405.094 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.405.095 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.405.096 I ggml_metal_init: simdgroup reduction   = true
0.00.405.096 I ggml_metal_init: simdgroup matrix mul. = true
0.00.405.097 I ggml_metal_init: has residency sets    = true
0.00.405.097 I ggml_metal_init: has bfloat            = true
0.00.405.097 I ggml_metal_init: use bfloat            = true
0.00.405.102 I ggml_metal_init: hasUnifiedMemory      = true
0.00.405.113 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.427.176 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.485.475 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.485.484 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.485.523 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.489.959 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.489.962 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.489.962 I llama_init_from_model: graph nodes  = 967
0.00.489.962 I llama_init_from_model: graph splits = 2
0.00.489.968 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.490.096 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.490.096 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.546.902 I main: llama threadpool init, n_threads = 4
0.00.546.947 I 
0.00.546.971 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.546.972 I 
0.00.547.148 I sampler seed: 1234
0.00.547.152 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.547.196 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.547.200 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.547.200 I 
I believe the meaning of life is the meaning of choice and the meaning of choice is the meaning of the chosen. I believe in the meaning of a chosen. I believe in the meaning of an under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-under-

0.01.219.258 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52475.98 tokens per second)
0.01.219.258 I llama_perf_context_print:        load time =     536.05 ms
0.01.219.259 I llama_perf_context_print: prompt eval time =      35.75 ms /     7 tokens (    5.11 ms per token,   195.79 tokens per second)
0.01.219.260 I llama_perf_context_print:        eval time =     633.44 ms /    63 runs   (   10.05 ms per token,    99.46 tokens per second)
0.01.219.260 I llama_perf_context_print:       total time =     673.06 ms /    70 tokens
0.01.219.521 I ggml_metal_free: deallocating

real	0m1.238s
user	0m0.112s
sys	0m0.182s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.107 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.911 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.835 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q2_k.gguf (version GGUF V3 (latest))
0.00.015.842 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.844 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.844 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.844 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.845 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.846 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.848 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.849 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.849 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.849 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.850 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.850 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.853 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.853 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.853 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.693 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.753 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.591 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.593 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.593 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.593 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.594 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.594 I llama_model_loader: - kv  22:                          general.file_type u32              = 10
0.00.024.595 I llama_model_loader: - type  f32:  194 tensors
0.00.024.595 I llama_model_loader: - type q2_K:   49 tensors
0.00.024.595 I llama_model_loader: - type q3_K:   48 tensors
0.00.024.596 I llama_model_loader: - type q6_K:    1 tensors
0.00.024.596 I print_info: file format = GGUF V3 (latest)
0.00.024.597 I print_info: file type   = Q2_K - Medium
0.00.024.599 I print_info: file size   = 542.04 MiB (3.21 BPW) 
0.00.032.458 I load: special tokens cache size = 25
0.00.038.475 I load: token to piece cache size = 0.2984 MB
0.00.038.479 I print_info: arch             = gptneox
0.00.038.479 I print_info: vocab_only       = 0
0.00.038.479 I print_info: n_ctx_train      = 2048
0.00.038.479 I print_info: n_embd           = 2048
0.00.038.480 I print_info: n_layer          = 24
0.00.038.483 I print_info: n_head           = 16
0.00.038.484 I print_info: n_head_kv        = 16
0.00.038.484 I print_info: n_rot            = 32
0.00.038.484 I print_info: n_swa            = 0
0.00.038.484 I print_info: n_embd_head_k    = 128
0.00.038.485 I print_info: n_embd_head_v    = 128
0.00.038.485 I print_info: n_gqa            = 1
0.00.038.488 I print_info: n_embd_k_gqa     = 2048
0.00.038.489 I print_info: n_embd_v_gqa     = 2048
0.00.038.489 I print_info: f_norm_eps       = 1.0e-05
0.00.038.489 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.490 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.490 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.490 I print_info: f_logit_scale    = 0.0e+00
0.00.038.491 I print_info: n_ff             = 8192
0.00.038.491 I print_info: n_expert         = 0
0.00.038.491 I print_info: n_expert_used    = 0
0.00.038.491 I print_info: causal attn      = 1
0.00.038.491 I print_info: pooling type     = 0
0.00.038.491 I print_info: rope type        = 2
0.00.038.492 I print_info: rope scaling     = linear
0.00.038.492 I print_info: freq_base_train  = 10000.0
0.00.038.493 I print_info: freq_scale_train = 1
0.00.038.493 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.493 I print_info: rope_finetuned   = unknown
0.00.038.493 I print_info: ssm_d_conv       = 0
0.00.038.495 I print_info: ssm_d_inner      = 0
0.00.038.495 I print_info: ssm_d_state      = 0
0.00.038.495 I print_info: ssm_dt_rank      = 0
0.00.038.495 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.496 I print_info: model type       = 1.4B
0.00.038.496 I print_info: model params     = 1.41 B
0.00.038.496 I print_info: general.name     = 1.4B
0.00.038.497 I print_info: vocab type       = BPE
0.00.038.497 I print_info: n_vocab          = 50304
0.00.038.497 I print_info: n_merges         = 50009
0.00.038.501 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.501 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.502 I print_info: LF token         = 187 'Ċ'
0.00.038.502 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.503 I print_info: max token length = 1024
0.00.374.597 I load_tensors: offloading 24 repeating layers to GPU
0.00.374.611 I load_tensors: offloading output layer to GPU
0.00.374.612 I load_tensors: offloaded 25/25 layers to GPU
0.00.374.641 I load_tensors: Metal_Mapped model buffer size =   542.05 MiB
0.00.374.642 I load_tensors:   CPU_Mapped model buffer size =    32.24 MiB
0.00.376.031 I llama_init_from_model: n_seq_max     = 1
0.00.376.036 I llama_init_from_model: n_ctx         = 128
0.00.376.040 I llama_init_from_model: n_ctx_per_seq = 128
0.00.376.041 I llama_init_from_model: n_batch       = 128
0.00.376.041 I llama_init_from_model: n_ubatch      = 128
0.00.376.042 I llama_init_from_model: flash_attn    = 0
0.00.376.044 I llama_init_from_model: freq_base     = 10000.0
0.00.376.056 I llama_init_from_model: freq_scale    = 1
0.00.376.057 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.376.059 I ggml_metal_init: allocating
0.00.376.135 I ggml_metal_init: found device: Apple M4
0.00.376.148 I ggml_metal_init: picking default device: Apple M4
0.00.378.010 I ggml_metal_init: using embedded metal library
0.00.383.534 I ggml_metal_init: GPU name:   Apple M4
0.00.383.547 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.383.548 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.383.549 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.383.549 I ggml_metal_init: simdgroup reduction   = true
0.00.383.550 I ggml_metal_init: simdgroup matrix mul. = true
0.00.383.550 I ggml_metal_init: has residency sets    = true
0.00.383.550 I ggml_metal_init: has bfloat            = true
0.00.383.551 I ggml_metal_init: use bfloat            = true
0.00.383.555 I ggml_metal_init: hasUnifiedMemory      = true
0.00.383.559 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.404.532 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.408.209 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.408.213 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.408.255 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.411.864 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.411.866 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.411.867 I llama_init_from_model: graph nodes  = 967
0.00.411.867 I llama_init_from_model: graph splits = 2
0.00.411.870 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.411.870 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.438.900 I 
0.00.438.981 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.438.987 I perplexity: tokenizing the input ..
0.00.446.420 I perplexity: tokenization took 7.428 ms
0.00.446.429 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.579.841 I perplexity: 0.13 seconds per pass - ETA 0.00 minutes
[1]70.7978,
0.00.581.361 I Final estimate: PPL = 70.7978 +/- 27.57202

0.00.581.385 I llama_perf_context_print:        load time =     429.98 ms
0.00.581.386 I llama_perf_context_print: prompt eval time =     132.55 ms /   128 tokens (    1.04 ms per token,   965.70 tokens per second)
0.00.581.387 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.581.388 I llama_perf_context_print:       total time =     142.49 ms /   129 tokens
0.00.581.756 I ggml_metal_free: deallocating

real	0m0.596s
user	0m0.083s
sys	0m0.093s
```
- q3_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.087 I main: llama backend init
0.00.000.089 I main: load the model and apply lora adapter, if any
0.00.008.827 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.288 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.016.294 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.295 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.296 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.296 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.297 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.297 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.298 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.298 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.298 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.299 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.299 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.301 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.302 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.305 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.305 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.305 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.194 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.242 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.005 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.006 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.007 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.008 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.008 I llama_model_loader: - type  f32:  194 tensors
0.00.025.009 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.009 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.009 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.009 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.010 I print_info: file format = GGUF V3 (latest)
0.00.025.010 I print_info: file type   = Q3_K - Medium
0.00.025.011 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.032.901 I load: special tokens cache size = 25
0.00.038.722 I load: token to piece cache size = 0.2984 MB
0.00.038.725 I print_info: arch             = gptneox
0.00.038.725 I print_info: vocab_only       = 0
0.00.038.726 I print_info: n_ctx_train      = 2048
0.00.038.726 I print_info: n_embd           = 2048
0.00.038.726 I print_info: n_layer          = 24
0.00.038.729 I print_info: n_head           = 16
0.00.038.729 I print_info: n_head_kv        = 16
0.00.038.730 I print_info: n_rot            = 32
0.00.038.730 I print_info: n_swa            = 0
0.00.038.732 I print_info: n_embd_head_k    = 128
0.00.038.732 I print_info: n_embd_head_v    = 128
0.00.038.732 I print_info: n_gqa            = 1
0.00.038.733 I print_info: n_embd_k_gqa     = 2048
0.00.038.738 I print_info: n_embd_v_gqa     = 2048
0.00.038.739 I print_info: f_norm_eps       = 1.0e-05
0.00.038.739 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.740 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.740 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.740 I print_info: f_logit_scale    = 0.0e+00
0.00.038.741 I print_info: n_ff             = 8192
0.00.038.741 I print_info: n_expert         = 0
0.00.038.741 I print_info: n_expert_used    = 0
0.00.038.743 I print_info: causal attn      = 1
0.00.038.743 I print_info: pooling type     = 0
0.00.038.743 I print_info: rope type        = 2
0.00.038.743 I print_info: rope scaling     = linear
0.00.038.744 I print_info: freq_base_train  = 10000.0
0.00.038.744 I print_info: freq_scale_train = 1
0.00.038.744 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.744 I print_info: rope_finetuned   = unknown
0.00.038.748 I print_info: ssm_d_conv       = 0
0.00.038.748 I print_info: ssm_d_inner      = 0
0.00.038.748 I print_info: ssm_d_state      = 0
0.00.038.748 I print_info: ssm_dt_rank      = 0
0.00.038.748 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.748 I print_info: model type       = 1.4B
0.00.038.748 I print_info: model params     = 1.41 B
0.00.038.749 I print_info: general.name     = 1.4B
0.00.038.749 I print_info: vocab type       = BPE
0.00.038.749 I print_info: n_vocab          = 50304
0.00.038.750 I print_info: n_merges         = 50009
0.00.038.750 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.750 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.751 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.751 I print_info: LF token         = 187 'Ċ'
0.00.038.751 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.751 I print_info: max token length = 1024
0.00.443.830 I load_tensors: offloading 24 repeating layers to GPU
0.00.443.846 I load_tensors: offloading output layer to GPU
0.00.443.847 I load_tensors: offloaded 25/25 layers to GPU
0.00.443.885 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.443.887 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.445.401 I llama_init_from_model: n_seq_max     = 1
0.00.445.407 I llama_init_from_model: n_ctx         = 2048
0.00.445.408 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.445.408 I llama_init_from_model: n_batch       = 2048
0.00.445.408 I llama_init_from_model: n_ubatch      = 512
0.00.445.409 I llama_init_from_model: flash_attn    = 0
0.00.445.415 I llama_init_from_model: freq_base     = 10000.0
0.00.445.418 I llama_init_from_model: freq_scale    = 1
0.00.445.420 I ggml_metal_init: allocating
0.00.445.494 I ggml_metal_init: found device: Apple M4
0.00.445.509 I ggml_metal_init: picking default device: Apple M4
0.00.447.383 I ggml_metal_init: using embedded metal library
0.00.453.267 I ggml_metal_init: GPU name:   Apple M4
0.00.453.272 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.453.273 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.453.274 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.453.275 I ggml_metal_init: simdgroup reduction   = true
0.00.453.275 I ggml_metal_init: simdgroup matrix mul. = true
0.00.453.276 I ggml_metal_init: has residency sets    = true
0.00.453.276 I ggml_metal_init: has bfloat            = true
0.00.453.276 I ggml_metal_init: use bfloat            = true
0.00.453.277 I ggml_metal_init: hasUnifiedMemory      = true
0.00.453.279 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.472.691 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.530.935 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.530.942 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.530.981 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.534.946 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.534.947 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.534.948 I llama_init_from_model: graph nodes  = 967
0.00.534.948 I llama_init_from_model: graph splits = 2
0.00.534.953 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.535.081 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.535.082 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.590.487 I main: llama threadpool init, n_threads = 4
0.00.590.528 I 
0.00.590.548 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.590.550 I 
0.00.590.724 I sampler seed: 1234
0.00.590.729 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.590.747 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.590.748 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.590.748 I 
I believe the meaning of life is that one should always seek to be understood.

"The question is not, 'What is the best and most beautiful thing that I can do?'

"The question is, 'What can I do,

"To be better and wiser and happier and more

"Loved than anyone

0.01.341.635 I llama_perf_sampler_print:    sampling time =       1.34 ms /    71 runs   (    0.02 ms per token, 52827.38 tokens per second)
0.01.341.635 I llama_perf_context_print:        load time =     580.95 ms
0.01.341.636 I llama_perf_context_print: prompt eval time =      49.89 ms /     7 tokens (    7.13 ms per token,   140.31 tokens per second)
0.01.341.637 I llama_perf_context_print:        eval time =     698.11 ms /    63 runs   (   11.08 ms per token,    90.24 tokens per second)
0.01.341.637 I llama_perf_context_print:       total time =     751.86 ms /    70 tokens
0.01.341.923 I ggml_metal_free: deallocating

real	0m1.357s
user	0m0.110s
sys	0m0.186s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.101 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.163 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.194 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q3_k.gguf (version GGUF V3 (latest))
0.00.017.200 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.207 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.208 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.208 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.210 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.210 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.211 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.211 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.212 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.212 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.212 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.213 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.217 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.218 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.218 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.220 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.083 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.175 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.916 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.917 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.917 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.918 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.918 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.918 I llama_model_loader: - kv  22:                          general.file_type u32              = 12
0.00.025.919 I llama_model_loader: - type  f32:  194 tensors
0.00.025.919 I llama_model_loader: - type q3_K:   25 tensors
0.00.025.919 I llama_model_loader: - type q4_K:   71 tensors
0.00.025.920 I llama_model_loader: - type q5_K:    1 tensors
0.00.025.920 I llama_model_loader: - type q6_K:    1 tensors
0.00.025.920 I print_info: file format = GGUF V3 (latest)
0.00.025.921 I print_info: file type   = Q3_K - Medium
0.00.025.922 I print_info: file size   = 724.27 MiB (4.29 BPW) 
0.00.034.059 I load: special tokens cache size = 25
0.00.040.139 I load: token to piece cache size = 0.2984 MB
0.00.040.142 I print_info: arch             = gptneox
0.00.040.142 I print_info: vocab_only       = 0
0.00.040.142 I print_info: n_ctx_train      = 2048
0.00.040.142 I print_info: n_embd           = 2048
0.00.040.143 I print_info: n_layer          = 24
0.00.040.146 I print_info: n_head           = 16
0.00.040.147 I print_info: n_head_kv        = 16
0.00.040.147 I print_info: n_rot            = 32
0.00.040.147 I print_info: n_swa            = 0
0.00.040.148 I print_info: n_embd_head_k    = 128
0.00.040.148 I print_info: n_embd_head_v    = 128
0.00.040.149 I print_info: n_gqa            = 1
0.00.040.150 I print_info: n_embd_k_gqa     = 2048
0.00.040.151 I print_info: n_embd_v_gqa     = 2048
0.00.040.151 I print_info: f_norm_eps       = 1.0e-05
0.00.040.152 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.152 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.152 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.153 I print_info: f_logit_scale    = 0.0e+00
0.00.040.153 I print_info: n_ff             = 8192
0.00.040.153 I print_info: n_expert         = 0
0.00.040.154 I print_info: n_expert_used    = 0
0.00.040.154 I print_info: causal attn      = 1
0.00.040.154 I print_info: pooling type     = 0
0.00.040.154 I print_info: rope type        = 2
0.00.040.154 I print_info: rope scaling     = linear
0.00.040.155 I print_info: freq_base_train  = 10000.0
0.00.040.157 I print_info: freq_scale_train = 1
0.00.040.157 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.157 I print_info: rope_finetuned   = unknown
0.00.040.157 I print_info: ssm_d_conv       = 0
0.00.040.157 I print_info: ssm_d_inner      = 0
0.00.040.157 I print_info: ssm_d_state      = 0
0.00.040.158 I print_info: ssm_dt_rank      = 0
0.00.040.158 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.158 I print_info: model type       = 1.4B
0.00.040.158 I print_info: model params     = 1.41 B
0.00.040.159 I print_info: general.name     = 1.4B
0.00.040.160 I print_info: vocab type       = BPE
0.00.040.160 I print_info: n_vocab          = 50304
0.00.040.160 I print_info: n_merges         = 50009
0.00.040.161 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.161 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.162 I print_info: LF token         = 187 'Ċ'
0.00.040.162 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.162 I print_info: max token length = 1024
0.00.464.811 I load_tensors: offloading 24 repeating layers to GPU
0.00.464.820 I load_tensors: offloading output layer to GPU
0.00.464.821 I load_tensors: offloaded 25/25 layers to GPU
0.00.464.850 I load_tensors: Metal_Mapped model buffer size =   724.28 MiB
0.00.464.852 I load_tensors:   CPU_Mapped model buffer size =    42.22 MiB
0.00.466.248 I llama_init_from_model: n_seq_max     = 1
0.00.466.256 I llama_init_from_model: n_ctx         = 128
0.00.466.257 I llama_init_from_model: n_ctx_per_seq = 128
0.00.466.257 I llama_init_from_model: n_batch       = 128
0.00.466.257 I llama_init_from_model: n_ubatch      = 128
0.00.466.258 I llama_init_from_model: flash_attn    = 0
0.00.466.259 I llama_init_from_model: freq_base     = 10000.0
0.00.466.260 I llama_init_from_model: freq_scale    = 1
0.00.466.260 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.466.265 I ggml_metal_init: allocating
0.00.466.321 I ggml_metal_init: found device: Apple M4
0.00.466.336 I ggml_metal_init: picking default device: Apple M4
0.00.468.457 I ggml_metal_init: using embedded metal library
0.00.474.746 I ggml_metal_init: GPU name:   Apple M4
0.00.474.751 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.474.752 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.474.753 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.474.754 I ggml_metal_init: simdgroup reduction   = true
0.00.474.754 I ggml_metal_init: simdgroup matrix mul. = true
0.00.474.755 I ggml_metal_init: has residency sets    = true
0.00.474.755 I ggml_metal_init: has bfloat            = true
0.00.474.755 I ggml_metal_init: use bfloat            = true
0.00.474.756 I ggml_metal_init: hasUnifiedMemory      = true
0.00.474.758 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.494.573 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.498.210 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.498.214 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.498.267 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.501.662 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.501.664 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.501.664 I llama_init_from_model: graph nodes  = 967
0.00.501.665 I llama_init_from_model: graph splits = 2
0.00.501.668 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.501.668 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.533.230 I 
0.00.533.315 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.533.323 I perplexity: tokenizing the input ..
0.00.540.164 I perplexity: tokenization took 6.839 ms
0.00.540.168 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.678.890 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]12.0517,
0.00.680.406 I Final estimate: PPL = 12.0517 +/- 3.93502

0.00.680.428 I llama_perf_context_print:        load time =     523.06 ms
0.00.680.429 I llama_perf_context_print: prompt eval time =     138.49 ms /   128 tokens (    1.08 ms per token,   924.24 tokens per second)
0.00.680.430 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.680.430 I llama_perf_context_print:       total time =     147.20 ms /   129 tokens
0.00.680.804 I ggml_metal_free: deallocating

real	0m0.696s
user	0m0.081s
sys	0m0.127s
```
- q4_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.053 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.086 I main: llama backend init
0.00.000.088 I main: load the model and apply lora adapter, if any
0.00.008.856 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.771 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.016.777 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.782 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.783 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.784 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.784 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.784 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.785 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.786 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.786 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.786 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.787 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.789 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.789 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.791 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.792 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.792 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.020.645 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.021.644 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.458 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.459 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.459 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.459 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.460 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.460 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.461 I llama_model_loader: - type  f32:  194 tensors
0.00.025.461 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.461 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.461 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.462 I print_info: file format = GGUF V3 (latest)
0.00.025.462 I print_info: file type   = Q4_K - Medium
0.00.025.463 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.033.410 I load: special tokens cache size = 25
0.00.039.488 I load: token to piece cache size = 0.2984 MB
0.00.039.491 I print_info: arch             = gptneox
0.00.039.491 I print_info: vocab_only       = 0
0.00.039.492 I print_info: n_ctx_train      = 2048
0.00.039.492 I print_info: n_embd           = 2048
0.00.039.492 I print_info: n_layer          = 24
0.00.039.495 I print_info: n_head           = 16
0.00.039.495 I print_info: n_head_kv        = 16
0.00.039.496 I print_info: n_rot            = 32
0.00.039.496 I print_info: n_swa            = 0
0.00.039.496 I print_info: n_embd_head_k    = 128
0.00.039.496 I print_info: n_embd_head_v    = 128
0.00.039.497 I print_info: n_gqa            = 1
0.00.039.498 I print_info: n_embd_k_gqa     = 2048
0.00.039.498 I print_info: n_embd_v_gqa     = 2048
0.00.039.499 I print_info: f_norm_eps       = 1.0e-05
0.00.039.499 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.039.499 I print_info: f_clamp_kqv      = 0.0e+00
0.00.039.500 I print_info: f_max_alibi_bias = 0.0e+00
0.00.039.500 I print_info: f_logit_scale    = 0.0e+00
0.00.039.500 I print_info: n_ff             = 8192
0.00.039.501 I print_info: n_expert         = 0
0.00.039.501 I print_info: n_expert_used    = 0
0.00.039.501 I print_info: causal attn      = 1
0.00.039.501 I print_info: pooling type     = 0
0.00.039.501 I print_info: rope type        = 2
0.00.039.502 I print_info: rope scaling     = linear
0.00.039.502 I print_info: freq_base_train  = 10000.0
0.00.039.502 I print_info: freq_scale_train = 1
0.00.039.502 I print_info: n_ctx_orig_yarn  = 2048
0.00.039.503 I print_info: rope_finetuned   = unknown
0.00.039.505 I print_info: ssm_d_conv       = 0
0.00.039.505 I print_info: ssm_d_inner      = 0
0.00.039.505 I print_info: ssm_d_state      = 0
0.00.039.505 I print_info: ssm_dt_rank      = 0
0.00.039.505 I print_info: ssm_dt_b_c_rms   = 0
0.00.039.506 I print_info: model type       = 1.4B
0.00.039.506 I print_info: model params     = 1.41 B
0.00.039.506 I print_info: general.name     = 1.4B
0.00.039.507 I print_info: vocab type       = BPE
0.00.039.507 I print_info: n_vocab          = 50304
0.00.039.507 I print_info: n_merges         = 50009
0.00.039.507 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.039.507 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: LF token         = 187 'Ċ'
0.00.039.508 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.039.508 I print_info: max token length = 1024
0.00.516.736 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.751 I load_tensors: offloading output layer to GPU
0.00.516.751 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.784 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.786 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.518.155 I llama_init_from_model: n_seq_max     = 1
0.00.518.160 I llama_init_from_model: n_ctx         = 2048
0.00.518.160 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.518.161 I llama_init_from_model: n_batch       = 2048
0.00.518.161 I llama_init_from_model: n_ubatch      = 512
0.00.518.162 I llama_init_from_model: flash_attn    = 0
0.00.518.164 I llama_init_from_model: freq_base     = 10000.0
0.00.518.164 I llama_init_from_model: freq_scale    = 1
0.00.518.170 I ggml_metal_init: allocating
0.00.518.248 I ggml_metal_init: found device: Apple M4
0.00.518.262 I ggml_metal_init: picking default device: Apple M4
0.00.520.098 I ggml_metal_init: using embedded metal library
0.00.526.755 I ggml_metal_init: GPU name:   Apple M4
0.00.526.759 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.760 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.761 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.762 I ggml_metal_init: simdgroup reduction   = true
0.00.526.762 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.762 I ggml_metal_init: has residency sets    = true
0.00.526.762 I ggml_metal_init: has bfloat            = true
0.00.526.763 I ggml_metal_init: use bfloat            = true
0.00.526.764 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.765 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.545.071 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.600.980 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.600.986 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.601.025 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.605.176 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.605.178 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.605.178 I llama_init_from_model: graph nodes  = 967
0.00.605.178 I llama_init_from_model: graph splits = 2
0.00.605.184 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.605.312 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.605.313 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.662.898 I main: llama threadpool init, n_threads = 4
0.00.662.941 I 
0.00.662.963 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.662.963 I 
0.00.663.112 I sampler seed: 1234
0.00.663.116 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.663.127 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.663.127 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.663.127 I 
I believe the meaning of life is that there is a lot of suffering and pain in the world. That is why I believe that we have the capability to help each other out of the suffering in the world. I believe that if we just believe and feel like we are part of a bigger, more loving, more compassionate world, that we can help each

0.01.426.220 I llama_perf_sampler_print:    sampling time =       1.48 ms /    71 runs   (    0.02 ms per token, 48037.89 tokens per second)
0.01.426.221 I llama_perf_context_print:        load time =     653.33 ms
0.01.426.226 I llama_perf_context_print: prompt eval time =      47.43 ms /     7 tokens (    6.78 ms per token,   147.58 tokens per second)
0.01.426.227 I llama_perf_context_print:        eval time =     712.90 ms /    63 runs   (   11.32 ms per token,    88.37 tokens per second)
0.01.426.227 I llama_perf_context_print:       total time =     764.03 ms /    70 tokens
0.01.426.507 I ggml_metal_free: deallocating

real	0m1.442s
user	0m0.109s
sys	0m0.192s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.111 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.010.096 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.202 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_k.gguf (version GGUF V3 (latest))
0.00.017.207 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.213 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.214 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.214 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.215 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.215 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.216 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.216 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.217 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.217 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.217 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.218 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.218 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.220 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.220 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.221 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.007 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.011 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.025.802 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.025.803 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.025.803 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.025.804 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.025.804 I llama_model_loader: - kv  22:                          general.file_type u32              = 15
0.00.025.805 I llama_model_loader: - type  f32:  194 tensors
0.00.025.805 I llama_model_loader: - type q4_K:   61 tensors
0.00.025.805 I llama_model_loader: - type q5_K:   24 tensors
0.00.025.806 I llama_model_loader: - type q6_K:   13 tensors
0.00.025.806 I print_info: file format = GGUF V3 (latest)
0.00.025.807 I print_info: file type   = Q4_K - Medium
0.00.025.808 I print_info: file size   = 871.81 MiB (5.17 BPW) 
0.00.034.036 I load: special tokens cache size = 25
0.00.040.053 I load: token to piece cache size = 0.2984 MB
0.00.040.056 I print_info: arch             = gptneox
0.00.040.056 I print_info: vocab_only       = 0
0.00.040.056 I print_info: n_ctx_train      = 2048
0.00.040.056 I print_info: n_embd           = 2048
0.00.040.057 I print_info: n_layer          = 24
0.00.040.060 I print_info: n_head           = 16
0.00.040.060 I print_info: n_head_kv        = 16
0.00.040.060 I print_info: n_rot            = 32
0.00.040.061 I print_info: n_swa            = 0
0.00.040.061 I print_info: n_embd_head_k    = 128
0.00.040.061 I print_info: n_embd_head_v    = 128
0.00.040.062 I print_info: n_gqa            = 1
0.00.040.062 I print_info: n_embd_k_gqa     = 2048
0.00.040.063 I print_info: n_embd_v_gqa     = 2048
0.00.040.064 I print_info: f_norm_eps       = 1.0e-05
0.00.040.064 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.064 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.064 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.065 I print_info: f_logit_scale    = 0.0e+00
0.00.040.065 I print_info: n_ff             = 8192
0.00.040.065 I print_info: n_expert         = 0
0.00.040.066 I print_info: n_expert_used    = 0
0.00.040.066 I print_info: causal attn      = 1
0.00.040.066 I print_info: pooling type     = 0
0.00.040.066 I print_info: rope type        = 2
0.00.040.067 I print_info: rope scaling     = linear
0.00.040.069 I print_info: freq_base_train  = 10000.0
0.00.040.070 I print_info: freq_scale_train = 1
0.00.040.070 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.070 I print_info: rope_finetuned   = unknown
0.00.040.070 I print_info: ssm_d_conv       = 0
0.00.040.070 I print_info: ssm_d_inner      = 0
0.00.040.070 I print_info: ssm_d_state      = 0
0.00.040.071 I print_info: ssm_dt_rank      = 0
0.00.040.071 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.071 I print_info: model type       = 1.4B
0.00.040.071 I print_info: model params     = 1.41 B
0.00.040.071 I print_info: general.name     = 1.4B
0.00.040.072 I print_info: vocab type       = BPE
0.00.040.072 I print_info: n_vocab          = 50304
0.00.040.072 I print_info: n_merges         = 50009
0.00.040.072 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.073 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.073 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.074 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: LF token         = 187 'Ċ'
0.00.040.078 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.078 I print_info: max token length = 1024
0.00.516.165 I load_tensors: offloading 24 repeating layers to GPU
0.00.516.183 I load_tensors: offloading output layer to GPU
0.00.516.183 I load_tensors: offloaded 25/25 layers to GPU
0.00.516.218 I load_tensors: Metal_Mapped model buffer size =   871.83 MiB
0.00.516.219 I load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
0.00.517.748 I llama_init_from_model: n_seq_max     = 1
0.00.517.754 I llama_init_from_model: n_ctx         = 128
0.00.517.754 I llama_init_from_model: n_ctx_per_seq = 128
0.00.517.755 I llama_init_from_model: n_batch       = 128
0.00.517.755 I llama_init_from_model: n_ubatch      = 128
0.00.517.756 I llama_init_from_model: flash_attn    = 0
0.00.517.758 I llama_init_from_model: freq_base     = 10000.0
0.00.517.758 I llama_init_from_model: freq_scale    = 1
0.00.517.759 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.517.762 I ggml_metal_init: allocating
0.00.517.833 I ggml_metal_init: found device: Apple M4
0.00.517.847 I ggml_metal_init: picking default device: Apple M4
0.00.519.597 I ggml_metal_init: using embedded metal library
0.00.526.301 I ggml_metal_init: GPU name:   Apple M4
0.00.526.306 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.526.307 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.526.307 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.526.311 I ggml_metal_init: simdgroup reduction   = true
0.00.526.311 I ggml_metal_init: simdgroup matrix mul. = true
0.00.526.312 I ggml_metal_init: has residency sets    = true
0.00.526.312 I ggml_metal_init: has bfloat            = true
0.00.526.312 I ggml_metal_init: use bfloat            = true
0.00.526.313 I ggml_metal_init: hasUnifiedMemory      = true
0.00.526.318 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.544.917 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.548.412 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.548.416 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.548.467 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.551.679 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.551.681 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.551.682 I llama_init_from_model: graph nodes  = 967
0.00.551.682 I llama_init_from_model: graph splits = 2
0.00.551.685 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.551.686 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.582.966 I 
0.00.583.065 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.583.086 I perplexity: tokenizing the input ..
0.00.590.073 I perplexity: tokenization took 6.983 ms
0.00.590.079 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.731.971 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.1031,
0.00.733.507 I Final estimate: PPL = 10.1031 +/- 3.22057

0.00.733.534 I llama_perf_context_print:        load time =     572.86 ms
0.00.733.535 I llama_perf_context_print: prompt eval time =     140.99 ms /   128 tokens (    1.10 ms per token,   907.89 tokens per second)
0.00.733.536 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.733.536 I llama_perf_context_print:       total time =     150.57 ms /   129 tokens
0.00.733.952 I ggml_metal_free: deallocating

real	0m0.750s
user	0m0.081s
sys	0m0.121s
```
- q5_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.097 I main: load the model and apply lora adapter, if any
0.00.011.359 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.018.150 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.018.156 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.018.158 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.018.158 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.018.158 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.018.159 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.018.159 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.018.160 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.018.160 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.018.161 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.018.161 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.018.162 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.018.162 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.018.162 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.018.164 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.018.165 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.018.165 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.022.007 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.023.096 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.027.020 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.027.021 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.027.022 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.027.022 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.027.022 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.027.023 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.027.023 I llama_model_loader: - type  f32:  194 tensors
0.00.027.024 I llama_model_loader: - type q5_K:   61 tensors
0.00.027.024 I llama_model_loader: - type q6_K:   37 tensors
0.00.027.025 I print_info: file format = GGUF V3 (latest)
0.00.027.025 I print_info: file type   = Q5_K - Medium
0.00.027.026 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.035.306 I load: special tokens cache size = 25
0.00.041.415 I load: token to piece cache size = 0.2984 MB
0.00.041.422 I print_info: arch             = gptneox
0.00.041.423 I print_info: vocab_only       = 0
0.00.041.423 I print_info: n_ctx_train      = 2048
0.00.041.423 I print_info: n_embd           = 2048
0.00.041.423 I print_info: n_layer          = 24
0.00.041.427 I print_info: n_head           = 16
0.00.041.428 I print_info: n_head_kv        = 16
0.00.041.428 I print_info: n_rot            = 32
0.00.041.428 I print_info: n_swa            = 0
0.00.041.428 I print_info: n_embd_head_k    = 128
0.00.041.429 I print_info: n_embd_head_v    = 128
0.00.041.429 I print_info: n_gqa            = 1
0.00.041.433 I print_info: n_embd_k_gqa     = 2048
0.00.041.433 I print_info: n_embd_v_gqa     = 2048
0.00.041.434 I print_info: f_norm_eps       = 1.0e-05
0.00.041.434 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.041.434 I print_info: f_clamp_kqv      = 0.0e+00
0.00.041.435 I print_info: f_max_alibi_bias = 0.0e+00
0.00.041.435 I print_info: f_logit_scale    = 0.0e+00
0.00.041.435 I print_info: n_ff             = 8192
0.00.041.436 I print_info: n_expert         = 0
0.00.041.437 I print_info: n_expert_used    = 0
0.00.041.437 I print_info: causal attn      = 1
0.00.041.437 I print_info: pooling type     = 0
0.00.041.437 I print_info: rope type        = 2
0.00.041.438 I print_info: rope scaling     = linear
0.00.041.438 I print_info: freq_base_train  = 10000.0
0.00.041.438 I print_info: freq_scale_train = 1
0.00.041.438 I print_info: n_ctx_orig_yarn  = 2048
0.00.041.438 I print_info: rope_finetuned   = unknown
0.00.041.439 I print_info: ssm_d_conv       = 0
0.00.041.439 I print_info: ssm_d_inner      = 0
0.00.041.439 I print_info: ssm_d_state      = 0
0.00.041.439 I print_info: ssm_dt_rank      = 0
0.00.041.439 I print_info: ssm_dt_b_c_rms   = 0
0.00.041.439 I print_info: model type       = 1.4B
0.00.041.440 I print_info: model params     = 1.41 B
0.00.041.440 I print_info: general.name     = 1.4B
0.00.041.440 I print_info: vocab type       = BPE
0.00.041.440 I print_info: n_vocab          = 50304
0.00.041.440 I print_info: n_merges         = 50009
0.00.041.441 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.041.441 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.041.441 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.041.441 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.041.441 I print_info: LF token         = 187 'Ċ'
0.00.041.442 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.041.442 I print_info: max token length = 1024
0.00.614.568 I load_tensors: offloading 24 repeating layers to GPU
0.00.614.578 I load_tensors: offloading output layer to GPU
0.00.614.578 I load_tensors: offloaded 25/25 layers to GPU
0.00.614.608 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.614.609 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.616.074 I llama_init_from_model: n_seq_max     = 1
0.00.616.078 I llama_init_from_model: n_ctx         = 2048
0.00.616.078 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.616.079 I llama_init_from_model: n_batch       = 2048
0.00.616.079 I llama_init_from_model: n_ubatch      = 512
0.00.616.080 I llama_init_from_model: flash_attn    = 0
0.00.616.081 I llama_init_from_model: freq_base     = 10000.0
0.00.616.081 I llama_init_from_model: freq_scale    = 1
0.00.616.083 I ggml_metal_init: allocating
0.00.616.135 I ggml_metal_init: found device: Apple M4
0.00.616.147 I ggml_metal_init: picking default device: Apple M4
0.00.617.743 I ggml_metal_init: using embedded metal library
0.00.623.917 I ggml_metal_init: GPU name:   Apple M4
0.00.623.922 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.623.923 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.623.923 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.623.924 I ggml_metal_init: simdgroup reduction   = true
0.00.623.924 I ggml_metal_init: simdgroup matrix mul. = true
0.00.623.925 I ggml_metal_init: has residency sets    = true
0.00.623.925 I ggml_metal_init: has bfloat            = true
0.00.623.925 I ggml_metal_init: use bfloat            = true
0.00.623.926 I ggml_metal_init: hasUnifiedMemory      = true
0.00.623.928 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.640.267 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.695.268 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.695.276 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.695.314 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.700.174 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.700.176 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.700.177 I llama_init_from_model: graph nodes  = 967
0.00.700.177 I llama_init_from_model: graph splits = 2
0.00.700.183 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.700.308 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.700.308 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.751.658 I main: llama threadpool init, n_threads = 4
0.00.751.698 I 
0.00.751.718 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.751.718 I 
0.00.751.854 I sampler seed: 1234
0.00.751.858 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.751.868 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.751.868 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.751.868 I 
I believe the meaning of life is to find love."

"Then why did you say you don't have any?"

"Because you're a girl. It's not the same thing."

"So what is love?"

"I don't know."

"You don't know?"

"I don't

0.01.602.211 I llama_perf_sampler_print:    sampling time =       1.46 ms /    71 runs   (    0.02 ms per token, 48630.14 tokens per second)
0.01.602.212 I llama_perf_context_print:        load time =     739.59 ms
0.01.602.213 I llama_perf_context_print: prompt eval time =      51.24 ms /     7 tokens (    7.32 ms per token,   136.62 tokens per second)
0.01.602.213 I llama_perf_context_print:        eval time =     796.66 ms /    63 runs   (   12.65 ms per token,    79.08 tokens per second)
0.01.602.214 I llama_perf_context_print:       total time =     851.26 ms /    70 tokens
0.01.602.505 I ggml_metal_free: deallocating

real	0m1.620s
user	0m0.109s
sys	0m0.187s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.096 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.008.891 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.016.023 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q5_k.gguf (version GGUF V3 (latest))
0.00.016.029 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.016.035 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.016.036 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.016.036 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.016.036 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.016.037 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.016.038 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.016.038 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.016.039 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.016.039 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.016.040 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.016.040 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.016.042 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.016.042 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.016.043 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.876 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.915 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.763 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.764 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.764 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.764 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.765 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.765 I llama_model_loader: - kv  22:                          general.file_type u32              = 17
0.00.024.765 I llama_model_loader: - type  f32:  194 tensors
0.00.024.766 I llama_model_loader: - type q5_K:   61 tensors
0.00.024.766 I llama_model_loader: - type q6_K:   37 tensors
0.00.024.767 I print_info: file format = GGUF V3 (latest)
0.00.024.767 I print_info: file type   = Q5_K - Medium
0.00.024.768 I print_info: file size   = 1006.35 MiB (5.97 BPW) 
0.00.032.617 I load: special tokens cache size = 25
0.00.038.691 I load: token to piece cache size = 0.2984 MB
0.00.038.693 I print_info: arch             = gptneox
0.00.038.694 I print_info: vocab_only       = 0
0.00.038.694 I print_info: n_ctx_train      = 2048
0.00.038.694 I print_info: n_embd           = 2048
0.00.038.694 I print_info: n_layer          = 24
0.00.038.697 I print_info: n_head           = 16
0.00.038.698 I print_info: n_head_kv        = 16
0.00.038.698 I print_info: n_rot            = 32
0.00.038.699 I print_info: n_swa            = 0
0.00.038.699 I print_info: n_embd_head_k    = 128
0.00.038.699 I print_info: n_embd_head_v    = 128
0.00.038.700 I print_info: n_gqa            = 1
0.00.038.700 I print_info: n_embd_k_gqa     = 2048
0.00.038.701 I print_info: n_embd_v_gqa     = 2048
0.00.038.702 I print_info: f_norm_eps       = 1.0e-05
0.00.038.704 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.705 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.705 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.705 I print_info: f_logit_scale    = 0.0e+00
0.00.038.707 I print_info: n_ff             = 8192
0.00.038.707 I print_info: n_expert         = 0
0.00.038.707 I print_info: n_expert_used    = 0
0.00.038.707 I print_info: causal attn      = 1
0.00.038.707 I print_info: pooling type     = 0
0.00.038.708 I print_info: rope type        = 2
0.00.038.708 I print_info: rope scaling     = linear
0.00.038.708 I print_info: freq_base_train  = 10000.0
0.00.038.708 I print_info: freq_scale_train = 1
0.00.038.709 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.709 I print_info: rope_finetuned   = unknown
0.00.038.709 I print_info: ssm_d_conv       = 0
0.00.038.709 I print_info: ssm_d_inner      = 0
0.00.038.709 I print_info: ssm_d_state      = 0
0.00.038.709 I print_info: ssm_dt_rank      = 0
0.00.038.710 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.710 I print_info: model type       = 1.4B
0.00.038.710 I print_info: model params     = 1.41 B
0.00.038.711 I print_info: general.name     = 1.4B
0.00.038.715 I print_info: vocab type       = BPE
0.00.038.715 I print_info: n_vocab          = 50304
0.00.038.715 I print_info: n_merges         = 50009
0.00.038.715 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.716 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.717 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.717 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.717 I print_info: LF token         = 187 'Ċ'
0.00.038.717 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.718 I print_info: max token length = 1024
0.00.581.852 I load_tensors: offloading 24 repeating layers to GPU
0.00.581.857 I load_tensors: offloading output layer to GPU
0.00.581.859 I load_tensors: offloaded 25/25 layers to GPU
0.00.581.884 I load_tensors: Metal_Mapped model buffer size =  1006.36 MiB
0.00.581.887 I load_tensors:   CPU_Mapped model buffer size =    67.55 MiB
0.00.583.327 I llama_init_from_model: n_seq_max     = 1
0.00.583.329 I llama_init_from_model: n_ctx         = 128
0.00.583.330 I llama_init_from_model: n_ctx_per_seq = 128
0.00.583.330 I llama_init_from_model: n_batch       = 128
0.00.583.331 I llama_init_from_model: n_ubatch      = 128
0.00.583.332 I llama_init_from_model: flash_attn    = 0
0.00.583.333 I llama_init_from_model: freq_base     = 10000.0
0.00.583.334 I llama_init_from_model: freq_scale    = 1
0.00.583.334 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.583.335 I ggml_metal_init: allocating
0.00.583.352 I ggml_metal_init: found device: Apple M4
0.00.583.361 I ggml_metal_init: picking default device: Apple M4
0.00.584.861 I ggml_metal_init: using embedded metal library
0.00.590.989 I ggml_metal_init: GPU name:   Apple M4
0.00.590.993 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.590.993 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.590.994 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.590.999 I ggml_metal_init: simdgroup reduction   = true
0.00.590.999 I ggml_metal_init: simdgroup matrix mul. = true
0.00.591.000 I ggml_metal_init: has residency sets    = true
0.00.591.000 I ggml_metal_init: has bfloat            = true
0.00.591.000 I ggml_metal_init: use bfloat            = true
0.00.591.001 I ggml_metal_init: hasUnifiedMemory      = true
0.00.591.003 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.607.472 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.611.031 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.611.035 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.611.075 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.614.286 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.614.288 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.614.288 I llama_init_from_model: graph nodes  = 967
0.00.614.289 I llama_init_from_model: graph splits = 2
0.00.614.292 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.614.292 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.645.735 I 
0.00.645.820 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.645.827 I perplexity: tokenizing the input ..
0.00.653.304 I perplexity: tokenization took 7.473 ms
0.00.653.311 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.795.349 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.2433,
0.00.796.964 I Final estimate: PPL = 10.2433 +/- 3.24778

0.00.796.987 I llama_perf_context_print:        load time =     636.83 ms
0.00.796.988 I llama_perf_context_print: prompt eval time =     141.12 ms /   128 tokens (    1.10 ms per token,   907.04 tokens per second)
0.00.796.988 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.796.989 I llama_perf_context_print:       total time =     151.26 ms /   129 tokens
0.00.797.373 I ggml_metal_free: deallocating

real	0m0.811s
user	0m0.079s
sys	0m0.129s
```
- q6_k:
```
+ ./bin/llama-cli -no-cnv --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -ngl 99 -c 0 -s 1234 -n 64 --ignore-eos -p 'I believe the meaning of life is'
0.00.000.058 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.000.095 I main: llama backend init
0.00.000.098 I main: load the model and apply lora adapter, if any
0.00.010.518 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.017.365 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.017.369 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.017.371 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.017.371 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.017.371 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.017.372 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.017.376 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.017.377 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.017.377 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.017.377 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.017.377 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.017.378 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.017.378 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.017.381 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.017.381 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.017.381 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.021.290 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.022.375 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.026.327 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.026.329 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.026.330 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.026.330 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.026.330 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.026.330 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.026.331 I llama_model_loader: - type  f32:  194 tensors
0.00.026.331 I llama_model_loader: - type q6_K:   98 tensors
0.00.026.332 I print_info: file format = GGUF V3 (latest)
0.00.026.332 I print_info: file type   = Q6_K
0.00.026.333 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.034.235 I load: special tokens cache size = 25
0.00.040.296 I load: token to piece cache size = 0.2984 MB
0.00.040.299 I print_info: arch             = gptneox
0.00.040.299 I print_info: vocab_only       = 0
0.00.040.299 I print_info: n_ctx_train      = 2048
0.00.040.300 I print_info: n_embd           = 2048
0.00.040.300 I print_info: n_layer          = 24
0.00.040.303 I print_info: n_head           = 16
0.00.040.304 I print_info: n_head_kv        = 16
0.00.040.304 I print_info: n_rot            = 32
0.00.040.305 I print_info: n_swa            = 0
0.00.040.305 I print_info: n_embd_head_k    = 128
0.00.040.305 I print_info: n_embd_head_v    = 128
0.00.040.306 I print_info: n_gqa            = 1
0.00.040.307 I print_info: n_embd_k_gqa     = 2048
0.00.040.308 I print_info: n_embd_v_gqa     = 2048
0.00.040.308 I print_info: f_norm_eps       = 1.0e-05
0.00.040.309 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.040.309 I print_info: f_clamp_kqv      = 0.0e+00
0.00.040.309 I print_info: f_max_alibi_bias = 0.0e+00
0.00.040.309 I print_info: f_logit_scale    = 0.0e+00
0.00.040.310 I print_info: n_ff             = 8192
0.00.040.310 I print_info: n_expert         = 0
0.00.040.310 I print_info: n_expert_used    = 0
0.00.040.311 I print_info: causal attn      = 1
0.00.040.311 I print_info: pooling type     = 0
0.00.040.311 I print_info: rope type        = 2
0.00.040.311 I print_info: rope scaling     = linear
0.00.040.313 I print_info: freq_base_train  = 10000.0
0.00.040.314 I print_info: freq_scale_train = 1
0.00.040.314 I print_info: n_ctx_orig_yarn  = 2048
0.00.040.315 I print_info: rope_finetuned   = unknown
0.00.040.315 I print_info: ssm_d_conv       = 0
0.00.040.315 I print_info: ssm_d_inner      = 0
0.00.040.315 I print_info: ssm_d_state      = 0
0.00.040.315 I print_info: ssm_dt_rank      = 0
0.00.040.315 I print_info: ssm_dt_b_c_rms   = 0
0.00.040.315 I print_info: model type       = 1.4B
0.00.040.316 I print_info: model params     = 1.41 B
0.00.040.316 I print_info: general.name     = 1.4B
0.00.040.316 I print_info: vocab type       = BPE
0.00.040.316 I print_info: n_vocab          = 50304
0.00.040.317 I print_info: n_merges         = 50009
0.00.040.318 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.040.318 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.040.318 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.040.318 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.040.318 I print_info: LF token         = 187 'Ċ'
0.00.040.318 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.040.319 I print_info: max token length = 1024
0.00.656.993 I load_tensors: offloading 24 repeating layers to GPU
0.00.657.000 I load_tensors: offloading output layer to GPU
0.00.657.001 I load_tensors: offloaded 25/25 layers to GPU
0.00.657.032 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.657.035 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.658.201 I llama_init_from_model: n_seq_max     = 1
0.00.658.205 I llama_init_from_model: n_ctx         = 2048
0.00.658.205 I llama_init_from_model: n_ctx_per_seq = 2048
0.00.658.206 I llama_init_from_model: n_batch       = 2048
0.00.658.206 I llama_init_from_model: n_ubatch      = 512
0.00.658.207 I llama_init_from_model: flash_attn    = 0
0.00.658.208 I llama_init_from_model: freq_base     = 10000.0
0.00.658.208 I llama_init_from_model: freq_scale    = 1
0.00.658.210 I ggml_metal_init: allocating
0.00.658.257 I ggml_metal_init: found device: Apple M4
0.00.658.272 I ggml_metal_init: picking default device: Apple M4
0.00.659.816 I ggml_metal_init: using embedded metal library
0.00.666.508 I ggml_metal_init: GPU name:   Apple M4
0.00.666.512 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.666.513 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.666.514 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.666.515 I ggml_metal_init: simdgroup reduction   = true
0.00.666.515 I ggml_metal_init: simdgroup matrix mul. = true
0.00.666.516 I ggml_metal_init: has residency sets    = true
0.00.666.516 I ggml_metal_init: has bfloat            = true
0.00.666.516 I ggml_metal_init: use bfloat            = true
0.00.666.517 I ggml_metal_init: hasUnifiedMemory      = true
0.00.666.519 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.683.942 I llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.743.815 I llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
0.00.743.821 I llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
0.00.743.857 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.748.318 I llama_init_from_model:      Metal compute buffer size =   102.25 MiB
0.00.748.320 I llama_init_from_model:        CPU compute buffer size =     8.01 MiB
0.00.748.321 I llama_init_from_model: graph nodes  = 967
0.00.748.321 I llama_init_from_model: graph splits = 2
0.00.748.329 I common_init_from_params: added <|endoftext|> logit bias = -inf
0.00.748.457 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.748.457 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.817.870 I main: llama threadpool init, n_threads = 4
0.00.817.912 I 
0.00.817.942 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.817.942 I 
0.00.818.127 I sampler seed: 1234
0.00.818.132 I sampler params: 
	repeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000
	dry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = 2048
	top_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800
	mirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000
0.00.818.143 I sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist 
0.00.818.143 I generate: n_ctx = 2048, n_batch = 2048, n_predict = 64, n_keep = 0
0.00.818.143 I 
I believe the meaning of life is to become as big as possible, but I also believe that life is short and we should enjoy it as we have it.

“I believe in life and death, and I think we should use every minute that we have to live our lives to the fullest.”

He added: “I’ve

0.01.693.810 I llama_perf_sampler_print:    sampling time =       1.35 ms /    71 runs   (    0.02 ms per token, 52670.62 tokens per second)
0.01.693.810 I llama_perf_context_print:        load time =     806.64 ms
0.01.693.812 I llama_perf_context_print: prompt eval time =      54.09 ms /     7 tokens (    7.73 ms per token,   129.43 tokens per second)
0.01.693.813 I llama_perf_context_print:        eval time =     818.60 ms /    63 runs   (   12.99 ms per token,    76.96 tokens per second)
0.01.693.813 I llama_perf_context_print:       total time =     876.65 ms /    70 tokens
0.01.694.040 I ggml_metal_free: deallocating

real	0m1.714s
user	0m0.109s
sys	0m0.215s
+ ./bin/llama-perplexity --model ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf -f ../models-mnt/wikitext/wikitext-2-raw/wiki.test-60.raw -ngl 99 -c 128 -b 128 --chunks 1
0.00.000.104 I build: 4651 (c0d48432) with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
0.00.009.084 I llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
0.00.015.893 I llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q6_k.gguf (version GGUF V3 (latest))
0.00.015.898 I llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
0.00.015.901 I llama_model_loader: - kv   0:                       general.architecture str              = gptneox
0.00.015.901 I llama_model_loader: - kv   1:                               general.type str              = model
0.00.015.901 I llama_model_loader: - kv   2:                               general.name str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
0.00.015.902 I llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
0.00.015.903 I llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
0.00.015.903 I llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
0.00.015.904 I llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
0.00.015.904 I llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
0.00.015.904 I llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
0.00.015.905 I llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
0.00.015.905 I llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
0.00.015.907 I llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
0.00.015.907 I llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
0.00.015.908 I llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
0.00.019.772 I llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
0.00.020.789 I llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
0.00.024.601 I llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
0.00.024.602 I llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
0.00.024.602 I llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
0.00.024.603 I llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
0.00.024.603 I llama_model_loader: - kv  21:               general.quantization_version u32              = 2
0.00.024.603 I llama_model_loader: - kv  22:                          general.file_type u32              = 18
0.00.024.604 I llama_model_loader: - type  f32:  194 tensors
0.00.024.604 I llama_model_loader: - type q6_K:   98 tensors
0.00.024.605 I print_info: file format = GGUF V3 (latest)
0.00.024.605 I print_info: file type   = Q6_K
0.00.024.606 I print_info: file size   = 1.08 GiB (6.57 BPW) 
0.00.032.487 I load: special tokens cache size = 25
0.00.038.551 I load: token to piece cache size = 0.2984 MB
0.00.038.554 I print_info: arch             = gptneox
0.00.038.554 I print_info: vocab_only       = 0
0.00.038.555 I print_info: n_ctx_train      = 2048
0.00.038.555 I print_info: n_embd           = 2048
0.00.038.555 I print_info: n_layer          = 24
0.00.038.558 I print_info: n_head           = 16
0.00.038.559 I print_info: n_head_kv        = 16
0.00.038.559 I print_info: n_rot            = 32
0.00.038.559 I print_info: n_swa            = 0
0.00.038.560 I print_info: n_embd_head_k    = 128
0.00.038.560 I print_info: n_embd_head_v    = 128
0.00.038.560 I print_info: n_gqa            = 1
0.00.038.561 I print_info: n_embd_k_gqa     = 2048
0.00.038.564 I print_info: n_embd_v_gqa     = 2048
0.00.038.565 I print_info: f_norm_eps       = 1.0e-05
0.00.038.565 I print_info: f_norm_rms_eps   = 0.0e+00
0.00.038.565 I print_info: f_clamp_kqv      = 0.0e+00
0.00.038.566 I print_info: f_max_alibi_bias = 0.0e+00
0.00.038.566 I print_info: f_logit_scale    = 0.0e+00
0.00.038.566 I print_info: n_ff             = 8192
0.00.038.566 I print_info: n_expert         = 0
0.00.038.568 I print_info: n_expert_used    = 0
0.00.038.568 I print_info: causal attn      = 1
0.00.038.568 I print_info: pooling type     = 0
0.00.038.568 I print_info: rope type        = 2
0.00.038.569 I print_info: rope scaling     = linear
0.00.038.569 I print_info: freq_base_train  = 10000.0
0.00.038.569 I print_info: freq_scale_train = 1
0.00.038.569 I print_info: n_ctx_orig_yarn  = 2048
0.00.038.570 I print_info: rope_finetuned   = unknown
0.00.038.570 I print_info: ssm_d_conv       = 0
0.00.038.570 I print_info: ssm_d_inner      = 0
0.00.038.570 I print_info: ssm_d_state      = 0
0.00.038.570 I print_info: ssm_dt_rank      = 0
0.00.038.570 I print_info: ssm_dt_b_c_rms   = 0
0.00.038.570 I print_info: model type       = 1.4B
0.00.038.571 I print_info: model params     = 1.41 B
0.00.038.571 I print_info: general.name     = 1.4B
0.00.038.571 I print_info: vocab type       = BPE
0.00.038.571 I print_info: n_vocab          = 50304
0.00.038.572 I print_info: n_merges         = 50009
0.00.038.576 I print_info: BOS token        = 0 '<|endoftext|>'
0.00.038.576 I print_info: EOS token        = 0 '<|endoftext|>'
0.00.038.576 I print_info: EOT token        = 0 '<|endoftext|>'
0.00.038.576 I print_info: UNK token        = 0 '<|endoftext|>'
0.00.038.577 I print_info: LF token         = 187 'Ċ'
0.00.038.577 I print_info: EOG token        = 0 '<|endoftext|>'
0.00.038.577 I print_info: max token length = 1024
0.00.592.144 I load_tensors: offloading 24 repeating layers to GPU
0.00.592.155 I load_tensors: offloading output layer to GPU
0.00.592.155 I load_tensors: offloaded 25/25 layers to GPU
0.00.592.187 I load_tensors: Metal_Mapped model buffer size =  1108.66 MiB
0.00.592.189 I load_tensors:   CPU_Mapped model buffer size =    80.60 MiB
0.00.593.627 I llama_init_from_model: n_seq_max     = 1
0.00.593.637 I llama_init_from_model: n_ctx         = 128
0.00.593.638 I llama_init_from_model: n_ctx_per_seq = 128
0.00.593.638 I llama_init_from_model: n_batch       = 128
0.00.593.638 I llama_init_from_model: n_ubatch      = 128
0.00.593.639 I llama_init_from_model: flash_attn    = 0
0.00.593.641 I llama_init_from_model: freq_base     = 10000.0
0.00.593.641 I llama_init_from_model: freq_scale    = 1
0.00.593.642 W llama_init_from_model: n_ctx_per_seq (128) < n_ctx_train (2048) -- the full capacity of the model will not be utilized
0.00.593.644 I ggml_metal_init: allocating
0.00.593.704 I ggml_metal_init: found device: Apple M4
0.00.593.717 I ggml_metal_init: picking default device: Apple M4
0.00.595.390 I ggml_metal_init: using embedded metal library
0.00.602.044 I ggml_metal_init: GPU name:   Apple M4
0.00.602.047 I ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
0.00.602.048 I ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
0.00.602.049 I ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
0.00.602.050 I ggml_metal_init: simdgroup reduction   = true
0.00.602.050 I ggml_metal_init: simdgroup matrix mul. = true
0.00.602.050 I ggml_metal_init: has residency sets    = true
0.00.602.051 I ggml_metal_init: has bfloat            = true
0.00.602.051 I ggml_metal_init: use bfloat            = true
0.00.602.052 I ggml_metal_init: hasUnifiedMemory      = true
0.00.602.053 I ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
0.00.619.730 I llama_kv_cache_init: kv_size = 128, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
0.00.623.453 I llama_kv_cache_init:      Metal KV buffer size =    24.00 MiB
0.00.623.457 I llama_init_from_model: KV self size  =   24.00 MiB, K (f16):   12.00 MiB, V (f16):   12.00 MiB
0.00.623.503 I llama_init_from_model:        CPU  output buffer size =     0.19 MiB
0.00.626.817 I llama_init_from_model:      Metal compute buffer size =    25.56 MiB
0.00.626.819 I llama_init_from_model:        CPU compute buffer size =     1.06 MiB
0.00.626.819 I llama_init_from_model: graph nodes  = 967
0.00.626.820 I llama_init_from_model: graph splits = 2
0.00.626.822 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 128
0.00.626.823 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
0.00.664.384 I 
0.00.664.466 I system_info: n_threads = 4 (n_threads_batch = 4) / 10 | Metal : EMBED_LIBRARY = 1 | BF16 = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | 
0.00.664.472 I perplexity: tokenizing the input ..
0.00.670.364 I perplexity: tokenization took 5.888 ms
0.00.670.371 I perplexity: calculating perplexity over 1 chunks, n_ctx=128, batch_size=128, n_seq=1
0.00.809.755 I perplexity: 0.14 seconds per pass - ETA 0.00 minutes
[1]10.3179,
0.00.811.439 I Final estimate: PPL = 10.3179 +/- 3.28637

0.00.811.468 I llama_perf_context_print:        load time =     655.29 ms
0.00.811.469 I llama_perf_context_print: prompt eval time =     139.02 ms /   128 tokens (    1.09 ms per token,   920.75 tokens per second)
0.00.811.470 I llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)
0.00.811.470 I llama_perf_context_print:       total time =     147.09 ms /   129 tokens
0.00.811.825 I ggml_metal_free: deallocating

real	0m0.827s
user	0m0.079s
sys	0m0.147s
```
- save-load-state: 
```
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0
main: build = 4651 (c0d48432)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120b058d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120b05f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120b063b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120b09030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120b094a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120b09910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120b09ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120b0a470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120b0aa20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120b0af20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120b0b420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120b0b920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120b0c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120b0cbf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120b0d400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120b0db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120b0e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120b0e960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120b0f080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120b0f850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120b0ff70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120b10690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120b10db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120b11650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120b11d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120b12030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120b12640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120b132b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120b137f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120b13ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120b13f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120b14210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120b14aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120b14fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120b152a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120b15740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120b15be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120b16080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120b16520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120b169c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120b16e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120b17300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120b177a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120b17c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120b17f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120b18510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120b18b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120b19440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120b19a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120b1a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120b1a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120b1ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120b1b290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120b1b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120b1c090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120b1c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120b1c9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120b1cc90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120b1d2a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120b1da90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120b1dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120b1e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120b1e690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120b1eb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120b1efd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120b1f470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120b1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120b1fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120b20250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120b206f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120b20b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120b21030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120b214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120b21a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120b21f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120b224c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120b22a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120b22f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120b234b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120b23a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120b23f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120b244a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120b249f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120b24f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120b25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120b259e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120b25f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120b26480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120b269d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120b26f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120b27470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120b279c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120b27f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120b28460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120b289b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120b28f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120b29450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120b19130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120b298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120b2a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120b2a5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120b2ab10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120b2b060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120b2b5b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120b2bb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120b2c050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120b2c5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120b2caf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120b2d040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120b2d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120b2dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120b2e030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120b2e580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120b2ea20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120b2eec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120b2f360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120b2f800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120b2fca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120b30140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120b305e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120b30a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120b30f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120b313c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120b31860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120b31d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120b321a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120b32640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120b32ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120b32f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120b33420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120b338c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120b33d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120b34200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120b346a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120b34b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120b34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120b35480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120b35920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120b35dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120b36260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120b36700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120b36ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120b37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120b374e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120b37980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120b37e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120b382c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120b38760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120b38c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120b390a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120b39540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120b399e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120b39e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120b3a320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120b3a7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120b3ac60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120b3b100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120b3b5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120b3ba40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120b3bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120b3c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120b3c820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120b3ccc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120b3d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120b3d600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120b3daa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120b3df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120b3e3e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120b3e880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120b3ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120b3f1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120b3f660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120b3fb00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120b3ffa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120b40440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120b408e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120b40d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120b41220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120b416c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120b41b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120b42000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120b424a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120b42940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120b42de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120b43280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120b43720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120b43bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120b44060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120b44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120b449a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120b44e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120b452e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120b45780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120b45cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120b46220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120b46770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120b46cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120b46f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120b47590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120b47ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120b481b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120b489a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120b48e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120b49100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120b49710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120b49d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120b4a510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120b4a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120b4ae50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120b4b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120b4baa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120b4bff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120b4c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120b4ca90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120b4cfe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120b4d530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120b4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120b4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120b4e520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120b4ea70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120b4efc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120b4f510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120b4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120b4ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120b50500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120b50a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120b50fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120b514f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120b51a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120b51f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120b524e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120b52a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120b52f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120b534d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120b53a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120b53f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120b544c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120b54a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120b54f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120b554b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120b55a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120b55f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120b564a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120b569f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120b56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120b57490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120b579e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120b57f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120b58480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120b589d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120b58f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120b59470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120b599c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120b59f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120b5a460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120b5a9b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120b5af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120b5b450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120b5b9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120b5bef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120b5c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120b5c990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120b5cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120b5d430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120b5d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120b5ded0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120b5e420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120b5e8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120b5ed60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120b5f200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120b5f6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120b5fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120b5ffe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120b60480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120b60920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120b60dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120b61260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120b61700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120a04080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120a044f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120a04960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120a04dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120a05240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120a05d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120a06470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120a06b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120a072b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120a07570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120a07830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120a07ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120a08110 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
0.00.719.886 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.719.890 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x108f04b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x108f04f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x108f05400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x108f05870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x108f05ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x108f06150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x108f065c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x108f06a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x108f06ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x108f07310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x108f07780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x108f07e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x108f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x108f09140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x108f09950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x108f0a070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x108f0a790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x108f0aeb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x108f0b5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x108f0bd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x108f0c420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x108f0cb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x108f0d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x108f0d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x108f0e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x108f0e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x108f0e620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x108f0ea90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x108f0ef00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x108f0f370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x108f0f7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x108f0fd10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x108f10180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x108f10440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x108f108b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x108f10d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x108f11190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x108f11600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x108f11a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x108f11ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x108f12350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x108f127c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x108f12c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x108f130a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x108f13510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x108f13980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x108f13df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x108f14260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x108f146d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x108f14b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x108f14fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x108f15420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x108f15890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x108f15d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x108f16170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x108f165e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x108f16b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x108f17050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x108f174c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x108f17930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x108f17da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x108f18210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x108f18680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x108f18af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x108f18f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x108f193d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x108f19840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x108f19cb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x108f1a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x108f1a590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x108f1aa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x108f1ae70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x108f1b2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x108f1b750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x108f1bbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x108f1c030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x108f1c4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x108f1c910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x108f1cd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x108f1d1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x108f1d660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x108f1dad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x108f1df40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x108f1e3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x108f1e820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x108f1ec90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x108f1f100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x108f1f570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x108f1f9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x108f1fe50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x108f202c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x108f20730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x108f20ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x108f21010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x108f21480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x108f218f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x108f21d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x108f221d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x108f22640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x108f22ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x108f22f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x108f23390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x108f23800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x108f23c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x108f240e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x108f24550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x108f249c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x108f24e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x108f252a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x108f25710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x108f25b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x108f25ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x108f26460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x108f268d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x108f26d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x108f271b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x108f27620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x108f27a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x108f27f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x108f28370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x108f287e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x108f28c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x108f290c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x108f29530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x108f299a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x108f29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x108f2a280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x108f2a6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x108f2ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x108f2afd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x108f2b440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x108f2b8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x108f2bd20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x108f2c190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x108f2c600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x108f2ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x108f2cee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x108f2d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x108f2d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x108f2dc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x108f2e0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x108f2e510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x108f2e980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x108f2edf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x108f2f260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x108f2f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x108f2fb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x108f2ffb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x108f30420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x108f30890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x108f30d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x108f31170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x108f315e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x108f31a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x108f31ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x108f32330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x108f327a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x108f32c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x108f33080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x108f334f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x108f33960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x108f33dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x108f34240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x108f346b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x108f34b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x108f34f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x108f35bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x108f35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x108f36140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x108f365b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x108f36a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x108f36e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x108f37300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x108f37770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x108f37be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x108f38050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x108f384c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x108f38930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x108f38da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x108f39210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x108f39680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x108f39af0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x108f39f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x108f3a3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x108f3a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x108f3acb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x108f3b120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x108f3b590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x108f3ba00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x108f3be70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x108f3c2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x108f3c750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x108f3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x108f3d030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x108f3d4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x108f3d910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x108f3dd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x108f3e1f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x108f3e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x108f3ead0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x108f3ef40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x108f3f3b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x108f3f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x108f3fe20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x108f40290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x108f40700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x108f40b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x108f40fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x108f41500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x108f41a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x108f42580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x108f42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x108f42e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x108f433c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x108f43980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x108f43f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x108f44500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x108f44ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x108f45080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x108f45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x108f45c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x108f461c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x108f46780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x108f46d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x108f47300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x108f478c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x108f47e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x108f48440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x108f48a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x108f48fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x108f49580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x108f49b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x108f4a100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x108f4a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x108f4ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x108f4b240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x108f4b800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x108f4bdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x108f4c380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x108f4c940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x108f4cf00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x108f4d4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x108f4da80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x108f4e040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x108f4e600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x108f4ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x108f4f180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x108f4f740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x108f4fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x108f502c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x108f50880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x108f50e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x108f51400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x108f519c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x108f51f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x108f52540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x108f52b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x108f530c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x108f53680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x108f53c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x108f54200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x108f547c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x108f54d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x108f55340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x108f55900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x108f55ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x108f56480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x108f56a40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x108f56f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x108f57440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x108f57940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x108f57e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x108f58340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x108f58840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x108f58d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x108f59240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x108f59740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x108f59c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x108f5a140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x108f5a640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x108f5ab40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x108f5b040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x108f5b540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x108f5bf50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x108f5c670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x108f5cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x108f5d4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x108f5d770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x108f5df60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x108f5e220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x108f5e830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 0
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x120a08680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x120a08940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x120a08c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x120a09160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x120a09420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x120a096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x120a099a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x120a09c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x120a09f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x120a0a1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x120a0a4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x120a0a960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x120a0b480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x120a0bc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x120a0c440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x120a0cb60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x120a0d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x120a0d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x120a0e0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x120a0e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x120a0efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x120a0f6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x120a0fdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x120a10510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x120a10c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x120a10ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x120a111b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x120a11620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x120a11a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x120a11f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x120a12370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x120a128a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x120a12d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x120a12fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x120a13440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x120a138b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x120a13d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x120a14190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x120a14600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x120a14a70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x120a14ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x120a15350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x120a157c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x120a15c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x120a160a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x120a16510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x120a16980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x120a16df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x120a17260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x120a176d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x120a17b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x120a17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x120a18420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x120a18890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x120a18d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x120a19170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x120a196e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x120a19be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x120a1a050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x120a1a4c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x120a1a930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x120a1ada0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x120a1b210 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x120a1b680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x120a1baf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x120a1bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x120a1c3d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x120a1c840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x120a1ccb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x120a1d120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x120a1d590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x120a1da00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x120a1de70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x120a1e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x120a1e750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x120a1ebc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x120a1f030 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x120a1f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x120a1f910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x120a1fd80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x120a201f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x120a20660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x120a20ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x120a20f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x120a213b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x120a21820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x120a21c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x120a22100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x120a22570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x120a229e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x120a22e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x120a232c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x120a23730 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x120a23ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x120a24010 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x120a24480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x120a248f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x120a24d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x120a251d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x120a25640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x120a25ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x120a25f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x120a26390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x120a26c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x120a26ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x120a27350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x120a277c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x120a27c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x120a280a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x120a28510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x120a28980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x120a28df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x120a29260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x120a296d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x120a29b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x120a29fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x120a2a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x120a2a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x120a2ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x120a2b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x120a2b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x120a2ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x120a2bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x120a2c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x120a2c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x120a2cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x120a2d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x120a2d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x120a2d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x120a2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x120a2e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x120a2e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x120a2eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x120a2ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x120a2f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x120a2f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x120a2fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x120a30150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x120a305c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x120a30a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x120a30ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x120a31310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x120a31780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x120a31bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x120a32060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x120a324d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x120a32940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x120a32db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x120a33220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x120a33690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x120a33b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x120a33f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x120a343e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x120a34850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x120a34cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x120a35130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x120a355a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x120a35a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x120a35e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x120a362f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x120a36760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x120a36bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x120a37040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x120a374b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x120a37920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x120a37d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x120a38200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x120a38670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x120a38ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x120a38f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x120a393c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x120a39830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x120a39ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x120a3a110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x120a3a580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x120a3a9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x120a3ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x120a3b2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x120a3b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x120a3bbb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x120a3c020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x120a3c490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x120a3c900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x120a3cd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x120a3d1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x120a3d650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x120a3dac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x120a3df30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x120a3e3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x120a3e810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x120a3ec80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x120a3f0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x120a3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x120a3f9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x120a3fe40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x120a402b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x120a40720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x120a40b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x120a41000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x120a41470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x120a418e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x120a41d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x120a421c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x120a42630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x120a42aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x120a42f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x120a43380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x120a437f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x120a43c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x120a440d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x120a44c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x120a44f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x120a451d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x120a45640 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x120a45ab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x120a45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x120a46390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x120a46800 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x120a46c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x120a470e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x120a47550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x120a479c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x120a47e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x120a482a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x120a48710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x120a48b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x120a48ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x120a49460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x120a498d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x120a49d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x120a4a1b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x120a4a620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x120a4aa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x120a4af00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x120a4b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x120a4b7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x120a4bc50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x120a4c0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x120a4c530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x120a4c9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x120a4ce10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x120a4d280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x120a4d6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x120a4db60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x120a4dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x120a4e440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x120a4e8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x120a4ed20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x120a4f190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x120a4f600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x120a4fa70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x120a4fee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x120a50350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x120a507c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x120a50c30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x120a510a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x120a51510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x120a51980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x120a51df0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x120a52260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x120a526d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x120a52b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x120a52fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x120a53420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x120a53890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x120a53d00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x120a54170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x120a545e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x120a54a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x120a54ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x120a55330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x120a557a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x120a55c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x120a56080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x120a564f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x120a56960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x120a56dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x120a57240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x120a576b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x120a57b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x120a57f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x120a58400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x120a58870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x120a592e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x120a59a00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x120a5a120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x120a5a840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x120a5ab00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x120a5af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x120a5b570 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x120a5bb80 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 967
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have said something about the lazy

real	0m1.784s
user	0m0.283s
sys	0m0.299s
+ ./bin/llama-save-load-state --model ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf -ngl 99 -c 0 -fa
main: build = 4651 (c0d48432)
main: built with Apple clang version 16.0.0 (clang-1600.0.26.4) for arm64-apple-darwin24.1.0
llama_model_load_from_file_impl: using device Metal (Apple M4) - 10922 MiB free
llama_model_loader: loaded meta data with 23 key-value pairs and 292 tensors from ../models-mnt/pythia/1.4B/ggml-model-q4_0.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = gptneox
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = 1.4B
llama_model_loader: - kv   3:                           general.finetune str              = 1.4B
llama_model_loader: - kv   4:                         general.size_label str              = 1.4B
llama_model_loader: - kv   5:                     gptneox.context_length u32              = 2048
llama_model_loader: - kv   6:                   gptneox.embedding_length u32              = 2048
llama_model_loader: - kv   7:                        gptneox.block_count u32              = 24
llama_model_loader: - kv   8:                gptneox.feed_forward_length u32              = 8192
llama_model_loader: - kv   9:               gptneox.rope.dimension_count u32              = 32
llama_model_loader: - kv  10:               gptneox.attention.head_count u32              = 16
llama_model_loader: - kv  11:              gptneox.use_parallel_residual bool             = true
llama_model_loader: - kv  12:       gptneox.attention.layer_norm_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = olmo
llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,50304]   = ["<|endoftext|>", "<|padding|>", "!",...
llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,50304]   = [3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,50009]   = ["Ġ Ġ", "Ġ t", "Ġ a", "h e", "i n...
llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  21:               general.quantization_version u32              = 2
llama_model_loader: - kv  22:                          general.file_type u32              = 2
llama_model_loader: - type  f32:  194 tensors
llama_model_loader: - type q4_0:   97 tensors
llama_model_loader: - type q6_K:    1 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = Q4_0
print_info: file size   = 786.31 MiB (4.66 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token:      1 '<|padding|>' is not marked as EOG
load: special tokens cache size = 25
load: token to piece cache size = 0.2984 MB
print_info: arch             = gptneox
print_info: vocab_only       = 0
print_info: n_ctx_train      = 2048
print_info: n_embd           = 2048
print_info: n_layer          = 24
print_info: n_head           = 16
print_info: n_head_kv        = 16
print_info: n_rot            = 32
print_info: n_swa            = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 1
print_info: n_embd_k_gqa     = 2048
print_info: n_embd_v_gqa     = 2048
print_info: f_norm_eps       = 1.0e-05
print_info: f_norm_rms_eps   = 0.0e+00
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: n_ff             = 8192
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: causal attn      = 1
print_info: pooling type     = 0
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 10000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 2048
print_info: rope_finetuned   = unknown
print_info: ssm_d_conv       = 0
print_info: ssm_d_inner      = 0
print_info: ssm_d_state      = 0
print_info: ssm_dt_rank      = 0
print_info: ssm_dt_b_c_rms   = 0
print_info: model type       = 1.4B
print_info: model params     = 1.41 B
print_info: general.name     = 1.4B
print_info: vocab type       = BPE
print_info: n_vocab          = 50304
print_info: n_merges         = 50009
print_info: BOS token        = 0 '<|endoftext|>'
print_info: EOS token        = 0 '<|endoftext|>'
print_info: EOT token        = 0 '<|endoftext|>'
print_info: UNK token        = 0 '<|endoftext|>'
print_info: LF token         = 187 'Ċ'
print_info: EOG token        = 0 '<|endoftext|>'
print_info: max token length = 1024
load_tensors: layer   0 assigned to device Metal
load_tensors: layer   1 assigned to device Metal
load_tensors: layer   2 assigned to device Metal
load_tensors: layer   3 assigned to device Metal
load_tensors: layer   4 assigned to device Metal
load_tensors: layer   5 assigned to device Metal
load_tensors: layer   6 assigned to device Metal
load_tensors: layer   7 assigned to device Metal
load_tensors: layer   8 assigned to device Metal
load_tensors: layer   9 assigned to device Metal
load_tensors: layer  10 assigned to device Metal
load_tensors: layer  11 assigned to device Metal
load_tensors: layer  12 assigned to device Metal
load_tensors: layer  13 assigned to device Metal
load_tensors: layer  14 assigned to device Metal
load_tensors: layer  15 assigned to device Metal
load_tensors: layer  16 assigned to device Metal
load_tensors: layer  17 assigned to device Metal
load_tensors: layer  18 assigned to device Metal
load_tensors: layer  19 assigned to device Metal
load_tensors: layer  20 assigned to device Metal
load_tensors: layer  21 assigned to device Metal
load_tensors: layer  22 assigned to device Metal
load_tensors: layer  23 assigned to device Metal
load_tensors: layer  24 assigned to device Metal
load_tensors: tensor 'token_embd.weight' (q4_0) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead
ggml_backend_metal_log_allocated_size: allocated buffer, size =   786.34 MiB, (  786.42 / 10922.67)
load_tensors: offloading 24 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 25/25 layers to GPU
load_tensors: Metal_Mapped model buffer size =   786.33 MiB
load_tensors:   CPU_Mapped model buffer size =    55.27 MiB
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134e0e4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134e0ebb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134e0f160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134e0f710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134e0fcc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134e10270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134e10820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134e10dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134e11380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134e11880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134e11d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134e12280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134e12da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134e13550 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134e13d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134e14480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134e14ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134e152c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134e159e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134e161b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134e168d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134e16ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134e17710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134e17fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134e186d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134e18990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134e18fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134e19c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134e1a150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134e1a410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134e1a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134e1ab70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134e1b400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134e1b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134e1bc00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134e1c0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134e1c540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134e1c9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134e1ce80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134e1d320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134e1d7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134e1dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134e1e100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134e1e5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134e1e860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134e1ee70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134e1f480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134e1fda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134e203b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134e209c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134e20fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134e215e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134e21bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134e22200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134e229f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134e22e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134e23330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134e235f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134e23c00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134e243f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134e246b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134e24b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134e24ff0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134e25490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134e25930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134e25dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134e26270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134e26710 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134e26bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134e27050 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134e274f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134e27990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134e27e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134e28380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134e288d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134e28e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134e29370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134e298c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134e29e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134e2a360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134e2a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134e2ae00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134e2b350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134e2b8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134e2bdf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134e2c340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134e2c890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134e2cde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134e2d330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134e2d880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134e2ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134e2e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134e2e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134e2edc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134e2f310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134e2f860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134e2fdb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134e1fa90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134e30220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134e309d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134e30f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134e31470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134e319c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134e31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134e32460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134e329b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134e32f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134e33450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134e339a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134e33ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134e34440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134e34990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134e34ee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134e35380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134e35820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134e35cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134e36160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134e36600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134e36aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134e36f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134e373e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134e37880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134e37d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134e381c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134e38660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134e38b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134e38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134e39440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134e398e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134e39d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134e3a220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134e3a6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134e3ab60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134e3b000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134e3b4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134e3b940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134e3bde0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134e3c280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134e3c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134e3cbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134e3d060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134e3d500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134e3d9a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134e3de40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134e3e2e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134e3e780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134e3ec20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134e3f0c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134e3f560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134e3fa00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134e3fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134e40340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134e407e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134e40c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134e41120 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134e415c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134e41a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134e41f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134e423a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134e42840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134e42ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134e43180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134e43620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134e43ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134e43f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134e44400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134e448a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134e44d40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134e451e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134e45680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134e45b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134e45fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134e46460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134e46900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134e46da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134e47240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134e476e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134e47b80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134e48020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134e484c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134e48960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134e48e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134e492a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134e49740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134e49be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134e4a080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134e4a520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134e4a9c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134e4ae60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134e4b300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134e4b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134e4bc40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134e4c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134e4c630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134e4cb80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134e4d0d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134e4d620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134e4d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134e4def0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134e4e500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134e4eb10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134e4f300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134e4f7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134e4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134e50070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134e50680 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134e50e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134e51310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134e517b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134e51c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134e52400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134e52950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134e52ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134e533f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134e53940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134e53e90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134e543e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134e54930 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134e54e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134e553d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134e55920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134e55e70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134e563c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134e56910 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134e56e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134e573b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134e57900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134e57e50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134e583a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134e588f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134e58e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134e59390 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134e598e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134e59e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134e5a380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134e5a8d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134e5ae20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134e5b370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134e5b8c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134e5be10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134e5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134e5c8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134e5ce00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134e5d350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134e5d8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134e5ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134e5e340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134e5e890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134e5ede0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134e5f330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134e5f880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134e5fdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134e60320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134e60870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134e60dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134e61310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134e61860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134e61db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134e62300 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134e62850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134e62da0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134e632f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134e63840 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134e63d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134e642e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134e64830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134e64d80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134e65220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134e656c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134e65b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134e66000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134e664a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134e66940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134e66de0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134e67280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134e67720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134e67bc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134e68060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134e68500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134e689a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134e68e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134e692e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134e69830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134e69f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134e6a670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134e6ad90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134e6b4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134e6b770 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134e6bf60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134e6c220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134e6c830 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
0.00.100.688 I common_init_from_params: setting dry_penalty_last_n to ctx_size = 2048
0.00.100.692 W common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)
main : serialized state into 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x134f07c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x134f080b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x134f08520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x134f08990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x134f08e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x134f09270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x134f096e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x134f09b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x134f09fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x134f0a430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x134f0a8a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x134f0af90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x134f0bab0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x134f0c260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x134f0ca70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x134f0d190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x134f0d8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x134f0dfd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x134f0e6f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x134f0ee20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x134f0f540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x134f0fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x134f10380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x134f10aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x134f111c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x134f11480 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x134f11740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x134f11bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x134f12020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x134f12490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x134f12900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x134f12e30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x134f132a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x134f13560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x134f139d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x134f13e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x134f142b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x134f14720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x134f14b90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x134f15000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x134f15470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x134f158e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x134f15d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x134f161c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x134f16630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x134f16aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x134f16f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x134f17380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x134f177f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x134f17c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x134f180d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x134f18540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x134f189b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x134f18e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x134f19290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x134f19700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x134f19c70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x134f1a170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x134f1a5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x134f1aa50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x134f1aec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x134f1b330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x134f1b7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x134f1bc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x134f1c080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x134f1c4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x134f1c960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x134f1cdd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x134f1d240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x134f1d6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x134f1db20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x134f1df90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x134f1e400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x134f1e870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x134f1ece0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x134f1f150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x134f1f5c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x134f1fa30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x134f1fea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x134f20310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x134f20780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x134f20bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x134f21060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x134f214d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x134f21940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x134f21db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x134f22220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x134f22690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x134f22b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x134f22f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x134f233e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x134f23850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x134f23cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x134f24130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x134f245a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x134f24a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x134f24e80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x134f252f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x134f25760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x134f25bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x134f26040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x134f264b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x134f26920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x134f26d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x134f27200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x134f27670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x134f27ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x134f27f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x134f283c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x134f28830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x134f28ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x134f29110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x134f29580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x134f299f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x134f29e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x134f2a2d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x134f2a740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x134f2abb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x134f2b020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x134f2b490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x134f2b900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x134f2bd70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x134f2c1e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x134f2c650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x134f2cac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x134f2cf30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x134f2d3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x134f2d810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x134f2dc80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x134f2e0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x134f2e560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x134f2e9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x134f2ee40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x134f2f2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x134f2f720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x134f2fb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x134f30000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x134f30470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x134f308e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x134f30d50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x134f311c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x134f31630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x134f31aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x134f31f10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x134f32380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x134f327f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x134f32c60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x134f330d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x134f33540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x134f339b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x134f33e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x134f34290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x134f34700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x134f34b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x134f34fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x134f35450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x134f358c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x134f35d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x134f361a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x134f36610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x134f36a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x134f36ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x134f37360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x134f377d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x134f37c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x134f380b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x134f38ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x134f38fa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x134f39260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x134f396d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x134f39b40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x134f39fb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x134f3a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x134f3a890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x134f3ad00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x134f3b170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x134f3b5e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x134f3ba50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x134f3bec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x134f3c330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x134f3c7a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x134f3cc10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x134f3d080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x134f3d4f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x134f3d960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x134f3ddd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x134f3e240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x134f3e6b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x134f3eb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x134f3ef90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x134f3f400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x134f3f870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x134f3fce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x134f40150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x134f405c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x134f40a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x134f40ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x134f41310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x134f41780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x134f41bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x134f42060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x134f424d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x134f42a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x134f42f40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x134f433b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x134f43820 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x134f43c90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x134f44100 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x134f44620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x134f44b30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x134f456a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x134f45960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x134f45f20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x134f464e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x134f46aa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x134f47060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x134f47620 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x134f47be0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x134f481a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x134f48760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x134f48d20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x134f492e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x134f498a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x134f49e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x134f4a420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x134f4a9e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x134f4afa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x134f4b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x134f4bb20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x134f4c0e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x134f4c6a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x134f4cc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x134f4d220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x134f4d7e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x134f4dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x134f4e360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x134f4e920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x134f4eee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x134f4f4a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x134f4fa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x134f50020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x134f505e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x134f50ba0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x134f51160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x134f51720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x134f51ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x134f522a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x134f52860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x134f52e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x134f533e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x134f539a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x134f53f60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x134f54520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x134f54ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x134f550a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x134f55660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x134f55c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x134f561e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x134f567a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x134f56d60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x134f57320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x134f578e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x134f57ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x134f58460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x134f58a20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x134f58fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x134f595a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x134f59b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x134f5a060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x134f5a560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x134f5aa60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x134f5af60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x134f5b460 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x134f5b960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x134f5be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x134f5c360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x134f5c860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x134f5cd60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x134f5d260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x134f5d760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x134f5dc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x134f5e160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x134f5e660 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x134f5f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x134f5f790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x134f5feb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x134f605d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x134f60890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x134f61080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x134f61340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x134f61950 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
llama_init_from_model: n_seq_max     = 1
llama_init_from_model: n_ctx         = 2048
llama_init_from_model: n_ctx_per_seq = 2048
llama_init_from_model: n_batch       = 2048
llama_init_from_model: n_ubatch      = 512
llama_init_from_model: flash_attn    = 1
llama_init_from_model: freq_base     = 10000.0
llama_init_from_model: freq_scale    = 1
ggml_metal_init: allocating
ggml_metal_init: found device: Apple M4
ggml_metal_init: picking default device: Apple M4
ggml_metal_init: using embedded metal library
ggml_metal_init: GPU name:   Apple M4
ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)
ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)
ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)
ggml_metal_init: simdgroup reduction   = true
ggml_metal_init: simdgroup matrix mul. = true
ggml_metal_init: has residency sets    = true
ggml_metal_init: has bfloat            = true
ggml_metal_init: use bfloat            = true
ggml_metal_init: hasUnifiedMemory      = true
ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB
ggml_metal_init: loaded kernel_add                                    0x1360044e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_add_row                                0x136004950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub                                    0x136004dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sub_row                                0x136005230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul                                    0x1360056a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_row                                0x136005b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div                                    0x136005f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_div_row                                0x1360063f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f32                             0x136006860 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_f16                             0x136006cd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i32                             0x136007140 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_repeat_i16                             0x136007810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale                                  0x136008330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_scale_4                                0x136008ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_clamp                                  0x1360092f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_tanh                                   0x136009a10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_relu                                   0x13600a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sigmoid                                0x13600a850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu                                   0x13600af70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_4                                 0x13600b740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick                             0x13600be60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_gelu_quick_4                           0x13600c580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu                                   0x13600cca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_silu_4                                 0x13600d3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_elu                                    0x13600dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16                           0x13600dda0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f16_4                         0x13600e060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32                           0x13600e4d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_soft_max_f32_4                         0x13600e940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf                          0x13600edb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x13600f220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f32                           0x13600f750 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_f16                           0x13600fbc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_bf16                          0x13600fe80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1360102f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_1                          0x136010760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_0                          0x136010bd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_1                          0x136011040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1360114b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q2_K                          0x136011920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q3_K                          0x136011d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q4_K                          0x136012200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q5_K                          0x136012670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_q6_K                          0x136012ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x136012f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1360133c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x136013830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x136013ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x136014110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x136014580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x1360149f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x136014e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x1360152d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_get_rows_i32                           0x136015740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rms_norm                               0x136015bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_group_norm                             0x136016020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_norm                                   0x136016590 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_conv_f32                           0x136016a90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_ssm_scan_f32                           0x136016f00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x136017370 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32                        0x1360177e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_1row                   0x136017c50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_f32_l4                     0x1360180c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_bf16_bf16                       0x136018530 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1360189a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x136018e10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x136019280 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1360196f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x136019b60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x136019fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x13601a440 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x13601a8b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x13601ad20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x13601b190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x13601b600 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x13601ba70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x13601bee0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x13601c350 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x13601c7c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x13601cc30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x13601d0a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x13601d510 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x13601d980 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x13601ddf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x13601e260 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x13601e6d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x13601eb40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x13601efb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x13601f420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x13601f890 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x13601fd00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x136020170 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1360205e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x136020a50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x136020ec0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x136021330 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1360217a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x136021c10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x136022080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1360224f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x136022960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x136022dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x136023240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x136023ad0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x136023d90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x136024200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x136024670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x136024ae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x136024f50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x1360253c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x136025830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x136025ca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x136026110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x136026580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1360269f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x136026e60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1360272d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x136027740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x136027bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x136028020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x136028490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x136028900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x136028d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1360291e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x136029650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x136029ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x136029f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x13602a3a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x13602a810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_bf16_f32                     0x13602ac80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x13602b0f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x13602b560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x13602b9d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x13602be40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x13602c2b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x13602c720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x13602cb90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x13602d000 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x13602d470 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x13602d8e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x13602dd50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x13602e1c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x13602e630 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x13602eaa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x13602ef10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x13602f380 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x13602f7f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x13602fc60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1360300d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x136030540 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1360309b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_bf16_f32                        0x136030e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x136031290 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x136031700 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x136031b70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x136031fe0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x136032450 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x1360328c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x136032d30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1360331a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x136033610 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x136033a80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x136033ef0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x136034360 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1360347d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x136034c40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1360350b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x136035520 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x136035990 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x136035e00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x136036270 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x1360366e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x136036b50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_bf16_f32                     0x136036fc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x136037430 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x1360378a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x136037d10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x136038180 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x1360385f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x136038a60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x136038ed0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x136039340 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x1360397b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x136039c20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x13603a090 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x13603a500 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x13603a970 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x13603ade0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x13603b250 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x13603b6c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x13603bb30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x13603bfa0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x13603c410 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f32                          0x13603c880 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_norm_f16                          0x13603ccf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f32                          0x13603d160 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_rope_neox_f16                          0x13603d5d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f16                             0x13603da40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_f32                             0x13603deb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f16                         0x13603e320 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_im2col_ext_f32                         0x13603e790 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x13603ec00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x13603f070 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_upscale_f32                            0x13603f4e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_f32                                0x13603f950 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x13603fdc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x136040230 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_arange_f32                             0x1360406a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x136040b10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x136040f80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_leaky_relu_f32                         0x136041b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x136041dc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x136042080 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x1360424f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x136042960 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x136042dd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x136043240 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h64                0x1360436b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h80                0x136043b20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h96                0x136043f90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h112               0x136044400 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h128               0x136044870 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_bf16_h256               0x136044ce0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x136045150 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x1360455c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x136045a30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x136045ea0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x136046310 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x136046780 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x136046bf0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x136047060 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1360474d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x136047940 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x136047db0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x136048220 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x136048690 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x136048b00 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x136048f70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1360493e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x136049850 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x136049cc0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x13604a130 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x13604a5a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x13604aa10 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x13604ae80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x13604b2f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x13604b760 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x13604bbd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x13604c040 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x13604c4b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x13604c920 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x13604cd90 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x13604d200 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x13604d670 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h128           0x13604dae0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x13604df50 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x13604e3c0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x13604e830 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x13604eca0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x13604f110 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x13604f580 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_bf16_h256           0x13604f9f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x13604fe60 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1360502d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x136050740 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x136050bb0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x136051020 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_f32                                0x136051490 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_set_i32                                0x136051900 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f32                            0x136051d70 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_f16                            0x1360521e0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_bf16                           0x136052650 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f32                            0x136052ac0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f16_f16                            0x136052f30 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_f32                           0x1360533a0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_bf16_bf16                          0x136053810 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x136053c80 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1360540f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x136054560 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x1360549d0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x136054e40 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1360552b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_concat                                 0x136055720 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqr                                    0x136056190 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sqrt                                   0x1360568b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sin                                    0x136056fd0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_cos                                    0x1360576f0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_sum_rows                               0x1360579b0 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_argmax                                 0x136057e20 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x136058420 | th_max = 1024 | th_width =   32
ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x136058a30 | th_max = 1024 | th_width =   32
llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 24, can_shift = 1
llama_kv_cache_init: layer 0: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 1: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 2: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 3: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 4: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 5: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 6: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 7: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 8: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 9: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 10: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 11: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 12: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 13: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 14: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 15: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 16: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 17: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 18: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 19: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 20: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 21: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 22: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init: layer 23: n_embd_k_gqa = 2048, n_embd_v_gqa = 2048
llama_kv_cache_init:      Metal KV buffer size =   384.00 MiB
llama_init_from_model: KV self size  =  384.00 MiB, K (f16):  192.00 MiB, V (f16):  192.00 MiB
llama_init_from_model:        CPU  output buffer size =     0.19 MiB
llama_init_from_model:      Metal compute buffer size =   102.25 MiB
llama_init_from_model:        CPU compute buffer size =     8.01 MiB
llama_init_from_model: graph nodes  = 872
llama_init_from_model: graph splits = 2
main : deserialized state from 988319 out of a maximum of 988319 bytes
main : seq 0 copied, 787052 bytes
main : kv cache cleared
main : seq 1 restored, 787052 bytes

main : success
ggml_metal_free: deallocating

first run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


second run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."


single seq run: The quick brown fox jumps over the lazy Dog." "Maybe you should have done that little trick."

real	0m0.967s
user	0m0.237s
sys	0m0.191s
```
### ctest_with_model_debug

Runs ctest with model files in debug mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-debug
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.45 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    1.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   1.75 sec*proc (2 tests)

Total Test time (real) =   1.76 sec
        1.78 real         0.52 user         0.23 sys
```
### ctest_with_model_release

Runs ctest with model files in release mode
- status: 0
```
+ LLAMACPP_TEST_MODELFILE=/Users/ggml/mnt/llama.cpp/models/pythia/1.4B/ggml-model-f16.gguf
+ time ctest --output-on-failure -L model
Test project /Users/ggml/work/llama.cpp/build-ci-release
    Start 26: test-model-load-cancel
1/2 Test #26: test-model-load-cancel ...........   Passed    0.23 sec
    Start 27: test-autorelease
2/2 Test #27: test-autorelease .................   Passed    0.31 sec

100% tests passed, 0 tests failed out of 2

Label Time Summary:
model    =   0.54 sec*proc (2 tests)

Total Test time (real) =   0.55 sec
        0.55 real         0.13 user         0.08 sys
```
